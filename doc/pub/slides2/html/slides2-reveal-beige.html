<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Slides from FYS3150/4150 Lectures">

<title>Slides from FYS3150/4150 Lectures</title>






<!-- reveal.js: http://lab.hakim.se/reveal-js/ -->

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<!--
<link rel="stylesheet" href="reveal.js/css/reveal.css">
<link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/beigesmall.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/serif.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/moon.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/sky.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/darkgray.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/default.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/cbc.css" id="theme">
<link rel="stylesheet" href="reveal.js/css/theme/simula.css" id="theme">
-->

<!-- For syntax highlighting -->
<link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">

<!-- If the query includes 'print-pdf', use the PDF print sheet -->
<script>
document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
</script>

<style type="text/css">
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    p.caption { width: 80%; font-size: 60%; font-style: italic; text-align: left; }
    hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .reveal .alert-text-small   { font-size: 80%;  }
    .reveal .alert-text-large   { font-size: 130%; }
    .reveal .alert-text-normal  { font-size: 90%;  }
    .reveal .alert {
             padding:8px 35px 8px 14px; margin-bottom:18px;
             text-shadow:0 1px 0 rgba(255,255,255,0.5);
             border:5px solid #bababa;
             -webkit-border-radius: 14px; -moz-border-radius: 14px;
             border-radius:14px
             background-position: 10px 10px;
             background-repeat: no-repeat;
             background-size: 38px;
             padding-left: 30px; /* 55px; if icon */
     }
     .reveal .alert-block {padding-top:14px; padding-bottom:14px}
     .reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
     /*.reveal .alert li {margin-top: 1em}*/
     .reveal .alert-block p+p {margin-top:5px}
     /*.reveal .alert-notice { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_notice.png); }
     .reveal .alert-summary  { background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_summary.png); }
     .reveal .alert-warning { background-image: url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_warning.png); }
     .reveal .alert-question {background-image:url(http://hplgit.github.io/doconce/bundled/html_images/small_gray_question.png); } */

</style>



<!-- Styles for table layout of slides -->
<style type="text/css">
td.padding {
  padding-top:20px;
  padding-bottom:20px;
  padding-right:50px;
  padding-left:50px;
}
</style>

</head>

<body>
<div class="reveal">

<!-- Any section element inside the <div class="slides"> container
     is displayed as a slide -->

<div class="slides">





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<!-- newcommands.tex -->
$$
\newcommand{\prob}[0]{\mathrm{Prob}} %probability
\newcommand{\cov}[0]{\mathrm{Cov}}   %covariance
\newcommand{\var}[0]{\mathrm{Var}}   %variancd
\newcommand{\mean}[1]{\langle #1 \rangle}
\newcommand{\meanb}[1]{\big\langle #1 \big\rangle}
\newcommand{\OP}[1]{{\bf\widehat{#1}}}
$$




    



<section>
<!-- ------------------- main content ---------------------- -->



<center><h1>Slides from FYS3150/4150 Lectures</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>


<p>&nbsp;<br>
<!-- institution(s) -->

<center>[1] <b>Department of Physics and Center of Mathematics for Applications, University of Oslo</b></center>
<center>[2] <b>National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<p>&nbsp;<br>
<center><h4>Aug 31, 2014</h4></center> <!-- date -->

<h1>Overview of week 37  <a name="___sec0"></a></h1>


</section>


<section>

<h2>Overview of week 37  <a name="___sec1"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Linear Algebra and differential equations.</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Discussion of some Armadillo examples and matrix operations.</li>
  <p><li> Discussion of classes in C++</li>
  <p><li> Iterative methods, Gauss-Seidel. Cubic spline and interpolation (chapter 6).</li>
  <p><li> Tuesday: Eigenvalue problems</li>
  <p><li> We start discussing Jacobi's algorithm, chapter 7.</li>
  <p><li> Presentation of project 2.</li>
  <p><li> Computer-Lab: Project 1 and project 2.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Iterative methods, Chapter 6  <a name="___sec2"></a></h2>

<ul>
  <p><li> Direct solvers such as Gauss elimination and LU decomposition discussed last week.</li>
  <p><li> Iterative solvers such as Basic iterative solvers, Jacobi, Gauss-Seidel, Successive over-relaxation. These methods are easy to parallelize, as we will se later. Much used in solutions of partial differential equations.</li>
  <p><li> Other iterative methods such as Krylov subspace methods with Generalized minimum residual (GMRES) and Conjugate gradient etc will not be discussed.</li>
</ul>
<p>


</section>


<section>

<h2>Iterative methods, Jacobi's method  <a name="___sec3"></a></h2>

It is a simple method for solving

<p>&nbsp;<br>
$$ \hat{A}{\bf x}={\bf b},$$
<p>&nbsp;<br>

where \( \hat{A} \) is a matrix and \( {\bf x} \) and \( {\bf b} \) are vectors. The vector \( {\bf x} \) is
the unknown.

<p>
It is an iterative scheme where we start with a guess for the unknown, and
after \( k+1 \) iterations we have

<p>&nbsp;<br>
$$ {\bf x}^{(k+1)}= \hat{D}^{-1}({\bf b}-(\hat{L}+\hat{U}){\bf x}^{(k)}),$$
<p>&nbsp;<br>

with \( \hat{A}=\hat{D}+\hat{U}+\hat{L} \) and
\( \hat{D} \) being a diagonal matrix, \( \hat{U} \) an upper triangular matrix and \( \hat{L} \) a  lower triangular
matrix.

<p>
If the matrix \( \hat{A} \) is positive definite or diagonally dominant, one can show that this method will always converge to the exact solution.

<p>

</section>


<section>

<h2>Iterative methods, Jacobi's method  <a name="___sec4"></a></h2>

We can demonstrate Jacobi's method by this \( 4\times 4 \) matrix problem. We assume a guess
for the vector elements \( x_i^{(0)} \), a guess which represents our first iteration. The new
values are obtained by substitution

<p>&nbsp;<br>
$$
\begin{align}
 x_1^{(1)} =&(b_1-a_{12}x_2^{(0)} -a_{13}x_3^{(0)} - a_{14}x_4^{(0)})/a_{11} \nonumber \\
 x_2^{(1)} =&(b_2-a_{21}x_1^{(0)} - a_{23}x_3^{(0)} - a_{24}x_4^{(0)})/a_{22} \nonumber \\
 x_3^{(1)} =&(b_3- a_{31}x_1^{(0)} -a_{32}x_2^{(0)} -a_{34}x_4^{(0)})/a_{33} \nonumber \\
 x_4^{(1)}=&(b_4-a_{41}x_1^{(0)} -a_{42}x_2^{(0)} - a_{43}x_3^{(0)})/a_{44},  \nonumber
\end{align}
$$
<p>&nbsp;<br>

which after \( k+1 \) iterations reads

<p>&nbsp;<br>
$$
\begin{align}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \nonumber
\end{align}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Iterative methods, Jacobi's method  <a name="___sec5"></a></h2>

We can generalize the above equations to

<p>&nbsp;<br>
$$
 x_i^{(k+1)}=(b_i-\sum_{j=1, j\ne i}^{n}a_{ij}x_j^{(k)})/a_{ii}
$$
<p>&nbsp;<br>

or in an even more compact form as

<p>&nbsp;<br>
$$ {\bf x}^{(k+1)}= \hat{D}^{-1}({\bf b}-(\hat{L}+\hat{U}){\bf x}^{(k)}),$$
<p>&nbsp;<br>

with \( \hat{A}=\hat{D}+\hat{U}+\hat{L} \) and
\( \hat{D} \) being a diagonal matrix, \( \hat{U} \) an upper triangular matrix and \( \hat{L} \) a  lower triangular
matrix.

<p>

</section>


<section>

<h2>Iterative methods, Gauss-Seidel's method  <a name="___sec6"></a></h2>

Our \( 4\times 4 \) matrix problem

<p>&nbsp;<br>
$$
\begin{align}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \nonumber
\end{align}
$$
<p>&nbsp;<br>

can be rewritten as

<p>&nbsp;<br>
$$
\begin{align}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k+1)} -a_{32}x_2^{(k+1)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k+1)} -a_{42}x_2^{(k+1)} - a_{43}x_3^{(k+1)})/a_{44},  \nonumber
\end{align}
$$
<p>&nbsp;<br>

which allows us to utilize the preceding solution (forward substitution). This improves normally the convergence
behavior and leads to the Gauss-Seidel method!

<p>

</section>


<section>

<h2>Iterative methods, Gauss-Seidel's method  <a name="___sec7"></a></h2>

We can generalize

<p>&nbsp;<br>
$$
\begin{align}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k+1)} -a_{32}x_2^{(k+1)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k+1)} -a_{42}x_2^{(k+1)} - a_{43}x_3^{(k+1)})/a_{44},  \nonumber
\end{align}
$$
<p>&nbsp;<br>

to the following form

<p>&nbsp;<br>
$$
 x^{(k+1)}_i = \frac{1}{a_{ii}} \left(
b_i - \sum_{j > i} a_{ij}x^{(k)}_j - \sum_{j < i}a_{ij}x^{(k+1)}_j
\right), \quad i=1,2,\ldots,n.
$$
<p>&nbsp;<br>


<p>
The procedure is generally continued until the changes made by an iteration are below some tolerance.

<p>
The convergence properties of the Jacobi method and the
Gauss-Seidel method are dependent on the matrix \( \hat{A} \). These methods converge when
the matrix is symmetric positive-definite, or is strictly or irreducibly diagonally dominant.
Both methods sometimes converge even if these conditions are not satisfied.

<p>

</section>


<section>

<h2>Iterative methods, Successive over-relaxation  <a name="___sec8"></a></h2>

Given a square system of n linear equations with unknown \( \mathbf x \):

<p>&nbsp;<br>
$$
    \hat{A}\mathbf x = \mathbf b
$$
<p>&nbsp;<br>

where:

<p>&nbsp;<br>
$$
    \hat{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix}, \qquad \mathbf{x} = \begin{bmatrix} x_{1} \\ x_2 \\ \vdots \\ x_n \end{bmatrix} , \qquad \mathbf{b} = \begin{bmatrix} b_{1} \\ b_2 \\ \vdots \\ b_n \end{bmatrix}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Iterative methods, Successive over-relaxation  <a name="___sec9"></a></h2>

Then A can be decomposed into a diagonal component D, and strictly lower and upper triangular components L and U:

<p>&nbsp;<br>
$$
    \hat{A} =\hat{D} + \hat{L} + \hat{U},
$$
<p>&nbsp;<br>

where

<p>&nbsp;<br>
$$
    D = \begin{bmatrix} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & a_{nn} \end{bmatrix}, \quad L = \begin{bmatrix} 0 & 0 & \cdots & 0 \\ a_{21} & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\a_{n1} & a_{n2} & \cdots & 0 \end{bmatrix}, \quad U = \begin{bmatrix} 0 & a_{12} & \cdots & a_{1n} \\ 0 & 0 & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & 0 \end{bmatrix}.
$$
<p>&nbsp;<br>

The system of linear equations may be rewritten as:

<p>&nbsp;<br>
$$
(D+\omega L) \mathbf{x} = \omega \mathbf{b} - [\omega U + (\omega-1) D ] \mathbf{x}
$$
<p>&nbsp;<br>

for a constant \( \omega > 1 \).

<p>

</section>


<section>

<h2>Iterative methods, Successive over-relaxation  <a name="___sec10"></a></h2>

The method of successive over-relaxation is an iterative technique that solves the left hand side of this expression for \( x \), using previous value for \( x \) on the right hand side. Analytically, this may be written as:

<p>&nbsp;<br>
$$
\mathbf{x}^{(k+1)} = (D+\omega L)^{-1} \big(\omega \mathbf{b} - [\omega U + (\omega-1) D ] \mathbf{x}^{(k)}\big).
$$
<p>&nbsp;<br>

However, by taking advantage of the triangular form of \( (D+\omega L) \), the elements of \( x^{(k+1)} \) can be computed sequentially using forward substitution:

<p>&nbsp;<br>
$$
 x^{(k+1)}_i = (1-\omega)x^{(k)}_i + \frac{\omega}{a_{ii}}
\left(b_i - \sum_{j>i} a_{ij}x^{(k)}_j -
\sum_{j < i} a_{ij}x^{(k+1)}_j \right),\quad i=1,2,\ldots,n.
$$
<p>&nbsp;<br>

The choice of relaxation factor is not necessarily easy, and depends upon the properties of the coefficient matrix. For symmetric, positive-definite matrices it can be proven that \( 0 < \omega < 2 \) will lead to convergence, but we are generally interested in faster convergence rather than just convergence.

<p>

</section>


<section>

<h2>Cubic Splines, Chapter 6  <a name="___sec11"></a></h2>

Cubic spline interpolation is among one of the mostly used
methods for interpolating between data points where the arguments
are organized as ascending series. In the library program we supply
such a function, based on the so-called cubic spline method to be
described below.

<p>
A spline function consists of polynomial pieces defined on
subintervals. The different subintervals are connected via
various continuity relations.

<p>
Assume we have at our disposal \( n+1 \) points \( x_0, x_1, \dots x_n \)
arranged so that \( x_0 < x_1 < x_2 < \dots x_{n-1} < x_n \) (such points are called
knots). A spline function \( s \) of degree \( k \) with \( n+1 \) knots is defined
as follows

<ul>
  <p><li> On every subinterval \( [x_{i-1},x_i) \) $s$ is a polynomial of degree \( \le k \).</li>
  <p><li> \( s \) has \( k-1 \) continuous derivatives in the whole interval \( [x_0,x_n] \).</li>
</ul>
<p>


</section>


<section>

<h2>Splines  <a name="___sec12"></a></h2>

As an example, consider a spline function of degree \( k=1 \) defined as follows

<p>&nbsp;<br>
$$
    s(x)=\left\{\begin{array}{cc} s_0(x)=a_0x+b_0 & x\in [x_0, x_1) \\
                             s_1(x)=a_1x+b_1 & x\in [x_1, x_2) \\
                             \dots & \dots \\
                             s_{n-1}(x)=a_{n-1}x+b_{n-1} & x\in
                             [x_{n-1}, x_n] \end{array}\right.
$$
<p>&nbsp;<br>


<p>
In this case the polynomial consists of series of straight lines
connected to each other at every endpoint. The number of continuous
derivatives is then \( k-1=0 \), as expected when we deal with straight lines.
Such a polynomial is quite easy to construct given
\( n+1 \) points \( x_0, x_1, \dots x_n \) and their corresponding
function values.

<p>

</section>


<section>

<h2>Splines  <a name="___sec13"></a></h2>

The most commonly used spline function is the one with \( k=3 \), the so-called
cubic spline function.
Assume that we have in adddition to the \( n+1 \) knots a series of
functions values \( y_0=f(x_0), y_1=f(x_1), \dots y_n=f(x_n) \).
By definition, the polynomials \( s_{i-1} \) and \( s_i \)
are thence supposed to interpolate
the same point \( i \), i.e.,

<p>&nbsp;<br>
$$
    s_{i-1}(x_i)= y_i = s_i(x_i),
$$
<p>&nbsp;<br>

with \( 1 \le i \le n-1 \). In total we have \( n \) polynomials of the
type

<p>&nbsp;<br>
$$
    s_i(x)=a_{i0}+a_{i1}x+a_{i2}x^2+a_{i2}x^3,
$$
<p>&nbsp;<br>

yielding \( 4n \) coefficients to determine.

<p>

</section>


<section>

<h2>Splines  <a name="___sec14"></a></h2>

 Every subinterval provides
in addition the \( 2n \) conditions

<p>&nbsp;<br>
$$
    y_i = s(x_i),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
    s(x_{i+1})= y_{i+1},
$$
<p>&nbsp;<br>

to be fulfilled. If we also assume that \( s' \) and \( s'' \) are continuous,
then

<p>&nbsp;<br>
$$
       s'_{i-1}(x_i)= s'_i(x_i),
$$
<p>&nbsp;<br>

yields \( n-1 \) conditions. Similarly,

<p>&nbsp;<br>
$$
       s''_{i-1}(x_i)= s''_i(x_i),
$$
<p>&nbsp;<br>

results in additional \( n-1 \) conditions. In total we have \( 4n \) coefficients
and \( 4n-2 \) equations to determine them, leaving us with \( 2 \) degrees of
freedom to be determined.

<p>

</section>


<section>

<h2>Splines  <a name="___sec15"></a></h2>

Using the last equation we define two values for the second derivative,
namely

<p>&nbsp;<br>
$$
       s''_{i}(x_i)= f_i,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
       s''_{i}(x_{i+1})= f_{i+1},
$$
<p>&nbsp;<br>

and setting up a straight line between \( f_i \) and \( f_{i+1} \) we have

<p>&nbsp;<br>
$$
   s_i''(x) = \frac{f_i}{x_{i+1}-x_i}(x_{i+1}-x)+
               \frac{f_{i+1}}{x_{i+1}-x_i}(x-x_i),
$$
<p>&nbsp;<br>

and integrating twice one obtains

<p>&nbsp;<br>
$$
   s_i(x) = \frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3
             +c(x-x_i)+d(x_{i+1}-x).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Splines  <a name="___sec16"></a></h2>

Using the conditions \( s_i(x_i)=y_i \) and \( s_i(x_{i+1})=y_{i+1} \)
we can in turn determine the constants \( c \) and \( d \) resulting in

<p>&nbsp;<br>
$$
\begin{align}
   s_i(x) =&\frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3 \nonumber  \\
            +&(\frac{y_{i+1}}{x_{i+1}-x_i}-\frac{f_{i+1}(x_{i+1}-x_i)}{6})
              (x-x_i)+
             (\frac{y_{i}}{x_{i+1}-x_i}-\frac{f_{i}(x_{i+1}-x_i)}{6})
             (x_{i+1}-x).
\end{align}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Splines  <a name="___sec17"></a></h2>

How to determine the values of the second
derivatives \( f_{i} \) and \( f_{i+1} \)? We use the continuity assumption
of the first derivatives

<p>&nbsp;<br>
$$
    s'_{i-1}(x_i)= s'_i(x_i),
$$
<p>&nbsp;<br>

and set \( x=x_i \). Defining \( h_i=x_{i+1}-x_i \) we obtain finally
the following expression

<p>&nbsp;<br>
$$
   h_{i-1}f_{i-1}+2(h_{i}+h_{i-1})f_i+h_if_{i+1}=
   \frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1}),
$$
<p>&nbsp;<br>

and introducing the shorthands \( u_i=2(h_{i}+h_{i-1}) \),
\( v_i=\frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1}) \),
we can reformulate the problem as a set of linear equations to be
solved  through e.g., Gaussian elemination

<p>

</section>


<section>

<h2>Splines  <a name="___sec18"></a></h2>

Gaussian elimination

<p>&nbsp;<br>
$$
   \left[\begin{array}{cccccccc} u_1 & h_1 &0 &\dots & & & & \\
                                 h_1 & u_2 & h_2 &0 &\dots & & & \\
                                  0   & h_2 & u_3 & h_3 &0 &\dots & & \\
                               \dots& & \dots &\dots &\dots &\dots &\dots & \\
                                 &\dots & & &0 &h_{n-3} &u_{n-2} &h_{n-2} \\
                                 & && & &0 &h_{n-2} &u_{n-1} \end{array}\right]
   \left[\begin{array}{c} f_1 \\
                          f_2 \\
                          f_3\\
                          \dots \\
                          f_{n-2} \\
                          f_{n-1} \end{array} \right] =
   \left[\begin{array}{c} v_1 \\
                          v_2 \\
                          v_3\\
                          \dots \\
                          v_{n-2}\\
                          v_{n-1} \end{array} \right].
$$
<p>&nbsp;<br>

Note that this is a set of tridiagonal equations and can be solved
through only \( O(n) \) operations.

<p>

</section>


<section>

<h2>Splines  <a name="___sec19"></a></h2>

The functions supplied in the program library are \( spline \) and \( splint \).
In order to use cubic spline interpolation you need first to call

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">spline(<span style="color: #a7a7a7; font-weight: bold">double</span> x[], <span style="color: #a7a7a7; font-weight: bold">double</span> y[], <span style="color: #a7a7a7; font-weight: bold">int</span> n, <span style="color: #a7a7a7; font-weight: bold">double</span> yp1,
         <span style="color: #a7a7a7; font-weight: bold">double</span> yp2, <span style="color: #a7a7a7; font-weight: bold">double</span> y2[])
</pre></div>
<p>
This function takes as
input \( x[0,..,n - 1] \) and \( y[0,..,n - 1] \) containing a tabulation
\( y_i = f(x_i) \) with \( x_0 < x_1 < .. < x_{n - 1} \)
together with the
first derivatives  of \( f(x) \) at \( x_0 \) and \( x_{n-1} \), respectively. Then the
function returns \( y2[0,..,n-1] \) which contains the second derivatives of
\( f(x_i) \) at each point \( x_i \). \( n \) is the number of points.
This function provides the cubic spline interpolation for all subintervals
and is called only once.

<p>

</section>


<section>

<h2>Splines  <a name="___sec20"></a></h2>

 Thereafter, if you wish to make
various interpolations, you need to call the function

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">splint(<span style="color: #a7a7a7; font-weight: bold">double</span> x[], <span style="color: #a7a7a7; font-weight: bold">double</span> y[], <span style="color: #a7a7a7; font-weight: bold">double</span> y2a[], <span style="color: #a7a7a7; font-weight: bold">int</span> n,
                    <span style="color: #a7a7a7; font-weight: bold">double</span> x, <span style="color: #a7a7a7; font-weight: bold">double</span> *y)
</pre></div>
<p>
which takes as input
the tabulated values \( x[0,..,n - 1] \) and \( y[0,..,n - 1] \) and the output
y2a[0,..,n - 1] from \( spline \). It returns the value \( y \) corresponding
to the point \( x \).

<h1>Overview of week 38  <a name="___sec21"></a></h1>


</section>


<section>

<h2>Overview of week 38  <a name="___sec22"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Eigenvalue problems and project 2.</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week and discussion of project 2</li>
  <p><li> Jacobi's algorithm</li>
  <p><li> Tuesday:</li>
  <p><li> Householder's algorithm and Francis' algorithm</li>
  <p><li> Iterative methods for symmetric matrices: Lanczos' algorithm</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec23"></a></h2>

Let us consider the matrix {\bf A} of dimension n. The eigenvalues of
{\bf A} is defined through the matrix equation

<p>&nbsp;<br>
$$
   {\bf A}{\bf x^{(\nu)}} = \lambda^{(\nu)}{\bf x^{(\nu)}},
$$
<p>&nbsp;<br>


<p>
where \( \lambda^{(\nu)} \) are the eigenvalues and \( {\bf x^{(\nu)}} \) the
corresponding eigenvectors.
Unless otherwise stated, when we use the wording eigenvector we mean the
right eigenvector. The left eigenvector is defined as

<p>&nbsp;<br>
$$
{\bf x^{(\nu)}}_L{\bf A} = \lambda^{(\nu)}{\bf x^{(\nu)}}_L
$$
<p>&nbsp;<br>

The above right eigenvector problem is equivalent to a set of \( n \) equations with \( n \) unknowns
\( x_i \).

<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec24"></a></h2>

The eigenvalue problem can be rewritten as

<p>&nbsp;<br>
$$
   \left( {\bf A}-\lambda^{(\nu)} I \right) {\bf x^{(\nu)}} = 0,
$$
<p>&nbsp;<br>


<p>
with \( I \) being the unity matrix. This equation provides
a solution to the problem if and only if the determinant
is zero, namely

<p>&nbsp;<br>
$$
   \left| {\bf A}-\lambda^{(\nu)}{\bf I}\right| = 0,
$$
<p>&nbsp;<br>


<p>
which in turn means that the determinant is a polynomial
of degree \( n \) in \( \lambda \) and in general we will have
\( n \) distinct zeros.

<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec25"></a></h2>

The eigenvalues of a matrix
\( {\bf A}\in {\mathbb{C}}^{n\times n} \)
are thus the \( n \) roots of its characteristic polynomial

<p>&nbsp;<br>
$$
P(\lambda) = det(\lambda{\bf I}-{\bf A}),
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
  P(\lambda)= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right).
$$
<p>&nbsp;<br>

The set of these roots is called the spectrum and is denoted as
\( \lambda({\bf A}) \).
If \( \lambda({\bf A})=\left\{\lambda_1,\lambda_2,\dots ,\lambda_n\right\} \) then we have

<p>&nbsp;<br>
$$
   det({\bf A})= \lambda_1\lambda_2\dots\lambda_n,
$$
<p>&nbsp;<br>

and if we define the trace of \( {\bf A} \) as

<p>&nbsp;<br>
$$
Tr({\bf A})=\sum_{i=1}^n a_{ii}$$
<p>&nbsp;<br>

then
\( Tr({\bf A})=\lambda_1+\lambda_2+\dots+\lambda_n \).

<p>

</section>


<section>

<h2>Abel-Ruffini Impossibility Theorem  <a name="___sec26"></a></h2>

The Abel-Ruffini theorem (also known as Abel's impossibility theorem)
states that there is no general solution in radicals to polynomial equations of degree five or higher.

<p>
The content of this theorem is frequently misunderstood. It does not assert that higher-degree polynomial equations are unsolvable.
In fact, if the polynomial has real or complex coefficients, and we allow complex solutions, then every polynomial equation has solutions; this is the fundamental theorem of algebra. Although these solutions cannot always be computed exactly with radicals, they can be computed to any desired degree of accuracy using numerical methods such as the Newton-Raphson method or Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees.

<p>
The theorem only concerns the form that such a solution must take. The content of the theorem is
that the solution of a higher-degree equation cannot in all cases be expressed in terms of the polynomial coefficients with a finite number of operations of addition, subtraction, multiplication, division and root extraction. Some polynomials of arbitrary degree, of which the simplest nontrivial example is the monomial equation \( ax^n = b \), are always solvable with a radical.

<p>

</section>


<section>

<h2>Abel-Ruffini Impossibility Theorem  <a name="___sec27"></a></h2>

The Abel-Ruffini theorem says that there are some fifth-degree equations whose solution cannot be so expressed.
The equation \( x^5 - x + 1 = 0 \) is an example. Some other fifth degree equations can be solved by radicals,
for example \( x^5 - x^4 - x + 1 = 0 \). The precise criterion that distinguishes between those equations that can be solved
by radicals and those that cannot was given by Galois and is now part of Galois theory:
a polynomial equation can be solved by radicals if and only if its Galois group is a solvable group.

<p>
Today, in the modern algebraic context, we say that second, third and fourth degree polynomial
equations can always be solved by radicals because the symmetric groups \( S_2, S_3 \) and \( S_4 \) are solvable groups,
whereas \( S_n \) is not solvable for \( n \ge 5 \).

<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec28"></a></h2>

In the present discussion we assume that our matrix is real and symmetric, that is
\( {\bf A}\in {\mathbb{R}}^{n\times n} \).
The matrix \( {\bf A} \) has \( n \) eigenvalues
\( \lambda_1\dots \lambda_n \) (distinct or not). Let \( {\bf D} \) be the
diagonal matrix with the eigenvalues on the diagonal

<p>&nbsp;<br>
$$
{\bf D}=    \left( \begin{array}{ccccccc} \lambda_1 & 0 & 0   & 0    & \dots  &0     & 0 \\
                                0 & \lambda_2 & 0 & 0    & \dots  &0     &0 \\
                                0   & 0 & \lambda_3 & 0  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\lambda_{n-1} & \\
                                0   & \dots & \dots & \dots  &\dots       &0 & \lambda_n

             \end{array} \right).
$$
<p>&nbsp;<br>


<p>
If \( {\bf A} \) is real and symmetric then there exists a real orthogonal matrix \( {\bf S} \) such that

<p>&nbsp;<br>
$$
     {\bf S}^T {\bf A}{\bf S}= \mbox{diag}(\lambda_1,\lambda_2,\dots ,\lambda_n),
$$
<p>&nbsp;<br>

and for \( j=1:n \) we have \( {\bf A}{\bf S}(:,j) = \lambda_j {\bf S}(:,j) \).

<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec29"></a></h2>

To obtain the eigenvalues of \( {\bf A}\in {\mathbb{R}}^{n\times n} \),
the strategy is to
perform a series of similarity transformations on the original
matrix \( {\bf A} \), in order to reduce it either into a  diagonal form as above
or into a  tridiagonal form.

<p>
We say that a matrix \( {\bf B} \) is a similarity
transform  of  \( {\bf A} \) if

<p>&nbsp;<br>
$$
     {\bf B}= {\bf S}^T {\bf A}{\bf S}, \quad \mbox{where}\quad
     {\bf S}^T{\bf S}={\bf S}^{-1}{\bf S} ={\bf I}.
$$
<p>&nbsp;<br>


<p>
The importance of a similarity transformation lies in the fact that
the resulting matrix has the same
eigenvalues, but the eigenvectors are in general different.

<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec30"></a></h2>

To prove this we
start with  the eigenvalue problem and a similarity transformed matrix \( {\bf B} \).

<p>&nbsp;<br>
$$
   {\bf Ax}=\lambda{\bf x} \mbox{ and }
    {\bf B}= {\bf S}^T {\bf A}{\bf S}.
$$
<p>&nbsp;<br>


<p>
We multiply the first equation on the left by \( {\bf S}^T \) and insert
\( {\bf S}^{T}{\bf S} ={\bf I} \) between \( {\bf A} \) and \( {\bf x} \). Then we get

<p>&nbsp;<br>
$$
\begin{equation}
   ({\bf S^TAS})({\bf S^Tx})=\lambda{\bf S^Tx} ,
\end{equation}
$$
<p>&nbsp;<br>


<p>
which is the same as

<p>&nbsp;<br>
$$
   {\bf B} \left ( {\bf S^Tx} \right ) = \lambda \left ({\bf S^Tx}
                             \right ).
$$
<p>&nbsp;<br>


<p>
The variable  \( \lambda \) is an eigenvalue of \( {\bf B} \) as well, but with
eigenvector \( {\bf S^Tx} \).

<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec31"></a></h2>

The basic philosophy is to

<p>
either apply subsequent similarity transformations (direct method) so that

<p>&nbsp;<br>
$$
\begin{equation}
   {\bf S_N^T\dots S_1^TAS_1\dots S_N }={\bf D} ,
\end{equation}
$$
<p>&nbsp;<br>


<p>
or apply subsequent similarity transformations so that
\( \bf A \) becomes tridiagonal (Householder) or upper/lower triangular (\( \bf QR \) method).
Thereafter, techniques for obtaining
eigenvalues from tridiagonal matrices can be used.

<p>
or use so-called power methods

<p>
or use iterative methods (Krylov, Lanczos, Arnoldi). These methods are popular for huge matrix problems.

<p>

</section>


<section>

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec32"></a></h2>

In project 1 we rewrote our original differential equation in terms of a discretized equation with approximations to the
derivatives as

<p>&nbsp;<br>
$$
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
$$
<p>&nbsp;<br>

with \( i=1,2,\dots, n \). We need to add to this system the two boundary conditions \( u(a) =u_0 \) and \( u(b) = u_{n+1} \).
If we define a matrix

<p>&nbsp;<br>
$$
    {\bf A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 & 2 & -1 & & & \\
                           & -1 & 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 & 2 \\
                      \end{array} \right)
$$
<p>&nbsp;<br>

and the corresponding vectors \( {\bf u} = (u_1, u_2, \dots,u_n)^T \) and
\( {\bf f}({\bf u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T \)  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns

<p>&nbsp;<br>
$$
   {\bf A}{\bf u} = {\bf f}({\bf u}).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part a-b)  <a name="___sec33"></a></h2>

We are first interested in the solution of the radial part of Schr\"odinger's equation for one electron. This equation reads

<p>&nbsp;<br>
$$
  -\frac{\hbar^2}{2 m} \left ( \frac{1}{r^2} \frac{d}{dr} r^2
  \frac{d}{dr} - \frac{l (l + 1)}{r^2} \right )R(r)
     + V(r) R(r) = E R(r).
$$
<p>&nbsp;<br>

In our case \( V(r) \) is the harmonic oscillator potential \( (1/2)kr^2 \) with
\( k=m\omega^2 \) and \( E \) is
the energy of the harmonic oscillator in three dimensions.
The oscillator frequency is \( \omega \) and the energies are

<p>&nbsp;<br>
$$
E_{nl}=  \hbar \omega \left(2n+l+\frac{3}{2}\right),
$$
<p>&nbsp;<br>

with \( n=0,1,2,\dots \) and \( l=0,1,2,\dots \).

<p>
Since we have made a transformation to spherical coordinates it means that
\( r\in [0,\infty) \).
The quantum number
\( l \) is the orbital momentum of the electron.

<p>
Then we substitute \( R(r) = (1/r) u(r) \) and obtain

<p>&nbsp;<br>
$$
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r)
       + \left ( V(r) + \frac{l (l + 1)}{r^2}\frac{\hbar^2}{2 m}
                                    \right ) u(r)  = E u(r) .
$$
<p>&nbsp;<br>


<p>
The boundary conditions are \( u(0)=0 \) and \( u(\infty)=0 \).

<p>

</section>


<section>

<h2>Student project 2, part a-b)  <a name="___sec34"></a></h2>

We introduce a dimensionless variable \( \rho = (1/\alpha) r \)
where \( \alpha \) is a constant with dimension length and get

<p>&nbsp;<br>
$$
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho)
       + \left ( V(\rho) + \frac{l (l + 1)}{\rho^2}
         \frac{\hbar^2}{2 m\alpha^2} \right ) u(\rho)  = E u(\rho) .
$$
<p>&nbsp;<br>


<p>
In project 2 we set \( l=0 \).
Inserting \( V(\rho) = (1/2) k \alpha^2\rho^2 \) we end up with

<p>&nbsp;<br>
$$
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho)
       + \frac{k}{2} \alpha^2\rho^2u(\rho)  = E u(\rho) .
$$
<p>&nbsp;<br>

We multiply thereafter with \( 2m\alpha^2/\hbar^2 \) on both sides and obtain

<p>&nbsp;<br>
$$
  -\frac{d^2}{d\rho^2} u(\rho)
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part a-b)  <a name="___sec35"></a></h2>

We have from the previous slide

<p>&nbsp;<br>
$$
  -\frac{d^2}{d\rho^2} u(\rho)
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
$$
<p>&nbsp;<br>

The constant \( \alpha \) can now be fixed
so that

<p>&nbsp;<br>
$$
\frac{mk}{\hbar^2} \alpha^4 = 1,
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
\alpha = \left(\frac{\hbar^2}{mk}\right)^{1/4}.
$$
<p>&nbsp;<br>

Defining

<p>&nbsp;<br>
$$
\lambda = \frac{2m\alpha^2}{\hbar^2}E,
$$
<p>&nbsp;<br>

we can rewrite Schr\"odinger's equation as

<p>&nbsp;<br>
$$
  -\frac{d^2}{d\rho^2} u(\rho) + \rho^2u(\rho)  = \lambda u(\rho) .
$$
<p>&nbsp;<br>

This is the first equation to solve numerically. In three dimensions
the eigenvalues for \( l=0 \) are
\( \lambda_0=3,\lambda_1=7,\lambda_2=11,\dots . \)

<p>

</section>


<section>

<h2>Student project 2, part a-b)  <a name="___sec36"></a></h2>

We use the by now standard
expression for the second derivative of a function \( u \)

<p>&nbsp;<br>
$$
\begin{equation}
    u''=\frac{u(\rho+h) -2u(\rho) +u(\rho-h)}{h^2} +O(h^2),
    \tag{1}
\end{equation}
$$
<p>&nbsp;<br>

where \( h \) is our step.
Next we define minimum and maximum values for the variable \( \rho \),
\( \rho_{\mbox{min}}=0 \)  and \( \rho_{\mbox{max}} \), respectively.
You need to check your results for the energies against different values
\( \rho_{\mbox{max}} \), since we cannot set
\( \rho_{\mbox{max}}=\infty \).

<p>

</section>


<section>

<h2>Student project 2, part a-b)  <a name="___sec37"></a></h2>

With a given number of steps, \( n_{\mbox{step}} \), we then
define the step \( h \) as

<p>&nbsp;<br>
$$
  h=\frac{\rho_{\mbox{max}}-\rho_{\mbox{min}} }{n_{\mbox{step}}}.
$$
<p>&nbsp;<br>

Define an arbitrary value of \( \rho \) as

<p>&nbsp;<br>
$$
    \rho_i= \rho_{\mbox{min}} + ih\quad i=0,1,2,\dots , n_{\mbox{step}}
$$
<p>&nbsp;<br>

we can rewrite the Schr\"odinger equation for \( \rho_i \) as

<p>&nbsp;<br>
$$
-\frac{u(\rho_i+h) -2u(\rho_i) +u(\rho_i-h)}{h^2}+\rho_i^2u(\rho_i)  = \lambda u(\rho_i),
$$
<p>&nbsp;<br>

or in  a more compact way

<p>&nbsp;<br>
$$
-\frac{u_{i+1} -2u_i +u_{i-1}}{h^2}+\rho_i^2u_i=-\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}+V_iu_i  = \lambda u_i,
$$
<p>&nbsp;<br>

where \( V_i=\rho_i^2 \) is the harmonic oscillator potential.

<p>

</section>


<section>

<h2>Student project 2, part a-b)  <a name="___sec38"></a></h2>

Define first the diagonal matrix element

<p>&nbsp;<br>
$$
   d_i=\frac{2}{h^2}+V_i,
$$
<p>&nbsp;<br>

and the non-diagonal matrix element

<p>&nbsp;<br>
$$
   e_i=-\frac{1}{h^2}.
$$
<p>&nbsp;<br>

In this case the non-diagonal matrix elements are given by a mere constant.
<em>All non-diagonal matrix elements are equal</em>.
With these definitions the Schr\"odinger equation takes the following form

<p>&nbsp;<br>
$$
d_iu_i+e_{i-1}u_{i-1}+e_{i+1}u_{i+1}  = \lambda u_i,
$$
<p>&nbsp;<br>

where \( u_i \) is unknown. We can write the
latter equation as a matrix eigenvalue problem

<p>&nbsp;<br>
$$
\begin{equation}
    \left( \begin{array}{ccccccc} d_1 & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2 & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3 & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{n_{\mbox{step}}-2} & e_{n_{\mbox{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n_{\mbox{step}}-1} & d_{n_{\mbox{step}}-1}

             \end{array} \right)      \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mbox{step}}-1}
             \end{array} \right)=\lambda \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mbox{step}}-1}
             \end{array} \right)
      \tag{2}
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part a-b)  <a name="___sec39"></a></h2>

or if we wish to be more detailed, we can write the tridiagonal matrix as

<p>&nbsp;<br>
$$
\begin{equation}
    \left( \begin{array}{ccccccc} \frac{2}{h^2}+V_1 & -\frac{1}{h^2} & 0   & 0    & \dots  &0     & 0 \\
                                -\frac{1}{h^2} & \frac{2}{h^2}+V_2 & -\frac{1}{h^2} & 0    & \dots  &0     &0 \\
                                0   & -\frac{1}{h^2} & \frac{2}{h^2}+V_3 & -\frac{1}{h^2}  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\frac{2}{h^2}+V_{n_{\mbox{step}}-2} & -\frac{1}{h^2}\\
                                0   & \dots & \dots & \dots  &\dots       &-\frac{1}{h^2} & \frac{2}{h^2}+V_{n_{\mbox{step}}-1}

             \end{array} \right)
\tag{3}
\end{equation}
$$
<p>&nbsp;<br>


<p>
Recall that the solutions are known via the boundary conditions at
\( i=n_{\mbox{step}} \) and at the other end point, that is for  \( \rho_0 \).
The solution is zero in both cases.

<p>

</section>


<section>

<h2>Student project 2, part c)  <a name="___sec40"></a></h2>

We are going to study two electrons in a harmonic oscillator well which
also interact via a repulsive Coulomb interaction.
Let us start with the single-electron equation written as

<p>&nbsp;<br>
$$
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r)
       + \frac{1}{2}k r^2u(r)  = E^{(1)} u(r),
$$
<p>&nbsp;<br>

where \( E^{(1)} \) stands for the energy with one electron only.
For two electrons with no repulsive Coulomb interaction, we have the following
Schr\"odinger equation

<p>&nbsp;<br>
$$
\left(  -\frac{\hbar^2}{2 m} \frac{d^2}{dr_1^2} -\frac{\hbar^2}{2 m} \frac{d^2}{dr_2^2}+ \frac{1}{2}k r_1^2+ \frac{1}{2}k r_2^2\right)u(r_1,r_2)  = E^{(2)} u(r_1,r_2) .
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part c)  <a name="___sec41"></a></h2>

Note that we deal with a two-electron wave function \( u(r_1,r_2) \) and
two-electron energy \( E^{(2)} \).

<p>
With no interaction this can be written out as the product of two
single-electron wave functions, that is we have a solution on closed form.

<p>
We introduce the relative coordinate \( {\bf r} = {\bf r}_1-{\bf r}_2 \)
and the center-of-mass coordinate \( {\bf R} = 1/2({\bf r}_1+{\bf r}_2) \).
With these new coordinates, the radial Schr\"odinger equation reads

<p>&nbsp;<br>
$$
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2} -\frac{\hbar^2}{4 m} \frac{d^2}{dR^2}+ \frac{1}{4} k r^2+  kR^2\right)u(r,R)  = E^{(2)} u(r,R).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part c)  <a name="___sec42"></a></h2>

The equations for \( r \) and \( R \) can be separated via the ansatz for the
wave function \( u(r,R) = \psi(r)\phi(R) \) and the energy is given by the sum
of the relative energy \( E_r \) and the center-of-mass energy \( E_R \), that
is

<p>&nbsp;<br>
$$
E^{(2)}=E_r+E_R.
$$
<p>&nbsp;<br>


<p>
We add then the repulsive Coulomb interaction between two electrons,
namely a term

<p>&nbsp;<br>
$$
V(r_1,r_2) = \frac{\beta e^2}{|{\bf r}_1-{\bf r}_2|}=\frac{\beta e^2}{r},
$$
<p>&nbsp;<br>

with \( \beta e^2=1.44 \) eVnm.

<p>

</section>


<section>

<h2>Student project 2, part c)  <a name="___sec43"></a></h2>

Adding this term, the \( r \)-dependent Schr\"odinger equation becomes

<p>&nbsp;<br>
$$
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2}+ \frac{1}{4}k r^2+\frac{\beta e^2}{r}\right)\psi(r)  = E_r \psi(r).
$$
<p>&nbsp;<br>

This equation is similar to the one we had previously in parts (a) and (b)
and we introduce
again a dimensionless variable \( \rho = r/\alpha \). Repeating the same
steps, we arrive at

<p>&nbsp;<br>
$$
  -\frac{d^2}{d\rho^2} \psi(\rho)
       + \frac{mk}{4\hbar^2} \alpha^4\rho^2\psi(\rho)+\frac{m\alpha \beta e^2}{\rho\hbar^2}\psi(\rho)  =
\frac{m\alpha^2}{\hbar^2}E_r \psi(\rho) .
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part c)  <a name="___sec44"></a></h2>

We want to manipulate this equation further to make it as similar to that in (a)
as possible. We define a 'frequency'

<p>&nbsp;<br>
$$
\omega_r^2=\frac{1}{4}\frac{mk}{\hbar^2} \alpha^4,
$$
<p>&nbsp;<br>

and fix the constant \( \alpha \) by requiring

<p>&nbsp;<br>
$$
\frac{m\alpha \beta e^2}{\hbar^2}=1
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
\alpha = \frac{\hbar^2}{m\beta e^2}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part c)  <a name="___sec45"></a></h2>

Defining

<p>&nbsp;<br>
$$
\lambda = \frac{m\alpha^2}{\hbar^2}E,
$$
<p>&nbsp;<br>

we can rewrite Schr\"odinger's equation as

<p>&nbsp;<br>
$$
  -\frac{d^2}{d\rho^2} \psi(\rho) + \omega_r^2\rho^2\psi(\rho) +\frac{1}{\rho}\psi(\rho) = \lambda \psi(\rho).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Student project 2, part c)  <a name="___sec46"></a></h2>

We treat \( \omega_r \) as a parameter which reflects the strength of the oscillator potential.

<p>
Here we will study the cases \( \omega_r = 0.01 \), \( \omega_r = 0.5 \), \( \omega_r =1 \),
and \( \omega_r = 5 \)
for the ground state only, that is the lowest-lying state.

<p>
With no repulsive Coulomb interaction
you should get a result which corresponds to
the relative energy of a non-interacting system.
Make sure your results are
stable as functions of \( \rho_{\mbox{max}} \) and the number of steps.

<p>
We are only interested in the ground state with \( l=0 \). We omit the
center-of-mass energy.

<p>
For specific oscillator frequencies, the above equation has closed answers,
see the article by M. Taut, Phys. Rev. A 48, 3561 - 3566 (1993).
The article can be retrieved from the following web address
<a href="http://prola.aps.org/abstract/PRA/v48/i5/p3561_1" target="_blank"><tt>http://prola.aps.org/abstract/PRA/v48/i5/p3561_1</tt></a>.

<p>

</section>


<section>

<h2>Eigenvalue Solvers  <a name="___sec47"></a></h2>

One speaks normally of two main approaches to solving the eigenvalue problem.

<p>
The first is the formal method, involving determinants and the
\textit{characteristic polynomial}. This proves how many eigenvalues
there are, and is the way most of you
learned about how to solve the eigenvalue problem, but for
matrices of dimensions greater than 2 or 3, it is rather
impractical.

<p>
The other general approach is to use similarity or unitary
tranformations  to reduce a matrix to diagonal form. Almost always
this is done in two steps: first reduce to for example a \textit{tridiagonal}
form, and then to diagonal form. The main algorithms we will discuss
in detail, Jacobi's and  Householder's  (so-called direct method) and Lanczos algorithms (an iterative method), follow this
methodology.

<p>

</section>


<section>

<h2>Diagonalization methods, direct methods  <a name="___sec48"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Direct or non-iterative methods.</b>
<p>
...require for matrices of dimensionality \( n\times n \) typically \( O(n^3) \) operations. These methods are normally called standard methods and are used for dimensionalities
\( n \sim 10^5 \) or smaller. A brief historical overview

<p>
<table border="1">
<thead>
<tr><th align="center">       Year      </th> <td align="center">     \( n \)     </td> <th align="center">                 </th> </tr>
</thead>
<tbody>
<tr><td align="left">   1950                 </td> <td align="right">   \( n=20 \)           </td> <td align="left">   (Wilkinson)          </td> </tr>
<tr><td align="left">   1965                 </td> <td align="right">   \( n=200 \)          </td> <td align="left">   (Forsythe et al.)    </td> </tr>
<tr><td align="left">   1980                 </td> <td align="right">   \( n=2000 \)         </td> <td align="left">   Linpack              </td> </tr>
<tr><td align="left">   1995                 </td> <td align="right">   \( n=20000 \)        </td> <td align="left">   Lapack               </td> </tr>
<tr><td align="left">   2012                 </td> <td align="right">   \( n\sim 10^5 \)     </td> <td align="left">   Lapack               </td> </tr>
</tbody>
</table>
<p>
shows that in the course of 60 years the dimension that  direct diagonalization methods can handle  has increased by almost a factor of
\( 10^4 \). However, it pales beside the progress achieved by computer hardware, from flops to petaflops, a factor of almost \( 10^{15} \). We see clearly played out in history the \( O(n^3) \) bottleneck  of direct matrix algorithms.
Sloppily speaking, when  \( n\sim 10^4 \) is cubed we have \( O(10^{12}) \) operations, which is smaller than the \( 10^{15} \) increase in flops.
</div>


<p>

</section>


<section>

<h2>Diagonalization methods  <a name="___sec49"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Why iterative methods?</b>
<p>
If the matrix to diagonalize is large and sparse, direct methods simply become impractical,
also because
many of the direct methods tend to destroy sparsity. As a result large dense matrices may arise during the diagonalization procedure.  The idea behind iterative methods is to project the
$n-$dimensional problem in smaller spaces, so-called Krylov subspaces.
Given a matrix \( \hat{A} \) and a vector \( \hat{v} \), the associated Krylov sequences of vectors
(and thereby subspaces)
\( \hat{v} \), \( \hat{A}\hat{v} \), \( \hat{A}^2\hat{v} \), \( \hat{A}^3\hat{v},\dots \), represent
successively larger Krylov subspaces.

<p>
<table border="1">
<thead>
<tr><th align="center">               Matrix              </th> <td align="center">    \( \hat{A}\hat{x}=\hat{b} \)   </td> <td align="center">\( \hat{A}\hat{x}=\lambda\hat{x} \)</td> </tr>
</thead>
<tbody>
<tr><td align="left">   \( \hat{A}=\hat{A}^* \)                </td> <td align="left">   Conjugate gradient                     </td> <td align="left">   Lanczos                                </td> </tr>
<tr><td align="left">   \( \hat{A}\ne \hat{A}^* \)             </td> <td align="left">   GMRES etc                              </td> <td align="left">   Arnoldi                                </td> </tr>
</tbody>
</table>

</div>


<p>

</section>


<section>

<h2>Important Matrix and vector handling packages  <a name="___sec50"></a></h2>

The Numerical Recipes codes have been rewritten in Fortran 90/95 and C/C++ by us.
The original source codes are taken from the widely used software
package LAPACK, which follows two other popular packages developed in the 1970s,
namely EISPACK
and LINPACK.

<ul>
  <p><li> LINPACK: package for linear equations and least square problems.</li>
  <p><li> LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <a href="http://www.netlib.org" target="_blank"><tt>http://www.netlib.org</tt></a> it is possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.</li>
  <p><li> BLAS (I, II and III): (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. Highly parallelized and efficient codes, all available for download from <a href="http://www.netlib.org" target="_blank"><tt>http://www.netlib.org</tt></a>.</li>
</ul>
<p>


</section>


<section>

<h2>Eigenvalue Solvers, Jacobi  <a name="___sec51"></a></h2>

Consider an  (\( n\times n \)) orthogonal transformation matrix

<p>&nbsp;<br>
$$
{\bf S}=
 \left(
   \begin{array}{cccccccc}
   1  &    0  & \dots &   0        &    0  & \dots & 0 &   0       \\
   0  &    1  & \dots &   0        &    0  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\
   0  &    0  & \dots & \cos\theta  &    0  & \dots & 0 & \sin\theta \\
   0  &    0  & \dots &   0        &    1  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\
   0  &    0  & \dots &  -\sin\theta        &    0  & \dots & 1 &   \cos\theta       \\
   0  &    0  & \dots & 0 & \dots & \dots & 0 & 1
   \end{array}
 \right)
$$
<p>&nbsp;<br>


<p>
with property \( {\bf S^{T}} = {\bf S^{-1}} \).
It performs a plane rotation around an angle \( \theta \) in the Euclidean
$n-$dimensional space.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Jacobi  <a name="___sec52"></a></h2>

It means that its matrix elements that differ
from zero are given by

<p>&nbsp;<br>
$$
    s_{kk}= s_{ll}=\cos\theta,
    s_{kl}=-s_{lk}= -\sin\theta,
    s_{ii}=-s_{ii}=1\quad i\ne k \quad i \ne l,
$$
<p>&nbsp;<br>


<p>
A similarity transformation

<p>&nbsp;<br>
$$
     {\bf B}= {\bf S}^T {\bf A}{\bf S},
$$
<p>&nbsp;<br>


<p>
results in

<p>&nbsp;<br>
$$
\begin{align*}
b_{ik} &= a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} &= a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} &= a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} &= a_{ll}\cos^2\theta +2a_{kl}\cos\theta sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} &= (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber
\end{align*}
$$
<p>&nbsp;<br>


<p>
The angle \( \theta \) is  arbitrary. The recipe is to choose  \( \theta \) so that all
non-diagonal matrix elements \( b_{kl} \) become zero.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Jacobi  <a name="___sec53"></a></h2>

The main idea is thus to reduce systematically the
norm of the
off-diagonal matrix elements  of a matrix  \( {\bf A} \)

<p>&nbsp;<br>
$$
\mbox{off}({\bf A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}.
$$
<p>&nbsp;<br>

 To demonstrate the algorithm, we consider the  simple \( 2\times 2 \)  similarity transformation
of the full matrix. The matrix is symmetric, we single out $ 1\le k < l \le n$  and
use the abbreviations \( c=\cos\theta \) and \( s=\sin\theta \) to obtain

<p>&nbsp;<br>
$$
 \left( \begin{array}{cc} b_{kk} & 0 \\
                          0 & b_{ll} \\\end{array} \right)  =  \left( \begin{array}{cc} c & -s \\
                          s &c \\\end{array} \right)  \left( \begin{array}{cc} a_{kk} & a_{kl} \\
                          a_{lk} &a_{ll} \\\end{array} \right) \left( \begin{array}{cc} c & s \\
                          -s & c \\\end{array} \right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Jacobi  <a name="___sec54"></a></h2>

We require that the non-diagonal matrix elements \( b_{kl}=b_{lk}=0 \), implying that

<p>&nbsp;<br>
$$
a_{kl}(c^2-s^2)+(a_{kk}-a_{ll})cs = b_{kl} = 0.
$$
<p>&nbsp;<br>

If \( a_{kl}=0 \) one sees immediately that \( \cos\theta = 1 \) and \( \sin\theta=0 \).

<p>
The Frobenius norm of an orthogonal transformation is always preserved. The Frobenius norm is defined
as

<p>&nbsp;<br>
$$
||{\bf A}||_F =  \sqrt{\sum_{i=1}^n\sum_{j=1}^n |a_{ij}|^2}.
$$
<p>&nbsp;<br>

This means that for our \( 2\times 2 \) case  we have

<p>&nbsp;<br>
$$
2a_{kl}^2+a_{kk}^2+a_{ll}^2 = b_{kk}^2+b_{ll}^2,
$$
<p>&nbsp;<br>

which leads to

<p>&nbsp;<br>
$$
\mbox{off}({\bf B})^2 = ||{\bf B}||_F^2-\sum_{i=1}^nb_{ii}^2=\mbox{off}({\bf A})^2-2a_{kl}^2,
$$
<p>&nbsp;<br>

since

<p>&nbsp;<br>
$$
||{\bf B}||_F^2-\sum_{i=1}^nb_{ii}^2=||{\bf A}||_F^2-\sum_{i=1}^na_{ii}^2+(a_{kk}^2+a_{ll}^2 -b_{kk}^2-b_{ll}^2).
$$
<p>&nbsp;<br>

This results means that  the matrix \( {\bf A} \) moves closer to diagonal form  for each transformation.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Jacobi  <a name="___sec55"></a></h2>

Defining the quantities \( \tan\theta = t= s/c \) and

<p>&nbsp;<br>
$$\cot 2\theta=\tau = \frac{a_{ll}-a_{kk}}{2a_{kl}},
$$
<p>&nbsp;<br>

we obtain the quadratic equation (using \( \cot 2\theta=1/2(\cot \theta-\tan\theta) \)

<p>&nbsp;<br>
$$
t^2+2\tau t-1= 0,
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
  t = -\tau \pm \sqrt{1+\tau^2},
$$
<p>&nbsp;<br>

and \( c \) and \( s \) are easily obtained via

<p>&nbsp;<br>
$$
   c = \frac{1}{\sqrt{1+t^2}},
$$
<p>&nbsp;<br>

and \( s=tc \).  Choosing \( t \) to be the smaller of the roots ensures that \( |\theta| \le \pi/4 \) and has the
effect of minimizing the difference between the matrices \( {\bf B} \) and \( {\bf A} \) since

<p>&nbsp;<br>
$$
||{\bf B}-{\bf A}||_F^2=4(1-c)\sum_{i=1,i\ne k,l}^n(a_{ik}^2+a_{il}^2) +\frac{2a_{kl}^2}{c^2}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Jacobi algo  <a name="___sec56"></a></h2>

<b>Step 1.</b>
Choose a tolerance \( \epsilon \), making it a small number, typically \( 10^{-8} \) or smaller.

<p>
<b>Step 2.</b>
Setup a <code>while</code>-test  where one compares the norm of the newly computed off-diagonal matrix elements

<p>&nbsp;<br>
$$ \mbox{off}({\bf A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}   >  \epsilon.
$$
<p>&nbsp;<br>


<p>
<b>Step 3.</b>
Now choose the matrix elements \( a_{kl} \) so that we have those with largest value, that is \( |a_{kl}|=\mbox{max}_{i\ne j} |a_{ij}| \).

<p>
<b>Step 4.</b>
Compute thereafter \( \tau = (a_{ll}-a_{kk})/2a_{kl} \), \( \tan\theta \), \( \cos\theta \) and \( \sin\theta \).

<p>
<b>Step 5.</b>
Compute thereafter the similarity transformation for this set of values \( (k,l) \), obtaining the new matrix \( {\bf B}= {\bf S}(k,l,\theta)^T {\bf A}{\bf S}(k,l,\theta) \).

<p>
<b>Step 6.</b>
Compute the new norm of the off-diagonal matrix elements and continue till you have satisfied \( \mbox{off}({\bf B})  \le  \epsilon \)

<p>
The convergence rate of the Jacobi method is however poor, one needs typically \( 3n^2-5n^2 \) rotations and each rotation
requires \( 4n \) operations, resulting in a total of \( 12n^3-20n^3 \) operations in order to zero out non-diagonal matrix elements.

<p>

</section>


<section>

<h2>Jacobi's method, an example to convice you about the algorithm  <a name="___sec57"></a></h2>

We specialize to a symmetric \( 3\times 3  \) matrix \( {\bf A} \).
We start the process as follows (assuming that \( a_{23}=a_{32} \) is the largest non-diagonal)
with \( c=\cos{\theta} \) and \( s=\sin{\theta} \)

<p>&nbsp;<br>
$$
 {\bf B} =
      \left( \begin{array}{ccc}
                1 & 0 & 0    \\
                0 & c & -s     \\
                0 & s & c
             \end{array} \right)\left( \begin{array}{ccc}
                a_{11} & a_{12} & a_{13}    \\
                a_{21} & a_{22} & a_{23}     \\
                a_{31} & a_{32} & a_{33}
             \end{array} \right)
              \left( \begin{array}{ccc}
                1 & 0 & 0    \\
                0 & c & s     \\
                0 & -s & c
             \end{array} \right).
$$
<p>&nbsp;<br>

We will choose the angle \( \theta \) in order to have \( a_{23}=a_{32}=0 \).
We get (symmetric matrix)

<p>&nbsp;<br>
$$
 {\bf B} =\left( \begin{array}{ccc}
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
$$
<p>&nbsp;<br>

Note that \( a_{11} \) is unchanged! As it should.

<p>

</section>


<section>

<h2>Jacobi's method, an example to convice you about the algorithm  <a name="___sec58"></a></h2>

We have

<p>&nbsp;<br>
$$
 {\bf B} =\left( \begin{array}{ccc}
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
\begin{align*}
b_{11} &= a_{11} \\
b_{12} &= a_{12}\cos\theta - a_{13}\sin\theta , 1 \ne 2, 1 \ne 3 \\
b_{13} &= a_{13}\cos\theta + a_{12}\sin\theta , 1 \ne 2, 1 \ne 3 \nonumber\\
b_{22} &= a_{22}\cos^2\theta - 2a_{23}\cos\theta \sin\theta +a_{33}\sin^2\theta\nonumber\\
b_{33} &= a_{33}\cos^2\theta +2a_{23}\cos\theta \sin\theta +a_{22}\sin^2\theta\nonumber\\
b_{23} &= (a_{22}-a_{33})\cos\theta \sin\theta +a_{23}(\cos^2\theta-\sin^2\theta)\nonumber
\end{align*}
$$
<p>&nbsp;<br>

We will fix the angle \( \theta \) so that \( b_{23}=0 \).

<p>

</section>


<section>

<h2>Jacobi's method, an example to convice you about the algorithm  <a name="___sec59"></a></h2>

We get then a new matrix

<p>&nbsp;<br>
$$
 {\bf B} =\left( \begin{array}{ccc}
                b_{11} & b_{12}& b_{13}    \\
                b_{12}& b_{22}& 0    \\
                b_{13}& 0& a_{33}
             \end{array} \right).
$$
<p>&nbsp;<br>

We repeat then assuming that \( b_{12} \) is the largest non-diagonal matrix element and get a
new matrix

<p>&nbsp;<br>
$$
 {\bf C} =
      \left( \begin{array}{ccc}
                c & -s & 0    \\
                s & c & 0     \\
                0 & 0 & 1
             \end{array} \right)\left( \begin{array}{ccc}
                b_{11} & b_{12} & b_{13}    \\
                b_{12} & b_{22} & 0     \\
                b_{13} & 0 & b_{33}
             \end{array} \right)
              \left( \begin{array}{ccc}
                c & s & 0    \\
                -s & c & 0     \\
                0 & 0 & 1
             \end{array} \right).
$$
<p>&nbsp;<br>

We continue this process till all non-diagonal matrix elements are zero (ideally).
You will notice that performing the above operations that the matrix element
\( b_{23} \) which was previous zero becomes different from zero.  This is one of the problems which slows
down the jacobi procedure.

<p>

</section>


<section>

<h2>Jacobi's method, an example to convice you about the algorithm  <a name="___sec60"></a></h2>

The more general expression for the new matrix elements are

<p>&nbsp;<br>
$$
\begin{align*}
b_{ii} &= a_{ii}, i \ne k, i \ne l \\
b_{ik} &= a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} &= a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} &= a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} &= a_{ll}\cos^2\theta +2a_{kl}\cos\theta \sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} &= (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber
\end{align*}
$$
<p>&nbsp;<br>

This is what we will need to code.

<p>

</section>


<section>

<h2>Jacobi code example  <a name="___sec61"></a></h2>

Main part

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  we have defined a matrix A and a matrix R for the eigenvector, both of dim n x n</span>
<span style="color: #228B22">//  The final matrix R has the eigenvectors in its row elements, it is set to one</span>
<span style="color: #228B22">//  for the diagonal elements in the beginning, zero else.</span>
....
<span style="color: #a7a7a7; font-weight: bold">double</span> tolerance = <span style="color: #B452CD">1.0E-10</span>;
<span style="color: #a7a7a7; font-weight: bold">int</span> iterations = <span style="color: #B452CD">0</span>;
<span style="color: #8B008B; font-weight: bold">while</span> ( maxnondiag &gt; tolerance &amp;&amp; iterations &lt;= maxiter)
{
   <span style="color: #a7a7a7; font-weight: bold">int</span> p, q;
   maxnondiag  = offdiag(A, p, q, n);
   Jacobi_rotate(A, R, p, q, n);
   iterations++;

...
</pre></div>
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">
</pre></div>
<p>

</section>


<section>

<h2>Jacobi code example  <a name="___sec62"></a></h2>

Finding the max nondiagonal element

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  the offdiag function</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">offdiag</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> **A, <span style="color: #a7a7a7; font-weight: bold">int</span> p, <span style="color: #a7a7a7; font-weight: bold">int</span> q, <span style="color: #a7a7a7; font-weight: bold">int</span> n);
{
   <span style="color: #a7a7a7; font-weight: bold">double</span> max;
   <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; n; ++i)
   {
       <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> j = i+<span style="color: #B452CD">1</span>; j &lt; n; ++j)
       {
           <span style="color: #a7a7a7; font-weight: bold">double</span> aij = fabs(A[i][j]);
           <span style="color: #8B008B; font-weight: bold">if</span> ( aij &gt; max)
           {
              max = aij;  p = i; q = j;

   <span style="color: #8B008B; font-weight: bold">return</span> max;

...
</pre></div>
<p>

</section>


<section>

<h2>Jacobi code example  <a name="___sec63"></a></h2>

Finding the new matrix elements

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">Jacobi_rotate</span> ( <span style="color: #a7a7a7; font-weight: bold">double</span> ** A, <span style="color: #a7a7a7; font-weight: bold">double</span> ** R, <span style="color: #a7a7a7; font-weight: bold">int</span> k, <span style="color: #a7a7a7; font-weight: bold">int</span> l, <span style="color: #a7a7a7; font-weight: bold">int</span> n )
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> s, c;
  <span style="color: #8B008B; font-weight: bold">if</span> ( A[k][l] != <span style="color: #B452CD">0.0</span> ) {
    <span style="color: #a7a7a7; font-weight: bold">double</span> t, tau;
    tau = (A[l][l] - A[k][k])/(<span style="color: #B452CD">2</span>*A[k][l]);

    <span style="color: #8B008B; font-weight: bold">if</span> ( tau &gt;= <span style="color: #B452CD">0</span> ) {
      t = <span style="color: #B452CD">1.0</span>/(tau + sqrt(<span style="color: #B452CD">1.0</span> + tau*tau));
    } <span style="color: #8B008B; font-weight: bold">else</span> {
      t = -<span style="color: #B452CD">1.0</span>/(-tau +sqrt(<span style="color: #B452CD">1.0</span> + tau*tau));


    c = <span style="color: #B452CD">1</span>/sqrt(<span style="color: #B452CD">1</span>+t*t);
    s = c*t;
  } <span style="color: #8B008B; font-weight: bold">else</span> {
    c = <span style="color: #B452CD">1.0</span>;
    s = <span style="color: #B452CD">0.0</span>;
</pre></div>
<p>

</section>


<section>

<h2>Jacobi code example  <a name="___sec64"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #a7a7a7; font-weight: bold">double</span> a_kk, a_ll, a_ik, a_il, r_ik, r_il;
  a_kk = A[k][k];
  a_ll = A[l][l];
  A[k][k] = c*c*a_kk - <span style="color: #B452CD">2.0</span>*c*s*A[k][l] + s*s*a_ll;
  A[l][l] = s*s*a_kk + <span style="color: #B452CD">2.0</span>*c*s*A[k][l] + c*c*a_ll;
  A[k][l] = <span style="color: #B452CD">0.0</span>;  <span style="color: #228B22">// hard-coding non-diagonal elements by hand</span>
  A[l][k] = <span style="color: #B452CD">0.0</span>;  <span style="color: #228B22">// same here</span>
  <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; n; i++ ) {
    <span style="color: #8B008B; font-weight: bold">if</span> ( i != k &amp;&amp; i != l ) {
      a_ik = A[i][k];
      a_il = A[i][l];
      A[i][k] = c*a_ik - s*a_il;
      A[k][i] = A[i][k];
      A[i][l] = c*a_il + s*a_ik;
      A[l][i] = A[i][l];
</pre></div>
<p>

</section>


<section>

<h2>Jacobi code example  <a name="___sec65"></a></h2>

and finally the new eigenvectors

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    r_ik = R[i][k];
    r_il = R[i][l];

    R[i][k] = c*r_ik - s*r_il;
    R[i][l] = c*r_il + s*r_ik;

  <span style="color: #8B008B; font-weight: bold">return</span>;
} <span style="color: #228B22">// end of function jacobi_rotate</span>
</pre></div>

<h1>Week 39  <a name="___sec66"></a></h1>


</section>


<section>

<h2>Overview of week 39  <a name="___sec67"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Eigenvalue problems and differential equations (change of plans).</b>
<p>

<ul>
  <p><li> Monday: Brief repetition from last week, with discussion of project 2.</li>
  <p><li> Discussion of Householder's and Francis' algorithm (not finished last week)</li>
  <p><li> Discussion of Lanczos' method (also optional part of project 2)</li>
  <p><li> Tuesday:</li>
  <p><li> Introduction to differential equations</li>
  <p><li> Differential equations, general properties.</li>
  <p><li> Runge-Kutta methods The material on differential equations is covered by chapters 8, 9 and 10.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec68"></a></h2>

The first step  consists in finding
an orthogonal  matrix \( {\bf S} \) which is the product of \( (n-2) \) orthogonal matrices

<p>&nbsp;<br>
$$
   {\bf S}={\bf S}_1{\bf S}_2\dots{\bf S}_{n-2},
$$
<p>&nbsp;<br>


<p>
each of which successively transforms one row and one column of \( {\bf A} \) into the
required tridiagonal form. Only \( n-2 \) transformations are required, since the last two
elements are already in tridiagonal form. In order to determine each \( {\bf S_i} \) let us
see what happens after the first multiplication, namely,

<p>&nbsp;<br>
$$
    {\bf S}_1^T{\bf A}{\bf S}_1=    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &a'_{23}  & \dots    & \dots  &\dots &a'_{2n} \\
                                0   & a'_{32} &a'_{33}  & \dots    & \dots  &\dots &a'_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & a'_{n2} &a'_{n3}  & \dots    & \dots  &\dots &a'_{nn} \\

             \end{array} \right)
$$
<p>&nbsp;<br>


<p>
where the primed quantities represent a matrix \( {\bf A}' \) of dimension
\( n-1 \) which will subsequentely be transformed by \( {\bf S_2} \).

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec69"></a></h2>

The factor  \( e_1 \) is a possibly non-vanishing element. The next
transformation produced by \( {\bf S_2} \) has the same effect as  \( {\bf
S_1} \) but now on the submatirx \( {\bf A^{'}} \) only

<p>&nbsp;<br>
$$
   \left ({\bf S}_{1}{\bf S}_{2} \right )^{T} {\bf A}{\bf S}_{1} {\bf S}_{2}
 = \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &e_2  & 0   & \dots  &\dots &0 \\
                                0   & e_2 &a''_{33}  & \dots    & \dots  &\dots &a''_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & 0 &a''_{n3}  & \dots    & \dots  &\dots &a''_{nn} \\

             \end{array} \right)
$$
<p>&nbsp;<br>


<p>
{\bf Note that the effective size of the matrix on which we apply the transformation reduces
for every new step. In the previous Jacobi method each similarity
transformation is in principle performed on the full size of the original matrix.}

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec70"></a></h2>

After a series of such transformations, we end with a set of diagonal
matrix elements

<p>&nbsp;<br>
$$
  a_{11}, a'_{22}, a''_{33}\dots a^{n-1}_{nn},
$$
<p>&nbsp;<br>


<p>
and off-diagonal matrix elements

<p>&nbsp;<br>
$$
   e_1, e_2,e_3,  \dots, e_{n-1}.
$$
<p>&nbsp;<br>


<p>
The resulting matrix reads

<p>&nbsp;<br>
$$
{\bf S}^{T} {\bf A} {\bf S} =
    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & a''_{33} & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &a^{(n-1)}_{n-2} & e_{n-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n-1} & a^{(n-1)}_{n-1}

             \end{array} \right) .
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec71"></a></h2>

It remains to find a recipe for determining the transformation \( {\bf S}_n \).
We illustrate the method for \( {\bf S}_1 \) which we assume takes the form

<p>&nbsp;<br>
$$
    {\bf S_{1}} = \left( \begin{array}{cc} 1 & {\bf 0^{T}} \\
                              {\bf 0}& {\bf P} \end{array} \right),
$$
<p>&nbsp;<br>


<p>
with \( {\bf 0^{T}} \) being a zero row vector, \( {\bf 0^{T}} = \{0,0,\cdots\} \)
of dimension \( (n-1) \). The matrix \( {\bf P} \)  is symmetric
with dimension (\( (n-1) \times (n-1) \)) satisfying
\( {\bf P}^2={\bf I} \)  and \( {\bf P}^T={\bf P} \).
A possible choice which fullfils the latter two requirements is

<p>&nbsp;<br>
$$
    {\bf P}={\bf I}-2{\bf u}{\bf u}^T,
$$
<p>&nbsp;<br>


<p>
where \( {\bf I} \) is the \( (n-1) \) unity matrix and \( {\bf u} \) is an \( n-1 \)
column vector with norm \( {\bf u}^T{\bf u} \) (inner product).

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec72"></a></h2>

 Note that \( {\bf u}{\bf u}^T \) is an outer product giving a
matrix of dimension (\( (n-1) \times (n-1) \)).
Each matrix element of \( {\bf P} \) then reads

<p>&nbsp;<br>
$$
   P_{ij}=\delta_{ij}-2u_iu_j,
$$
<p>&nbsp;<br>


<p>
where \( i \) and \( j \) range from \( 1 \) to \( n-1 \). Applying the transformation
\( {\bf S}_1 \) results in

<p>&nbsp;<br>
$$
   {\bf S}_1^T{\bf A}{\bf S}_1 =  \left( \begin{array}{cc} a_{11} & ({\bf Pv})^T \\
                              {\bf Pv}& {\bf A}' \end{array} \right) ,
$$
<p>&nbsp;<br>


<p>
where \( {\bf v^{T}} = \{a_{21}, a_{31},\cdots, a_{n1}\} \) and {\bf P}
must satisfy (\( {\bf Pv})^{T} = \{k, 0, 0,\cdots \} \). Then

<p>&nbsp;<br>
$$
\begin{equation}
    {\bf Pv} = {\bf v} -2{\bf u}( {\bf u}^T{\bf v})= k {\bf e},
    \tag{4}
\end{equation}
$$
<p>&nbsp;<br>

with \( {\bf e^{T}} = \{1,0,0,\dots 0\} \).

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec73"></a></h2>

Solving the latter equation gives us \( {\bf u} \) and thus the needed transformation
\( {\bf P} \). We do first however need to compute the scalar \( k \) by taking the scalar
product of the last equation with its transpose and using the fact that \( {\bf P}^2={\bf I} \).
We get then

<p>&nbsp;<br>
$$
   ({\bf Pv})^T{\bf Pv} = k^{2} = {\bf v}^T{\bf v}=
   |v|^2 = \sum_{i=2}^{n}a_{i1}^2,
$$
<p>&nbsp;<br>


<p>
which determines the constant $ k = \pm v$.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec74"></a></h2>

 Now we can rewrite Eq. <a href="#mjx-eqn-4">(4)</a>
as

<p>&nbsp;<br>
$$
    {\bf v} - k{\bf e} = 2{\bf u}( {\bf u}^T{\bf v}),
$$
<p>&nbsp;<br>

and taking the scalar product of this equation with itself and obtain
<p>&nbsp;<br>
$$
\begin{equation}
    2( {\bf u}^T{\bf v})^2=(v^2\pm a_{21}v),
    \tag{5}
\end{equation}
$$
<p>&nbsp;<br>

which finally determines

<p>&nbsp;<br>
$$
    {\bf u}=\frac{{\bf v}-k{\bf e}}{2( {\bf u}^T{\bf v})}.
$$
<p>&nbsp;<br>

In solving Eq. <a href="#mjx-eqn-5">(5)</a> great care has to be exercised so as to choose
those values which make the right-hand largest in order to avoid loss of numerical
precision.
The above steps are then repeated for every transformations till we have a
tridiagonal matrix suitable for obtaining the eigenvalues.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder, brute force  <a name="___sec75"></a></h2>

Our Householder transformation has given us a tridiagonal matrix. We discuss here how one can use
Jacobi's iterative procedure to obtain the eigenvalues.
Letus specialize to a \( 4\times 4  \) matrix.
The tridiagonal matrix takes the form

<p>&nbsp;<br>
$$
 {\bf A} =
      \left( \begin{array}{cccc}
                d_{1} & e_{1} & 0     &  0    \\
                e_{1} & d_{2} & e_{2} &  0    \\
                 0    & e_{2} & d_{3} & e_{3} \\
                 0    &   0   & e_{3} & d_{4}
             \end{array} \right).
$$
<p>&nbsp;<br>


<p>
As a first observation, if any of the elements \( e_{i} \) are zero the
matrix can be separated into smaller pieces before
diagonalization. Specifically, if \( e_{1} = 0 \) then \( d_{1} \) is an
eigenvalue.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec76"></a></h2>

 Thus, let us introduce  a transformation \( {\bf S_{1}} \) which operates like

<p>&nbsp;<br>
$$
 {\bf S_{1}} =
      \left( \begin{array}{cccc}
                \cos \theta & 0 & 0 & \sin \theta\\
                 0       & 0 & 0 &      0      \\
                   0        & 0 & 0 &      0      \\
               \cos \theta & 0 & 0 & \cos \theta
             \end{array} \right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec77"></a></h2>

Then the similarity transformation

<p>&nbsp;<br>
$$
{\bf S_{1}^{T} A  S_{1}} = {\bf A'} =
      \left( \begin{array}{cccc}
              d'_{1} & e'_{1} &   0    &   0   \\
              e'_{1}  & d_{2}  & e_{2}  &   0   \\
                0    & e_{2}  & d_{3}  & e'{3} \\
                0    &   0    & e'_{3} & d'_{4}
             \end{array} \right)
$$
<p>&nbsp;<br>


<p>
produces a matrix where the primed elements in \( {\bf A'} \) have been
changed by the transformation whereas the unprimed elements are unchanged.
If we now choose \( \theta \) to
give the element \( a_{21}^{'} = e^{'}= 0 \) then we have the first
eigenvalue  \( = a_{11}^{'} = d_{1}^{'} \).
(This is actually what you are doing in project 2!!)

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder's and  Francis' algorithm  <a name="___sec78"></a></h2>

This procedure can be continued on the remaining three-dimensional
submatrix for the next eigenvalue. Thus after few transformations
we have the wanted diagonal form.

<p>
What we see here is just a special case of the more general procedure
developed by Francis in two articles in 1961 and 1962. Using Jacobi's method is not very efficient ether.

<p>
The algorithm is based on the so-called {\bf QR} method (or just {\bf QR}-algorithm). It follows from a theorem by Schur which states that any square matrix can be written out in terms of an orthogonal matrix \( \hat{Q} \) and an upper triangular matrix \( \hat{U} \). Historically \( R \) was used instead of
\( U \) since the wording right triangular matrix was first used.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder's and  Francis' algorithm  <a name="___sec79"></a></h2>

The method is based on an iterative procedure similar to Jacobi's method, by a succession of
planar rotations. For a tridiagonal matrix it is simple to carry out in principle, but complicated in detail!

<p>
Schur's theorem

<p>&nbsp;<br>
$$
\hat{A} = \hat{Q}\hat{U},
$$
<p>&nbsp;<br>

is used to rewrite any square matrix into a unitary matrix times an upper triangular matrix.
We say that a square matrix is similar to a triangular matrix.

<p>
Householder's algorithm which we have derived is just a special case of the general Householder algorithm. For a symmetric square matrix we obtain a tridiagonal matrix.

<p>
There is a corollary to Schur's theorem which states that every Hermitian matrix is unitarily similar to a diagonal matrix.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder's and  Francis' algorithm  <a name="___sec80"></a></h2>

It follows that we can define a new matrix

<p>&nbsp;<br>
$$
\hat{A}\hat{Q} = \hat{Q}\hat{U}\hat{Q},
$$
<p>&nbsp;<br>

and multiply from the left with \( \hat{Q}^{-1} \) we get

<p>&nbsp;<br>
$$
\hat{Q}^{-1}\hat{A}\hat{Q} = \hat{B}=\hat{U}\hat{Q},
$$
<p>&nbsp;<br>

where the matrix \( \hat{B} \) is a similarity transformation of \( \hat{A} \) and has the same eigenvalues
as \( \hat{B} \).

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder's and  Francis' algorithm  <a name="___sec81"></a></h2>

Suppose \( \hat{A} \) is the triangular matrix we obtained after the Householder  transformation,

<p>&nbsp;<br>
$$
\hat{A} = \hat{Q}\hat{U},
$$
<p>&nbsp;<br>

and multiply from the left with \( \hat{Q}^{-1} \) resulting in

<p>&nbsp;<br>
$$
\hat{Q}^{-1}\hat{A} = \hat{U}.
$$
<p>&nbsp;<br>

Suppose that \( \hat{Q} \) consists of a series of planar Jacobi like rotations acting on sub blocks
of \( \hat{A} \) so that all elements below the diagonal are zeroed out

<p>&nbsp;<br>
$$
\hat{Q}=\hat{R}_{12}\hat{R}_{23}\dots\hat{R}_{n-1,n}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder's and  Francis' algorithm  <a name="___sec82"></a></h2>

A transformation of the type \( \hat{R}_{12} \) looks like

<p>&nbsp;<br>
$$
 \hat{R}_{12} =
      \left( \begin{array}{ccccccccc}
                 c&s &0 &0 &0 &  \dots &0 & 0 & 0\\
                 -s&c &0 &0 &0 &   \dots &0 & 0 & 0\\
                 0&0 &1 &0 &0 &   \dots &0 & 0 & 0\\
                 \dots&\dots &\dots &\dots &\dots &\dots      \\
                 0&0 &0 & 0 & 0 & \dots &1 &0 &0      \\
                 0&0 &0 & 0 & 0 & \dots &0 &1 &0      \\
                 0&0 &0 & 0 & 0 & \dots &0 &0 & 1
             \end{array} \right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder's and Francis' algorithm  <a name="___sec83"></a></h2>

The matrix \( \hat{U} \) takes then the form

<p>&nbsp;<br>
$$
 \hat{U} =
      \left( \begin{array}{ccccccccc}
                 x&x &x &0 &0 &  \dots &0 & 0 & 0\\
                 0&x &x &x &0 &   \dots &0 & 0 & 0\\
                 0&0 &x &x &x &   \dots &0 & 0 & 0\\
                 \dots&\dots &\dots &\dots &\dots &\dots      \\
                 0&0 &0 & 0 & 0 & \dots &x &x &x      \\
                 0&0 &0 & 0 & 0 & \dots &0 &x &x      \\
                 0&0 &0 & 0 & 0 & \dots &0 &0 & x
             \end{array} \right)
$$
<p>&nbsp;<br>

which has a second superdiagonal.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder's and Francis' algorithm  <a name="___sec84"></a></h2>

We have now found \( \hat{Q} \) and \( \hat{U} \) and this allows us to find the matrix \( \hat{B} \)
which is, due to Schur's theorem,  unitarily similar to a triangular matrix (upper in our case)
since we have that

<p>&nbsp;<br>
$$
\hat{Q}^{-1}\hat{A}\hat{Q} = \hat{B},
$$
<p>&nbsp;<br>

from Schur's theorem the  matrix \( \hat{B} \) is triangular and the eigenvalues the same as those of
\( \hat{A} \) and are given by the diagonal matrix elements of
\( \hat{B} \). Why?

<p>
Our matrix \( \hat{B}=\hat{U}\hat{Q} \).

<p>

</section>


<section>

<h2>Another iterative procedure  <a name="___sec85"></a></h2>

The matrix \( \hat{A} \) is transformed into a tridiagonal form and the last
step is to transform it into a diagonal matrix giving the eigenvalues
on the diagonal.

<p>
The eigenvalues of a  matrix can be obtained using the characteristic polynomial

<p>&nbsp;<br>
$$
P(\lambda) = det(\lambda{\bf I}-{\bf A})= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right),
$$
<p>&nbsp;<br>

which rewritten in matrix form reads

<p>&nbsp;<br>
$$
P(\lambda)= \left( \begin{array}{ccccccc} d_1-\lambda & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2-\lambda & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3-\lambda & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{N_{\mbox{step}}-2}-\lambda & e_{N_{\mbox{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{N_{\mbox{step}}-1} & d_{N_{\mbox{step}}-1}-\lambda

             \end{array} \right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder  <a name="___sec86"></a></h2>

We can solve this equation in an iterative manner.
We let \( P_k(\lambda) \) be the value of \( k \) subdeterminant of the above matrix of dimension
\( n\times n \). The polynomial \( P_k(\lambda) \) is clearly a polynomial of degree \( k \).
Starting with \( P_1(\lambda) \) we have \( P_1(\lambda)=d_1-\lambda \). The next polynomial reads
\( P_2(\lambda)=(d_2-\lambda)P_1(\lambda)-e_1^2 \). By expanding the determinant for \( P_k(\lambda) \)
in terms of the minors of the $n$th column we arrive at the recursion relation

<p>&nbsp;<br>
$$
   P_k(\lambda)=(d_k-\lambda)P_{k-1}(\lambda)-e_{k-1}^2P_{k-2}(\lambda).
$$
<p>&nbsp;<br>

Together with the starting values \( P_1(\lambda) \) and \( P_2(\lambda) \) and good root searching methods
we arrive at an efficient computational scheme for finding the roots of \( P_n(\lambda) \).
However, for large matrices this algorithm is rather inefficient and time-consuming.

<p>

</section>


<section>

<h2>Eigenvalue Solvers, Householder functions  <a name="___sec87"></a></h2>

The programs  which performs these transformations are
\( \mbox{matrix} \quad {\bf A} \longrightarrow \mbox{tridiagonal matrix}
 \longrightarrow \mbox{diagonal matrix} \)

<p>
C++:

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">void</span> trd2(<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #a61717; background-color: #e3d2d2">$</span>**<span style="color: #a61717; background-color: #e3d2d2">$</span>a, <span style="color: #a7a7a7; font-weight: bold">int</span> n, <span style="color: #a7a7a7; font-weight: bold">double</span> d[], <span style="color: #a7a7a7; font-weight: bold">double</span> e[])
<span style="color: #a7a7a7; font-weight: bold">void</span> tqli(<span style="color: #a7a7a7; font-weight: bold">double</span> d[], <span style="color: #a7a7a7; font-weight: bold">double</span>[], <span style="color: #a7a7a7; font-weight: bold">int</span> n, <span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #a61717; background-color: #e3d2d2">$</span>**<span style="color: #a61717; background-color: #e3d2d2">$</span>z)
</pre></div>
<p>
Fortran:

<p>

<!-- code=fortran (!bc fcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #8B008B; font-weight: bold">CALL </span><span style="color: #00688B">tred2</span>(<span style="color: #00688B">a</span>, <span style="color: #00688B">n</span>, <span style="color: #00688B">d</span>, <span style="color: #00688B">e</span>)
<span style="color: #8B008B; font-weight: bold">CALL </span><span style="color: #00688B">tqli</span>(<span style="color: #00688B">d</span>, <span style="color: #00688B">e</span>, <span style="color: #00688B">n</span>, <span style="color: #00688B">z</span>)
</pre></div>
<p>

</section>


<section>

<h2>Using Lapack to solve linear algebra and eigenvalue problems  <a name="___sec88"></a></h2>

Suppose you wanted to solve a general system of linear equations \( \hat{A}{\bf x} =  {\bf b} \), where $\hat{A}$is an \( n \times n \) square matrix and \( x \) and \( b \) are \( n \)-element column vectors. You opt to use the routine
{\bf dgesv}. The man page abstract obtained with

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">
</pre></div>
<p>

</section>


<section>

<h2>Lapack example  <a name="___sec89"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#include &lt;iostream&gt;</span>
<span style="color: #1e889b">#define MAX 10</span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>(){
   <span style="color: #228B22">// Values needed for dgesv</span>
   <span style="color: #a7a7a7; font-weight: bold">int</span> n;
   <span style="color: #a7a7a7; font-weight: bold">int</span> nrhs = <span style="color: #B452CD">1</span>;
   <span style="color: #a7a7a7; font-weight: bold">double</span> a[MAX][MAX];
   <span style="color: #a7a7a7; font-weight: bold">double</span> b[<span style="color: #B452CD">1</span>][MAX];
   <span style="color: #a7a7a7; font-weight: bold">int</span> lda = MAX;
   <span style="color: #a7a7a7; font-weight: bold">int</span> ldb = MAX;
   <span style="color: #a7a7a7; font-weight: bold">int</span> ipiv[MAX];
   <span style="color: #a7a7a7; font-weight: bold">int</span> info;
   <span style="color: #228B22">// Other values</span>
   <span style="color: #a7a7a7; font-weight: bold">int</span> i,j;
</pre></div>
<p>

</section>


<section>

<h2>Lapack example  <a name="___sec90"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   <span style="color: #228B22">// Read the values of the matrix</span>
   cout &lt;&lt; <span style="color: #CD5555">&quot;Enter n \n&quot;</span>;
   cin &gt;&gt; n;
   cout &lt;&lt; <span style="color: #CD5555">&quot;On each line type a row of the matrix A followed by one element of b:\n&quot;</span>;
   <span style="color: #8B008B; font-weight: bold">for</span>(i = <span style="color: #B452CD">0</span>; i &lt; n; i++){
     cout &lt;&lt; <span style="color: #CD5555">&quot;row &quot;</span> &lt;&lt; i &lt;&lt; <span style="color: #CD5555">&quot; &quot;</span>;
     <span style="color: #8B008B; font-weight: bold">for</span>(j = <span style="color: #B452CD">0</span>; j &lt; n; j++)std::cin &gt;&gt; a[j][i];
     cin &gt;&gt; b[<span style="color: #B452CD">0</span>][i];
</pre></div>
<p>

</section>


<section>

<h2>Lapack example  <a name="___sec91"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   <span style="color: #228B22">// Solve the linear system</span>
   dgesv(n, nrhs, &amp;a[<span style="color: #B452CD">0</span>][<span style="color: #B452CD">0</span>], lda, ipiv, &amp;b[<span style="color: #B452CD">0</span>][<span style="color: #B452CD">0</span>], ldb, &amp;info);
   <span style="color: #228B22">// Check for success</span>
   <span style="color: #8B008B; font-weight: bold">if</span>(info == <span style="color: #B452CD">0</span>)
   {
      <span style="color: #228B22">// Write the answer</span>
      cout &lt;&lt; <span style="color: #CD5555">&quot;The answer is\n&quot;</span>;
      <span style="color: #8B008B; font-weight: bold">for</span>(i = <span style="color: #B452CD">0</span>; i &lt; n; i++)
        cout &lt;&lt; <span style="color: #CD5555">&quot;b[&quot;</span> &lt;&lt; i &lt;&lt; <span style="color: #CD5555">&quot;]\t&quot;</span> &lt;&lt; b[<span style="color: #B452CD">0</span>][i] &lt;&lt; <span style="color: #CD5555">&quot;\n&quot;</span>;

   <span style="color: #8B008B; font-weight: bold">else</span>
   {
      <span style="color: #228B22">// Write an error message</span>
      cerr &lt;&lt; <span style="color: #CD5555">&quot;dgesv returned error &quot;</span> &lt;&lt; info &lt;&lt; <span style="color: #CD5555">&quot;\n&quot;</span>;

   <span style="color: #8B008B; font-weight: bold">return</span> info;
</pre></div>
<p>

</section>


<section>

<h2>Lanczos' iteration  <a name="___sec92"></a></h2>

Basic features with a real symmetric matrix (and normally huge \( n > 10^6 \) and sparse)
\( \hat{A} \) of dimension \( n\times n \):

<p>
Lanczos' algorithm generates a sequence of real tridiagonal matrices \( T_k \) of dimension \( k\times k \) with \( k\le n \), with the property that the extremal eigenvalues of \( T_k \) are progressively better estimates of \( \hat{A} \)' extremal eigenvalues.

<p>
The method converges to the extremal eigenvalues.

<p>
The similarity transformation is

<p>&nbsp;<br>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$
<p>&nbsp;<br>

with the first vector \( \hat{Q}\hat{e}_1=\hat{q}_1 \).

<p>

</section>


<section>

<h2>Lanczos' iteration  <a name="___sec93"></a></h2>

We are going to solve iteratively

<p>&nbsp;<br>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$
<p>&nbsp;<br>

with the first vector \( \hat{Q}\hat{e}_1=\hat{q}_1 \).
We can write out the matrix \( \hat{Q} \) in terms of its column vectors

<p>&nbsp;<br>
$$
\hat{Q}=\left[\hat{q}_1\hat{q}_2\dots\hat{q}_n\right].
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Lanczos' iteration  <a name="___sec94"></a></h2>

The matrix

<p>&nbsp;<br>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$
<p>&nbsp;<br>

can be written as

<p>&nbsp;<br>
$$
    \hat{T} = \left(\begin{array}{cccccc}
                           \alpha_1& \beta_1 & 0 &\dots   & \dots &0 \\
                           \beta_1 & \alpha_2 & \beta_2 &0 &\dots &0 \\
                           0& \beta_2 & \alpha_3 & \beta_3 & \dots &0 \\
                           \dots& \dots   & \dots &\dots   &\dots & 0 \\
                           \dots&   &  &\beta_{n-2}  &\alpha_{n-1}& \beta_{n-1} \\
                           0&  \dots  &\dots  &0   &\beta_{n-1} & \alpha_{n} \\
                      \end{array} \right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Lanczos' iteration  <a name="___sec95"></a></h2>

Using the fact that \( \hat{Q}\hat{Q}^T=\hat{I} \),
we can rewrite

<p>&nbsp;<br>
$$
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
$$
<p>&nbsp;<br>

as

<p>&nbsp;<br>
$$
\hat{Q}\hat{T}= \hat{A}\hat{Q},
$$
<p>&nbsp;<br>

and if we equate columns (recall from the previous slide)

<p>&nbsp;<br>
$$
    \hat{T} = \left(\begin{array}{cccccc}
                           \alpha_1& \beta_1 & 0 &\dots   & \dots &0 \\
                           \beta_1 & \alpha_2 & \beta_2 &0 &\dots &0 \\
                           0& \beta_2 & \alpha_3 & \beta_3 & \dots &0 \\
                           \dots& \dots   & \dots &\dots   &\dots & 0 \\
                           \dots&   &  &\beta_{n-2}  &\alpha_{n-1}& \beta_{n-1} \\
                           0&  \dots  &\dots  &0   &\beta_{n-1} & \alpha_{n} \\
                      \end{array} \right)
$$
<p>&nbsp;<br>

we obtain

<p>&nbsp;<br>
$$
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Lanczos' iteration  <a name="___sec96"></a></h2>

We have thus

<p>&nbsp;<br>
$$
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1},
$$
<p>&nbsp;<br>

with \( \beta_0\hat{q}_0=0 \) for \( k=1:n-1 \).
Remember that the vectors \( \hat{q}_k \)  are orthornormal and this implies

<p>&nbsp;<br>
$$
\alpha_k=\hat{q}_k^T\hat{A}\hat{q}_k,
$$
<p>&nbsp;<br>

and these vectors are called Lanczos vectors.

<p>

</section>


<section>

<h2>Lanczos' iteration, the algorithm  <a name="___sec97"></a></h2>

We have thus

<p>&nbsp;<br>
$$
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1},
$$
<p>&nbsp;<br>

with \( \beta_0\hat{q}_0=0 \) for \( k=1:n-1 \) and

<p>&nbsp;<br>
$$
\alpha_k=\hat{q}_k^T\hat{A}\hat{q}_k.
$$
<p>&nbsp;<br>

If

<p>&nbsp;<br>
$$
\hat{r}_k=(\hat{A}-\alpha_k\hat{I})\hat{q}_k-\beta_{k-1}\hat{q}_{k-1},
$$
<p>&nbsp;<br>

is non-zero, then

<p>&nbsp;<br>
$$
\hat{q}_{k+1}=\hat{r}_{k}/\beta_k,
$$
<p>&nbsp;<br>

with \( \beta_k=\pm ||\hat{r}_{k}||_2 \).

<p>

</section>


<section>

<h2>A simple implementation of the Lanczos algorithm  <a name="___sec98"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  r_0 = q_1; beta_0=<span style="color: #B452CD">1</span>; q_0=<span style="color: #B452CD">0</span>; <span style="color: #a7a7a7; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>;
  <span style="color: #8B008B; font-weight: bold">while</span> (beta_k != <span style="color: #B452CD">0</span>)
      q_{k+<span style="color: #B452CD">1</span>} = r_k/beta_k
      k = k+<span style="color: #B452CD">1</span>
      alpha_k = q_k^T A q_k
      r_k = (A-alpha_k I) q_k  -beta_{k-<span style="color: #B452CD">1</span>}q_{k-<span style="color: #B452CD">1</span>}
      beta_k = || r_k||_2
  end <span style="color: #8B008B; font-weight: bold">while</span>
</pre></div>
<p>

</section>


<section>

<h2>Differential equations program  <a name="___sec99"></a></h2>

<ul>
  <p><li> Ordinary differential equations, Runge-Kutta method,chapter 8</li>
  <p><li> Ordinary differential equations with boundary conditions: one-variable equations to be solved by shooting and Green's function methods, chapter 9</li>
  <p><li> We can solve such equations by a finite difference scheme as well, turning the equation into an eigenvalue problem. Still one variable. Done in projects 1 and 2.</li>
  <p><li> If we have more than one variable, we need to solve partial differential equations, see Chapter 10</li>
  <p><li> Fourier transforms and Fast Fourier transforms if we get time. Project 3 deals with ordinary differential equations (most likely the solar system).</li>
</ul>
<p>


</section>


<section>

<h2>Differential Equations, chapter 8  <a name="___sec100"></a></h2>

The order of the ODE refers to the order of the derivative
on the left-hand side in the equation

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{dy}{dt}=f(t,y).
\end{equation}
$$
<p>&nbsp;<br>

This equation is of first order and \( f \) is an arbitrary function.
A second-order equation goes typically like

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{d^2y}{dt^2}=f(t,\frac{dy}{dt},y).
\end{equation}
$$
<p>&nbsp;<br>

A well-known second-order equation is Newton's second law

<p>&nbsp;<br>
$$
\begin{equation}
   m\frac{d^2x}{dt^2}=-kx,
   \tag{6}
\end{equation}
$$
<p>&nbsp;<br>

where \( k \) is the force constant. ODE depend only on one
variable

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec101"></a></h2>

partial differential equations like the time-dependent Schr\"odinger
equation

<p>&nbsp;<br>
$$
\begin{equation}
   i\hbar\frac{\partial \psi({\bf x},t)}{\partial t}=
   -\frac{\hbar^2}{2m}\left( \frac{\partial^2 \psi({\bf r},t)}{\partial x^2} +
                            \frac{\partial^2 \psi({\bf r},t)}{\partial y^2}+
                            \frac{\partial^2 \psi({\bf r},t)}{\partial z^2}\right) + V({\bf x})\psi({\bf x},t),
\end{equation}
$$
<p>&nbsp;<br>

may depend on several variables. In certain cases, like the above
equation, the wave function can be factorized in functions of the separate
variables, so that the Schr\"odinger equation
can be rewritten in terms of sets of ordinary differential equations.
These equations are discussed in chapter 10. Involve boundary conditions in addition to initial conditions.

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec102"></a></h2>

We distinguish also between linear and non-linear differential
equation where e.g.,

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{dy}{dt}=g^3(t)y(t),
\end{equation}
$$
<p>&nbsp;<br>

is an example of a linear equation, while

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{dy}{dt}=g^3(t)y(t)-g(t)y^2(t),
\end{equation}
$$
<p>&nbsp;<br>

     is a non-linear ODE.

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec103"></a></h2>

Another concept which dictates the numerical method chosen
for solving an ODE, is that of initial and boundary conditions.
To give an example, if we study white dwarf stars or neutron stars
we will need to solve two coupled first-order differential
equations, one for the total mass \( m \) and one for the
pressure \( P \) as functions of
\( \rho \)

<p>&nbsp;<br>
$$
\frac{dm}{dr}=4\pi r^{2}\rho (r)/c^2,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\frac{dP}{dr}=-\frac{Gm(r)}{r^{2}}\rho (r)/c^2.
$$
<p>&nbsp;<br>

where \( \rho \) is the mass-energy density.
The initial conditions are dictated by the mass being
zero at the center of the star, i.e., when \( r=0 \),
yielding \( m(r=0)=0 \). The other condition is that
the pressure vanishes at the surface of the star.

<p>
In the solution of the Schr\"odinger equation for a particle
in a potential, we may need to apply boundary conditions as well,
such as demanding continuity of the wave function and its derivative.

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec104"></a></h2>

In many cases it is possible to rewrite a second-order
differential equation in terms of two first-order differential
equations. Consider again the case of Newton's second law in Eq.
<a href="#mjx-eqn-6">(6)</a>. If we define the position \( x(t)=y^{(1)}(t) \)
and the velocity \( v(t)=y^{(2)}(t) \) as its derivative

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{dy^{(1)}(t)}{dt}=\frac{dx(t)}{dt}=y^{(2)}(t),
\end{equation}
$$
<p>&nbsp;<br>

we can rewrite Newton's second law as two coupled first-order
differential equations

<p>&nbsp;<br>
$$
\begin{equation}
   m\frac{dy^{(2)}(t)}{dt}=-kx(t)=-ky^{(1)}(t),
    \tag{7}
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
\frac{dy^{(1)}(t)}{dt}=y^{(2)}(t).
    \tag{8}
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Differential Equations, Finite Difference  <a name="___sec105"></a></h2>

These methods fall under the general class of one-step methods.
The algoritm is rather simple.
Suppose we have an initial value for the function \( y(t) \) given by

<p>&nbsp;<br>
$$
\begin{equation}
  y_0=y(t=t_0).
\end{equation}
$$
<p>&nbsp;<br>

We are interested in solving a differential equation in a region
in space [a,b]. We define a step \( h \) by splitting the interval
in \( N \) sub intervals, so that we have

<p>&nbsp;<br>
$$
\begin{equation}
  h=\frac{b-a}{N}.
\end{equation}
$$
<p>&nbsp;<br>

With this step and the derivative of \( y \) we can construct the
next value of the function \( y \) at

<p>&nbsp;<br>
$$
\begin{equation}
   y_1=y(t_1=t_0+h),
\end{equation}
$$
<p>&nbsp;<br>

and so forth.

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec106"></a></h2>

If the function is rather well-behaved in the domain
[a,b], we can use a fixed step size. If not, adaptive steps
may be needed. Here we concentrate on fixed-step
methods only.
Let us try to generalize the above procedure by writing the
step \( y_{i+1} \) in terms of the previous step \( y_i \)

<p>&nbsp;<br>
$$
\begin{equation}
  y_{i+1}=y(t=t_i+h)=y(t_i) + h\Delta(t_i,y_i(t_i)) + O(h^{p+1}),
\end{equation}
$$
<p>&nbsp;<br>

where \( O(h^{p+1}) \) represents the truncation error. To determine
\( \Delta \), we Taylor expand our function \( y \)

<p>&nbsp;<br>
$$
\begin{equation}
     y_{i+1}=y(t=t_i+h)=y(t_i) + h(y'(t_i)+\dots +y^{(p)}(t_i)\frac{h^{p-1}}{p!}) + O(h^{p+1}),
\tag{9}
\end{equation}
$$
<p>&nbsp;<br>

where we will associate the derivatives in the parenthesis with

<p>&nbsp;<br>
$$
\begin{equation}
\Delta(t_i,y_i(t_i))=(y'(t_i)+\dots +y^{(p)}(t_i)\frac{h^{p-1}}{p!}).
\tag{10}
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec107"></a></h2>

We define

<p>&nbsp;<br>
$$
\begin{equation}
  y'(t_i)=f(t_i,y_i)
\end{equation}
$$
<p>&nbsp;<br>

and if we truncate \( \Delta \) at the first derivative, we have

<p>&nbsp;<br>
$$
\begin{equation}
   y_{i+1}=y(t_i) + hf(t_i,y_i) + O(h^2),
   \tag{11}
\end{equation}
$$
<p>&nbsp;<br>

which when complemented with \( t_{i+1}=t_i+h \) forms
the algorithm for the well-known Euler method.
Note that at every step we make an approximation error
of the order of \( O(h^2) \), however the total error is the sum over all
steps \( N=(b-a)/h \), yielding thus a global error which goes like
\( NO(h^2)\approx O(h) \).

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec108"></a></h2>

To make Euler's method more precise we can obviously
decrease \( h \) (increase \( N \)). However, if we are computing the
derivative \( f \) numerically
by e.g., the two-steps formula

<p>&nbsp;<br>
$$
    f'_{2c}(x)= \frac{f(x+h)-f(x)}{h}+O(h),
$$
<p>&nbsp;<br>

we can enter into roundoff error problems when we subtract
two almost equal numbers \( f(x+h)-f(x)\approx 0 \).
Euler's method is not recommended for precision calculation,
although it is handy to use in order to get a first
view on how a solution may look like. As an example,
consider Newton's equation rewritten in Eqs.
<a href="#mjx-eqn-7">(7)</a> and <a href="#mjx-eqn-8">(8)</a>. We define \( y_0=y^{(1)}(t=0) \)
an \( v_0=y^{(2)}(t=0) \). The first steps in Newton's equations
are then

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(1)}_1=y_0+hv_0+O(h^2)
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
      y^{(2)}_1=v_0-hy_0k/m+O(h^2).
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec109"></a></h2>

The Euler method is asymmetric in time, since it uses information about the derivative at the beginning
of the time interval. This means that we evaluate the position at \( y^{(1)}_1 \) using the velocity
at \( y^{(2)}_0=v_0 \). A simple variation is to determine \( y^{(1)}_{n+1} \) using the velocity at
\( y^{(2)}_{n+1} \), that is (in a slightly more generalized form)

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(1)}_{n+1}=y^{(1)}_{n}+h y^{(2)}_{n+1}+O(h^2)
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(2)}_{n+1}=y^{(2)}_{n}+h a_{n}+O(h^2).
\end{equation}
$$
<p>&nbsp;<br>

The acceleration \( a_n \) is a function of \( a_n(y^{(1)}_{n}, y^{(2)}_{n},t) \) and needs to be evaluated
as well. This is the Euler-Cromer method.

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec110"></a></h2>

Let us then include the second derivative in our Taylor expansion.
We have then

<p>&nbsp;<br>
$$
\begin{equation}
 \Delta(t_i,y_i(t_i))=f(t_i)+\frac{h}{2}\frac{df(t_i,y_i)}{dt}+O(h^3).
\end{equation}
$$
<p>&nbsp;<br>

The second derivative can be rewritten as

<p>&nbsp;<br>
$$
\begin{equation}
  y''=f'=\frac{df}{dt}=\frac{\partial f}{\partial t}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial t}=\frac{\partial f}{\partial t}+\frac{\partial f}{\partial y}f
\tag{12}
\end{equation}
$$
<p>&nbsp;<br>

and we can rewrite Eq. <a href="#mjx-eqn-9">(9)</a> as

<p>&nbsp;<br>
$$
\begin{equation}
     y_{i+1}=y(t=t_i+h)=y(t_i) +hf(t_i)+
     \frac{h^2}{2}\left(\frac{\partial f}{\partial t}+\frac{\partial f}{\partial y}f\right) + O(h^{3  }),
\end{equation}
$$
<p>&nbsp;<br>

which has a local approximation error \( O(h^{3  }) \) and a global
error \( O(h^{2}) \).

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec111"></a></h2>

These approximations can be generalized by using the derivative \( f \) to
arbitrary order so that we have

<p>&nbsp;<br>
$$
\begin{equation}
     y_{i+1}=y(t=t_i+h)=y(t_i) + h(f(t_i,y_i)+\dots f^{(p-1)}(t_i,y_i)
     \frac{h^{p-1}}{p!}) + O(h^{p+1}).
\end{equation}
$$
<p>&nbsp;<br>

These methods, based on higher-order derivatives, are in general not used
in numerical computation, since they rely on evaluating
derivatives several times. Unless one has analytical expressions
for these, the risk of roundoff errors is large.

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec112"></a></h2>

The most obvious improvements to Euler's and Euler-Cromer's algorithms,
avoiding in addition the need for computing a
second derivative, is the so-called midpoint method. We have then

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(1)}_{n+1}=y^{(1)}_{n}+\frac{h}{2}\left(y^{(2)}_{n+1}+y^{(2)}_{n}\right)+O(h^2)
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(2)}_{n+1}=y^{(2)}_{n}+h a_{n}+O(h^2),
\end{equation}
$$
<p>&nbsp;<br>

yielding

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(1)}_{n+1}=y^{(1)}_{n}+hy^{(2)}_{n}+\frac{h^2}{2}a_n+O(h^3)
\end{equation}
$$
<p>&nbsp;<br>

implying that the local truncation error in the position is now \( O(h^3) \), whereas Euler's or Euler-Cromer's
methods have a local error of  \( O(h^2) \).

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec113"></a></h2>

 Thus, the midpoint method yields a global error with
second-order accuracy for
the position and first-order accuracy for the velocity. However, although these methods yield exact results for
constant accelerations, the error increases in general with each time step.

<p>
One method that avoids this is the so-called half-step method. Here we define

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(2)}_{n+1/2}=y^{(2)}_{n-1/2}+h a_{n}+O(h^2),
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(1)}_{n+1}=y^{(1)}_{n}+hy^{(2)}_{n+1/2} +O(h^2).
\end{equation}
$$
<p>&nbsp;<br>

Note that this method needs the calculation of \( y^{(2)}_{1/2} \). This is done using
e.g., Euler's method

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(2)}_{1/2}=y^{(2)}_{0}+h a_{0}+O(h^2).
\end{equation}
$$
<p>&nbsp;<br>

As this method is numerically stable, it is often used instead of Euler's method.

<p>

</section>


<section>

<h2>Differential Equations  <a name="___sec114"></a></h2>

Another method which one may encounter is the Euler-Richardson method
with

<p>&nbsp;<br>
$$
\begin{equation}
   y^{(2)}_{n+1}=y^{(2)}_{n}+h a_{n+1/2}+O(h^2),
   \tag{13}
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
   \tag{14}
   y^{(1)}_{n+1}=y^{(1)}_{n}+hy^{(2)}_{n+1/2} +O(h^2).
\end{equation}
$$
<p>&nbsp;<br>

The program program2.cpp includes all of the above methods.

<p>

</section>


<section>

<h1>Overview of week 40  <a name="___sec115"></a></h1>


</section>


<section>

<h2>Overview of week 40  <a name="___sec116"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Ordinary differential equations (ODEs) and Partial differential equations (PDEs).</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Runge-Kutta methods, with adaptive methods as well</li>
  <p><li> Examples with codes</li>
  <p><li> Tuesday:</li>
  <p><li> Discussion of project 3</li>
  <p><li> Diffusion equation, implicit, explicit (if we get there!!) Chapter 9, ODEs with boundary conditions will not be discussed.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Differential Equations, Runge-Kutta methods  <a name="___sec117"></a></h2>

Runge-Kutta (RK) methods are based on Taylor expansion formulae, but yield
in general better algorithms for solutions of an ODE.
The basic philosophy is that it provides an intermediate step
in the computation of \( y_{i+1} \).

<p>
To see this, consider first the following definitions

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{dy}{dt}=f(t,y),
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
   y(t)=\int f(t,y) dt,
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
  y_{i+1}=y_i+ \int_{t_i}^{t_{i+1}} f(t,y) dt.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Differential Equations, Runge-Kutta methods  <a name="___sec118"></a></h2>

To demonstrate the philosophy behind RK methods, let us consider
the second-order RK method, RK2.
The first approximation consists in Taylor expanding \( f(t,y) \)
around the center of the integration interval \( t_i \) to \( t_{i+1} \),
i.e., at \( t_i+h/2 \), \( h \) being the step.
Using the midpoint formula for an integral,
defining \( y(t_i+h/2) = y_{i+1/2} \) and
\( t_i+h/2 = t_{i+1/2} \), we obtain

<p>&nbsp;<br>
$$
\begin{equation}
    \int_{t_i}^{t_{i+1}} f(t,y) dt \approx hf(t_{i+1/2},y_{i+1/2}) +O(h^3).
\end{equation}
$$
<p>&nbsp;<br>

This means in turn that we have

<p>&nbsp;<br>
$$
\begin{equation}
     y_{i+1}=y_i + hf(t_{i+1/2},y_{i+1/2}) +O(h^3).
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Differential Equations, Runge-Kutta methods  <a name="___sec119"></a></h2>

However, we do not know the value of   \( y_{i+1/2} \).
Here comes thus the next approximation, namely, we use Euler's
method to approximate \( y_{i+1/2} \). We have then

<p>&nbsp;<br>
$$
\begin{equation}
   y_{(i+1/2)}=y_i + \frac{h}{2}\frac{dy}{dt} =
   y(t_i) + \frac{h}{2}f(t_i,y_i).
\end{equation}
$$
<p>&nbsp;<br>

This means that we can define the following algorithm for
the second-order Runge-Kutta method, RK2.

<p>&nbsp;<br>
$$
\begin{equation}
  k_1=hf(t_i,y_i),
\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}
  k_2=hf(t_{i+1/2},y_i+k_1/2),
\end{equation}
$$
<p>&nbsp;<br>

with the final value

<p>&nbsp;<br>
$$
\begin{equation}
  y_{i+i}\approx y_i + k_2 +O(h^3).
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Differential Equations, Runge-Kutta methods  <a name="___sec120"></a></h2>

The difference between the previous one-step methods
is that we now need an intermediate step in our evaluation,
namely \( t_i+h/2 = t_{(i+1/2)} \) where we evaluate the derivative \( f \).
This involves more operations, but the gain is a better stability
in the solution.

<p>

</section>


<section>

<h2>Differential Equations, Runge-Kutta methods  <a name="___sec121"></a></h2>

The fourth-order Runge-Kutta, RK4, which we will employ in the solution
of various differential equations below, has the following
algorithm

<p>&nbsp;<br>
$$
\begin{equation}
  k_1=hf(t_i,y_i),
\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}
  k_2=hf(t_i+h/2,y_i+k_1/2),
\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}
  k_3=hf(t_i+h/2,y_i+k_2/2)
\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}
  k_4=hf(t_i+h,y_i+k_3)
\end{equation}
$$
<p>&nbsp;<br>

with the final value

<p>&nbsp;<br>
$$
\begin{equation}
  y_{i+1}=y_i +\frac{1}{6}\left( k_1 +2k_2+2k_3+k_4\right).
\end{equation}
$$
<p>&nbsp;<br>

Thus, the algorithm consists in first calculating \( k_1 \)
with \( t_i \), \( y_1 \) and \( f \) as inputs. Thereafter, we increase the step
size by \( h/2 \) and calculate \( k_2 \), then \( k_3 \) and finally \( k_4 \). Global error
as \( O(h^4) \).

<p>

</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec122"></a></h2>

Our first example is the classical case of simple harmonic
  oscillations, namely a block sliding on a horizontal frictionless
  surface. The block is tied to a wall with a spring.
  If the spring is not compressed or stretched too far, the force
  on the block at a given position \( x \) is

<p>&nbsp;<br>
$$
      F=-kx.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec123"></a></h2>

The negative sign means that the force acts to restore the object to an
  equilibrium position. Newton's equation of motion for this idealized system
  is then

<p>&nbsp;<br>
$$
    m\frac{d^2x}{dt^2}=-kx,
$$
<p>&nbsp;<br>

  or we could rephrase it as

<p>&nbsp;<br>
$$
   \frac{d^2x}{dt^2}=-\frac{k}{m}x=-\omega_0^2x,
    \tag{15}
$$
<p>&nbsp;<br>

  with the angular frequency \( \omega_0^2=k/m \).

<p>
  The above differential equation has the advantage that it can be solved
  analytically with solutions on the form

<p>&nbsp;<br>
$$
     x(t)=Acos(\omega_0t+\nu),
$$
<p>&nbsp;<br>

  where \( A \) is the amplitude and \( \nu \) the phase constant.
  This provides in turn an important test for the numerical
  solution and the development of a program for more complicated cases
  which cannot be solved analytically.

<p>

</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec124"></a></h2>

 With the position \( x(t) \) and the velocity
  \( v(t)=dx/dt \) we can reformulate Newton's equation in the following way

<p>&nbsp;<br>
$$
      \frac{dx(t)}{dt}=v(t),
$$
<p>&nbsp;<br>

  and

<p>&nbsp;<br>
$$
      \frac{dv(t)}{dt}=-\omega_0^2x(t).
$$
<p>&nbsp;<br>


<p>
  We are now going to solve these equations using the Runge-Kutta method
  to fourth order discussed previously.

<p>

</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec125"></a></h2>

 Before proceeding however, it is important to note that in addition
  to the exact solution, we have at least two further tests which can be
  used to check our solution.

<p>
  Since functions like \( cos \) are periodic with a period \( 2\pi \),
  then the solution
  \( x(t) \) has also to be periodic. This means that

<p>&nbsp;<br>
$$
     x(t+T)=x(t),
$$
<p>&nbsp;<br>

  with \( T \) the period defined as

<p>&nbsp;<br>
$$
     T=\frac{2\pi}{\omega_0}=\frac{2\pi}{\sqrt{k/m}}.
$$
<p>&nbsp;<br>


<p>
  Observe that \( T \) depends only on \( k/m \) and not on the amplitude
  of the solution.

<p>

</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec126"></a></h2>

  In addition to the periodicity test, the total energy has also to be
  conserved.

<p>
  Suppose we choose the initial conditions

<p>&nbsp;<br>
$$
     x(t=0)=1\quad \mbox{m}\quad v(t=0)=0\quad\mbox{m/s},
$$
<p>&nbsp;<br>

  meaning that block is at rest at \( t=0 \) but with a potential energy

<p>&nbsp;<br>
$$
    E_0=\frac{1}{2}kx(t=0)^2=\frac{1}{2}k.
$$
<p>&nbsp;<br>

  The total energy at any time \( t \) has however to be conserved, meaning
  that our solution has to fulfil the condition

<p>&nbsp;<br>
$$
    E_0=\frac{1}{2}kx(t)^2+\frac{1}{2}mv(t)^2.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec127"></a></h2>

 An algorithm which implements these equations is included below.

<ul>
  <p><li> Choose the initial position and speed, with the most common choice \( v(t=0)=0 \) and some fixed value for the position.</li>
  <p><li> Choose the method you wish to employ in solving the problem.</li>
  <p><li> Subdivide the time interval \( [t_i,t_f]  \) into a grid with step size \( h=\frac{t_f-t_i}{N} \), where \( N \) is the number of mesh points.</li>
  <p><li> Calculate now the total energy given by \( E_0=\frac{1}{2}kx(t=0)^2=\frac{1}{2}k \).</li>
  <p><li> The Runge-Kutta method is used to obtain \( x_{i+1} \) and \( v_{i+1} \) starting from the previous values \( x_i \) and \( v_i \)..</li>
  <p><li> When we have computed \( x(v)_{i+1} \) we upgrade \( t_{i+1}=t_i+h \).</li>
  <p><li> This iterative process continues till we reach the maximum time \( t_f \).</li>
  <p><li> The results are checked against the exact solution. Furthermore, one has to check the stability of the numerical solution against the chosen number of mesh points \( N \).</li>
</ul>
<p>


</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec128"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    y[<span style="color: #B452CD">0</span>] = initial_x;                 <span style="color: #228B22">// initial position</span>
    y[<span style="color: #B452CD">1</span>] = initial_v;                <span style="color: #228B22">// initial velocity</span>
    t=<span style="color: #B452CD">0.</span>;                             <span style="color: #228B22">// initial time</span>
    E0 = <span style="color: #B452CD">0.5</span>*y[<span style="color: #B452CD">0</span>]*y[<span style="color: #B452CD">0</span>]+<span style="color: #B452CD">0.5</span>*y[<span style="color: #B452CD">1</span>]*y[<span style="color: #B452CD">1</span>];  <span style="color: #228B22">// the initial total energy</span>
    <span style="color: #228B22">// now we start solving the differential</span>
    <span style="color: #228B22">// equations using the RK4 method</span>
    <span style="color: #8B008B; font-weight: bold">while</span> (t &lt;= tmax){
      derivatives(t, y, dydt);   <span style="color: #228B22">// initial derivatives</span>
      runge_kutta_4(y, dydt, n, t, h, yout, derivatives);
      <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++) {
	   y[i] = yout[i];

      t += h;
      output(t, y, E0);   <span style="color: #228B22">// write to file</span>
</pre></div>
<p>

</section>


<section>

<h2>Simple Example, Block tied to a Wall  <a name="___sec129"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">//   this function sets up the derivatives for this special case</span>
  <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">derivatives</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> t, <span style="color: #a7a7a7; font-weight: bold">double</span> *y, <span style="color: #a7a7a7; font-weight: bold">double</span> *dydt)
  {
    dydt[<span style="color: #B452CD">0</span>]=y[<span style="color: #B452CD">1</span>];    <span style="color: #228B22">// derivative of x</span>
    dydt[<span style="color: #B452CD">1</span>]=-y[<span style="color: #B452CD">0</span>]; <span style="color: #228B22">// derivative of v</span>
  } <span style="color: #228B22">// end of function derivatives</span>
</pre></div>
<p>

</section>


<section>

<h2>Runge-Kutta methods, code  <a name="___sec130"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">runge_kutta_4</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *y, <span style="color: #a7a7a7; font-weight: bold">double</span> *dydx, <span style="color: #a7a7a7; font-weight: bold">int</span> n,
                 <span style="color: #a7a7a7; font-weight: bold">double</span> x, <span style="color: #a7a7a7; font-weight: bold">double</span> h,
          <span style="color: #a7a7a7; font-weight: bold">double</span> *yout, <span style="color: #a7a7a7; font-weight: bold">void</span> (*derivs)(<span style="color: #a7a7a7; font-weight: bold">double</span>, <span style="color: #a7a7a7; font-weight: bold">double</span> *, <span style="color: #a7a7a7; font-weight: bold">double</span> *))
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> i;
  <span style="color: #a7a7a7; font-weight: bold">double</span>      xh,hh,h6;
  <span style="color: #a7a7a7; font-weight: bold">double</span> *dym, *dyt, *yt;
  <span style="color: #228B22">//   allocate space for local vectors</span>
  dym = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
  dyt =  <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
  yt =  <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
  hh = h*<span style="color: #B452CD">0.5</span>;
  h6 = h/<span style="color: #B452CD">6.</span>;
  xh = x+hh;
</pre></div>
<p>

</section>


<section>

<h2>Runge-Kutta methods, code  <a name="___sec131"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++) {
    yt[i] = y[i]+hh*dydx[i];

  (*derivs)(xh,yt,dyt);     <span style="color: #228B22">// computation of k2</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++) {
    yt[i] = y[i]+hh*dyt[i];

  (*derivs)(xh,yt,dym); <span style="color: #228B22">//  computation of k3</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i &lt; n; i++) {
    yt[i] = y[i]+h*dym[i];
    dym[i] += dyt[i];

  (*derivs)(x+h,yt,dyt);    <span style="color: #228B22">// computation of k4</span>
  <span style="color: #228B22">//      now we upgrade y in the array yout</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; n; i++){
    yout[i] = y[i]+h6*(dydx[i]+dyt[i]+<span style="color: #B452CD">2.0</span>*dym[i]);

  <span style="color: #8B008B; font-weight: bold">delete</span> []dym;
  <span style="color: #8B008B; font-weight: bold">delete</span> [] dyt;
  <span style="color: #8B008B; font-weight: bold">delete</span> [] yt;
}       <span style="color: #228B22">//  end of function Runge-kutta 4</span>
</pre></div>
<p>

</section>


<section>

<h2>The classical pendulum  <a name="___sec132"></a></h2>

The angular equation of motion of the pendulum is given by
Newton's equation and with no external force it reads

<p>&nbsp;<br>
$$
\begin{equation}
  ml\frac{d^2\theta}{dt^2}+mgsin(\theta)=0,
\end{equation}
$$
<p>&nbsp;<br>

with an angular velocity and acceleration given by

<p>&nbsp;<br>
$$
\begin{equation}
     v=l\frac{d\theta}{dt},
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
     a=l\frac{d^2\theta}{dt^2}.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>More on the Pendulum  <a name="___sec133"></a></h2>

We do however expect that the motion will gradually come to an end
due a viscous drag torque acting on the pendulum.
In the presence of the drag, the above equation becomes

<p>&nbsp;<br>
$$
\begin{equation}
   ml\frac{d^2\theta}{dt^2}+\nu\frac{d\theta}{dt}  +mgsin(\theta)=0,
\tag{16}
\end{equation}
$$
<p>&nbsp;<br>

where \( \nu \) is now a positive constant parameterizing the viscosity
of the medium in question. In order to maintain the motion against
viscosity, it is necessary to add some external driving force.
We choose here a periodic driving force. The last equation becomes then

<p>&nbsp;<br>
$$
\begin{equation}
   ml\frac{d^2\theta}{dt^2}+\nu\frac{d\theta}{dt}  +mgsin(\theta)=Asin(\omega t),
\tag{17}
\end{equation}
$$
<p>&nbsp;<br>

with \( A \) and \( \omega \) two constants representing the amplitude and
the angular frequency respectively. The latter is called the driving frequency.

<p>

</section>


<section>

<h2>More on the Pendulum  <a name="___sec134"></a></h2>

  We define

<p>&nbsp;<br>
$$
      \omega_0=\sqrt{g/l},
$$
<p>&nbsp;<br>

  the so-called natural frequency
  and the new dimensionless quantities

<p>&nbsp;<br>
$$
      \hat{t}=\omega_0t,
$$
<p>&nbsp;<br>

with the dimensionless driving frequency

<p>&nbsp;<br>
$$
     \hat{\omega}=\frac{\omega}{\omega_0},
$$
<p>&nbsp;<br>

  and introducing the quantity \( Q \), called the <em>quality factor</em>,

<p>&nbsp;<br>
$$
     Q=\frac{mg}{\omega_0\nu},
$$
<p>&nbsp;<br>

  and the dimensionless amplitude

<p>&nbsp;<br>
$$
     \hat{A}=\frac{A}{mg}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>More on the Pendulum  <a name="___sec135"></a></h2>

  we have

<p>&nbsp;<br>
$$
    \frac{d^2\theta}{d\hat{t}^2}+\frac{1}{Q}\frac{d\theta}{d\hat{t}}
     +sin(\theta)=\hat{A}cos(\hat{\omega}\hat{t}).
$$
<p>&nbsp;<br>


<p>
  This equation can in turn be recast in terms of two coupled first-order
  differential equations as follows

<p>&nbsp;<br>
$$
     \frac{d\theta}{d\hat{t}}=\hat{v},
$$
<p>&nbsp;<br>

  and

<p>&nbsp;<br>
$$
     \frac{d\hat{v}}{d\hat{t}}=-\frac{\hat{v}}{Q}-sin(\theta)+\hat{A}cos(\hat{\omega}\hat{t}).
$$
<p>&nbsp;<br>


<p>
  These are the equations to be solved.
  The factor \( Q \) represents the number of oscillations of the undriven system that must occur
  before its energy is significantly reduced due to the viscous  drag. The amplitude \( \hat{A} \)
  is measured in units of the maximum possible gravitational torque while
  \( \hat{\omega} \) is the angular frequency of the external torque measured in units of the pendulum's
  natural frequency.

<p>

</section>


<section>

<h2>Classes for ODE methods  <a name="___sec136"></a></h2>

It can be very useful to make a Class which contains all possible methods discussed. In Fortran we can use the MODULE
keyword in order to can methods and keep the variables private and hidden from other parts of our code.
This allows for a generalization which can be used to tackle other ODEs as well.

<p>

</section>


<section>

<h2>Classes for ODE methods  <a name="___sec137"></a></h2>

In <code>program2.cpp</code> of chapter 8 we have canned the following methods

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">euler</span>();
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">euler_cromer</span>();
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">midpoint</span>();
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">euler_richardson</span>();
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">half_step</span>();
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">rk2</span>(); <span style="color: #228B22">//runge-kutta-second-order</span>
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">rk4_step</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>,<span style="color: #a7a7a7; font-weight: bold">double</span>*,<span style="color: #a7a7a7; font-weight: bold">double</span>*,<span style="color: #a7a7a7; font-weight: bold">double</span>); <span style="color: #228B22">// we need it in function rk4() and asc()</span>
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">rk4</span>(); <span style="color: #228B22">//runge-kutta-fourth-order</span>
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">asc</span>(); <span style="color: #228B22">//runge-kutta-fourth-order with adaptive stepsize control</span>
</pre></div>
<p>

</section>


<section>

<h2>Classes for ODE methods  <a name="___sec138"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #8B008B; font-weight: bold">class</span> <span style="color: #008b45; font-weight: bold">pendulum</span>
 {
 private:
   <span style="color: #a7a7a7; font-weight: bold">double</span> Q, A_roof, omega_0, omega_roof,g; <span style="color: #228B22">//</span>
   <span style="color: #a7a7a7; font-weight: bold">double</span> y[<span style="color: #B452CD">2</span>];          <span style="color: #228B22">//for the initial-values of phi and v</span>
   <span style="color: #a7a7a7; font-weight: bold">int</span> n;                <span style="color: #228B22">// how many steps</span>
   <span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #a7a7a7; font-weight: bold">delta_t</span>,delta_t_roof;

 public:
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">derivatives</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>,<span style="color: #a7a7a7; font-weight: bold">double</span>*,<span style="color: #a7a7a7; font-weight: bold">double</span>*);
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">initialise</span>();
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">euler</span>();
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">euler_cromer</span>();
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">midpoint</span>();
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">euler_richardson</span>();
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">half_step</span>();
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">rk2</span>(); <span style="color: #228B22">//runge-kutta-second-order</span>
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">rk4_step</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>,<span style="color: #a7a7a7; font-weight: bold">double</span>*,<span style="color: #a7a7a7; font-weight: bold">double</span>*,<span style="color: #a7a7a7; font-weight: bold">double</span>); <span style="color: #228B22">// we need it in function rk4() and asc()</span>
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">rk4</span>(); <span style="color: #228B22">//runge-kutta-fourth-order</span>
   <span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">asc</span>(); <span style="color: #228B22">//runge-kutta-fourth-order with adaptive stepsize control</span>
 };
</pre></div>
<p>

</section>


<section>

<h2>Classes for ODE methods  <a name="___sec139"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">void</span> pendulum::derivatives(<span style="color: #a7a7a7; font-weight: bold">double</span> t, <span style="color: #a7a7a7; font-weight: bold">double</span>* in, <span style="color: #a7a7a7; font-weight: bold">double</span>* out)
{ <span style="color: #228B22">/* Here we are calculating the derivatives at (dimensionless) time t</span>
<span style="color: #228B22">     &#39;in&#39; are the values of phi and v, which are used for the calculation</span>
<span style="color: #228B22">     The results are given to &#39;out&#39; */</span>

  out[<span style="color: #B452CD">0</span>]=in[<span style="color: #B452CD">1</span>];             <span style="color: #228B22">//out[0] = (phi)&#39;  = v</span>
  <span style="color: #8B008B; font-weight: bold">if</span>(Q)
    out[<span style="color: #B452CD">1</span>]=-in[<span style="color: #B452CD">1</span>]/((<span style="color: #a7a7a7; font-weight: bold">double</span>)Q)-sin(in[<span style="color: #B452CD">0</span>])+A_roof*cos(omega_roof*t);  <span style="color: #228B22">//out[1] = (phi)&#39;&#39;</span>
  <span style="color: #8B008B; font-weight: bold">else</span>
    out[<span style="color: #B452CD">1</span>]=-sin(in[<span style="color: #B452CD">0</span>])+A_roof*cos(omega_roof*t);  <span style="color: #228B22">//out[1] = (phi)&#39;&#39;</span>
</pre></div>
<p>

</section>


<section>

<h2>Classes for ODE methods  <a name="___sec140"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>()
{
  pendulum testcase;
  testcase.initialise();
  testcase.euler();
  testcase.euler_cromer();
  testcase.midpoint();
  testcase.euler_richardson();
  testcase.half_step();
  testcase.rk2();
  testcase.rk4();
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  <span style="color: #228B22">// end of main function</span>
</pre></div>
<p>

</section>


<section>

<h2>Classes for ODE methods  <a name="___sec141"></a></h2>

In Fortran we would use

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">MODULE pendulum
   USE CONSTANTS
   IMPLICIT NONE
   REAL(DP), PRIVATE :: Q, A_roof, omega_0, omega_roof,g
   REAL(DP), PRIVATE :: y(<span style="color: #B452CD">2</span>)         ! <span style="color: #8B008B; font-weight: bold">for</span> the initial-values of phi and v
   INTEGER, PRIVATE ::  n               ! how many steps
   REAL(DP), PRIVATE :: <span style="color: #a7a7a7; font-weight: bold">delta_t</span>,delta_t_roof

   CONTAINS
    SUBROUTINE derivatives(..)
    SUBROUTINE initialise(..)
    SUBROUTINE euler(..)
    SUBROUTINE euler_cromer(..)
    SUBROUTINE midpoint(..)
    etc

END MODULE pendulum
</pre></div>
<p>

</section>


<section>

<h2>The report: how to write a good scienfitic/technical report  <a name="___sec142"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should it contain? A typical structure.</b>
<p>

<ul>
  <p><li> An introduction where you explain the aims and rationale for the physics case and what you have done. At the end of the introduction you should give a brief summary of the structure of the report</li>
  <p><li> Theoretical models and technicalities. This is the methods section.</li>
  <p><li> Results and discussion</li>
  <p><li> Conclusions and perspectives</li>
  <p><li> Appendix with extra material</li>
  <p><li> Bibliography Keep always a good log of what you do.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec143"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Introduction.</b>
<p>
You don't need to answer all questions in a chronological order.  When you write the introduction you could focus on the following aspects

<ul>
  <p><li> Motivate the reader, the first part of the introduction gives always a motivation and tries to give the overarching ideas</li>
  <p><li> What I have done</li>
  <p><li> The structure of the report, how it is organized etc</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec144"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Methods sections.</b>
<p>

<ul>
  <p><li> Describe the methods and algorithms</li>
  <p><li> You need to explain how you implemented the methods and also say something about the structure of your algorithm and present some parts of your code</li>
  <p><li> You should plug in some calculations to demonstrate your code, such as selected runs used to validate and verify your results. The latter is extremely important!! A reader needs to understand that your code reproduces selected benchmarks and reproduces previous results, either numerical and/or well-known closed form expressions.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec145"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Results.</b>
<p>

<ul>
  <p><li> Present your results</li>
  <p><li> Give a critical discussion of your work and place it in the correct context.</li>
  <p><li> Relate your work to other calculations/studies</li>
  <p><li> An eventual reader should be able to reproduce your calculations if she/he wants to do so. All input variables should be properly explained.</li>
  <p><li> Make sure that figures and tables should contain enough information in their captions, axis labels etc so that an eventual reader can gain a first impression of your work by studying figures and tables only.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec146"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Conclusions.</b>
<p>

<ul>
  <p><li> State your main findings and interpretations</li>
  <p><li> Try as far as possible to present perspectives for future work</li>
  <p><li> Try to discuss the pros and cons of the methods and possible improvements</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec147"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? additional material.</b>
<p>

<ul>
  <p><li> Additional calculations used to validate the codes</li>
  <p><li> Selected calculations, these can be listed with few comments</li>
  <p><li> Listing of the code if you feel this is necessary You can consider moving parts of the material from the methods section to the appendix. You can also place additional material on your webpage.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec148"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? References.</b>
<p>

<ul>
  <p><li> Give always references to material you base your work on, either scientific articles/reports or books.</li>
  <p><li> <em>Wikipedia is not accepted as a scientific reference</em>. Under no circumstances.</li>
  <p><li> Refer to articles as: name(s) of author(s), journal, volume (boldfaced), page and year in parenthesis.</li>
  <p><li> Refer to books as: name(s) of author(s), title of book, publisher, place and year, eventual page numbers</li>
</ul>
</div>


<p>

</section>


<section>

<h1>Overview of week 41  <a name="___sec149"></a></h1>


</section>


<section>

<h2>Overview of week 41  <a name="___sec150"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Ordinary differential equations (ODEs) and Partial differential equations (PDEs).</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Adaptive Runge-Kutta methods and stiff equations</li>
  <p><li> Examples with codes</li>
  <p><li> Discussion of project 3</li>
  <p><li> Begin partial differential equations, discussion of the diffusion equation</li>
  <p><li> Tuesday:</li>
  <p><li> Diffusion equation in one spatial dimension, implicit and explicit scheme and the Crank-Nicolson scheme Chapter 9, ODEs with boundary conditions will not be discussed.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Student project 3: An object oriented example program for project 3  <a name="___sec151"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>See <a href="http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem" target="_blank"><tt>http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem</tt></a>.</b>
<p>

<ul>
  <p><li> <a href="http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/solarsystem.cpp" target="_blank"><tt>http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/solarsystem.cpp</tt></a></li>
  <p><li> <a href="http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/planet.cpp" target="_blank"><tt>http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/planet.cpp</tt></a></li>
  <p><li> <a href="http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/planet.h" target="_blank"><tt>http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/planet.h</tt></a></li>
  <p><li> <a href="http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/constants.cpp" target="_blank"><tt>http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/constants.cpp</tt></a></li>
  <p><li> <a href="http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/constants.h" target="_blank"><tt>http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/constants.h</tt></a></li>
</ul>
</div>


<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec152"></a></h2>

In case the function to integrate varies slowly or fast in different integration domains, adaptive methods are normally used. One strategy is always to decrease the step size. As we have seen earlier, this leads to more CPU cycles and may lead to loss or numerical precision. An alternative is to use higher-order RK methods for example. However, this leads again to more cycles, furthermore, there is no guarantee that higher-order leads to an improved error.

<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec153"></a></h2>

Assume the exact result is \( \tilde{x} \) and that we are using an RKM method. Suppose we run two calculations, one with \( h \) (called \( x_1 \)) and one with \( h/2 \) (called \( x_2 \)). Then

<p>&nbsp;<br>
$$
\tilde{x}=x_1+Ch^{M+1}+O(h^{M+2}),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\tilde{x}=x_2+2C(h/2)^{M+1}+O(h^{M+2}),
$$
<p>&nbsp;<br>

with \( C \) a constant. Note that we calculate two halves in the last equation. We get then

<p>&nbsp;<br>
$$
|x_1-x_2| = Ch^{M+1}(1-\frac{1}{2^M}).
$$
<p>&nbsp;<br>

yielding

<p>&nbsp;<br>
$$
C=\frac{|x_1-x_2|}{(1-2^{-M})h^{M+1}}.
$$
<p>&nbsp;<br>

We rewrite

<p>&nbsp;<br>
$$
\tilde{x}=x_2+\epsilon+O((h)^{M+2}),
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
\epsilon = \frac{|x_1-x_2|}{2^M-1}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec154"></a></h2>

With RK4 the expressions become

<p>&nbsp;<br>
$$
\tilde{x}=x_2+\epsilon+O((h)^{6}),
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
\epsilon = \frac{|x_1-x_2|}{15}.
$$
<p>&nbsp;<br>

The estimate is one order higher than the original RK4. But this method is normally rather inefficient since it requires a lot of computations. We solve typically the equation three times at each time step.
However, we can compare the estimate \( \epsilon \) with some by us given accuracy \( \xi \).
We can then ask the question: what is, with a given \( x_j \) and \( t_j \), the largest possible step size \( \tilde{h} \) that leads to a truncation error below \( \xi \)?
We want

<p>&nbsp;<br>
$$
C\tilde{h} \le \xi,
$$
<p>&nbsp;<br>

which leads to

<p>&nbsp;<br>
$$
\left(\frac{\tilde{h}}{h}\right)^{M+1}\frac{|x_1-x_2|}{(1-2^{-M})}\le \xi,
$$
<p>&nbsp;<br>

meaning that

<p>&nbsp;<br>
$$
\tilde{h}=h\left(\frac{\xi}{\epsilon}\right)^{1+1/M}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec155"></a></h2>

With

<p>&nbsp;<br>
$$
\tilde{h}=h\left(\frac{\xi}{\epsilon}\right)^{1+1/M}.
$$
<p>&nbsp;<br>

we can design the following algorithm:

<ul>
  <p><li> If the two answers are close, keep the approximation to \( h \).</li>
  <p><li> If \( \epsilon > \xi \) we need to decrease the step size in the next time step.</li>
  <p><li> If \( \epsilon < \xi \) we need to increase the step size in the next time step. A much used algorithm is the so-called RKF45 which uses a combination of a fourth and fifth order RK methods.</li>
</ul>
<p>


</section>


<section>

<h2>Adaptive methods, RKF45  <a name="___sec156"></a></h2>

At
each step, two different approximations for the solution are made and compared. If the
two answers are in close agreement, the approximation is accepted. If the two answers
do not agree to a specified accuracy, the step size is reduced. If the answers agree to
more significant digits than required, the step size is increased.
Each step requires the use of the following six values:

<p>&nbsp;<br>
$$
k_1 = h f (t_k , y_k ),
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
k_2 = h f (t_k + \frac{1}{4}h, y_k + \frac{1}{4}k_1) ,
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
k_3 = h f (t_k + \frac{3}{8}h, y_k + \frac{3}{32}k_1 + \frac{9}{32}k_2) ,
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
k_4 = h f (t_k + \frac{12}{13}h, y_k + \frac{1932}{2197}k_1 + \frac{7200}{2197}k_2+\frac{7296}{2197}k_3),
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
k_5 = h f (t_k + h, y_k + \frac{439}{216}k_1 -8k_2+ \frac{3680}{513}k_3+\frac{845}{4104}k_4),
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
k_6 = h f (t_k + \frac{1}{2}h, y_k - \frac{8}{27}k_1 + 2k_2-\frac{3544}{2565}k_2+\frac{1859}{4104}k_4-+\frac{11}{40}k_5).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Adaptive methods, RKF45  <a name="___sec157"></a></h2>

Then an approximation to the solution of the ODE is made using a Runge-Kutta
method of order 4:

<p>&nbsp;<br>
$$
y_{k+1} = y_k  + \frac{25}{216}k_1+\frac{1408}{2565}k_3 +\frac{2197}{4101}k_4-\frac{1}{5}k_5,
$$
<p>&nbsp;<br>

where the four function values \( k_1 \) , \( k_3 \) , \( k_4 \) , and \( k_5 \) are used. Notice that \( k_2 \) is not used  here.
A better value for the solution is determined using a Runge-Kutta
method of order 5:

<p>&nbsp;<br>
$$
z_{k+1} = y_k + \frac{16}{135}k_1+\frac{6656}{12825}k_3 +\frac{28561}{56430}k_4-\frac{9}{50}k_5+\frac{2}{55}k_6.
$$
<p>&nbsp;<br>


<p>
The optimal time step \( \alpha h \) is then determined by

<p>&nbsp;<br>
$$
\alpha = \left( \frac{\xi h}{2|z_{k+1}-y_{k+1}|}\right)^{1/4},
$$
<p>&nbsp;<br>

with \( \xi \) our defined tolerance.

<p>

</section>


<section>

<h2>Partial Differential Equations, chapter 10  <a name="___sec158"></a></h2>

General 2+1-dim PDE

<p>&nbsp;<br>
$$
A(x,y)\frac{\partial^2 U}{\partial x^2}+B(x,y)\frac{\partial^2 U}{\partial x\partial y}
+C(x,y)\frac{\partial^2 U}{\partial y^2}=F(x,y,U,\frac{\partial U}{\partial x}, \frac{\partial U}{\partial y})
$$
<p>&nbsp;<br>

Examples

<p>&nbsp;<br>
$$
  B=C=0,
$$
<p>&nbsp;<br>

give e.g., 1+1-dim diffusion equation

<p>&nbsp;<br>
$$
 A\frac{\partial^2 U}{\partial x^2}=\frac{\partial U}{\partial t}
$$
<p>&nbsp;<br>

and is an example of a parabolic PDE

<p>

</section>


<section>

<h2>Partial Differential Equations  <a name="___sec159"></a></h2>

More examples
2+1-dim wave equation

<p>&nbsp;<br>
$$
 A\frac{\partial^2 U}{\partial x^2}+C\frac{\partial^2 U}{\partial y^2}=\frac{\partial^2 U}{\partial t^2}
$$
<p>&nbsp;<br>

Poisson's (Laplace's \( \rho =0 \))  equation

<p>&nbsp;<br>
$$
 \nabla^2 U({\bf x})=-4\pi \rho({\bf x}).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Heat/Diffusion Equation  <a name="___sec160"></a></h2>

Diffusion equation

<p>&nbsp;<br>
$$
 \frac{\kappa}{C\rho}\nabla^2 T({\bf x},t) =\frac{\partial T({\bf x},t)}{\partial t}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
 \frac{\kappa}{C\rho({\bf x},t)}\nabla^2 T({\bf x},t) =\frac{\partial T({\bf x},t)}{\partial t}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Explicit Scheme for the Diffusion Equation  <a name="___sec161"></a></h2>

In one dimension we have thus the following equation
<p>&nbsp;<br>
$$
 \nabla^2 u(x,t) =\frac{\partial u(x,t)}{\partial t},
$$
<p>&nbsp;<br>

or
<p>&nbsp;<br>
$$
u_{xx} = u_t,
$$
<p>&nbsp;<br>

with initial conditions, i.e., the conditions at \( t=0 \),
<p>&nbsp;<br>
$$
u(x,0)= g(x) \quad 0 \le x \le L
$$
<p>&nbsp;<br>

with \( L=1 \) the length of the \( x \)-region of interest. The
boundary conditions are
<p>&nbsp;<br>
$$
u(0,t)= a(t) \quad t \ge 0,
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
u(L,t)= b(t) \quad t \ge 0,
$$
<p>&nbsp;<br>

where \( a(t) \) and \( b(t) \) are two functions which depend on time only, while
\( g(x) \) depends only on the position \( x \).

<p>

</section>


<section>

<h2>Explicit Scheme, Forward Euler  <a name="___sec162"></a></h2>

<p>&nbsp;<br>
$$
u_t\approx \frac{u_{i,j+1}-u_{i,j}}{\Delta t},
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
u_{xx}\approx \frac{u_{i+i,j}-2u_{i,j}+u_{i-1,j}}{\Delta x^2}.
$$
<p>&nbsp;<br>

The one-dimensional diffusion equation can then be rewritten in its
discretized version as
<p>&nbsp;<br>
$$
\frac{u_{i,j+1}-u_{i,j}}{\Delta t}=\frac{u_{i+i,j}-2u_{i,j}+u_{i-1,j}}{\Delta x^2}.
$$
<p>&nbsp;<br>

Defining \( \alpha = \Delta t/\Delta x^2 \) results in the explicit scheme
<p>&nbsp;<br>
$$
\tag{18}
 u_{i,j+1}= \alpha u_{i-1,j}+(1-2\alpha)u_{i,j}+\alpha u_{i+1,j}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Explicit Scheme  <a name="___sec163"></a></h2>

<p>&nbsp;<br>
$$
   V_{j+1} = AV_{j}
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
 A=\left(\begin{array}{cccc}1-2\alpha&\alpha&0& 0\dots\\
                            \alpha&1-2\alpha&\alpha & 0\dots \\
                            \dots & \dots & \dots & \dots \\
                      0\dots & 0\dots &\alpha& 1-2\alpha\end{array}
\right)
$$
<p>&nbsp;<br>

yielding

<p>&nbsp;<br>
$$
   V_{j+1} = AV_{j}=\dots = A^jV_0
$$
<p>&nbsp;<br>

The explicit scheme, although being rather simple to implement has a very weak
stability condition given by
<p>&nbsp;<br>
$$
  \Delta t/\Delta x^2 \le 1/2
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Implicit Scheme  <a name="___sec164"></a></h2>

Choose now

<p>&nbsp;<br>
$$
u_t\approx \frac{u(x_i,t_j)-u(x_i,t_j-k)}{k}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
u_{xx}\approx \frac{u(x_i+h,t_j)-2u(x_i,t_j)+u(x_i-h,t_j)}{h^2}
$$
<p>&nbsp;<br>

Define \( \alpha = k/h^2 \). Gives

<p>&nbsp;<br>
$$
 u_{i,j-1}= -\alpha u_{i-1,j}+(1-2\alpha)u_{i,j}-\alpha u_{i+1,j}
$$
<p>&nbsp;<br>

Here \( u_{i,j-1} \) is the only unknown quantity.

<p>
Have

<p>&nbsp;<br>
$$
   AV_{j} = V_{j-1}
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
 A=\left(\begin{array}{cccc}1+2\alpha&-\alpha&0& 0\dots\\
                            -\alpha&1+2\alpha&-\alpha & 0\dots \\
                            \dots & \dots & \dots & \dots \\
                      0\dots & 0\dots &-\alpha& 1+2\alpha\end{array}
\right)
$$
<p>&nbsp;<br>

which gives

<p>&nbsp;<br>
$$
   V_{j} = A^{-1}V_{j-1}=\dots = A^{-j}V_0
$$
<p>&nbsp;<br>

Need only to invert a matrix

<p>

</section>


<section>

<h2>Brute Force Implicit Scheme, inefficient algo  <a name="___sec165"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">!  now invert the matrix
       CALL matinv( a, ndim, det)
       DO i = <span style="color: #B452CD">1</span>, m
          DO l=<span style="color: #B452CD">1</span>, ndim
             u(l) = DOT_PRODUCT(a(l,:),v(:))
          ENDDO
          v = u
          t = i*k
          DO  j=<span style="color: #B452CD">1</span>, ndim
              WRITE(<span style="color: #B452CD">6</span>,*) t, j*h, v(j)
          ENDDO
       ENDDO
</pre></div>
<p>

</section>


<section>

<h2>Brief Summary of the  Explicit and the Implicit Methods  <a name="___sec166"></a></h2>

Explicit is straightforward to code, but avoid doing the matrix vector
multiplication since the matrix is tridiagonal.

<p>&nbsp;<br>
$$
u_t\approx \frac{u(x,t+\Delta t)-u(x,t)}{\Delta t}=\frac{u(x_i,t_j+\Delta t)-u(x_i,t_j)}{\Delta t}
$$
<p>&nbsp;<br>


<p>
The implicit method can be applied in a brute force way as well as long as the
element of the matrix are constants.

<p>&nbsp;<br>
$$
u_t\approx \frac{u(x,t)-u(x,t-\Delta t)}{\Delta t}=\frac{u(x_i,t_j)-u(x_i,t_j-\Delta t)}{\Delta t}
$$
<p>&nbsp;<br>


<p>
However, it is more efficient to use a linear algebra solver for tridiagonal matrices.

<p>

</section>


<section>

<h2>Crank-Nicolson  <a name="___sec167"></a></h2>

<p>&nbsp;<br>
$$
  \frac{\theta}{\Delta x^2}\left(u_{i-1,j}-2u_{i,j}+u_{i+1,j}\right)+
  \frac{1-\theta}{\Delta x^2}\left(u_{i+1,j-1}-2u_{i,j-1}+u_{i-1,j-1}\right)=
  \frac{1}{\Delta t}\left(u_{i,j}-u_{i,j-1}\right),
$$
<p>&nbsp;<br>

which for \( \theta=0 \) yields the forward formula for the first derivative and
the explicit scheme, while \( \theta=1 \) yields the backward formula and the implicit
scheme. These two schemes are called the backward and forward Euler schemes, respectively.
For \( \theta = 1/2 \) we obtain a new scheme after its inventors, Crank and Nicolson.

<p>

</section>


<section>

<h2>Crank Nicolson  <a name="___sec168"></a></h2>

Using our previous definition of \( \alpha=\Delta t/\Delta x^2 \) we can rewrite the latter
equation as

<p>&nbsp;<br>
$$
  -\alpha u_{i-1,j}+\left(2+2\alpha\right)u_{i,j}-\alpha u_{i+1,j}=
  \alpha u_{i-1,j-1}+\left(2-2\alpha\right)u_{i,j-1}+\alpha u_{i+1,j-1},
$$
<p>&nbsp;<br>

or in matrix-vector form as

<p>&nbsp;<br>
$$
  \left(2\hat{I}+\alpha\hat{B}\right)V_{j}=
  \left(2\hat{I}-\alpha\hat{B}\right)V_{j-1},
$$
<p>&nbsp;<br>

 where the vector \( V_{j} \) is the same as defined in the implicit case while the matrix
\( \hat{B} \) is

<p>&nbsp;<br>
$$
 \hat{B}=\left(\begin{array}{cccc}2&-1&0& 0\dots\\
                            -1&2&-1 & 0\dots \\
                            \dots & \dots & \dots & \dots \\
                      0\dots & 0\dots && 2\end{array}
\right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec169"></a></h2>

We start with the forward Euler scheme and Taylor expand \( u(x,t+\Delta t) \),
\( u(x+\Delta x, t) \) and \( u(x-\Delta x,t) \)

<p>&nbsp;<br>
$$
\begin{align}
u(x+\Delta x,t)&=u(x,t)+\frac{\partial u(x,t)}{\partial x} \Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2}\Delta x^2+\mathcal{O}(\Delta x^3), \nonumber\\
u(x-\Delta x,t)&=u(x,t)-\frac{\partial u(x,t)}{\partial x}\Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2} \Delta x^2+\mathcal{O}(\Delta x^3), \nonumber\\
u(x,t+\Delta t)&=u(x,t)+\frac{\partial u(x,t)}{\partial t}\Delta t+  \mathcal{O}(\Delta t^2).
\tag{19}
\end{align}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec170"></a></h2>

With these Taylor expansions the approximations for the derivatives takes the form

<p>&nbsp;<br>
$$
\begin{align}
&\left[\frac{\partial u(x,t)}{\partial t}\right]_{\text{approx}}
 =\frac{\partial u(x,t)}{\partial t}+\mathcal{O}(\Delta t) , \nonumber\\
&\left[\frac{\partial^2 u(x,t)}{\partial x^2}\right]_{\text{approx}}
 =\frac{\partial^2 u(x,t)}{\partial x^2}+\mathcal{O}(\Delta x^2).
\tag{20}
\end{align}
$$
<p>&nbsp;<br>

It is easy to convince oneself that the backward Euler method must have the same truncation errors as the forward Euler scheme.

<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec171"></a></h2>

For the Crank-Nicolson scheme we also need to Taylor expand \( u(x+\Delta x, t+\Delta t) \) and \( u(x-\Delta x, t+\Delta t) \) around \( t'=t+\Delta t/2 \).

<p>&nbsp;<br>
$$
\begin{align}
u(x+\Delta x, t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\notag \\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\ \nonumber
u(x-\Delta x, t+\Delta t)&=u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} - \notag\\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\
u(x+\Delta x,t)&=u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} -\notag \\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\  \nonumber
u(x-\Delta x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\notag \\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\  \nonumber
u(x,t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial t}\frac{\Delta_t}{2} +\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 + \mathcal{O}(\Delta t^3)  \nonumber\\
u(x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2}+\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 + \mathcal{O}(\Delta t^3)
\tag{21}
\end{align}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec172"></a></h2>

We now insert these expansions in the approximations for the derivatives to find

<p>&nbsp;<br>
$$
\begin{align}
&\left[\frac{\partial u(x,t')}{\partial t}\right]_{\text{approx}} =\frac{\partial u(x,t')}{\partial t}+\mathcal{O}(\Delta t^2) , \\ \nonumber
&\left[\frac{\partial^2 u(x,t')}{\partial x^2}\right]_{\text{approx}}=\frac{\partial^2 u(x,t')}{\partial x^2}+\mathcal{O}(\Delta x^2).
\end{align}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec173"></a></h2>

The following table summarizes the three methods.

<p>
<table border="1">
<thead>
<tr><th align="center">                <em>Scheme:</em>                </th> <th align="center">           <em>Truncation Error:</em>           </th> <th align="center">        <em>Stability requirements:</em>        </th> </tr>
</thead>
<tbody>
<tr><td align="left">   Crank-Nicolson                                      </td> <td align="left">   \( \mathcal{O}(\Delta x^2,\Delta t^2) \)            </td> <td align="left">   Stable for all \( \Delta t \) and \( \Delta x \)    </td> </tr>
<tr><td align="left">   Backward Euler                                      </td> <td align="left">   \( \mathcal{O}(\Delta x^2,\Delta t) \)              </td> <td align="left">   Stable for all \( \Delta t \) and \( \Delta x \)    </td> </tr>
<tr><td align="left">   Forward Euler                                       </td> <td align="left">   \( \mathcal{O}(\Delta x^2,\Delta t) \)              </td> <td align="left">   \( \Delta t\leq \frac{1}{2}\Delta x^2 \)            </td> </tr>
</tbody>
</table>
<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec174"></a></h2>

It cannot be repeated enough, it is always useful to find cases where one can compare the numerics
and the developed algorithms and codes with analytic solution.  The above case is also particularly simple.
We have the following partial differential equation

<p>&nbsp;<br>
$$
 \nabla^2 u(x,t) =\frac{\partial u(x,t)}{\partial t},
$$
<p>&nbsp;<br>

with initial conditions

<p>&nbsp;<br>
$$
u(x,0)= g(x) \quad 0 < x < L.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec175"></a></h2>

The
boundary conditions are

<p>&nbsp;<br>
$$
u(0,t)= 0 \quad t \ge 0,  \quad  u(L,t)= 0 \quad t \ge 0,
$$
<p>&nbsp;<br>


<p>
 We assume that we have solutions of the form (separation of variable)

<p>&nbsp;<br>
$$
\begin{equation}
u(x,t)=F(x)G(t),
\end{equation}
$$
<p>&nbsp;<br>

which inserted in the partial differential equation results in

<p>&nbsp;<br>
$$
\begin{equation}
\frac{F''}{F}=\frac{G'}{G},
\end{equation}
$$
<p>&nbsp;<br>

where the derivative is with respect to \( x \) on the left hand side and with respect to \( t \) on right hand side.
This equation  should hold for all \( x \) and \( t \). We must require the rhs and lhs to be equal to a constant.

<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec176"></a></h2>

We call this constant \( -\lambda^2 \). This gives us the two differential equations,

<p>&nbsp;<br>
$$
\begin{equation}
F''+\lambda^2F=0;  \quad G'=-\lambda^2G,
\end{equation}
$$
<p>&nbsp;<br>

with general solutions

<p>&nbsp;<br>
$$
\begin{equation}
F(x)=A\sin(\lambda x)+B\cos(\lambda x); \quad G(t)=Ce^{-\lambda^2t}.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec177"></a></h2>

To satisfy the boundary conditions we require \( B=0 \) and \( \lambda=n\pi/L \). One solution is therefore found to be

<p>&nbsp;<br>
$$
\begin{equation}
u(x,t)=A_n\sin(n\pi x/L)e^{-n^2\pi^2 t/L^2}.
\end{equation}
$$
<p>&nbsp;<br>

But there are infinitely many  possible \( n \) values (infinite number of solutions). Moreover,
the diffusion equation is linear and because of this we know that a superposition of solutions
will also be a solution of the equation. We may therefore write

<p>&nbsp;<br>
$$
\begin{equation}
u(x,t)=\sum_{n=1}^{\infty} A_n \sin(n\pi x/L) e^{-n^2\pi^2 t/L^2}.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analysis of diffusion equation  <a name="___sec178"></a></h2>

The coefficient \( A_n \) is in turn determined from the initial condition. We require

<p>&nbsp;<br>
$$
\begin{equation}
u(x,0)=g(x)=\sum_{n=1}^{\infty} A_n \sin(n\pi x/L).
\end{equation}
$$
<p>&nbsp;<br>

The coefficient \( A_n \) is the Fourier coefficients for the function \( g(x) \). Because of this, \( A_n \) is given by (from the theory on Fourier series)

<p>&nbsp;<br>
$$
\begin{equation}
A_n=\frac{2}{L}\int_0^L g(x)\sin(n\pi x/L) \mbox{d}x.
\end{equation}
$$
<p>&nbsp;<br>

Different \( g(x) \) functions will obviously result in different results for \( A_n \).

<p>

</section>


<section>

<h1>Overview of week 42  <a name="___sec179"></a></h1>


</section>


<section>

<h2>Overview of week 42  <a name="___sec180"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Partial differential equations (chapter 10) and begin numerical integration (chapter 5).</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Discussion of project 3, with an emphasis on object orientation</li>
  <p><li> Diffusion equation in two spatial dimensions</li>
  <p><li> Poisson's and Laplace's equations</li>
  <p><li> Tuesday:</li>
  <p><li> Wave equation in one and two dimensions.</li>
  <p><li> If we get time, we will start with numerical integration</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Laplace's and Poisson's equations  <a name="___sec181"></a></h2>

Laplace's equation reads
<p>&nbsp;<br>
$$
 \nabla^2 u({\bf x})=u_{xx}+u_{yy}=0.
$$
<p>&nbsp;<br>

with possible boundary conditions
\( u(x,y) = g(x,y)  \) on the border. There is no time-dependence.
Choosing equally many steps in both directions we have a quadratic or rectangular
grid, depending on whether we choose equal steps lengths or not in the \( x \) and
the \( y \) directions. Here we set \( \Delta x = \Delta y = h \) and obtain
a discretized version
<p>&nbsp;<br>
$$
u_{xx}\approx \frac{u(x+h,y)-2u(x,y)+u(x-h,y)}{h^2},
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
u_{yy}\approx \frac{u(x,y+h)-2u(x,y)+u(x,y-h)}{h^2},
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Laplace's and Poisson's equations  <a name="___sec182"></a></h2>

<p>&nbsp;<br>
$$
u_{xx}\approx \frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2},
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
u_{yy}\approx \frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2},
$$
<p>&nbsp;<br>

which gives when inserted in Laplace's equation
<p>&nbsp;<br>
$$
\begin{equation}
\tag{22}
  u_{i,j}= \frac{1}{4}\left[u_{i,j+1}+u_{i,j-1}+u_{i+1,j}+u_{i-1,j}\right].
\end{equation}
$$
<p>&nbsp;<br>

This is our final numerical scheme for solving Laplace's equation.
Poisson's equation adds only a minor complication
to the above equation since in this case we have

<p>&nbsp;<br>
$$
    u_{xx}+u_{yy}=-\rho({\bf x}),
$$
<p>&nbsp;<br>

and we need only to add a discretized version of \( \rho({\bf x}) \)
resulting in
<p>&nbsp;<br>
$$
\begin{equation}
\tag{23}
  u_{i,j}= \frac{1}{4}\left[u_{i,j+1}+u_{i,j-1}+u_{i+1,j}+u_{i-1,j}\right]
           +\rho_{i,j}.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Solution Approach  <a name="___sec183"></a></h2>

The way we solve these equations is based on an iterative scheme we discussed in connection
with linear algebra, namely the so-called Jacobi, Gauss-Seidel and
relaxation methods. The steps are rather simple. We start with an initial guess
for \( u_{i,j}^{(0)} \) where all values are known. To obtain a new solution we
solve Eq. <a href="#mjx-eqn-22">(22)</a> or Eq. <a href="#mjx-eqn-23">(23)</a>
in order to obtain a new solution \( u_{i,j}^{(1)} \).
Most likely this solution will not be a solution to
Eq. <a href="#mjx-eqn-22">(22)</a>. This solution is in turn
used to obtain a new and improved \( u_{i,j}^{(2)} \). We continue this process
till we obtain a result which satisfies some specific convergence criterion.

<p>

</section>


<section>

<h2>Code example for the two-dimensional diff equation/Laplace  <a name="___sec184"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">DiffusionJacobi</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> N, <span style="color: #a7a7a7; font-weight: bold">double</span> dx, <span style="color: #a7a7a7; font-weight: bold">double</span> dt,
		      <span style="color: #a7a7a7; font-weight: bold">double</span> **A, <span style="color: #a7a7a7; font-weight: bold">double</span> **q, <span style="color: #a7a7a7; font-weight: bold">double</span> abstol){
  <span style="color: #a7a7a7; font-weight: bold">int</span> i,j,k;
  <span style="color: #a7a7a7; font-weight: bold">int</span> maxit = <span style="color: #B452CD">100000</span>;
  <span style="color: #a7a7a7; font-weight: bold">double</span> sum;
  <span style="color: #a7a7a7; font-weight: bold">double</span> ** Aold = CreateMatrix(N,N);

  <span style="color: #a7a7a7; font-weight: bold">double</span> D = dt/(dx*dx);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the two-dimensional diff equation/Laplace  <a name="___sec185"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">1</span>; i&lt;N-<span style="color: #B452CD">1</span>; i++)
    <span style="color: #8B008B; font-weight: bold">for</span>(j=<span style="color: #B452CD">1</span>;j&lt;N-<span style="color: #B452CD">1</span>;j++)
      Aold[i][j] = <span style="color: #B452CD">1.0</span>;
  <span style="color: #228B22">/* Boundary Conditions -- all zeros */</span>
  <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">0</span>;i&lt;N;i++){
    A[<span style="color: #B452CD">0</span>][i] = <span style="color: #B452CD">0.0</span>;
    A[N-<span style="color: #B452CD">1</span>][i] = <span style="color: #B452CD">0.0</span>;
    A[i][<span style="color: #B452CD">0</span>] = <span style="color: #B452CD">0.0</span>;
    A[i][N-<span style="color: #B452CD">1</span>] = <span style="color: #B452CD">0.0</span>;
</pre></div>
<p>

</section>


<section>

<h2>Code example for the two-dimensional diff equation/Laplace  <a name="___sec186"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">for</span>(k=<span style="color: #B452CD">0</span>; k&lt;maxit; k++){
    <span style="color: #8B008B; font-weight: bold">for</span>(i = <span style="color: #B452CD">1</span>; i&lt;N-<span style="color: #B452CD">1</span>; i++){
      <span style="color: #8B008B; font-weight: bold">for</span>(j=<span style="color: #B452CD">1</span>; j&lt;N-<span style="color: #B452CD">1</span>; j++){
	A[i][j] = dt*q[i][j] + Aold[i][j] +
	  D*(Aold[i+<span style="color: #B452CD">1</span>][j] + Aold[i][j+<span style="color: #B452CD">1</span>] - <span style="color: #B452CD">4.0</span>*Aold[i][j] +
	     Aold[i-<span style="color: #B452CD">1</span>][j] + Aold[i][j-<span style="color: #B452CD">1</span>]);

    sum = <span style="color: #B452CD">0.0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">0</span>;i&lt;N;i++){
      <span style="color: #8B008B; font-weight: bold">for</span>(j=<span style="color: #B452CD">0</span>;j&lt;N;j++){
	sum += (Aold[i][j]-A[i][j])*(Aold[i][j]-A[i][j]);
	Aold[i][j] = A[i][j];

    <span style="color: #8B008B; font-weight: bold">if</span>(sqrt(sum)&lt;abstol){DestroyMatrix(Aold,N,N);
      <span style="color: #8B008B; font-weight: bold">return</span> k;
</pre></div>
<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec187"></a></h2>

Consider first the two-dimensional wave equation for a vibrating square membrane given by the
following initial and boundary conditions

<p>&nbsp;<br>
$$
\left\{\begin{array}{cc} \lambda\left(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}\right) = \frac{\partial^2u}{\partial t^2}& x,y\in[0,1], t \ge 0 \\
                         u(x,y,0) = sin(\pi x)sin(2\pi y)& x,y\in (0,1) \\
                         u = 0 \quad \mbox{boundary} & t \ge 0\\
                         \partial u/\partial t|_{t=0}=0 & x,y\in (0,1)\\
                       \end{array}\right. .
$$
<p>&nbsp;<br>

The boundary is defined by \( x=0 \), \( x=1 \), \( y=0 \) and \( y=1 \).
Here we set \( \lambda = 1 \).

<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec188"></a></h2>

Our equations depend on three variables whose discretized versions
are now
<p>&nbsp;<br>
$$
 \left\{\begin{array}{cc} t_l=l\Delta t& l \ge 0 \\
                          x_i=i\Delta x& 0 \le i \le n_x\\
                          y_j=j\Delta y& 0 \le j \le n_y\end{array}\right. ,
$$
<p>&nbsp;<br>

and we will let \( \Delta x=\Delta y = h \) and \( n_x=n_y \) for the sake of
simplicity.
We have now the following discretized partial derivatives
<p>&nbsp;<br>
$$
u_{xx}\approx \frac{u_{i+1,j}^l-2u_{i,j}^l+u_{i-1,j}^l}{h^2},
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
u_{yy}\approx \frac{u_{i,j+1}^l-2u_{i,j}^l+u_{i,j-1}^l}{h^2},
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
u_{tt}\approx \frac{u_{i,j}^{l+1}-2u_{i,j}^{l}+u_{i,j}^{l-1}}{\Delta t^2}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec189"></a></h2>

We merge this into the discretized \( 2+1 \)-dimensional wave equation
as
<p>&nbsp;<br>
$$
\tag{24}
u_{i,j}^{l+1}
=2u_{i,j}^{l}-u_{i,j}^{l-1}+\frac{\Delta t^2}{h^2}\left(u_{i+1,j}^l-4u_{i,j}^l+u_{i-1,j}^l+u_{i,j+1}^l+u_{i,j-1}^l\right),
$$
<p>&nbsp;<br>

where again we have an explicit scheme with \( u_{i,j}^{l+1} \) as the only
unknown quantity.
It is easy to account for different step lengths for \( x \) and \( y \).
The partial derivative is treated in much the same way
as for the one-dimensional case, except that we now have an additional
index due to the extra spatial dimension, viz., we need to compute
\( u_{i,j}^{-1} \) through
<p>&nbsp;<br>
$$
u_{i,j}^{-1}=u_{i,j}^0+\frac{\Delta t}{2h^2}\left(u_{i+1,j}^0-4u_{i,j}^0+u_{i-1,j}^0+u_{i,j+1}^0+u_{i,j-1}^0\right),
$$
<p>&nbsp;<br>

in our setup of the initial conditions.

<p>

</section>


<section>

<h2>Code example for the two-dimensional wave equation  <a name="___sec190"></a></h2>

We show here how to implement the two-dimensional wave equation

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">//  After initializations and declaration of variables</span>
  <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; n; i++ ) {
    u[i] = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
    uLast[i] = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
    uNext[i] = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
    x[i] = i*h;
    y[i] = x[i];

  <span style="color: #228B22">// initializing</span>
  <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; n; i++ ) {  <span style="color: #228B22">// setting initial step</span>
    <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; n; j++ ) {
      uLast[i][j] = sin(PI*x[i])*sin(<span style="color: #B452CD">2</span>*PI*y[j]);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the two-dimensional wave equation  <a name="___sec191"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>; i &lt; (n-<span style="color: #B452CD">1</span>); i++ ) {  <span style="color: #228B22">// setting first step using the initial derivative</span>
    <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">1</span>; j &lt; (n-<span style="color: #B452CD">1</span>); j++ ) {
      u[i][j] = uLast[i][j] - ((tStep*tStep)/(<span style="color: #B452CD">2.0</span>*h*h))*
	(<span style="color: #B452CD">4</span>*uLast[i][j] - uLast[i+<span style="color: #B452CD">1</span>][j] - uLast[i-<span style="color: #B452CD">1</span>][j] - uLast[i][j+<span style="color: #B452CD">1</span>] - uLast[i][j-<span style="color: #B452CD">1</span>]);

    u[i][<span style="color: #B452CD">0</span>] = <span style="color: #B452CD">0</span>;  <span style="color: #228B22">// setting boundaries once and for all</span>
    u[i][n-<span style="color: #B452CD">1</span>] = <span style="color: #B452CD">0</span>;
    u[<span style="color: #B452CD">0</span>][i] = <span style="color: #B452CD">0</span>;
    u[n-<span style="color: #B452CD">1</span>][i] = <span style="color: #B452CD">0</span>;

    uNext[i][<span style="color: #B452CD">0</span>] = <span style="color: #B452CD">0</span>;
    uNext[i][n-<span style="color: #B452CD">1</span>] = <span style="color: #B452CD">0</span>;
    uNext[<span style="color: #B452CD">0</span>][i] = <span style="color: #B452CD">0</span>;
    uNext[n-<span style="color: #B452CD">1</span>][i] = <span style="color: #B452CD">0</span>;
</pre></div>
<p>

</section>


<section>

<h2>Code example for the two-dimensional wave equation  <a name="___sec192"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// iterating in time</span>
  <span style="color: #a7a7a7; font-weight: bold">double</span> t = <span style="color: #B452CD">0.0</span> + tStep;
  <span style="color: #a7a7a7; font-weight: bold">int</span> iter = <span style="color: #B452CD">0</span>;

  <span style="color: #8B008B; font-weight: bold">while</span> ( t &lt; tFinal ) {
    iter ++;
    t = t + tStep;

    <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>; i &lt; (n-<span style="color: #B452CD">1</span>); i++ ) {  <span style="color: #228B22">// computing next step</span>
      <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">1</span>; j &lt; (n-<span style="color: #B452CD">1</span>); j++ ) {
	uNext[i][j] = <span style="color: #B452CD">2</span>*u[i][j] - uLast[i][j] - ((tStep*tStep)/(h*h))*
	  (<span style="color: #B452CD">4</span>*u[i][j] - u[i+<span style="color: #B452CD">1</span>][j] - u[i-<span style="color: #B452CD">1</span>][j] - u[i][j+<span style="color: #B452CD">1</span>] - u[i][j-<span style="color: #B452CD">1</span>]);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the two-dimensional wave equation  <a name="___sec193"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>; i &lt; (n-<span style="color: #B452CD">1</span>); i++ ) {  <span style="color: #228B22">// shifting results down</span>
      <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">1</span>; j &lt; (n-<span style="color: #B452CD">1</span>); j++ ) {
	uLast[i][j] = u[i][j];
	u[i][j] = uNext[i][j];
</pre></div>
<p>

</section>


<section>

<h2>Closed form solution of the wave equation  <a name="___sec194"></a></h2>

We develop here the analytic solution for the \( 2+1 \) dimensional wave equation with the following boundary and initial conditions

<p>&nbsp;<br>
$$
 \left\{\begin{array}{cc} c^2(u_{xx}+u_{yy}) = u_{tt}& x,y\in(0,L), t>0 \\
                         u(x,y,0) = f(x,y) & x,y\in (0,L) \\
                         u(0,0,t)=u(L,L,t)=0 & t > 0\\
                         \partial u/\partial t|_{t=0}= g(x,y) & x,y\in (0,L)\\
                       \end{array}\right. .
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Closed form solution of the wave equation  <a name="___sec195"></a></h2>

Our first step is to make the ansatz

<p>&nbsp;<br>
$$
   u(x,y,t) = F(x,y) G(t),
$$
<p>&nbsp;<br>

resulting  in the equation

<p>&nbsp;<br>
$$
   FG_{tt}= c^2(F_{xx}G+F_{yy}G),
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
   \frac{G_{tt}}{c^2G} =  \frac{1}{F}(F_{xx}+F_{yy}) = -\nu^2.
$$
<p>&nbsp;<br>

The lhs and rhs are independent of each other and we obtain two differential equations

<p>&nbsp;<br>
$$
   F_{xx}+F_{yy}+F\nu^2=0,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   G_{tt} + Gc^2\nu^2 =    G_{tt} + G\lambda^2 =  0,
$$
<p>&nbsp;<br>

with \( \lambda = c\nu \).

<p>

</section>


<section>

<h2>Closed form solution of the wave equation  <a name="___sec196"></a></h2>

We can in turn make the following ansatz for the \( x \)  and \( y \) dependent part

<p>&nbsp;<br>
$$
    F(x,y) = H(x)Q(y),
$$
<p>&nbsp;<br>

which results in

<p>&nbsp;<br>
$$
   \frac{1}{H}H_{xx} =  -\frac{1}{Q}(Q_{yy}+Q\nu^2)= -\kappa^2.
$$
<p>&nbsp;<br>

Since the lhs and rhs are again independent of each other, we can separate the latter equation into two independent
equations, one for \( x \) and one for \( y \), namely

<p>&nbsp;<br>
$$
   H_{xx} + \kappa^2H =  0,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   Q_{yy} + \rho^2Q = 0,
$$
<p>&nbsp;<br>

with \( \rho^2= \nu^2-\kappa^2 \).

<p>

</section>


<section>

<h2>Closed form solution of the wave equation  <a name="___sec197"></a></h2>

The second step is to solve these differential equations, which all have trigonometric functions as solutions, viz.

<p>&nbsp;<br>
$$
H(x) = A\cos(\kappa x)+B\sin(\kappa x),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
Q(y) = C\cos(\rho y)+D\sin(\rho y).
$$
<p>&nbsp;<br>

The boundary conditions require that \( F(x,y) = H(x)Q(y) \) are zero at the boundaries, meaning that
\( H(0)=H(L)=Q(0)=Q(L)=0 \).  This yields the solutions

<p>&nbsp;<br>
$$
  H_m(x) = \sin(\frac{m\pi x}{L}) \quad Q_n(y) = \sin(\frac{n\pi y}{L}),
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
  F_{mn}(x,y) = \sin(\frac{m\pi x}{L})\sin(\frac{n\pi y}{L}).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Closed form solution of the wave equation  <a name="___sec198"></a></h2>

With \( \rho^2= \nu^2-\kappa^2 \) and \( \lambda = c\nu \) we have an eigenspectrum \( \lambda=c\sqrt{\kappa^2+\rho^2} \)
or \( \lambda_{mn}= c\pi/L\sqrt{m^2+n^2} \).
The solution for \( G \) is

<p>&nbsp;<br>
$$
G_{mn}(t) = B_{mn}\cos(\lambda_{mn} t)+D_{mn}\sin(\lambda_{mn} t),
$$
<p>&nbsp;<br>

with the general solution of the form

<p>&nbsp;<br>
$$
u(x,y,t) = \sum_{mn=1}^{\infty} u_{mn}(x,y,t) = \sum_{mn=1}^{\infty}F_{mn}(x,y)G_{mn}(t).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Closed form solution of the wave equation  <a name="___sec199"></a></h2>

The final step is to determine the coefficients \( B_{mn} \) and \( D_{mn} \) from the Fourier coefficients.
The equations for these  are determined by the initial conditions \( u(x,y,0) = f(x,y) \) and
\( \partial u/\partial t|_{t=0}= g(x,y) \).
The final expressions are

<p>&nbsp;<br>
$$
B_{mn} = \frac{2}{L}\int_0^L\int_0^L dxdy f(x,y) \sin(\frac{m\pi x}{L})\sin(\frac{n\pi y}{L}),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
D_{mn} = \frac{2}{L}\int_0^L\int_0^L dxdy g(x,y) \sin(\frac{m\pi x}{L})\sin(\frac{n\pi y}{L}).
$$
<p>&nbsp;<br>

Inserting the particular functional forms of \( f(x,y) \) and \( g(x,y) \) one obtains the final analytic expressions.

<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec200"></a></h2>

We can check our results as function of the number of mesh points and in particular against
the stability condition

<p>&nbsp;<br>
$$
\Delta t \le \frac{1}{\sqrt{\lambda}}\left(\frac{1}{\Delta x^2}+\frac{1}{\Delta y^2}\right)^{-1/2}
$$
<p>&nbsp;<br>

where \( \Delta t \), \( \Delta x \) and \( \Delta y \) are the chosen step lengths. In our case
\( \Delta x=\Delta y=h \).   How do we find this condition?  In one dimension we can proceed
as we did for the diffusion equation.

<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec201"></a></h2>

The analytic solution of the wave equation in \( 2+1 \) dimensions has a characteristic
wave component which reads

<p>&nbsp;<br>
$$
u(x,y,t) = A \exp{ (i(k_xx+k_yy-\omega t))}
$$
<p>&nbsp;<br>

Then from

<p>&nbsp;<br>
$$
u_{xx}\approx \frac{u_{i+1,j}^l-2u_{i,j}^l+u_{i-1,j}^l}{\Delta x^2},
$$
<p>&nbsp;<br>

we get, with \( u_i=\exp{ikx_i} \)

<p>&nbsp;<br>
$$
u_{xx}\approx \frac{u_i}{\Delta x^2}\left(\exp{ik\Delta x}-2+\exp{(-ik\Delta x)}\right),
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
u_{xx}\approx 2\frac{u_i}{\Delta x^2}\left(cos(k\Delta x)-1\right)=
-4\frac{u_i}{\Delta x^2}sin^2(k\Delta x/2)
$$
<p>&nbsp;<br>

We get similar results for \( t \) and \( y \).

<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec202"></a></h2>

We have

<p>&nbsp;<br>
$$
\lambda\left(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}\right) = \frac{\partial^2u}{\partial t^2},
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
\lambda\left(-4\frac{u_{ij}^l}{\Delta x^2}\sin^2{(k_x\Delta x/2)}-4\frac{u_{ij}^l}{\Delta y^2}\sin^2{(k_y\Delta y/2)}\right)=-4\frac{u_{ij}^l}{\Delta t^2}\sin^2{(\omega\Delta t/2)},
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
\sin{(\omega\Delta t/2)}=\pm \sqrt{\lambda}\Delta t\left(\frac{1}{\Delta x^2}\sin^2{(k_x\Delta x/2)}+\frac{1}{\Delta y^2}\sin^2{(k_y\Delta y/2)}\right)^{1/2}.
$$
<p>&nbsp;<br>


<p>
The squared sine functions can at most be unity. The frequency
\( \omega \) is real and our wave is neither damped
nor amplified.

<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec203"></a></h2>

We have

<p>&nbsp;<br>
$$
\sin{(\omega\Delta t/2)}=\pm \sqrt{\lambda}\Delta t\left(\frac{1}{\Delta x^2}\sin^2{(k_x\Delta x/2)}+\frac{1}{\Delta y^2}\sin^2{(k_y\Delta y/2)}\right)^{1/2}.
$$
<p>&nbsp;<br>

The squared sine functions can at most be unity. \( \omega \) is real and our wave is neither damped
nor amplified. The numerical \( \omega \) must also be real  which is the case when
\( \sin{(\omega\Delta t/2)} \) is less than or equal to unity, meaning that

<p>&nbsp;<br>
$$
\Delta t \le \frac{1}{\sqrt{\lambda}}\left(\frac{1}{\Delta x^2}+\frac{1}{\Delta y^2}\right)^{-1/2}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec204"></a></h2>

 We modify now the wave equation in order to consider
a \( 2+1 \) dimensional wave equation with a position dependent velocity, given by

<p>&nbsp;<br>
$$
\frac{\partial^2 u}{\partial t^2} = \nabla\cdot (\lambda(x,y) \nabla u).
$$
<p>&nbsp;<br>

If \( \lambda \) is  constant, we obtain the standard wave equation discussed in the two previous points.
The solution \( u(x,y,t) \) could represent a model for  water waves. It represents then the surface elevation from still water.
We can model \( \lambda \) as

<p>&nbsp;<br>
$$
\lambda = gH(x,y),
$$
<p>&nbsp;<br>

with \( g \) being the acceleration of gravity and \( H(x,y) \) is the still water depth.

<p>
The function \( H(x,y) \) simulates the water depth using for example measurements of still water depths
in say a fjord or the north sea. The boundary conditions are then determined by the coast lines as discussed in point d) below.  We have assumed that the vertical motion is negligible and that
we deal with long wavelenghts \( \tilde{\lambda} \) compared with the depth of the sea \( H \), that
is \( \tilde{\lambda}/H \gg 1 \).  We neglect normally Coriolis effects in such calculations.

<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec205"></a></h2>

You can discretize

<p>&nbsp;<br>
$$
\nabla \cdot (\lambda(x,y) \nabla u)=  \frac{\partial }{\partial x}\left(\lambda(x,y)\frac{\partial u}{\partial x}\right)+
\frac{\partial }{\partial y}\left(\lambda(x,y)\frac{\partial u}{\partial y}\right),
$$
<p>&nbsp;<br>

as follows using  again a quadratic domain for \( x \) and \( y \):

<p>&nbsp;<br>
$$
\frac{\partial }{\partial x}\left(\lambda(x,y)\frac{\partial u}{\partial x}\right)\approx
\frac{1}{\Delta x} \left(\lambda_{i+1/2,j}\left[\frac{u_{i+1,j}^l-u_{i,j}^l}{\Delta x}\right]
-\lambda_{i-1/2,j}\left[\frac{u_{i,j}^l-u_{i-1,j}^l}{\Delta x}\right]\right),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\frac{\partial }{\partial y}\left(\lambda(x,y)\frac{\partial u}{\partial y}\right)\approx
\frac{1}{\Delta y} \left(\lambda_{i,j+1/2}\left[\frac{u_{i,j+1}^l-u_{i,j}^l}{\Delta y}\right]
-\lambda_{i,j-1/2}\left[\frac{u_{i,j}^l-u_{i,j-1}^l}{\Delta y}\right]\right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec206"></a></h2>

How did we do that? Look at the derivative wrt \( x \) only:
First we compute the derivative

<p>&nbsp;<br>
$$
   \frac{d}{dx}\left(\lambda(x)\frac{du}{dx}\right)|_{x=x_i} \approx
   \frac{1}{\Delta x}\left(\lambda\frac{du}{dx}|_{x=x_{i+1/2}}-\lambda\frac{du}{dx}|_{x=x_{i-1/2}}\right),
$$
<p>&nbsp;<br>

where we approximated it at the midpoint by going half a step to the right and half a step to
the left.  Then we approximate

<p>&nbsp;<br>
$$
\lambda\frac{du}{dx}|_{x=x_{i+1/2}}\approx \lambda_{x_{i+1/2}}\frac{u_{i+1}-u_i}{\Delta x},
$$
<p>&nbsp;<br>

and similarly for \( x = x_i-1/2 \).

<p>

</section>


<section>

<h1>Overview of week 43  <a name="___sec207"></a></h1>


</section>


<section>

<h2>Overview of week 43  <a name="___sec208"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Numerical integration (chapter 5).</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Numerical integration, Trapezoidal and Simpson's rules (equal step methods)</li>
  <p><li> Gaussian quadrature (better methods)</li>
  <p><li> Tuesday:</li>
  <p><li> Gaussian quadrature, continued</li>
  <p><li> Our first encounter with parallelization, this week MPI, next week also OpenMP with examples Next week we start also with Monte Carlo methods, which will keep us busy till the end of November.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Numerical integration and Equal Step Methods  <a name="___sec209"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Generalities.</b>
<p>

<p>
Choose a step size

<p>&nbsp;<br>
$$
        h=\frac{b-a}{N}
$$
<p>&nbsp;<br>

   where \( N \) is the number of steps and \( a \) and \( b \) the lower and upper limits
   of integration.

<p>
Choose then to stop the Taylor expansion of the function \( f(x) \) at a
         certain derivative.

<p>
With these approximations to \( f(x) \) perform the integration.

<p>&nbsp;<br>
$$
    \int_a^bf(x) dx= \int_a^{a+2h}f(x)dx + \int_{a+2h}^{a+4h}f(x)dx+\dots \int_{b-2h}^{b}f(x)dx.
$$
<p>&nbsp;<br>

The strategy then is to find a reliable Taylor expansion for \( f(x) \) in the smaller
sub intervals. Consider e.g., evaluating \( \int_{-h}^{+h}f(x)dx \)
</div>


<p>

</section>


<section>

<h2>Equal Step Methods  <a name="___sec210"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Trapezoidal Rule.</b>
<p>
Taylor expansion

<p>&nbsp;<br>
$$
   f(x)=f_0 + \frac{f_h-f_0}{h}x+O(x^2),
$$
<p>&nbsp;<br>

for \( x=x_0 \) to \( x=x_0+h \) and

<p>&nbsp;<br>
$$
   f(x)=f_0 + \frac{f_0-f_{-h}}{h}x+O(x^2),
$$
<p>&nbsp;<br>

for \( x=x_0-h \) to \( x=x_0 \). The error goes like \( O(x^2) \).
If we then evaluate the integral we obtain

<p>&nbsp;<br>
$$
   \int_{-h}^{+h}f(x)dx=\frac{h}{2}\left(f_h + 2f_0 + f_{-h}\right)+O(h^3),
$$
<p>&nbsp;<br>

which is the well-known trapezoidal rule.  Local error
\( O(h^3)=O((b-a)^3/N^3) \), and the <em>global error</em> goes like \( \approx O(h^2) \).
</div>


<p>

</section>


<section>

<h2>Equal Step Methods  <a name="___sec211"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Trapezoidal Rule.</b>
<p>
Easy to implement  numerically through the following simple algorithm

<ul>
  <p><li> Choose the number of mesh points and fix the step.</li>
  <p><li> calculate \( f(a) \) and \( f(b) \) and multiply with \( h/2 \)</li>
  <p><li> Perform a loop over \( n=1 \) to \( n-1 \) (\( f(a) \) and \( f(b) \) are known) and sum up the terms \( f(a+h) +f(a+2h)+f(a+3h)+\dots +f(b-h) \). Each step in the loop corresponds to a given value \( a+nh \).</li>
  <p><li> Multiply the final result by \( h \) and add \( hf(a)/2 \) and \( hf(b)/2 \).</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Trapezoidal Rule  <a name="___sec212"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">trapezoidal_rule</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> a, <span style="color: #a7a7a7; font-weight: bold">double</span> b, <span style="color: #a7a7a7; font-weight: bold">int</span> n,
                        <span style="color: #a7a7a7; font-weight: bold">double</span> (*func)(<span style="color: #a7a7a7; font-weight: bold">double</span>))
{
      <span style="color: #a7a7a7; font-weight: bold">double</span> trapez_sum;
      <span style="color: #a7a7a7; font-weight: bold">double</span> fa, fb, x, step;
      <span style="color: #a7a7a7; font-weight: bold">int</span>    j;
      step=(b-a)/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n);
      fa=(*func)(a)/<span style="color: #B452CD">2.</span> ;
      fb=(*func)(b)/<span style="color: #B452CD">2.</span> ;
      trapez_sum=<span style="color: #B452CD">0.</span>;
      <span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">1</span>; j &lt;= n-<span style="color: #B452CD">1</span>; j++){
         x=j*step+a;
         trapez_sum+=(*func)(x);

      trapez_sum=(trapez_sum+fb+fa)*step;
      <span style="color: #8B008B; font-weight: bold">return</span> trapez_sum;
}  <span style="color: #228B22">// end function for trapezoidal rule</span>
</pre></div>
<p>

</section>


<section>

<h2>Trapezoidal Rule  <a name="___sec213"></a></h2>

Pay attention to the way we transfer the name of a function. This gives us the possibility to define a general trapezoidal method, where we give as input the name of the function.

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">double</span> trapezoidal_rule(<span style="color: #a7a7a7; font-weight: bold">double</span> a, <span style="color: #a7a7a7; font-weight: bold">double</span> b, <span style="color: #a7a7a7; font-weight: bold">int</span> n,
                        <span style="color: #a7a7a7; font-weight: bold">double</span> (*func)(<span style="color: #a7a7a7; font-weight: bold">double</span>))
</pre></div>
<p>
We call this function simply as something like this

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">integral =  trapezoidal_rule(a, b, n,  mysuperduperfunction);
</pre></div>
<p>

</section>


<section>

<h2>Equal Step Methods  <a name="___sec214"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Simpson.</b>
<p>
The first and second
derivatives are given by

<p>&nbsp;<br>
$$
   \frac{f_h-f_{-h}}{2h}=f'_0+\sum_{j=1}^{\infty}\frac{f_0^{(2j+1)}}{(2j+1)!}h^{2j},
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
 \frac{ f_h -2f_0 +f_{-h}}{h^2}=f_0''+2\sum_{j=1}^{\infty}\frac{f_0^{(2j+2)}}{(2j+2)!}h^{2j},
$$
<p>&nbsp;<br>

results in
\( f(x)=f_0 + \frac{f_h-f_{-h}}{2h}x + \frac{ f_h -2f_0 +f_{-h}}{h^2}x^2 +O(x^3) \).
Inserting this formula in the integral

<p>&nbsp;<br>
$$
   \int_{-h}^{+h}f(x)dx=\frac{h}{3}\left(f_h + 4f_0 + f_{-h}\right)+O(h^5),
$$
<p>&nbsp;<br>

which is Simpson's rule.
</div>


<p>

</section>


<section>

<h2>Equal Step Methods  <a name="___sec215"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Simpson's rule.</b>
<p>
Note that the improved accuracy in the evaluation of
the derivatives gives a better error approximation, \( O(h^5) \) vs. \( O(h^3) \) .
But this is just the <em>local error approximation</em>.
Using Simpson's rule we arrive at the composite rule

<p>&nbsp;<br>
$$
   I=\int_a^bf(x) dx=\frac{h}{3}\left(f(a) + 4f(a+h) +2f(a+2h)+
                          \dots +4f(b-h)+ f_{b}\right),
   \tag{25}
$$
<p>&nbsp;<br>

with a global error which goes like \( O(h^4) \).
Algo

<ul>
  <p><li> Choose the number of mesh points and fix the step.</li>
  <p><li> calculate \( f(a) \) and \( f(b) \)</li>
  <p><li> Perform a loop over \( n=1 \) to \( n-1 \) (\( f(a) \) and \( f(b) \) are known) and sum up the terms \( 4f(a+h) +2f(a+2h)+4f(a+3h)+\dots +4f(b-h) \). Odd values of \( n \) give \( 4 \) as factor while even values yield \( 2 \) as factor.</li>
  <p><li> Multiply the final result by \( \frac{h}{3} \).</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Equal Step Methods  <a name="___sec216"></a></h2>

The basic idea behind all integration methods is to approximate the integral

<p>&nbsp;<br>
$$
   I=\int_a^bf(x)dx \approx \sum_{i=1}^N\omega_if(x_i),
$$
<p>&nbsp;<br>

where \( \omega \) and \( x \) are the weights and the chosen mesh points, respectively.
Simpson's rule gives

<p>&nbsp;<br>
$$
   \omega : \left\{h/3,4h/3,2h/3,4h/3,\dots,4h/3,h/3\right\},
$$
<p>&nbsp;<br>

for the weights, while the trapezoidal rule resulted in

<p>&nbsp;<br>
$$
   \omega : \left\{h/2,h,h,\dots,h,h/2\right\}.
$$
<p>&nbsp;<br>

In general, an integration formula which is based on a Taylor series using \( N \) points,
will integrate exactly a polynomial \( P \) of degree \( N-1 \). That is, the \( N \) weights
\( \omega_n \) can be chosen to satisfy \( N \) linear equations

<p>

</section>


<section>

<h2>Equal Step Methods, Polynomials and Newton-Cotes  <a name="___sec217"></a></h2>

Given \( n+1 \) distinct points \( x_0,\dots, x_n\in[a,b] \) and \( n+1 \) values \( y_0,\dots,y_n \) there exists a
unique polynomial \( p_n \) with the property

<p>&nbsp;<br>
$$
   p_n(x_j) = y_j\quad j=0,\dots,n
$$
<p>&nbsp;<br>

In the Lagrange representation this interpolation polynomial is given by

<p>&nbsp;<br>
$$
p_n = \sum_{k=0}^nl_ky_k,
$$
<p>&nbsp;<br>

with the Lagrange factors

<p>&nbsp;<br>
$$
   l_k(x) = \prod_{\begin{array}{c}i=0 \\ i\ne k\end{array}}^n\frac{x-x_i}{x_k-x_i}\quad k=0,\dots,n
$$
<p>&nbsp;<br>

Example: \( n=1 \)

<p>&nbsp;<br>
$$
p_1(x) = y_0\frac{x-x_1}{x_0-x_1}+y_1\frac{x-x_0}{x_1-x_0}=\frac{y_1-y_0}{x_1-x_0}x-\frac{y_1x_0+y_0x_1}{x_1-x_0},
$$
<p>&nbsp;<br>

which we recognize as the equation for a straight line.

<p>

</section>


<section>

<h2>Equal Step Methods, Polynomials and Newton-Cotes  <a name="___sec218"></a></h2>

The polynomial interpolatory quadrature of order \( n \) with equidistant quadrature points \( x_k=a+kh \)
and step \( h=(b-a)/n \) is called the Newton-Cotes quadrature formula of order \( n \).
The integral is

<p>&nbsp;<br>
$$
  \int_a^bf(x)dx \approx \int_a^bp_n(x)dx = \sum_{k=0}^nw_kf(x_k)
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
   w_k = h\frac{(-1)^{n-k}}{k!(n-k)!}\int_0^n\prod_{\begin{array}{c}j=0 \\ j\ne k\end{array}}^n(z-j)dz,
$$
<p>&nbsp;<br>

for \( k=0,\dots,n \).

<p>

</section>


<section>

<h2>Equal Step Methods, Polynomials and Newton-Cotes  <a name="___sec219"></a></h2>

The local error for the trapezoidal rule is

<p>&nbsp;<br>
$$
\int_a^bf(x)dx -\frac{b-a}{2}\left[f(a)+f(b)\right]=-\frac{h^3}{12}f^{(2)}(\xi),
$$
<p>&nbsp;<br>

and the global error (composite formula)

<p>&nbsp;<br>
$$
\int_a^bf(x)dx -T_h(f)=-\frac{b-a}{12}h^2f^{(2)}(\xi).
$$
<p>&nbsp;<br>

For Simpson's rule we have

<p>&nbsp;<br>
$$
\int_a^bf(x)dx -\frac{b-a}{6}\left[f(a)+4f((a+b)/2)+f(b)\right]=-\frac{h^5}{90}f^{(4)}(\xi),
$$
<p>&nbsp;<br>

and the global error

<p>&nbsp;<br>
$$
\int_a^bf(x)dx -S_h(f)=-\frac{b-a}{180}h^4f^{(4)}(\xi).
$$
<p>&nbsp;<br>

with \( \xi\in[a,b] \).

<p>

</section>


<section>

<h2>Gaussian Quadrature  <a name="___sec220"></a></h2>

Methods based on Taylor series using \( n+1 \) points will
        integrate exactly a polynomial \( P \) of degree \( n \). If a function \( f(x) \)
        can be approximated with a polynomial of degree \( n \)

<p>&nbsp;<br>
$$
          f(x)\approx P_{n}(x),
$$
<p>&nbsp;<br>

         with \( n+1 \) mesh points we should be able to integrate exactly the
         polynomial \( P_{n} \).

<p>
Gaussian quadrature methods promise more than this. We can get a better
         polynomial approximation with order greater than \( n+1 \)  to \( f(x) \) and still
         get away with only \( n+1 \) mesh points. More precisely, we approximate

<p>&nbsp;<br>
$$
            f(x) \approx P_{2n+1}(x),
$$
<p>&nbsp;<br>

         and with only \( n+1 \) mesh points these methods promise that

<p>&nbsp;<br>
$$
            \int f(x)dx \approx \int P_{2n+1}(x)dx=\sum_{i=0}^{n} P_{2n+1}(x_i)\omega_i,
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Gaussian Quadrature  <a name="___sec221"></a></h2>

What we have done till now is called Newton-Cotes quadrature. The numerical
approximation  goes like \( O(h^n) \), where \( n \) is method-dependent.

<p>
A greater precision for a given amount of numerical work can  be achieved
if we are willing to give up the requirement of equally spaced integration points.
In Gaussian quadrature (hereafter GQ), both the mesh points and the weights are to
be determined. The points will not be equally spaced
The theory behind GQ is to obtain an arbitrary weight \( \omega \) through the use of
so-called orthogonal polynomials. These polynomials are orthogonal in some
interval say e.g., [-1,1]. Our points \( x_i \) are chosen in some optimal sense subject
only to the constraint that they should lie in this interval. Together with the weights
we have then \( 2(n+1) \) (\( n+1 \) the number of points) parameters at our disposal.

<p>

</section>


<section>

<h2>Gaussian Quadrature  <a name="___sec222"></a></h2>

Even though the integrand is not smooth, we could render it smooth by extracting
from it the weight function of an orthogonal polynomial, i.e.,
we are rewriting

<p>&nbsp;<br>
$$
   I=\int_a^bf(x)dx =\int_a^bW(x)g(x)dx\approx \sum_{i=0}^n\omega_ig(x_i),
$$
<p>&nbsp;<br>

where \( g \) is smooth and \( W \) is the weight function, which is to  be associated with a given
orthogonal polynomial.

<p>

</section>


<section>

<h2>Gaussian Quadrature  <a name="___sec223"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Weight Functions.</b>
<p>
The weight function \( W \) is non-negative in the integration interval
\( x\in [a,b] \) such that
for any \( n \ge 0 \) $\int_a^b |x|^n W(x) dx$ is integrable. The naming
weight function arises from the fact that it may be used to give more emphasis
to one part of the interval than another.

<p>
<table border="1">
<thead>
<tr><th align="center">       Weight function        </th> <th align="center">           Interval           </th> <th align="center">          Polynomial          </th> </tr>
</thead>
<tbody>
<tr><td align="center">   \( W(x)=1 \)                      </td> <td align="center">   \( x\in [a,b] \)                  </td> <td align="center">   Legendre                          </td> </tr>
<tr><td align="center">   \( W(x)=e^{-x^2} \)               </td> <td align="center">   \( -\infty \le x \le \infty \)    </td> <td align="center">   Hermite                           </td> </tr>
<tr><td align="center">   \( W(x)=e^{-x} \)                 </td> <td align="center">   \( 0 \le x \le \infty  \)         </td> <td align="center">   Laguerre                          </td> </tr>
<tr><td align="center">   \( W(x)=1/(\sqrt{1-x^2}) \)       </td> <td align="center">   \( -1 \le x \le 1 \)              </td> <td align="center">   Chebyshev                         </td> </tr>
</tbody>
</table>

</div>


<p>

</section>


<section>

<h2>Legendre  <a name="___sec224"></a></h2>

<p>&nbsp;<br>
$$
   I=\int_{-1}^{1}f(x)dx
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
C(1-x^2)P-m_l^2P+(1-x^2)\frac{d}{dx}\left((1-x^2)\frac{dP}{dx}\right)=0.
$$
<p>&nbsp;<br>

\( C \) is a constant. For \( m_l=0 \) we obtain the Legendre polynomials
as solutions, whereas \( m_l \ne 0 \) yields the so-called associated Legendre
polynomials.
The corresponding polynomials \( P \) are

<p>&nbsp;<br>
$$
   L_k(x)=\frac{1}{2^kk!}\frac{d^k}{dx^k}(x^2-1)^k \quad k=0,1,2,\dots,
$$
<p>&nbsp;<br>

which, up to a factor, are the Legendre polynomials \( L_k \).
The latter fulfil the orthorgonality relation

<p>&nbsp;<br>
$$
  \int_{-1}^1L_i(x)L_j(x)dx=\frac{2}{2i+1}\delta_{ij},
$$
<p>&nbsp;<br>

and the recursion relation

<p>&nbsp;<br>
$$
  (j+1)L_{j+1}(x)+jL_{j-1}(x)-(2j+1)xL_j(x)=0.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Laguerre  <a name="___sec225"></a></h2>

<p>&nbsp;<br>
$$
   I=\int_0^{\infty}f(x)dx =\int_0^{\infty}x^{\alpha}e^{-x}g(x)dx.
$$
<p>&nbsp;<br>

These polynomials arise from the solution of the differential
equation

<p>&nbsp;<br>
$$
\left(\frac{d^2 }{dx^2}-\frac{d }{dx}+\frac{\lambda}{x}-\frac{l(l+1)}{x^2}\right){\cal L}(x)=0,
$$
<p>&nbsp;<br>

where \( l \) is an integer \( l\ge 0 \) and \( \lambda \) a constant.
They fulfil the orthorgonality relation

<p>&nbsp;<br>
$$
  \int_{-\infty}^{\infty}e^{-x}{\cal L}_n(x)^2dx=1,
$$
<p>&nbsp;<br>

and the recursion relation

<p>&nbsp;<br>
$$
  (n+1){\cal L}_{n+1}(x)=(2n+1-x){\cal L}_{n}(x)-n{\cal L}_{n-1}(x).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Hermite  <a name="___sec226"></a></h2>

In a similar way, for an integral which goes like

<p>&nbsp;<br>
$$
   I=\int_{-\infty}^{\infty}f(x)dx =\int_{-\infty}^{\infty}e^{-x^2}g(x)dx.
$$
<p>&nbsp;<br>

we could use the Hermite polynomials in order to extract weights and mesh points.
The Hermite polynomials are the solutions of the following differential
equation

<p>&nbsp;<br>
$$
   \frac{d^2H(x)}{dx^2}-2x\frac{dH(x)}{dx}+
       (\lambda-1)H(x)=0.
   \tag{26}
$$
<p>&nbsp;<br>

They fulfil the orthorgonality relation

<p>&nbsp;<br>
$$
  \int_{-\infty}^{\infty}e^{-x^2}H_n(x)^2dx=2^nn!\sqrt{\pi},
$$
<p>&nbsp;<br>

and the recursion relation

<p>&nbsp;<br>
$$
  H_{n+1}(x)=2xH_{n}(x)-2nH_{n-1}(x).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Gaussian Quadrature, general Properties  <a name="___sec227"></a></h2>

A quadrature formula

<p>&nbsp;<br>
$$ \int_a^bW(x)f(x)dx \approx \sum_{i=0}^n\omega_if(x_i), $$
<p>&nbsp;<br>

with \( n+1 \) distinct quadrature points (mesh points) is a called a Gaussian quadrature
formula if it integrates all polynomials \( p\in P_{2n+1} \) exactly, that is

<p>&nbsp;<br>
$$ \int_a^bW(x)p(x)dx =\sum_{i=0}^n\omega_ip(x_i), $$
<p>&nbsp;<br>

It is assumed that \( W(x) \) is continuous and positive and that the integral

<p>&nbsp;<br>
$$ \int_a^bW(x)dx , $$
<p>&nbsp;<br>

exists. Note that the replacement of \( f\rightarrow Wg \) is normally a better approximation
due to the fact that we may isolate possible singularities of \( W \) and its
derivatives at the endpoints of the interval.

<p>

</section>


<section>

<h2>Numerical integration: A simple example  <a name="___sec228"></a></h2>

We want to compute

<p>&nbsp;<br>
$$
I= \int_0^{\infty} x\exp{(-x)}\sin{x}=\frac{1}{2},
$$
<p>&nbsp;<br>

using brute force Trapezoidal rule, Simpson's rule, Gauss-Legendre, Gauss-Laguerre and Gauss-Legendre again but with a smarter mapping.

<ul>
  <p><li> Before we start it can be useful to study the integrand.</li>
  <p><li> How should we pick the integration limits?</li>
</ul>
<p>


</section>


<section>

<h2>Simple integral  <a name="___sec229"></a></h2>

Approximate

<p>&nbsp;<br>
$$ \int_0^{\infty}f(x)dx \approx \int_0^{\Lambda}f(x)dx
$$
<p>&nbsp;<br>


<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">     <span style="color: #a7a7a7; font-weight: bold">int</span> n;
     <span style="color: #a7a7a7; font-weight: bold">double</span> a, b, alf, xx;
     cout &lt;&lt; <span style="color: #CD5555">&quot;Read in the number of integration points&quot;</span> &lt;&lt; endl;
     cin &gt;&gt; n;
     cout &lt;&lt; <span style="color: #CD5555">&quot;Read in integration limits&quot;</span> &lt;&lt; endl;
     cin &gt;&gt; a &gt;&gt; b;
</pre></div>
<p>

</section>


<section>

<h2>Simple integral  <a name="___sec230"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//   reserve space in memory for vectors containing the mesh points</span>
<span style="color: #228B22">//   weights and function values for the use of the gauss-legendre</span>
<span style="color: #228B22">//   method</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> *x = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
     <span style="color: #a7a7a7; font-weight: bold">double</span> *w = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n];
     <span style="color: #228B22">// Gauss-Laguerre is old-fashioned translation of F77 --&gt; C++</span>
     <span style="color: #228B22">// arrays start at 1 and end at n</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> *xgl = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n+<span style="color: #B452CD">1</span>];
     <span style="color: #a7a7a7; font-weight: bold">double</span> *wgl = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [n+<span style="color: #B452CD">1</span>];
</pre></div>
<p>

</section>


<section>

<h2>Simple integral  <a name="___sec231"></a></h2>

Note the parameter \( alf \)  in \( x^{alpha}\exp{-x} \)

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//   set up the mesh points and weights</span>
     gauleg(a, b,x,w, n);
<span style="color: #228B22">//   set up the mesh points and weights</span>
     alf = <span style="color: #B452CD">1.0</span>;  <span style="color: #228B22">//  &lt;---  Note alf</span>
     gauss_laguerre(xgl,wgl, n, alf);
<span style="color: #228B22">//   evaluate the integral with the Gauss-Legendre method</span>
<span style="color: #228B22">//   Note that we initialize the sum. Here brute force gauleg</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> int_gauss = <span style="color: #B452CD">0.</span>;
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>;  i &lt; n; i++){
        int_gauss+=w[i]*int_function(x[i]);
</pre></div>
<p>

</section>


<section>

<h2>Simple integral, importance integration/sampling  <a name="___sec232"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//   evaluate the integral with the Gauss-Laguerre method</span>
<span style="color: #228B22">//   Note that we initialize the sum</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> int_gausslag = <span style="color: #B452CD">0.</span>;
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
       int_gausslag+=wgl[i]*sin(xgl[i]);     }
</pre></div>
<p>

</section>


<section>

<h2>Simple integral  <a name="___sec233"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//   evaluate the integral with the Gauss-Laguerre method</span>
<span style="color: #228B22">//   Here we change the mesh points with a mapping</span>
<span style="color: #228B22">//   Need to call gauleg from -1 to + 1</span>
     gauleg(-<span style="color: #B452CD">1.0</span>, <span style="color: #B452CD">1.0</span>,x,w, n);
     <span style="color: #a7a7a7; font-weight: bold">double</span> pi_4 = acos(-<span style="color: #B452CD">1.0</span>)*<span style="color: #B452CD">0.25</span>;
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>;  i &lt; n; i++){
       xx=pi_4*(x[i]+<span style="color: #B452CD">1.0</span>);
       r[i]= tan(xx);
       s[i]=pi_4/(cos(xx)*cos(xx))*w[i];

     <span style="color: #a7a7a7; font-weight: bold">double</span> int_gausslegimproved = <span style="color: #B452CD">0.</span>;
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>;  i &lt; n; i++){
       int_gausslegimproved += s[i]*int_function(r[i]);
</pre></div>
<p>

</section>


<section>

<h2>A six-dimensional integral, project 3 2010  <a name="___sec234"></a></h2>

The ansatz for the wave function for two electrons is given by the product of two
\( 1s \) wave functions as

<p>&nbsp;<br>
$$
   \Psi({\bf r}_1,{\bf r}_2)  =   e^{-\alpha (r_1+r_2)}.
$$
<p>&nbsp;<br>

Note that it is not possible to find a closed form  solution to Schr\"odinger's equation for
two interacting electrons in the helium atom.

<p>
The integral we need to solve is the quantum mechanical expectation value of the correlation
energy between two electrons, namely

<p>&nbsp;<br>
$$
\begin{equation}label{eq:correlationenergy}
   \langle \frac{1}{|{\bf r}_1-{\bf r}_2|} \rangle =
   \int_{-\infty}^{\infty} d{\bf r}_1d{\bf r}_2  e^{-2\alpha (r_1+r_2)}\frac{1}{|{\bf r}_1-{\bf r}_2|}=\frac{5\pi^2}{16^2}=0.192765711.
\end{equation}
$$
<p>&nbsp;<br>

Note that our wave function is not normalized. There is a normalization factor missing.

<p>

</section>


<section>

<h2>Brute force, six-dimensional integral, Gauss-Legendre  <a name="___sec235"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">     <span style="color: #a7a7a7; font-weight: bold">double</span> *x = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [N];
     <span style="color: #a7a7a7; font-weight: bold">double</span> *w = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [N];
<span style="color: #228B22">//   set up the mesh points and weights</span>
     gauleg(a,b,x,w, N);

<span style="color: #228B22">//   evaluate the integral with the Gauss-Legendre method</span>
<span style="color: #228B22">//   Note that we initialize the sum</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> int_gauss = <span style="color: #B452CD">0.</span>;
     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">0</span>;i&lt;N;i++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>;j&lt;N;j++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>;k&lt;N;k++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> l = <span style="color: #B452CD">0</span>;l&lt;N;l++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> m = <span style="color: #B452CD">0</span>;m&lt;N;m++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> n = <span style="color: #B452CD">0</span>;n&lt;N;n++){
        int_gauss+=w[i]*w[j]*w[k]*w[l]*w[m]*w[n]
       *int_function(x[i],x[j],x[k],x[l],x[m],x[n]);
     		}}}}}
	}
</pre></div>
<p>

</section>


<section>

<h2>The six-dimensional integral with Gauss-Legendre  <a name="___sec236"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  this function defines the function to integrate</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> x1, <span style="color: #a7a7a7; font-weight: bold">double</span> y1, <span style="color: #a7a7a7; font-weight: bold">double</span> z1, <span style="color: #a7a7a7; font-weight: bold">double</span> x2, <span style="color: #a7a7a7; font-weight: bold">double</span> y2, <span style="color: #a7a7a7; font-weight: bold">double</span> z2)
{
   <span style="color: #a7a7a7; font-weight: bold">double</span> alpha = <span style="color: #B452CD">2.</span>;
<span style="color: #228B22">// evaluate the different terms of the exponential</span>
   <span style="color: #a7a7a7; font-weight: bold">double</span> exp1=-<span style="color: #B452CD">2</span>*alpha*sqrt(x1*x1+y1*y1+z1*z1);
   <span style="color: #a7a7a7; font-weight: bold">double</span> exp2=-<span style="color: #B452CD">2</span>*alpha*sqrt(x2*x2+y2*y2+z2*z2);
   <span style="color: #a7a7a7; font-weight: bold">double</span> deno=sqrt(pow((x1-x2),<span style="color: #B452CD">2</span>)+pow((y1-y2),<span style="color: #B452CD">2</span>)+pow((z1-z2),<span style="color: #B452CD">2</span>));
  <span style="color: #8B008B; font-weight: bold">if</span>(deno &lt;pow(<span style="color: #B452CD">10.</span>,-<span style="color: #B452CD">6.</span>)) { <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;}
  <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #8B008B; font-weight: bold">return</span> exp(exp1+exp2)/deno;
} <span style="color: #228B22">// end of function to evaluate</span>
</pre></div>
<p>

</section>


<section>

<h2>Switch to spherical coordinates  <a name="___sec237"></a></h2>

Useful to change to spherical coordinates

<p>&nbsp;<br>
$$
   d{\bf r}_1d{\bf r}_2  = r_1^2dr_1 r_2^2dr_2 dcos(\theta_1)dcos(\theta_2)d\phi_1d\phi_2,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   \frac{1}{r_{12}}= \frac{1}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
\cos(\beta) = \cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Switch to spherical coordinates  <a name="___sec238"></a></h2>

This means that our integral becomes

<p>&nbsp;<br>
$$
   \int_0^{\infty} r_1^2dr_1 \int_0^{\infty}r_2^2dr_2 \int_0^{\pi}dcos(\theta_1)\int_0^{\pi}dcos(\theta_2)\int_0^{2\pi}d\phi_1\int_0^{2\pi}d\phi_2   \times
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\frac{e^{-2\alpha (r_1+r_2)}}{\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))   }}
$$
<p>&nbsp;<br>

since

<p>&nbsp;<br>
$$
   \frac{1}{r_{12}}= \frac{1}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
\cos(\beta) = \cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec239"></a></h2>

Before we abondon totally methods like the trapezoidal rule,
we mention breefly how
an adaptive integration method can be implemented.

<p>
The above methods are all based on a defined step length, normally provided by the user,
dividing the integration domain with a fixed number of subintervals.
This is rather simple to implement may be inefficient, in particular if the integrand
varies considerably in certain areas of the integration domain. In these areas the number of fixed integration points may not be adequate. In other regions, the integrand may vary slowly
and fewer integration points may be needed.

<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec240"></a></h2>

In order to account for such features, it may be convenient to first study the properties of
integrand, via for example a plot of the function to integrate. If this function
oscillates largely in some specific domain we may then opt for adding more integration points
to that particular domain. However, this procedure needs to be repeated for every new integrand and lacks obviously the advantages of a more generic code.

<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec241"></a></h2>

Assume that we want to compute an integral using say the trapezoidal rule. We limit ourselves
to a one-dimensional integral.
Our integration domain is defined by \( x\in [a,b] \). The algorithm goes as follows

<p>
<b>Step 1.</b>
We compute our first approximation by computing the integral for the full domain. We label this as \( I^{(0)} \). It is obtained by calling our previously discussed function <code>trapezoidal_rule</code> as

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">I0 = trapezoidal_rule(a, b, n, function);
</pre></div>
<p>
<b>Step 2.</b>
We split the integration in two, with \( c= (a+b)/2 \). We compute then the two integrals \( I^{(1L)} \) and \( I^{(1R)} \)

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">I1L = trapezoidal_rule(a, c, n, function);
</pre></div>
<p>
and
<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">I1R = trapezoidal_rule(c, b, n, function);
</pre></div>
<p>
With a given defined tolerance, being a small number provided by us, we estimate the difference \( |I^{(1L)}+I^{(1R)}-I^{(0)}| < \mbox{tolerance} \). If this test is satisfied, our first approximation is satisfactory.
If not, we can set up a recursive procedure where the integral is split into subsequent subintervals until our tolerance is satisfied.

<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec242"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//     Simple recursive function that implements the</span>
<span style="color: #228B22">//     adaptive integration using the trapezoidal rule</span>
<span style="color: #8B008B; font-weight: bold">const</span> <span style="color: #a7a7a7; font-weight: bold">int</span> maxrecursions = <span style="color: #B452CD">50</span>;
<span style="color: #8B008B; font-weight: bold">const</span> <span style="color: #a7a7a7; font-weight: bold">double</span> tolerance = <span style="color: #B452CD">1.0E-10</span>;
<span style="color: #228B22">//  Takes as input the integration  limits, number of points, function to integrate</span>
<span style="color: #228B22">//  and the number of steps</span>
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">adaptive_integration</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> a, <span style="color: #a7a7a7; font-weight: bold">double</span> b, <span style="color: #a7a7a7; font-weight: bold">double</span> *Integral, <span style="color: #a7a7a7; font-weight: bold">int</span> n, <span style="color: #a7a7a7; font-weight: bold">int</span> steps, <span style="color: #a7a7a7; font-weight: bold">double</span> (*func)(<span style="color: #a7a7a7; font-weight: bold">double</span>))
     <span style="color: #8B008B; font-weight: bold">if</span> ( steps &gt; maxrecursions){
        cout &lt;&lt; <span style="color: #a61717; background-color: #e3d2d2">&#39;</span>Too many recursive steps, the function varies too much<span style="color: #a61717; background-color: #e3d2d2">&#39;</span> &lt;&lt; endl;
        <span style="color: #8B008B; font-weight: bold">break</span>;
</pre></div>
<p>

</section>


<section>

<h2>Adaptive methods  <a name="___sec243"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">     <span style="color: #a7a7a7; font-weight: bold">double</span> c = (a+b)*<span style="color: #B452CD">0.5</span>;
     <span style="color: #228B22">// the whole integral</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> I0 = trapezoidal_rule(a, b,n, func);
     <span style="color: #228B22">//  the left half</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> I1L = trapezoidal_rule(a, c,n, func);
     <span style="color: #228B22">//  the right half</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> I1R = trapezoidal_rule(c, b,n, func);
     <span style="color: #8B008B; font-weight: bold">if</span> (fabs(I1L+I1R-I0) &lt; tolerance )  integral = I0;
     <span style="color: #8B008B; font-weight: bold">else</span>
     {
        adaptive_integration(a, c, integral, <span style="color: #a7a7a7; font-weight: bold">int</span> n, ++steps, func)
        adaptive_integration(c, b, integral, <span style="color: #a7a7a7; font-weight: bold">int</span> n, ++steps, func)

<span style="color: #228B22">// end function Adaptive integration</span>
</pre></div>
<p>
The variables {\bf Integral} and {\bf steps} should be initialized to zero by the function
that calls the adaptive procedure.

<p>

</section>


<section>

<h1>Overview of week 44  <a name="___sec244"></a></h1>


</section>


<section>

<h2>Overview of week 44  <a name="___sec245"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Monte Carlo methods.</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Parallelization using MPI</li>
  <p><li> Introduction to Monte Carlo methods and Monte Carlo integration</li>
  <p><li> Tuesday:</li>
  <p><li> Monte Carlo integration and importance sampling</li>
  <p><li> Discussion of project 4.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Going Parallel: Divide et impera - Divide and Conquer (accordingly after Julius Caesar)  <a name="___sec246"></a></h2>

We will first meet the concept of

<p>
{\bf Task parallelism}: the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize.
Monte Carlo simulation or integrations are examples of this.
It is almost embarrassingly trivial to parallelize
Monte Carlo and numerical integration codes.

<p>
We will use MPI=Message Passing Interface.
MPI is a message-passing library where all the routines
have corresponding C/C++-binding

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_Command_name
</pre></div>
<p>
and Fortran-binding (routine names are in uppercase, but can also be in lower case)

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_COMMAND_NAME
</pre></div>
<p>

</section>


<section>

<h2>What is Message Passing Interface (MPI)? Yet another library!  <a name="___sec247"></a></h2>

MPI is a library, not a language. It specifies the names, calling sequences and results of functions
or subroutines to be called from C or Fortran programs, and the classes and methods that make up the MPI C++
library. The programs that users write in Fortran, C or C++ are compiled with ordinary compilers and linked
with the MPI library.

<p>
MPI is a specification, not a particular implementation. MPI programs should be able to run
on all possible machines and run all MPI implementetations without change.

<p>
An MPI computation is a collection of processes communicating with messages.

<p>
See chapter 5.5 of lecture notes for more details.

<p>

</section>


<section>

<h2>MPI  <a name="___sec248"></a></h2>

MPI is a library specification for the message passing interface,
proposed as a standard.

<ul>
  <p><li> independent of hardware;</li>
  <p><li> not a language or compiler specification;</li>
  <p><li> not a specific implementation or product. A message passing standard for portability and ease-of-use. Designed for high performance. Insert communication and synchronization functions where necessary.</li>
</ul>
<p>


</section>


<section>

<h2>Demands from the HPC community  <a name="___sec249"></a></h2>

In the field of scientific computing, there is an ever-lasting wish
to do larger simulations using shorter computer time.

<p>
Development of the capacity for single-processor computers can
hardly keep up with the pace of scientific computing:

<ul>
  <p><li> processor speed</li>
  <p><li> memory size/speed Solution: parallel computing!</li>
</ul>
<p>


</section>


<section>

<h2>The basic ideas of parallel computing  <a name="___sec250"></a></h2>

<ul>
  <p><li> Pursuit of shorter computation time and larger simulation size gives rise to parallel computing.</li>
  <p><li> Multiple processors are involved to solve a global problem.</li>
  <p><li> The essence is to divide the entire computation evenly among collaborative processors. Divide and conquer.</li>
</ul>
<p>


</section>


<section>

<h2>A rough classification of hardware models  <a name="___sec251"></a></h2>

  *
Conventional single-processor computers can be called SISD
(single-instruction-single-data) machines.

<ul>
  <p><li> SIMD (single-instruction-multiple-data) machines incorporate the idea of parallel processing, which use a large number of process- ing units to execute the same instruction on different data.</li>
  <p><li> Modern parallel computers are so-called MIMD (multiple-instruction- multiple-data) machines and can execute different instruction streams in parallel on different data.</li>
</ul>
<p>


</section>


<section>

<h2>Shared memory and distributed memory  <a name="___sec252"></a></h2>

<ul>
  <p><li> One way of categorizing modern parallel computers is to look at the memory configuration.</li>
  <p><li> In shared memory systems the CPUs share the same address space. Any CPU can access any data in the global memory.</li>
  <p><li> In distributed memory systems each CPU has its own memory. The CPUs are connected by some network and may exchange messages.</li>
</ul>
<p>


</section>


<section>

<h2>Different parallel programming paradigms  <a name="___sec253"></a></h2>

<ul>
  <p><li> {\bf Task parallelism:} the work of a global problem can be divided into a number of independent tasks, which rarely need to synchronize. Monte Carlo simulation is one example. Integration is another. However this paradigm is of limited use.</li>
  <p><li> {\bf Data parallelism:} use of multiple threads (e.g. one thread per processor) to dissect loops over arrays etc. This paradigm requires a single memory address space. Communication and synchronization between processors are often hidden, thus easy to program. However, the user surrenders much control to a specialized compiler. Examples of data parallelism are compiler-based parallelization and OpenMP directives.</li>
</ul>
<p>


</section>


<section>

<h2>Today's situation of parallel computing  <a name="___sec254"></a></h2>

<ul>
  <p><li> Distributed memory is the dominant hardware configuration. There is a large diversity in these machines, from MPP (massively parallel processing) systems to clusters of off-the-shelf PCs, which are very cost-effective.</li>
  <p><li> Message-passing is a mature programming paradigm and widely accepted. It often provides an efficient match to the hardware. It is primarily used for the distributed memory systems, but can also be used on shared memory systems. In these lectures we consider only message-passing for writing parallel programs.</li>
</ul>
<p>


</section>


<section>

<h2>Overhead present in parallel computing  <a name="___sec255"></a></h2>

<ul>
  <p><li> {\bf Uneven load balance}: not all the processors can perform useful work at all time.</li>
  <p><li> {\bf Overhead of synchronization.}</li>
  <p><li> {\bf Overhead of communication}.</li>
  <p><li> {Extra computation due to parallelization}. Due to the above overhead and that certain part of a sequential algorithm cannot be parallelized we may not achieve an optimal parallelization.</li>
</ul>
<p>


</section>


<section>

<h2>Parallelizing a sequential algorithm  <a name="___sec256"></a></h2>

<ul>
  <p><li> Identify the part(s) of a sequential algorithm that can be executed in parallel. This is the difficult part,</li>
  <p><li> Distribute the global work and data among \( P \) processors.</li>
</ul>
<p>


</section>


<section>

<h2>Process and processor  <a name="___sec257"></a></h2>

<ul>
  <p><li> We refer to process as a logical unit which executes its own code, in an MIMD style.</li>
  <p><li> The processor is a physical device on which one or several processes are executed.</li>
  <p><li> The MPI standard uses the concept process consistently throughout its documentation.</li>
</ul>
<p>


</section>


<section>

<h2>Bindings to MPI routines  <a name="___sec258"></a></h2>

MPI is a message-passing library where all the routines
have corresponding C/C++-binding

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_Command_name
</pre></div>
<p>
and Fortran-binding (routine names are in uppercase, but can also be in lower case)

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   MPI_COMMAND_NAME
</pre></div>
<p>
The discussion in these slides focuses on the C++ binding.

<p>

</section>


<section>

<h2>The most important/used MPI routines  <a name="___sec259"></a></h2>

<ul>
  <p><li> <code>MPI_ Init</code> - initiate an MPI computation</li>
  <p><li> <code>MPI_Finalize</code> - terminate the MPI computation and clean up</li>
  <p><li> <code>MPI_Comm_size</code> - how many processes participate in a given MPI communicator?</li>
  <p><li> <code>MPI_Comm_rank</code> - which one am I? (A number between 0 and size-1.)</li>
  <p><li> <code>MPI_Reduce(Allreduce)</code> - Collect data from all nodes and either sum them up in one or all <code>(Allreduce)</code>. Useful for numerical integration</li>
  <p><li> <code>MPI_Send</code> - send a message to a particular process within an MPI communicator</li>
  <p><li> <code>MPI_Recv</code> - receive a message from a particular process within an MPI communicator</li>
</ul>
<p>


</section>


<section>

<h2>Note the <code>MPI_COMM_WORLD</code> declaration  <a name="___sec260"></a></h2>

<ul>
  <p><li> A group of MPI processes with a name (context).</li>
  <p><li> Any process is identified by its rank. The rank is only meaningful within a particular communicator.</li>
  <p><li> By default communicator <code>MPI_COMM_WORLD</code> contains all the MPI processes.</li>
  <p><li> Mechanism to identify subset of processes.</li>
  <p><li> Promotes modular design of parallel libraries.</li>
</ul>
<p>


</section>


<section>

<h2>The first MPI C/C++ program  <a name="___sec261"></a></h2>

Let every process write "Hello world" on the standard output.  This is program2.cpp of chapter 4.

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #1e889b">#include &lt;mpi.h&gt;</span>
<span style="color: #1e889b">#include &lt;iostream&gt;</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
<span style="color: #a7a7a7; font-weight: bold">int</span> numprocs, my_rank;
<span style="color: #228B22">//   MPI initializations</span>
MPI_Init (&amp;nargs, &amp;args);
MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world, I have  rank &quot;</span> &lt;&lt; my_rank &lt;&lt; <span style="color: #CD5555">&quot; out of &quot;</span>
     &lt;&lt; numprocs &lt;&lt; endl;
<span style="color: #228B22">//  End MPI</span>
MPI_Finalize ();
</pre></div>
<p>

</section>


<section>

<h2>The Fortran program  <a name="___sec262"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">PROGRAM hello
INCLUDE <span style="color: #CD5555">&quot;mpif.h&quot;</span>
INTEGER:: size, my_rank, ierr

CALL  MPI_INIT(ierr)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierr)
CALL MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)
WRITE(*,*)<span style="color: #CD5555">&quot;Hello world, I&#39;ve rank &quot;</span>,my_rank,<span style="color: #CD5555">&quot; out of &quot;</span>,size
CALL MPI_FINALIZE(ierr)

END PROGRAM hello
</pre></div>
<p>

</section>


<section>

<h2>Note 1  <a name="___sec263"></a></h2>

The output to screen is not ordered since all processes are trying to write  to screen simultaneously.
It is then the operating system which opts for an ordering.
If we wish to have an organized output, starting from the first process, we may rewrite our program as in the next example
(program3.cpp), see again chapter 5.7 of lecture notes.

<p>

</section>


<section>

<h2>Ordered output with <code>MPI_Barrier</code>  <a name="___sec264"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
 <span style="color: #a7a7a7; font-weight: bold">int</span> numprocs, my_rank, i;
 MPI_Init (&amp;nargs, &amp;args);
 MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
 MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
 <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; numprocs; i++) {}
 MPI_Barrier (MPI_COMM_WORLD);
 <span style="color: #8B008B; font-weight: bold">if</span> (i == my_rank) {
 cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world, I have  rank &quot;</span> &lt;&lt; my_rank &lt;&lt;
        <span style="color: #CD5555">&quot; out of &quot;</span> &lt;&lt; numprocs &lt;&lt; endl;}
      MPI_Finalize ();
</pre></div>
<p>

</section>


<section>

<h2>Note 2  <a name="___sec265"></a></h2>

Here we have used the <code>MPI_Barrier</code> function to ensure that
that every process has completed  its set of instructions in  a particular order.
A barrier is a special collective operation that does not allow the processes to continue
until all processes in the communicator (here <code>MPI_COMM_WORLD</code> have called
<code>MPI_Barrier</code>.
The barriers make sure that all processes have reached the same point in the code. Many of the collective operations
like <code>MPI_ALLREDUCE</code> to be discussed later, have the same property; viz. no process can exit the operation
until all processes have started.
However, this is slightly more time-consuming since the processes synchronize between themselves as many times as there
are processes.  In the next Hello world example we use the send and receive functions in order to a have a synchronized
action.

<p>

</section>


<section>

<h2>Strategies  <a name="___sec266"></a></h2>

<ul>
  <p><li> Develop codes locally, run with some few processes and test your codes. Do benchmarking, timing and so forth on local nodes, for example your laptop. You can install MPICH2 on your laptop (most new laptos come with dual cores). You can test with one node at the lab.</li>
  <p><li> When you are convinced that your codes run correctly, you start your production runs on available supercomputers. At UiO we have a new machine among the top 100, Abel.</li>
</ul>
<p>


</section>


<section>

<h2>How do I run MPI on the machines at the lab (MPICH2)  <a name="___sec267"></a></h2>

The machines at the lab are all quad-cores

<p>
<b>Step 1.</b>
Compile with mpicxx or mpic++

<p>
<b>Step 2.</b>
Set up collaboration between processes and run

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">mpd --ncpus=<span style="color: #B452CD">4</span> &amp; <span style="color: #a61717; background-color: #e3d2d2">#</span> run code with mpiexec -n <span style="color: #B452CD">4</span> ./nameofprog
</pre></div>
<p>
Here we declare that we will use 4 processes via the <code>-ncpus</code> option and via <code>-n 4</code> when running.

<p>
<b>Step 3.</b>
End with

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">mpdallexit
</pre></div>
<p>

</section>


<section>

<h2>Can I do it on my own PC/laptop?  <a name="___sec268"></a></h2>

Of course:

<ul>
  <p><li> go to <a href="http://www.mcs.anl.gov/research/projects/mpich2/" target="_blank"><tt>http://www.mcs.anl.gov/research/projects/mpich2/</tt></a></li>
  <p><li> follow the instructions and install it on your own PC/laptop It works on Windows, Mac and Linux. For Linux (Ubuntu, Linux Mint), go to synaptic package manager and search for MPI. You will get several options.</li>
</ul>
<p>


</section>


<section>

<h2>Ordered output with <code>MPI_Recv</code> and <code>MPI_Send</code>  <a name="___sec269"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">.....
<span style="color: #a7a7a7; font-weight: bold">int</span> numprocs, my_rank, flag;
MPI_Status status;
MPI_Init (&amp;nargs, &amp;args);
MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
<span style="color: #8B008B; font-weight: bold">if</span> (my_rank &gt; <span style="color: #B452CD">0</span>)
MPI_Recv (&amp;flag, <span style="color: #B452CD">1</span>, MPI_INT, my_rank-<span style="color: #B452CD">1</span>, <span style="color: #B452CD">100</span>,
           MPI_COMM_WORLD, &amp;status);
cout &lt;&lt; <span style="color: #CD5555">&quot;Hello world, I have  rank &quot;</span> &lt;&lt; my_rank &lt;&lt; <span style="color: #CD5555">&quot; out of &quot;</span>
&lt;&lt; numprocs &lt;&lt; endl;
<span style="color: #8B008B; font-weight: bold">if</span> (my_rank &lt; numprocs-<span style="color: #B452CD">1</span>)
MPI_Send (&amp;my_rank, <span style="color: #B452CD">1</span>, MPI_INT, my_rank+<span style="color: #B452CD">1</span>,
          <span style="color: #B452CD">100</span>, MPI_COMM_WORLD);
MPI_Finalize ();
</pre></div>
<p>

</section>


<section>

<h2>Note 3  <a name="___sec270"></a></h2>

The basic sending of messages is given by the function <code>MPI_SEND</code>, which in C/C++
is defined as

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> MPI_Send(<span style="color: #a7a7a7; font-weight: bold">void</span> *buf, <span style="color: #a7a7a7; font-weight: bold">int</span> count,
             MPI_Datatype datatype,
             <span style="color: #a7a7a7; font-weight: bold">int</span> dest, <span style="color: #a7a7a7; font-weight: bold">int</span> tag, MPI_Comm comm)}
</pre></div>
<p>
This single command allows the passing of any kind of variable, even a large array, to any group of tasks.
The variable {\bf buf} is the variable we wish to send while {\bf count}
is the  number of variables we are passing. If we are passing only a single value, this should be 1.
If we transfer an array, it is  the overall size of the array.
For example, if we want to send a 10 by 10 array, count would be \( 10\times 10=100 \)
since we are  actually passing 100 values.

<p>

</section>


<section>

<h2>Note 4  <a name="___sec271"></a></h2>

Once you have  sent a message, you must receive it on another task. The function <code>MPI_RECV</code> is similar to the send call.

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> MPI_Recv( <span style="color: #a7a7a7; font-weight: bold">void</span> *buf, <span style="color: #a7a7a7; font-weight: bold">int</span> count, MPI_Datatype datatype,
            <span style="color: #a7a7a7; font-weight: bold">int</span> source,
            <span style="color: #a7a7a7; font-weight: bold">int</span> tag, MPI_Comm comm, MPI_Status *status )
</pre></div>
<p>
The arguments that are different from those in <code>MPI_SEND</code> are
{\bf buf} which  is the name of the variable where you will  be storing the received data,
{\bf source} which  replaces the destination in the send command. This is the return ID of the sender.

<p>
Finally,  we have used  <code>MPI_Status status</code>
where one can check if the receive was completed.

<p>
The output of this code is the same as the previous example, but now
process 0 sends a message to process 1, which forwards it further
to process 2, and so forth.

<p>
Armed with this wisdom, performed all hello world greetings, we are now ready for serious work.

<p>

</section>


<section>

<h2>Integrating \( \pi \)  <a name="___sec272"></a></h2>


<table border="0">
<tr>
<td class="padding">
<p>
<!-- 2DOFIGURE: [pi.jpg, width=500 frac=0.8] -->

<p>
</td>
<td class="padding">
<div class="alert alert-block alert-block alert-text-normal">
<b>Examples.</b>
<p>

<ul>
  <p><li> Go to the program package</li>
  <p><li> Go to the MPI directory and then chapter 5</li>
  <p><li> Look at program6.cpp.</li>
  <p><li> This code computes \( \pi \) using the trapezoidal rule.</li>
</ul>
</div>

</td>
</tr>
</table>


<p>


<p>

</section>


<section>

<h2>Integration algos  <a name="___sec273"></a></h2>

The trapezoidal rule (program6.cpp)

<p>&nbsp;<br>
$$
   I=\int_a^bf(x) dx=h\left(f(a)/2 + f(a+h) +f(a+2h)+
                          \dots +f(b-h)+ f_{b}/2\right).
$$
<p>&nbsp;<br>


<p>
Another very simple approach is the so-called midpoint or rectangle method.
In this case the integration area is split in a given number of rectangles with length \( h \) and
heigth given by the mid-point value of the function.  This gives the following simple rule for
approximating an integral

<p>&nbsp;<br>
$$
   I=\int_a^bf(x) dx \approx  h\sum_{i=1}^N f(x_{i-1/2}),
$$
<p>&nbsp;<br>

where \( f(x_{i-1/2}) \) is the midpoint value of \( f \) for a given rectangle.  This is used in program5.cpp.

<p>

</section>


<section>

<h2><code>MPI_reduce</code> and <code>MPI_Allreduce</code>  <a name="___sec274"></a></h2>

The parallel integration MPI instructions are simple if we use the functions
<code>MPI_Reduce</code> or <code>MPI_Allreduce</code>.
The first function takes information from all processes and sends the result of the MPI operation to one process only,
typically the master node.  If we use <code>MPI_Allreduce</code>, the result is sent back to all processes, a feature which is
useful when all nodes need the value of a joint operation.  We limit ourselves to <code>MPI_Reduce</code> since it is only one
process which will print out the final number of our calculation, The arguments to <code>MPI_Allreduce</code> are the same.

<p>

</section>


<section>

<h2><code>MPI_reduce</code>  <a name="___sec275"></a></h2>

Call as

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">MPI_reduce( <span style="color: #a7a7a7; font-weight: bold">void</span> *senddata, <span style="color: #a7a7a7; font-weight: bold">void</span>* resultdata, <span style="color: #a7a7a7; font-weight: bold">int</span> count,
     MPI_Datatype datatype, MPI_Op, <span style="color: #a7a7a7; font-weight: bold">int</span> root, MPI_Comm comm)
</pre></div>
<p>
The two variables \( senddata \) and \( resultdata \) are obvious, besides the fact that one sends the address
of the variable or the first element of an array.  If they are arrays they need to have the same size.
The variable \( count \) represents the total dimensionality, 1 in case of just one variable,
while <code>MPI_Datatype</code>
defines the type of variable which is sent and received.

<p>
The new feature is <code>MPI_Op</code>. It defines the type
of operation we want to do.
In our case, since we are summing
the rectangle  contributions from every process we define  <code>MPI_Op = MPI_SUM</code>.
If we have an array or matrix we can search for the largest og smallest element by sending either <code>MPI_MAX</code> or
<code>MPI_MIN</code>.  If we want the location as well (which array element) we simply transfer
<code>MPI_MAXLOC</code> or <code>MPI_MINOC</code>. If we want the product we write <code>MPI_PROD</code>.

<p>
<code>MPI_Allreduce</code> is defined as

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">MPI_Alreduce( <span style="color: #a7a7a7; font-weight: bold">void</span> *senddata, <span style="color: #a7a7a7; font-weight: bold">void</span>* resultdata, <span style="color: #a7a7a7; font-weight: bold">int</span> count,
          MPI_Datatype datatype, MPI_Op, MPI_Comm comm)}.
</pre></div>
<p>

</section>


<section>

<h2>Dissection of example program6.cpp  <a name="___sec276"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//    Trapezoidal rule and numerical integration usign MPI, example program6.cpp</span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #1e889b">#include &lt;mpi.h&gt;</span>
<span style="color: #1e889b">#include &lt;iostream&gt;</span>

<span style="color: #228B22">//     Here we define various functions called by the main program</span>

<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> );
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">trapezoidal_rule</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> , <span style="color: #a7a7a7; font-weight: bold">double</span> , <span style="color: #a7a7a7; font-weight: bold">int</span> , <span style="color: #a7a7a7; font-weight: bold">double</span> (*)(<span style="color: #a7a7a7; font-weight: bold">double</span>));

<span style="color: #228B22">//   Main function begins here</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> nargs, <span style="color: #a7a7a7; font-weight: bold">char</span>* args[])
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> n, local_n, numprocs, my_rank;
  <span style="color: #a7a7a7; font-weight: bold">double</span> a, b, h, local_a, local_b, total_sum, local_sum;
  <span style="color: #a7a7a7; font-weight: bold">double</span>  time_start, time_end, total_time;
</pre></div>
<p>

</section>


<section>

<h2>Dissection of example program6.cpp  <a name="___sec277"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">//  MPI initializations</span>
  MPI_Init (&amp;nargs, &amp;args);
  MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
  time_start = MPI_Wtime();
  <span style="color: #228B22">//  Fixed values for a, b and n</span>
  a = <span style="color: #B452CD">0.0</span> ; b = <span style="color: #B452CD">1.0</span>;  n = <span style="color: #B452CD">1000</span>;
  h = (b-a)/n;    <span style="color: #228B22">// h is the same for all processes</span>
  local_n = n/numprocs;
  <span style="color: #228B22">// make sure n &gt; numprocs, else integer division gives zero</span>
  <span style="color: #228B22">// Length of each process&#39; interval of</span>
  <span style="color: #228B22">// integration = local_n*h.</span>
  local_a = a + my_rank*local_n*h;
  local_b = local_a + local_n*h;
</pre></div>
<p>

</section>


<section>

<h2>Dissection of example program6.cpp  <a name="___sec278"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  total_sum = <span style="color: #B452CD">0.0</span>;
  local_sum = trapezoidal_rule(local_a, local_b, local_n,
                               &amp;int_function);
  MPI_Reduce(&amp;local_sum, &amp;total_sum, <span style="color: #B452CD">1</span>, MPI_DOUBLE,
              MPI_SUM, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
  time_end = MPI_Wtime();
  total_time = time_end-time_start;
  <span style="color: #8B008B; font-weight: bold">if</span> ( my_rank == <span style="color: #B452CD">0</span>) {
    cout &lt;&lt; <span style="color: #CD5555">&quot;Trapezoidal rule = &quot;</span> &lt;&lt;  total_sum &lt;&lt; endl;
    cout &lt;&lt; <span style="color: #CD5555">&quot;Time = &quot;</span> &lt;&lt;  total_time
         &lt;&lt; <span style="color: #CD5555">&quot; on number of processors: &quot;</span>  &lt;&lt; numprocs  &lt;&lt; endl;

  <span style="color: #228B22">// End MPI</span>
  MPI_Finalize ();
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  <span style="color: #228B22">// end of main program</span>
</pre></div>
<p>

</section>


<section>

<h2>Dissection of example program6.cpp  <a name="___sec279"></a></h2>

We use <code>MPI_reduce</code> to collect data from each process. Note also the use of the function
<code>MPI_Wtime</code>. The final functions are

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  this function defines the function to integrate</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> x)
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> value = <span style="color: #B452CD">4.</span>/(<span style="color: #B452CD">1.</span>+x*x);
  <span style="color: #8B008B; font-weight: bold">return</span> value;
} <span style="color: #228B22">// end of function to evaluate</span>
</pre></div>
<p>

</section>


<section>

<h2>Dissection of example program6.cpp  <a name="___sec280"></a></h2>

Implementation of the trapezoidal rule.

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  this function defines the trapezoidal rule</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">trapezoidal_rule</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> a, <span style="color: #a7a7a7; font-weight: bold">double</span> b, <span style="color: #a7a7a7; font-weight: bold">int</span> n,
                         <span style="color: #a7a7a7; font-weight: bold">double</span> (*func)(<span style="color: #a7a7a7; font-weight: bold">double</span>))
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> trapez_sum;
  <span style="color: #a7a7a7; font-weight: bold">double</span> fa, fb, x, step;
  <span style="color: #a7a7a7; font-weight: bold">int</span>    j;
  step=(b-a)/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n);
  fa=(*func)(a)/<span style="color: #B452CD">2.</span> ;
  fb=(*func)(b)/<span style="color: #B452CD">2.</span> ;
  trapez_sum=<span style="color: #B452CD">0.</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (j=<span style="color: #B452CD">1</span>; j &lt;= n-<span style="color: #B452CD">1</span>; j++){
    x=j*step+a;
    trapez_sum+=(*func)(x);

  trapez_sum=(trapez_sum+fb+fa)*step;
  <span style="color: #8B008B; font-weight: bold">return</span> trapez_sum;
}  <span style="color: #228B22">// end trapezoidal_rule</span>
</pre></div>
<p>

</section>


<section>

<h2>Plan for Monte Carlo Lectures, chapters 11-14 in Lecture notes  <a name="___sec281"></a></h2>

<ul>
  <p><li> This week: intro, MC integration and probability distribution functions (PDFs)</li>
  <p><li> Next week: More on integration, PDFs, MC integration and random walks.</li>
  <p><li> Third week: random walks and statistical physics.</li>
  <p><li> Fourth week: Statistical physics.</li>
  <p><li> Fifth week: Most likely quantum Monte Carlo Approximately from this week till the end of November.</li>
</ul>
<p>


</section>


<section>

<h2>Monte Carlo Keywords  <a name="___sec282"></a></h2>

Consider it is a numerical experiment

<ul>
  <p><li> Be able to generate random variables following a given probability distribution function PDF</li>
  <p><li> Find a probability distribution function (PDF).</li>
  <p><li> Sampling rule for accepting a move</li>
  <p><li> Compute standard deviation and other expectation values</li>
  <p><li> Techniques for improving errors Enhances algorithmic thinking!</li>
</ul>
<p>


</section>


<section>

<h2>Probability Distribution Functions PDF  <a name="___sec283"></a></h2>

\begin{tabular}{lcc}\hline
   & Discrete PDF& continuous PDF\\\hline
Domain & \( \left\{x_1, x_2, x_3, \dots, x_N\right\} \) & \( [a,b] \) \\
probability & \( p(x_i) \) &  \( p(x)dx \) \\
Cumulative  & \( P_i=\sum_{l=1}^ip(x_l) \) & \( P(x)=\int_a^xp(t)dt \) \\
Positivity  & $ 0 \le p(x_i) \le 1$ & $ p(x) \ge 0$ \\
Positivity  & $ 0 \le P_i \le 1$ & $ 0 \le P(x) \le 1$ \\
Monotonuous    & \( P_i \ge P_j \) if \( x_i \ge x_j \) & \( P(x_i) \ge P(x_j) \) if \( x_i \ge x_j \) \\
Normalization & \( P_N=1 \) & $P(b)=1$\\
\hline
\end{tabular}

<p>

</section>


<section>

<h2>Expectation Values  <a name="___sec284"></a></h2>

Discrete PDF

<p>&nbsp;<br>
$$
    E[x^k]= \langle x^k\rangle=\frac{1}{N}\sum_{i=1}^{N}x_i^kp(x_i),
$$
<p>&nbsp;<br>

provided that the sums (or integrals) $ \sum_{i=1}^{N}p(x_i)$ converge absolutely (viz ,
$ \sum_{i=1}^{N}|p(x_i)|$ converges)

<p>
Continuous PDF

<p>&nbsp;<br>
$$
    E[x^k]=\langle x^k\rangle=\int_a^b x^kp(x)dx,
$$
<p>&nbsp;<br>


<p>
Function \( f(x) \)

<p>&nbsp;<br>
$$
    E[f^k]=\langle f^k\rangle=\int_a^b f^kp(x)dx,
$$
<p>&nbsp;<br>


<p>
Variance

<p>&nbsp;<br>
$$
    \sigma^2_f=E[f^2]-(E[f])^2=\langle f^2\rangle-\langle f\rangle^2
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Important PDFs  <a name="___sec285"></a></h2>

uniform distribution

<p>&nbsp;<br>
$$
  p(x)=\frac{1}{b-a}\Theta(x-a)\Theta(b-x),
$$
<p>&nbsp;<br>

which gives for \( a=0,b=1 \) $p(x)=1$ for \( x\in [0,1] \) and zero else.

<p>
exponential distribution

<p>&nbsp;<br>
$$
  p(x)=\alpha e^{-\alpha x},
$$
<p>&nbsp;<br>

with probability different from zero in  \( [0,\infty] \)

<p>
normal distribution (Gaussian)

<p>&nbsp;<br>
$$
   p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)}
$$
<p>&nbsp;<br>

 with probability different from zero in \( [-\infty,\infty] \)

<p>
All random number generators use the uniform distribution for \( x\in[0,1] \).

<p>

</section>


<section>

<h2>Why Monte Carlo?  <a name="___sec286"></a></h2>

An example from quantum mechanics: most
 problems of interest in e.g., atomic, molecular, nuclear and solid state
 physics consist of a large number of
 interacting electrons and ions or nucleons.
 The total number of particles \( N \) is usually sufficiently large
 that an exact solution cannot be found.
 Typically,
 the expectation value for a chosen hamiltonian for a system of
 \( N \) particles is

<p>&nbsp;<br>
$$
    \langle H \rangle =
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
    \frac{\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
          \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           H({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)}
         {\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
         \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
         \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)},
$$
<p>&nbsp;<br>


<p>
 an in general intractable problem.

<p>
 This integral is actually the starting point in a Variational Monte Carlo calculation.
 {\bf Gaussian quadrature: Forget it!} given 10 particles and 10 mesh points for each degree of freedom
and an
 ideal 1 petaflops machine (all operations take the same time), how long will it ta
ke to compute the above integral? Lifetime of the universe $T\approx 4.7 \times
10^{17}$s.

<p>

</section>


<section>

<h2>More on dimensionality  <a name="___sec287"></a></h2>

 As an example from the nuclear many-body problem, we have Schr\"odinger's
 equation as
 a differential equation

<p>&nbsp;<br>
$$
   \hat{H}\Psi({\bf r}_1,..,{\bf r}_A,\alpha_1,..,\alpha_A)=E\Psi({\bf r}_1,..,{
\bf r}_A,\alpha_1,..,\alpha_A)
$$
<p>&nbsp;<br>

 where

<p>&nbsp;<br>
$$
   {\bf r}_1,..,{\bf r}_A,
$$
<p>&nbsp;<br>

 are the coordinates and

<p>&nbsp;<br>
$$
   \alpha_1,..,\alpha_A,
$$
<p>&nbsp;<br>

 are sets of relevant quantum numbers such as spin and isospin for a system of
 \( A \) nucleons (\( A=N+Z \), \( N \) being the number of neutrons and \( Z \) the number of protons).

<p>

</section>


<section>

<h2>Even more on dimensionality  <a name="___sec288"></a></h2>

 There are

<p>&nbsp;<br>
$$
  2^A\times \left(\begin{array}{c} A\\ Z\end{array}\right)
$$
<p>&nbsp;<br>

 coupled second-order differential equations in \( 3A \) dimensions.

<p>
 For a nucleus like $^{10}$Be this number is
 {\bf 215040}.
 This is a truely challenging many-body problem.

<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec289"></a></h2>

A crude approach consists in setting all weights equal 1, \( \omega_i=1 \).
With \( dx=h=(b-a)/N \) where \( b=1 \), \( a=0 \) in our case and \( h \) is the
step size. The integral

<p>&nbsp;<br>
$$
   I=\int_0^1 f(x)dx\approx \frac{1}{N}\sum_{i=1}^Nf(x_i),
$$
<p>&nbsp;<br>

can be rewritten using the concept of the average of the function \( f \) for a given PDF \( p(x) \) as

<p>&nbsp;<br>
$$
   E[f]=\langle f \rangle = \frac{1}{N}\sum_{i=1}^Nf(x_i)p(x_i),
$$
<p>&nbsp;<br>

and identify \( p(x) \) with the uniform distribution, viz
$ p(x)=1$ when \( x\in [0,1] \) and zero for all other values of \( x \).
The integral
is then  the average of \( f \) over the interval \( x \in [0,1] \)

<p>&nbsp;<br>
$$
      I=\int_0^1 f(x)dx\approx E[f]=\langle f \rangle.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec290"></a></h2>

In addition to the average value \( \langle f \rangle \) the other
important quantity in a
Monte-Carlo calculation is the variance \( \sigma^2 \) and
the standard deviation \( \sigma \). We define first the variance
of the integral with \( f \) for a uniform distribution in the interval
\( x \in [0,1] \) to be

<p>&nbsp;<br>
$$
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^N(f(x_i)-\langle f\rangle)^2p(x_i),
$$
<p>&nbsp;<br>

and inserting the uniform distribution this yields

<p>&nbsp;<br>
$$
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^Nf(x_i)^2-
  \left(\frac{1}{N}\sum_{i=1}^Nf(x_i)\right)^2,
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
  \sigma^2_f=E[f^2]-(E[f])^2=\left(\langle f^2\rangle -
                                 \langle f \rangle^2\right).
$$
<p>&nbsp;<br>

which is nothing but a measure of the extent to
which \( f \) deviates from its average over the region of integration.
The standard deviation is defined as the square root of the variance.

<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec291"></a></h2>

If we consider the above results for
a fixed value of \( N \) as a measurement,
we could however recalculate the
above average and variance for a series of different measurements.
If each such measumerent produces a set of averages for the
integral \( I \) denoted \( \langle f\rangle_l \), we have for \( M \) measurements
that the integral is given by

<p>&nbsp;<br>
$$
   \langle I \rangle_M=\frac{1}{M}\sum_{l=1}^{M}\langle f\rangle_l.
$$
<p>&nbsp;<br>

If we can consider the probability of
correlated events to be zero, we can rewrite
the variance of these series of measurements as (equating \( M=N \))

<p>&nbsp;<br>
$$
  \sigma^2_N\approx \frac{1}{N}\left(\langle f^2\rangle -
                                 \langle f \rangle^2\right)=\frac{\sigma^2_f}{N}.
$$
<p>&nbsp;<br>


<p>
We note that the standard deviation is proportional with the inverse square root of
the number of measurements

<p>&nbsp;<br>
$$
   \sigma_N \sim \frac{1}{\sqrt{N}}.
$$
<p>&nbsp;<br>

<em>The aim in Monte Carlo calculations is to have \( \sigma_N \) as small as possible after \( N \) samples. </em>
The results from one  sample represents,
since we are using concepts from statistics,
a 'measurement'.

<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec292"></a></h2>

<ul>
  <p><li> We saw that the trapezoidal rule carries a truncation error \( O(h^2) \), with \( h \) the step length.</li>

<ul>
   <p><li> Quadrature rules such as Newton-Cotes have a truncation error which goes like \( \sim O(h^k) \), with \( k \ge 1 \). Recalling that the step size is defined as \( h=(b-a)/N \), we have an error which goes like \( \sim N^{-k} \).</li>
</ul>
<p>

  <p><li> Monte Carlo integration is more efficient in higher dimensions. Assume that our integration volume is a hypercube with side \( L \) and dimension \( d \). This cube contains hence \( N=(L/h)^d \) points and therefore the error in the result scales as \( N^{-k/d} \) for the traditional methods.</li>
  <p><li> The error in the Monte carlo integration is however independent of \( d \) and scales as \( \sigma\sim 1/\sqrt{N} \), always!</li>
</ul>
<p>

  *
Comparing
this with traditional methods, shows that
Monte Carlo integration is more efficient than an order-k algorithm
when \( d > 2k \)

<p>

</section>


<section>

<h2>Some simple examples: Example 1: Particles in a Box  <a name="___sec293"></a></h2>

Consider a box divided into two equal halves separated by a wall.
At the beginning, time \( t=0 \), there are \( N \) particles on the left
side. A small hole in the wall is then opened and one particle
can pass through the hole per unit time.

<p>
After some time the system reaches its equilibrium state with
equally many particles in both halves, \( N/2 \).
Instead of determining complicated initial conditions for a system
of \( N \) particles, we model the system by a simple statistical model.
In order to simulate this system, which may consist of \( N \gg 1 \) particles,
we assume that all particles in the left half have equal probabilities
of going to the right half.

<p>

</section>


<section>

<h2>Particles in a Box  <a name="___sec294"></a></h2>

We introduce the label \( n_l \) to denote the
 number of particles at every time on the left side, and \( n_r=N-n_l \) for those
on the right side.
The probability for a move to the right during a time step  \( \Delta t \)
is \( n_l/N \). The algorithm for simulating this problem may then look
like as follows

<ul>
  <p><li> Choose the number of particles \( N \).</li>
  <p><li> Make a loop over time, where the maximum time should be larger
         than the number of particles \( N \).</li>
  <p><li> For every time step \( \Delta t \) there is a probability \( n_l/N \) for a move to the right. Compare this probability with a random number \( x \).</li>
  <p><li> If $ x \le n_l/N$, decrease the number of particles in the left
         half by one, i.e., \( n_l=n_l-1 \). Else, move a particle from the
         right half to the left, i.e., \( n_l=n_l+1 \).
         {\bf This is our sampling rule}</li>
  <p><li> Increase the time by one unit (the external loop). In this case, a Monte Carlo sample corresponds to one time unit \( \Delta t \).</li>
</ul>
<p>


</section>


<section>

<h2>Particles in a Box  <a name="___sec295"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// setup of initial conditions</span>
  nleft = initial_n_particles;
  max_time = <span style="color: #B452CD">10</span>*initial_n_particles;
  idum = -<span style="color: #B452CD">1</span>;
  <span style="color: #228B22">// sampling over number of particles</span>
  <span style="color: #8B008B; font-weight: bold">for</span>( time=<span style="color: #B452CD">0</span>; time &lt;= max_time; time++){
    random_n = ((<span style="color: #a7a7a7; font-weight: bold">int</span>) initial_n_particles*ran0(&amp;idum));
    <span style="color: #8B008B; font-weight: bold">if</span> ( random_n &lt;= nleft){
      nleft -= <span style="color: #B452CD">1</span>;

    <span style="color: #8B008B; font-weight: bold">else</span>{
      nleft += <span style="color: #B452CD">1</span>;

    ofile &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; time;
    ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; nleft &lt;&lt; endl;
</pre></div>
<p>

</section>


<section>

<h2>Example 2: Radioactive Decay  <a name="___sec296"></a></h2>

Assume that a the time \( t=0 \) we have \( N(0) \) nuclei of type \( X \)
which can decay radioactively. At a time \( t > 0 \) we are left with
\( N(t) \) nuclei. With a transition probability \( \omega \),
which expresses the probability that the system will make a transition to
another state during a time step of one second, we have the following first-order
differential equation

<p>&nbsp;<br>
$$
   dN(t)=-\omega N(t)dt,
$$
<p>&nbsp;<br>

whose  solution is

<p>&nbsp;<br>
$$
   N(t)=N(0)e^{-\omega t},
$$
<p>&nbsp;<br>

where we have defined the mean lifetime \( \tau \) of \( X \) as

<p>&nbsp;<br>
$$
   \tau =\frac{1}{\omega}.
$$
<p>&nbsp;<br>


<p>
If a nucleus \( X \) decays to a daugther nucleus \( Y \) which also can decay, we get
the following coupled equations

<p>&nbsp;<br>
$$
   \frac{dN_X(t)}{dt}=-\omega_XN_X(t),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   \frac{dN_Y(t)}{dt}=-\omega_YN_Y(t)+\omega_XN_X(t).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Radioactive Decay  <a name="___sec297"></a></h2>

Probability for a decay of a particle during a time step
\( \Delta t \)
is

<p>&nbsp;<br>
$$
   \frac{\Delta N(t)}{N(t)\Delta t}= -\lambda
$$
<p>&nbsp;<br>

\( \lambda \) is inversely proportional to the lifetime

<ul>
  <p><li> Choose the number of particles \( N(t=0)=N_0 \).</li>
  <p><li> Make a loop over the number of time steps,
         with maximum time bigger than the number of particles \( N_0 \)</li>
  <p><li> At every time step there is a probability \( \lambda \) for decay. Compare this probability with a random number \( x \).</li>
  <p><li> If $ x \le \lambda$, reduce the number of particles with one
         i.e., \( N=N-1 \). If not, keep the same number of particles till the next
         time step.
         {\bf This is our sampling rule}</li>
  <p><li> Increase by one the time step (the external loop)</li>
</ul>
<p>


</section>


<section>

<h2>Radioactive Decay  <a name="___sec298"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  idum=-<span style="color: #B452CD">1</span>;  <span style="color: #228B22">// initialise random number generator</span>
  <span style="color: #228B22">// loop over monte carlo cycles</span>
  <span style="color: #228B22">// One monte carlo loop is one sample</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (cycles = <span style="color: #B452CD">1</span>; cycles &lt;= number_cycles; cycles++){
    n_unstable = initial_n_particles;
    <span style="color: #228B22">//  accumulate the number of particles per time step per trial</span>
    ncumulative[<span style="color: #B452CD">0</span>] += initial_n_particles;
    <span style="color: #228B22">// loop over each time step</span>
    <span style="color: #8B008B; font-weight: bold">for</span> (time=<span style="color: #B452CD">1</span>; time &lt;= max_time; time++){
      <span style="color: #228B22">// for each time step, we check each particle</span>
      particle_limit = n_unstable;
      <span style="color: #8B008B; font-weight: bold">for</span> ( np = <span style="color: #B452CD">1</span>; np &lt;=  particle_limit; np++) {
        <span style="color: #8B008B; font-weight: bold">if</span>( ran0(&amp;idum) &lt;= decay_probability) {
          n_unstable=n_unstable-<span style="color: #B452CD">1</span>;

      }  <span style="color: #228B22">// end of loop over particles</span>
      ncumulative[time] += n_unstable;
    }  <span style="color: #228B22">// end of loop over time steps</span>
  }    <span style="color: #228B22">// end of loop over MC trials</span>
}   <span style="color: #228B22">// end mc_sampling function</span>
</pre></div>
<p>

</section>


<section>

<h2>Monte Carlo Keywords  <a name="___sec299"></a></h2>

Consider it is a numerical experiment

<ul>
  <p><li> Be able to generate random variables following a given probability distribution function PDF</li>
  <p><li> Find a probability distribution function (PDF).</li>
  <p><li> Sampling rule for accepting a move</li>
  <p><li> Compute standard deviation and other expectation values</li>
  <p><li> Techniques for improving errors Enhances algorithmic thinking!</li>
</ul>
<p>


</section>


<section>

<h2>Why Monte Carlo integration?  <a name="___sec300"></a></h2>

An example from quantum mechanics: most
 problems of interest in e.g., atomic, molecular, nuclear and solid state
 physics consist of a large number of
 interacting electrons and ions or nucleons.
 The total number of particles \( N \) is usually sufficiently large
 that an exact solution cannot be found.
 Typically,
 the expectation value for a chosen hamiltonian for a system of
 \( N \) particles is

<p>&nbsp;<br>
$$
    \langle H \rangle =
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
    \frac{\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
          \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           H({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)}
         {\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
         \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
         \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)},
$$
<p>&nbsp;<br>


<p>
 an in general intractable problem.

<p>
 This integral is actually the starting point in a Variational Monte Carlo calculation.
 {\bf Gaussian quadrature: Forget it!} given 10 particles and 10 mesh points for each degree of freedom
and an
 ideal 1 petaflops machine (all operations take the same time), how long will it ta
ke to compute the above integral? Lifetime of the universe $T\approx 4.7 \times
10^{17}$s.

<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec301"></a></h2>

A crude approach consists in setting all weights equal 1, \( \omega_i=1 \).
With \( dx=h=(b-a)/N \) where \( b=1 \), \( a=0 \) in our case and \( h \) is the
step size. The integral

<p>&nbsp;<br>
$$
   I=\int_0^1 f(x)dx\approx \frac{1}{N}\sum_{i=1}^Nf(x_i),
$$
<p>&nbsp;<br>

can be rewritten using the concept of the average of the function \( f \) for a given PDF \( p(x) \) as

<p>&nbsp;<br>
$$
   E[f]=\langle f \rangle = \frac{1}{N}\sum_{i=1}^Nf(x_i)p(x_i),
$$
<p>&nbsp;<br>

and identify \( p(x) \) with the uniform distribution, viz
$ p(x)=1$ when \( x\in [0,1] \) and zero for all other values of \( x \).
The integral
is then  the average of \( f \) over the interval \( x \in [0,1] \)

<p>&nbsp;<br>
$$
      I=\int_0^1 f(x)dx\approx E[f]=\langle f \rangle.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec302"></a></h2>

In addition to the average value \( \langle f \rangle \) the other
important quantity in a
Monte-Carlo calculation is the variance \( \sigma^2 \) and
the standard deviation \( \sigma \). We define first the variance
of the integral with \( f \) for a uniform distribution in the interval
\( x \in [0,1] \) to be

<p>&nbsp;<br>
$$
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^N(f(x_i)-\langle f\rangle)^2p(x_i),
$$
<p>&nbsp;<br>

and inserting the uniform distribution this yields

<p>&nbsp;<br>
$$
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^Nf(x_i)^2-
  \left(\frac{1}{N}\sum_{i=1}^Nf(x_i)\right)^2,
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
  \sigma^2_f=E[f^2]-(E[f])^2=\left(\langle f^2\rangle -
                                 \langle f \rangle^2\right).
$$
<p>&nbsp;<br>

which is nothing but a measure of the extent to
which \( f \) deviates from its average over the region of integration.
The standard deviation is defined as the square root of the variance.

<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec303"></a></h2>

If we consider the above results for
a fixed value of \( N \) as a measurement,
we could however recalculate the
above average and variance for a series of different measurements.
If each such measumerent produces a set of averages for the
integral \( I \) denoted \( \langle f\rangle_l \), we have for \( M \) measurements
that the integral is given by

<p>&nbsp;<br>
$$
   \langle I \rangle_M=\frac{1}{M}\sum_{l=1}^{M}\langle f\rangle_l.
$$
<p>&nbsp;<br>

If we can consider the probability of
correlated events to be zero, we can rewrite
the variance of these series of measurements as (equating \( M=N \))

<p>&nbsp;<br>
$$
  \sigma^2_N\approx \frac{1}{N}\left(\langle f^2\rangle -
                                 \langle f \rangle^2\right)=\frac{\sigma^2_f}{N}.
$$
<p>&nbsp;<br>


<p>
We note that the standard deviation is proportional with the inverse square root of
the number of measurements

<p>&nbsp;<br>
$$
   \sigma_N \sim \frac{1}{\sqrt{N}}.
$$
<p>&nbsp;<br>

<em>The aim in Monte Carlo calculations is to have \( \sigma_N \) as small as possible after \( N \) samples. </em>
The results from one  sample represents,
since we are using concepts from statistics,
a 'measurement'.

<p>

</section>


<section>

<h2>But what do we gain by  Monte Carlo Integration?  <a name="___sec304"></a></h2>

<ul>
  <p><li> We saw that the trapezoidal rule carries a truncation error \( O(h^2) \), with \( h \) the step length.</li>

<ul>
   <p><li> Quadrature rules such as Newton-Cotes have a truncation error which goes like \( \sim O(h^k) \), with \( k \ge 1 \). Recalling that the step size is defined as \( h=(b-a)/N \), we have an error which goes like \( \sim N^{-k} \).</li>
</ul>
<p>

  <p><li> Monte Carlo integration is more efficient in higher dimensions. Assume that our integration volume is a hypercube with side \( L \) and dimension \( d \). This cube contains hence \( N=(L/h)^d \) points and therefore the error in the result scales as \( N^{-k/d} \) for the traditional methods.</li>
  <p><li> The error in the Monte carlo integration is however independent of \( d \) and scales as \( \sigma\sim 1/\sqrt{N} \), always!</li>
</ul>
<p>

  *
Comparing
this with traditional methods, shows that
Monte Carlo integration is more efficient than an order-k algorithm
when \( d>2k \)

<p>

</section>


<section>

<h2>Example: Acceptance-Rejection Method  <a name="___sec305"></a></h2>

This is a rather simple and appealing
method after von Neumann. Assume that we are looking at an interval
\( x\in [a,b] \), this being the domain of the PDF \( p(x) \). Suppose also that
the largest value our distribution function takes in this interval
is \( M \), that is

<p>&nbsp;<br>
$$
    p(x) \le M \quad  x\in [a,b].
$$
<p>&nbsp;<br>

Then we generate a random number \( x \) from the uniform distribution
for \( x\in [a,b] \) and a corresponding number \( s \) for the uniform
distribution between \( [0,M] \).
If

<p>&nbsp;<br>
$$
p(x) \ge s,
$$
<p>&nbsp;<br>

we accept the new value of \( x \), else we generate
again two new random numbers \( x \) and \( s \) and perform the test
in the latter equation again.

<p>

</section>


<section>

<h2>Acceptance-Rejection Method  <a name="___sec306"></a></h2>

As an example, consider the evaluation of the integral

<p>&nbsp;<br>
$$
   I=\int_0^3\exp{(x)}dx.
$$
<p>&nbsp;<br>

Obviously to derive it analytically is much easier, however the integrand could pose some more
difficult challenges. The aim here is simply to show how to implent the acceptance-rejection algorithm.
The integral is the area below the curve \( f(x)=\exp{(x)} \). If we uniformly fill the rectangle
spanned by \( x\in [0,3] \) and \( y\in [0,\exp{(3)}] \), the fraction below the curve obatained from a uniform distribution, and
multiplied by the area of the rectangle, should approximate the chosen integral. It is rather
easy to implement this numerically, as shown in the following code.

<p>

</section>


<section>

<h2>Simple Plot of the Accept-Reject Method  <a name="___sec307"></a></h2>

<!-- 2DOFIGURE: [acceptreject, width=500 frac=0.35] -->

<p>

</section>


<section>

<h2>Acceptance-Rejection Method  <a name="___sec308"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//   Loop over Monte Carlo trials n</span>
     integral =<span style="color: #B452CD">0.</span>;
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
<span style="color: #228B22">//   Finds a random value for x in the interval [0,3]</span>
          x = <span style="color: #B452CD">3</span>*ran0(&amp;idum);
<span style="color: #228B22">//   Finds y-value between [0,exp(3)]</span>
          y = exp(<span style="color: #B452CD">3.0</span>)*ran0(&amp;idum);
<span style="color: #228B22">//   if the value of y at exp(x) is below the curve, we accept</span>
<span style="color: #228B22">//   THIS IS OUR SAMPLING RULE</span>
          <span style="color: #8B008B; font-weight: bold">if</span> ( y  &lt; exp(x)) s = s+ <span style="color: #B452CD">1.0</span>;
<span style="color: #228B22">//   The integral is area enclosed below the line f(x)=exp(x)</span>

<span style="color: #228B22">//  Then we multiply with the area of the rectangle and</span>
<span style="color: #228B22">//  divide by the number of cycles</span>
    Integral = <span style="color: #B452CD">3.</span>*exp(<span style="color: #B452CD">3.</span>)*s/n
</pre></div>
<p>

</section>


<section>

<h2>Monte Carlo Integration  <a name="___sec309"></a></h2>

With uniform distribution \( p(x)=1 \) for \( x\in [0,1] \) and zero else

<p>&nbsp;<br>
$$
   I=\int_0^1 f(x)dx\approx \frac{1}{N}\sum_{i=1}^Nf(x_i),
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
      I=\int_0^1 f(x)dx\approx E[f]=\langle f \rangle.
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^Nf(x_i)^2-
  \left(\frac{1}{N}\sum_{i=1}^Nf(x_i)\right)^2,
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
  \sigma^2_f=E[f^2]-(E[f])^2=\left(\langle f^2\rangle -
                                 \langle f \rangle^2\right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Brute Force Algorithm for Monte Carlo Integration  <a name="___sec310"></a></h2>

<ul>
  <p><li> Choose the number of Monte Carlo samples \( N \).</li>
  <p><li> Make a loop over  \( N \) and for every step generate a random number
         \( x_i \) in the  interval \( x_i\in [0,1] \) by calling a random number generator.</li>
  <p><li> Use this number to compute \( f(x_i) \).</li>
  <p><li> Find the contribution to the variance and the mean value for every loop contribution.</li>
  <p><li> After \( N \) samplings, compute the final mean value and the standard deviation</li>
</ul>
<p>


</section>


<section>

<h2>Brute Force Integration  <a name="___sec311"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// crude mc function to calculate pi</span>
     <span style="color: #a7a7a7; font-weight: bold">int</span> i, n;
     <span style="color: #a7a7a7; font-weight: bold">long</span> idum;
     <span style="color: #a7a7a7; font-weight: bold">double</span> crude_mc, x, sum_sigma, fx, variance;
     cout &lt;&lt; <span style="color: #CD5555">&quot;Read in the number of Monte-Carlo samples&quot;</span> &lt;&lt; endl;
     cin &gt;&gt; n;
     crude_mc = sum_sigma=<span style="color: #B452CD">0.</span> ; idum=-<span style="color: #B452CD">1</span> ;
<span style="color: #228B22">//    evaluate the integral with the a crude Monte-Carlo method</span>
      <span style="color: #8B008B; font-weight: bold">for</span> ( i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
           x=ran0(&amp;idum);
           fx=func(x);
           crude_mc += fx;
           sum_sigma += fx*fx;

      crude_mc = crude_mc/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
      sum_sigma = sum_sigma/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
      variance=sum_sigma-crude_mc*crude_mc;
</pre></div>
<p>

</section>


<section>

<h2>Or: another Brute Force Integration  <a name="___sec312"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// crude mc function to calculate pi</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>()
{
  <span style="color: #8B008B; font-weight: bold">const</span> <span style="color: #a7a7a7; font-weight: bold">int</span> n = <span style="color: #B452CD">1000000</span>;
  <span style="color: #a7a7a7; font-weight: bold">double</span> x, fx, pi, invers_period, pi2;
  <span style="color: #a7a7a7; font-weight: bold">int</span> i;
  invers_period = <span style="color: #B452CD">1.</span>/RAND_MAX;
  srand(time(<span style="color: #658b00">NULL</span>));
  pi = pi2 = <span style="color: #B452CD">0.</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (i=<span style="color: #B452CD">0</span>; i&lt;n;i++)
    {
      x = <span style="color: #a7a7a7; font-weight: bold">double</span>(rand())*invers_period;
      <span style="color: #228B22">//   This is our sampling rule, all points accepted</span>
      fx = <span style="color: #B452CD">4.</span>/(<span style="color: #B452CD">1</span>+x*x);
      pi += fx;
      pi2 += fx*fx;

  pi /= n;  pi2 = pi2/n - pi*pi;
  cout &lt;&lt; <span style="color: #CD5555">&quot;pi=&quot;</span> &lt;&lt; pi &lt;&lt; <span style="color: #CD5555">&quot; sigma^2=&quot;</span> &lt;&lt; pi2 &lt;&lt; endl;
  <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
</pre></div>
<p>

</section>


<section>

<h2>Brute Force Integration  <a name="___sec313"></a></h2>

Note the call to a function which generates random numbers according to the uniform distribution

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">     <span style="color: #a7a7a7; font-weight: bold">long</span> idum;
     idum=-<span style="color: #B452CD">1</span> ;
     .....
     x=ran0(&amp;idum);
     ....
</pre></div>
<p>
or

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  ...
  invers_period = <span style="color: #B452CD">1.</span>/RAND_MAX;
  srand(time(<span style="color: #658b00">NULL</span>));
  ...
  x = <span style="color: #a7a7a7; font-weight: bold">double</span>(rand())*invers_period;
</pre></div>
<p>

</section>


<section>

<h2>Algorithm for Monte Carlo Integration, Results  <a name="___sec314"></a></h2>

\begin{tabular}{rll}\hline
$N$&$I$&$\sigma_N$\\\hline
10 &amp; 3.10263E+00 &amp; 3.98802E-01\\
100 &amp; 3.02933E+00 &amp; 4.04822E-01       \\
1000 &amp; 3.13395E+00 &amp; 4.22881E-01      \\
10000 &amp; 3.14195E+00 &amp; 4.11195E-01      \\
100000 &amp; 3.14003E+00 &amp; 4.14114E-01     \\
1000000&  3.14213E+00 &amp; 4.13838E-01   \\
10000000& 3.14177E+00 &amp; 4.13523E-01  \\
$10^{9}$& 3.14162E+00 &amp; 4.13581E-01  \\
\hline
\end{tabular}
We note that as \( N \) increases, the integral itself never reaches more than an agreement
to the fourth or fifth digit. The variance also oscillates around its exact value
\( 4.13581E-01 \). Note well that the variance need not be zero but can,
with appropriate redefinitions
of the integral be made smaller. A smaller variance yields also a smaller standard deviation.
This is the topic of importance sampling.

<p>

</section>


<section>

<h2>Transformation of Variables  <a name="___sec315"></a></h2>

The starting point is always the uniform distribution

<p>&nbsp;<br>
$$
p(x)dx=\left\{\begin{array}{cc} dx & 0 \le x \le 1\\
                                0  & else\end{array}\right.
$$
<p>&nbsp;<br>

with \( p(x)=1 \) and
satisfying

<p>&nbsp;<br>
$$
  \int_{-\infty}^{\infty}p(x)dx=1.
$$
<p>&nbsp;<br>

All random number generators provided in the program library
generate numbers in this domain.

<p>
When we attempt a
transformation to a new variable
\( x\rightarrow y \)
we have to conserve the probability

<p>&nbsp;<br>
$$
   p(y)dy=p(x)dx,
$$
<p>&nbsp;<br>

which for the uniform distribution implies

<p>&nbsp;<br>
$$
   p(y)dy=dx.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Transformation of Variables  <a name="___sec316"></a></h2>

Let us assume that \( p(y) \) is a  PDF different from the uniform
PDF \( p(x)=1 \) with \( x \in [0,1] \).
If we integrate the last expression we arrive at

<p>&nbsp;<br>
$$
   x(y)=\int_0^y p(y')dy',
$$
<p>&nbsp;<br>

which is nothing but the cumulative distribution of \( p(y) \), i.e.,

<p>&nbsp;<br>
$$
   x(y)=P(y)=\int_0^y p(y')dy'.
$$
<p>&nbsp;<br>


<p>
This is an important result which has consequences for eventual
improvements over the brute force Monte Carlo.

<p>

</section>


<section>

<h2>Example 1, a general Uniform Distribution  <a name="___sec317"></a></h2>

Suppose we have the general uniform distribution

<p>&nbsp;<br>
$$
p(y)dy=\left\{\begin{array}{cc} \frac{dy}{b-a} & a \le y \le b\\
                                0  & else\end{array}\right.
$$
<p>&nbsp;<br>

If we wish to relate this distribution to the one in the interval
\( x \in [0,1] \)
we have

<p>&nbsp;<br>
$$
   p(y)dy=\frac{dy}{b-a}=dx,
$$
<p>&nbsp;<br>

and integrating we obtain the cumulative function

<p>&nbsp;<br>
$$
   x(y)=\int_a^y \frac{dy'}{b-a},
$$
<p>&nbsp;<br>

yielding

<p>&nbsp;<br>
$$
    y=a+(b-a)x,
$$
<p>&nbsp;<br>

a well-known result!

<p>

</section>


<section>

<h2>Example 2, from Uniform to Exponential  <a name="___sec318"></a></h2>

Assume that

<p>&nbsp;<br>
$$
  p(y)=e^{-y},
$$
<p>&nbsp;<br>

which is the exponential distribution, important for the analysis
of e.g., radioactive decay. Again,
\( p(x) \) is given by the uniform distribution with
\( x \in [0,1] \), and
with the assumption that the probability is conserved we have

<p>&nbsp;<br>
$$
   p(y)dy=e^{-y}dy=dx,
$$
<p>&nbsp;<br>

which yields after integration

<p>&nbsp;<br>
$$
   x(y)=P(y)=\int_0^y \exp{(-y')}dy'=1-\exp{(-y)},
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
   y(x)=-ln(1-x).
$$
<p>&nbsp;<br>

This gives us the new random variable \( y \) in the domain
\( y \in [0,\infty) \)
determined through the random variable \( x \in [0,1] \) generated by
our favorite random generator.

<p>

</section>


<section>

<h2>Example 2, from Uniform to Exponential  <a name="___sec319"></a></h2>

This means that if we can factor out
\( \exp{(-y)} \) from an integrand we may have

<p>&nbsp;<br>
$$
   I=\int_0^{\infty}F(y)dy=\int_0^{\infty}\exp{(-y)}G(y)dy
$$
<p>&nbsp;<br>

which we rewrite as

<p>&nbsp;<br>
$$
  \int_0^{\infty}\exp{(-y)}G(y)dy=
   \int_0^{\infty}\frac{dx}{dy}G(y)dy\approx
   \frac{1}{N}\sum_{i=1}^NG(y(x_i)),
$$
<p>&nbsp;<br>

where \( x_i \) is a random number in the interval
[0,1].

<p>
Note that in practical implementations, our random number generators for the
uniform distribution never return exactly 0 or 1, but we we may come very close.
We  should thus in principle set \( x\in (0,1) \).

<p>

</section>


<section>

<h2>Example 2, from Uniform to Exponential  <a name="___sec320"></a></h2>

The algorithm is rather simple.
In the function which sets up the integral, we simply need
the random number generator for the uniform distribution
in order to obtain numbers
in the interval [0,1]. We obtain \( y \) by the taking the logarithm of
\( (1-x) \). Our calling function which sets up the new random
variable \( y \) may then include statements like

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">.....
idum=-<span style="color: #B452CD">1</span>;
x=ran0(&amp;idum);
y=-log(<span style="color: #B452CD">1.</span>-x);
.....
</pre></div>
<p>

</section>


<section>

<h2>Example 3  <a name="___sec321"></a></h2>

Another function which provides an example for a PDF is

<p>&nbsp;<br>
$$
   p(y)dy=\frac{dy}{(a+by)^n},
$$
<p>&nbsp;<br>

with \( n > 1 \). It is normalizable, positive definite, analytically
integrable and the integral is invertible, allowing thereby
the expression of a new variable in terms of the old one.
The integral

<p>&nbsp;<br>
$$
   \int_0^{\infty} \frac{dy}{(a+by)^n}=\frac{1}{(n-1)ba^{n-1}},
$$
<p>&nbsp;<br>

gives

<p>&nbsp;<br>
$$
   p(y)dy=\frac{(n-1)ba^{n-1}}{(a+by)^n}dy,
$$
<p>&nbsp;<br>

which in turn gives the cumulative function

<p>&nbsp;<br>
$$
   x(y)=P(y)=\int_0^y \frac{(n-1)ba^{n-1}}{(a+bx)^n}dy'=,
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
   y=\frac{a}{b}\left((1-x)^{-1/(n-1)}-1\right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Example 4, from Uniform to Normal  <a name="___sec322"></a></h2>

For the normal distribution, expressed here as

<p>&nbsp;<br>
$$
  g(x,y)=\exp{(-(x^2+y^2)/2)}dxdy.
$$
<p>&nbsp;<br>

it is rather difficult to find an inverse since the cumulative
distribution is given by the error function \( erf(x) \).

<p>
If we however switch to polar coordinates, we have
for \( x \) and \( y \)

<p>&nbsp;<br>
$$
   r=\left(x^2+y^2\right)^{1/2} \quad
   \theta =tan^{-1}\frac{x}{y},
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
  g(r,\theta)=r\exp{(-r^2/2)}drd\theta,
$$
<p>&nbsp;<br>

where the angle \( \theta \) could be given by a uniform
distribution in the region \( [0,2\pi] \).
Following example 1 above, this implies simply
multiplying random numbers
\( x\in [0,1] \) by \( 2\pi \).

<p>

</section>


<section>

<h2>Example 4, from Uniform to Normal  <a name="___sec323"></a></h2>

The variable
\( r \), defined for \( r \in [0,\infty) \) needs to be related to
to random numbers \( x'\in [0,1] \). To achieve that, we introduce a new variable

<p>&nbsp;<br>
$$
   u=\frac{1}{2}r^2,
$$
<p>&nbsp;<br>

and define a PDF

<p>&nbsp;<br>
$$
  \exp{(-u)}du,
$$
<p>&nbsp;<br>

with \( u\in [0,\infty) \).
Using the results from example 2, we have that

<p>&nbsp;<br>
$$
   u=-ln(1-x'),
$$
<p>&nbsp;<br>

where \( x' \) is a random number generated for \( x'\in [0,1] \).
With

<p>&nbsp;<br>
$$
  x=r\cos(\theta)=\sqrt{2u}\cos(\theta),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
  y=r\sin(\theta)=\sqrt{2u}\sin(\theta),
$$
<p>&nbsp;<br>

we can obtain new random numbers \( x,y \) through

<p>&nbsp;<br>
$$
  x=\sqrt{-2ln(1-x')}\cos(\theta),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
  y=\sqrt{-2ln(1-x')}\sin(\theta),
$$
<p>&nbsp;<br>

with \( x'\in [0,1] \) and \( \theta \in 2\pi [0,1] \).

<p>

</section>


<section>

<h2>Example 4, from Uniform to Normal  <a name="___sec324"></a></h2>

A function which yields such random numbers for the normal
distribution would include statements like

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">.....
idum=-<span style="color: #B452CD">1</span>;
radius=sqrt(-<span style="color: #B452CD">2</span>*ln(<span style="color: #B452CD">1.</span>-ran0(idum)));
theta=<span style="color: #B452CD">2</span>*pi*ran0(idum);
x=radius*cos(theta);
y=radius*sin(theta);
.....
</pre></div>
<p>

</section>


<section>

<h2>Box-Mueller Method for Normal Deviates  <a name="___sec325"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// random numbers with gaussian distribution</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">gaussian_deviate</span>(<span style="color: #a7a7a7; font-weight: bold">long</span> * idum)
{
  <span style="color: #8B008B; font-weight: bold">static</span> <span style="color: #a7a7a7; font-weight: bold">int</span> iset = <span style="color: #B452CD">0</span>;
  <span style="color: #8B008B; font-weight: bold">static</span> <span style="color: #a7a7a7; font-weight: bold">double</span> gset;
  <span style="color: #a7a7a7; font-weight: bold">double</span> fac, rsq, v1, v2;
  <span style="color: #8B008B; font-weight: bold">if</span> ( idum &lt; <span style="color: #B452CD">0</span>) iset =<span style="color: #B452CD">0</span>;
  <span style="color: #8B008B; font-weight: bold">if</span> (iset == <span style="color: #B452CD">0</span>) {
    <span style="color: #8B008B; font-weight: bold">do</span> {
      v1 = <span style="color: #B452CD">2.</span>*ran0(idum) -<span style="color: #B452CD">1.0</span>;
      v2 = <span style="color: #B452CD">2.</span>*ran0(idum) -<span style="color: #B452CD">1.0</span>;
      rsq = v1*v1+v2*v2;
    } <span style="color: #8B008B; font-weight: bold">while</span> (rsq &gt;= <span style="color: #B452CD">1.0</span> || rsq == <span style="color: #B452CD">0.</span>);
    fac = sqrt(-<span style="color: #B452CD">2.</span>*log(rsq)/rsq);
    gset = v1*fac;
    iset = <span style="color: #B452CD">1</span>;
    <span style="color: #8B008B; font-weight: bold">return</span> v2*fac;
  } <span style="color: #8B008B; font-weight: bold">else</span> {
    iset =<span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">return</span> gset;
</pre></div>
<p>

</section>


<section>

<h2>Importance Sampling, chapter 11.4  <a name="___sec326"></a></h2>

With the aid of the above variable transformations we address now
one of the most widely used approaches to Monte Carlo integration,
namely importance sampling.

<p>
Let us assume that  \( p(y) \) is a PDF whose behavior resembles that of a function
\( F \) defined in a certain interval \( [a,b] \). The normalization condition is

<p>&nbsp;<br>
$$
   \int_a^bp(y)dy=1.
$$
<p>&nbsp;<br>

We can rewrite our integral as

<p>&nbsp;<br>
$$
\begin{equation}
   I=\int_a^b F(y) dy =\int_a^b p(y)\frac{F(y)}{p(y)} dy.
   \tag{27}
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Importance Sampling  <a name="___sec327"></a></h2>

Since random numbers are generated for the uniform distribution \( p(x) \)
with \( x\in [0,1] \), we need to perform a change of variables \( x\rightarrow y \)
through

<p>&nbsp;<br>
$$
     x(y)=\int_a^y p(y')dy',
$$
<p>&nbsp;<br>

where we used

<p>&nbsp;<br>
$$
   p(x)dx=dx=p(y)dy.
$$
<p>&nbsp;<br>

If we can invert \( x(y) \), we find \( y(x) \) as well.

<p>

</section>


<section>

<h2>Importance Sampling  <a name="___sec328"></a></h2>

With this change of variables we can express the integral of
Eq. <a href="#mjx-eqn-27">(27)</a> as

<p>&nbsp;<br>
$$
   I=\int_a^b p(y)\frac{F(y)}{p(y)} dy=\int_a^b\frac{F(y(x))}{p(y(x))} dx,
$$
<p>&nbsp;<br>

meaning that a Monte Carlo evalutaion of the above integral gives

<p>&nbsp;<br>
$$
\int_a^b\frac{F(y(x))}{p(y(x))} dx=
\frac{1}{N}\sum_{i=1}^N\frac{F(y(x_i))}{p(y(x_i))}.
$$
<p>&nbsp;<br>

The advantage of such a change of variables in case \( p(y) \) follows
closely \( F \) is that the integrand becomes smooth and we can sample
over relevant values for the integrand. It is however not trivial
to find such a function \( p \).
The conditions on \( p \) which allow us to perform these transformations
are

<ul>
  <p><li> \( p \) is normalizable and positive definite,</li>
  <p><li> it is analytically integrable and</li>
  <p><li> the integral is invertible, allowing us thereby to express a new variable in terms of the old one.</li>
</ul>
<p>


</section>


<section>

<h2>Importance Sampling  <a name="___sec329"></a></h2>

The algorithm for this procedure is

<p>
Use the uniform distribution to find the random variable
  \( y \) in the interval [0,1]. \( p(x) \) is a user provided PDF.

<p>
Evaluate thereafter

<p>&nbsp;<br>
$$
   I=\int_a^b F(x) dx =\int_a^b p(x)\frac{F(x)}{p(x)} dx,
$$
<p>&nbsp;<br>

  by rewriting

<p>&nbsp;<br>
$$
   \int_a^b p(x)\frac{F(x)}{p(x)} dx =
   \int_a^b\frac{F(x(y))}{p(x(y))} dy,
$$
<p>&nbsp;<br>

since

<p>&nbsp;<br>
$$
   \frac{dy}{dx}=p(x).
$$
<p>&nbsp;<br>


<p>
Perform then a Monte Carlo sampling for

<p>&nbsp;<br>
$$
\int_a^b\frac{F(x(y))}{p(x(y))} dy,\approx
 \frac{1}{N}\sum_{i=1}^N\frac{F(x(y_i))}{p(x(y_i))},
$$
<p>&nbsp;<br>

with \( y_i\in [0,1] \),

<p>
Evaluate the variance

<p>

</section>


<section>

<h2>Demonstration of Importance Sampling  <a name="___sec330"></a></h2>

<p>&nbsp;<br>
$$
   I=\int_0^1 F(x) dx = \int_0^1 \frac{1}{1+x^2} dx = \frac{\pi}{4}.
$$
<p>&nbsp;<br>


<p>
We choose the following PDF (which follows closely the function to
integrate)

<p>&nbsp;<br>
$$
   p(x)=\frac{1}{3}\left(4-2x\right) \quad \int_0^1p(x)dx=1,
$$
<p>&nbsp;<br>

resulting

<p>&nbsp;<br>
$$
   \frac{F(0)}{p(0)}=\frac{F(1)}{p(1)}=\frac{3}{4}.
$$
<p>&nbsp;<br>

Check that it fullfils the requirements of a PDF.
We perform then the change of variables (via the Cumulative function)

<p>&nbsp;<br>
$$
     y(x)=\int_0^x p(x')dx'=\frac{1}{3}x\left(4-x\right),
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
   x=2-\left(4-3y\right)^{1/2}
$$
<p>&nbsp;<br>

We have that when \( y=0 \) then  \( x=0 \) and when  \( y=1 \) we have \( x=1 \).

<p>

</section>


<section>

<h2>Simple Code  <a name="___sec331"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//   evaluate the integral with importance sampling</span>
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
       x = ran0(&amp;idum);  <span style="color: #228B22">// random numbers in [0,1]</span>
       y = <span style="color: #B452CD">2</span> - sqrt(<span style="color: #B452CD">4</span>-<span style="color: #B452CD">3</span>*x);  <span style="color: #228B22">// new random numbers</span>
       fy=<span style="color: #B452CD">3</span>*func(y)/(<span style="color: #B452CD">4</span>-<span style="color: #B452CD">2</span>*y); <span style="color: #228B22">// weighted function</span>
       int_mc += fy;
       sum_sigma += fy*fy;

     int_mc = int_mc/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     sum_sigma = sum_sigma/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     variance=(sum_sigma-int_mc*int_mc);
</pre></div>
<p>

</section>


<section>

<h2>Test Runs and Comparison with Brute Force for \( \pi=3.14159 \)  <a name="___sec332"></a></h2>

The suffix \( cr \) stands for the brute force approach
while \( is \) stands for the use of importance sampling.
All calculations use ran0 as function to generate the uniform distribution.

<p>
\begin{tabular}{rllll}\hline
$N$&$I_{cr}$ &$\sigma_{cr}$   &$I_{is}$  &$\sigma_{is}$\\\hline
10000 &amp; 3.13395E+00 &amp; 4.22881E-01 &amp; 3.14163E+00 &amp; 6.49921E-03 \\
100000 &amp; 3.14195E+00&  4.11195E-01 &amp; 3.14163E+00 &amp; 6.36837E-03 \\
1000000& 3.14003E+00 &amp; 4.14114E-01 &amp; 3.14128E+00&  6.39217E-03\\
10000000 &3.14213E+00 &amp; 4.13838E-01 &amp; 3.14160E+00 &amp; 6.40784E-03 \\
\hline
\end{tabular}

<p>
However, it is unfair to study one-dimensional integrals with MC methods!

<p>

</section>


<section>

<h2>Multidimensional Integrals  <a name="___sec333"></a></h2>

When we deal with multidimensional integrals of the form

<p>&nbsp;<br>
$$
   I=\int_0^1dx_1\int_0^1dx_2\dots \int_0^1dx_d g(x_1,\dots,x_d),
$$
<p>&nbsp;<br>

with
\( x_i \) defined in the interval  \( [a_i,b_i] \) we would typically
need a transformation
of variables of the form

<p>&nbsp;<br>
$$
   x_i=a_i+(b_i-a_i)t_i,
$$
<p>&nbsp;<br>

if we were to use the uniform distribution on the interval \( [0,1] \).
In this case, we need a
Jacobi determinant

<p>&nbsp;<br>
$$
  \prod_{i=1}^d (b_i-a_i),
$$
<p>&nbsp;<br>

and to convert the function \( g(x_1,\dots,x_d) \) to

<p>&nbsp;<br>
$$
   g(x_1,\dots,x_d)\rightarrow
   g(a_1+(b_1-a_1)t_1,\dots,a_d+(b_d-a_d)t_d).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Example: 6-dimensional Integral  <a name="___sec334"></a></h2>

Consider the following six-dimensional
integral

<p>&nbsp;<br>
$$
   \int_{-\infty}^{\infty}{\bf dxdy}g({\bf x, y}),
$$
<p>&nbsp;<br>

where

<p>&nbsp;<br>
$$
  g({\bf x, y})=\exp{(-{\bf x}^2-{\bf y}^2-({\bf x-y})^2/2)},
$$
<p>&nbsp;<br>

with  \( d=6 \).

<p>

</section>


<section>

<h2>Example: 6-dimensional Integral  <a name="___sec335"></a></h2>

We can solve this integral by employing our brute force scheme,
or using importance sampling and random variables distributed
according to a gaussian PDF. For the latter, if we set
the mean value
\( \mu=0 \) and the standard deviation  \( \sigma=1/\sqrt{2} \), we have

<p>&nbsp;<br>
$$
   \frac{1}{\sqrt{\pi}}\exp{(-x^2)},
$$
<p>&nbsp;<br>

and through

<p>&nbsp;<br>
$$
   \pi^3\int\prod_{i=1}^6\left(
    \frac{1}{\sqrt{\pi}}\exp{(-x_i^2)}\right)
    \exp{(-({\bf x-y})^2/2)}dx_1.\dots dx_6,
$$
<p>&nbsp;<br>

we can rewrite our integral as

<p>&nbsp;<br>
$$
   \int f(x_1,\dots,x_d)F(x_1,\dots,x_d)\prod_{i=1}^6dx_i,
$$
<p>&nbsp;<br>

where \( f \) is the gaussian distribution.

<p>

</section>


<section>

<h2>Brute Force I  <a name="___sec336"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">.....
<span style="color: #228B22">//   evaluate the integral without importance sampling</span>
<span style="color: #228B22">//   Loop over Monte Carlo Cycles</span>
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
<span style="color: #228B22">//   x[] contains the random numbers for all dimensions</span>
       <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j&lt; <span style="color: #B452CD">6</span>; j++) {
           x[j]=-length+<span style="color: #B452CD">2</span>*length*ran0(&amp;idum);

       fx=brute_force_MC(x);
       int_mc += fx;
       sum_sigma += fx*fx;

     int_mc = int_mc/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     sum_sigma = sum_sigma/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     variance=sum_sigma-int_mc*int_mc;
......
</pre></div>
<p>

</section>


<section>

<h2>Brute Force II  <a name="___sec337"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">double</span>  <span style="color: #008b45">brute_force_MC</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *x)
{
   <span style="color: #a7a7a7; font-weight: bold">double</span> a = <span style="color: #B452CD">1.</span>; <span style="color: #a7a7a7; font-weight: bold">double</span> b = <span style="color: #B452CD">0.5</span>;
<span style="color: #228B22">// evaluate the different terms of the exponential</span>
   <span style="color: #a7a7a7; font-weight: bold">double</span> xx=x[<span style="color: #B452CD">0</span>]*x[<span style="color: #B452CD">0</span>]+x[<span style="color: #B452CD">1</span>]*x[<span style="color: #B452CD">1</span>]+x[<span style="color: #B452CD">2</span>]*x[<span style="color: #B452CD">2</span>];
   <span style="color: #a7a7a7; font-weight: bold">double</span> yy=x[<span style="color: #B452CD">3</span>]*x[<span style="color: #B452CD">3</span>]+x[<span style="color: #B452CD">4</span>]*x[<span style="color: #B452CD">4</span>]+x[<span style="color: #B452CD">5</span>]*x[<span style="color: #B452CD">5</span>];
   <span style="color: #a7a7a7; font-weight: bold">double</span> xy=pow((x[<span style="color: #B452CD">0</span>]-x[<span style="color: #B452CD">3</span>]),<span style="color: #B452CD">2</span>)+pow((x[<span style="color: #B452CD">1</span>]-x[<span style="color: #B452CD">4</span>]),<span style="color: #B452CD">2</span>)+pow((x[<span style="color: #B452CD">2</span>]-x[<span style="color: #B452CD">5</span>]),<span style="color: #B452CD">2</span>);
   <span style="color: #8B008B; font-weight: bold">return</span> exp(-a*xx-a*yy-b*xy);
</pre></div>
<p>

</section>


<section>

<h2>Importance Sampling I  <a name="___sec338"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">..........
<span style="color: #228B22">//   evaluate the integral with importance sampling</span>
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
<span style="color: #228B22">//   x[] contains the random numbers for all dimensions</span>
       <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; <span style="color: #B452CD">6</span>; j++) {
	 x[j] = gaussian_deviate(&amp;idum)*sqrt2;

       fx=gaussian_MC(x);
       int_mc += fx;
       sum_sigma += fx*fx;

     int_mc = int_mc/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     sum_sigma = sum_sigma/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     variance=sum_sigma-int_mc*int_mc;
.............
</pre></div>
<p>

</section>


<section>

<h2>Importance Sampling II  <a name="___sec339"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// this function defines the integrand to integrate</span>

<span style="color: #a7a7a7; font-weight: bold">double</span>  <span style="color: #008b45">gaussian_MC</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *x)
{
   <span style="color: #a7a7a7; font-weight: bold">double</span> a = <span style="color: #B452CD">0.5</span>;
<span style="color: #228B22">// evaluate the different terms of the exponential</span>
   <span style="color: #a7a7a7; font-weight: bold">double</span> xy=pow((x[<span style="color: #B452CD">0</span>]-x[<span style="color: #B452CD">3</span>]),<span style="color: #B452CD">2</span>)+pow((x[<span style="color: #B452CD">1</span>]-x[<span style="color: #B452CD">4</span>]),<span style="color: #B452CD">2</span>)+pow((x[<span style="color: #B452CD">2</span>]-x[<span style="color: #B452CD">5</span>]),<span style="color: #B452CD">2</span>);
   <span style="color: #8B008B; font-weight: bold">return</span> exp(-a*xy);
} <span style="color: #228B22">// end function for the integrand</span>
</pre></div>
<p>

</section>


<section>

<h2>Test Runs for six-dimensional Integral  <a name="___sec340"></a></h2>

Results for as function of number of Monte Carlo samples \( N \).
The exact answer is \( I\approx 10.9626 \) for the integral.
The suffix \( cr \) stands for the brute force approach
while \( gd \) stands for the use of a Gaussian distribution function.
All calculations use ran0 as function to generate the uniform distribution.

<p>
\begin{tabular}{rll}\hline
$N$&$I_{cr}$&$I_{gd}$\\\hline
10000 &amp; 1.15247E+01& 1.09128E+01   \\
100000 &amp; 1.29650E+01 &amp; 1.09522E+01   \\
1000000& 1.18226E+01 &amp; 1.09673E+01    \\
10000000&1.04925E+01 &amp; 1.09612E+01  \\
\hline
\end{tabular}

<p>

</section>


<section>

<h1>Overview of week 45  <a name="___sec341"></a></h1>


</section>


<section>

<h2>Overview of week 45  <a name="___sec342"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Monte Carlo methods.</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> More on importance sampling and multi-dimensional integrals</li>
  <p><li> Central limit theorem and discussion on variance, covariance and standard deviation</li>
  <p><li> Tuesday:</li>
  <p><li> Central limit theorem and discussion on variance, covariance and standard deviation</li>
  <p><li> Random number generators (RNG).</li>
  <p><li> Random walks, Markov chains, Diffusion and Master equation</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Monte Carlo Keywords  <a name="___sec343"></a></h2>

Consider it is a numerical experiment

<ul>
  <p><li> Be able to generate random variables following a given probability distribution function PDF</li>
  <p><li> Find a probability distribution function (PDF).</li>
  <p><li> Sampling rule for accepting a move</li>
  <p><li> Compute standard deviation and other expectation values</li>
  <p><li> Techniques for improving errors Enhances algorithmic thinking!</li>
</ul>
<p>


</section>


<section>

<h2>Example, six-dimensional integral (project 3 2012)  <a name="___sec344"></a></h2>

The task of this project is to integrate in a brute force manner a six-dimensional integral which is used
to determine the ground state correlation energy between two electrons
in a helium atom.  We will employ both Gaussian quadrature and Monte-Carlo integration.
We assume that the wave function of each electron can be modelled like the single-particle
wave function of an electron in the hydrogen atom. The single-particle wave function  for an electron \( i \) in the
\( 1s \) state
is given in terms of a dimensionless variable    (the wave function is not properly normalized)

<p>&nbsp;<br>
$$
   {\bf r}_i =  x_i {\bf e}_x + y_i {\bf e}_y +z_i {\bf e}_z ,
$$
<p>&nbsp;<br>

as

<p>&nbsp;<br>
$$
   \psi_{1s}({\bf r}_i)  =   e^{-\alpha r_i},
$$
<p>&nbsp;<br>

where \( \alpha \) is a parameter and

<p>&nbsp;<br>
$$
r_i = \sqrt{x_i^2+y_i^2+z_i^2}.
$$
<p>&nbsp;<br>

We will fix \( \alpha=2 \), which should correspond to the charge of the helium atom \( Z=2 \).

<p>

</section>


<section>

<h2>Switch to spherical coordinates  <a name="___sec345"></a></h2>

Useful to change to spherical coordinates

<p>&nbsp;<br>
$$
   d{\bf r}_1d{\bf r}_2  = r_1^2dr_1 r_2^2dr_2 dcos(\theta_1)dcos(\theta_2)d\phi_1d\phi_2,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   \frac{1}{r_{12}}= \frac{1}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
\cos(\beta) = \cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>How do I do importance sampling in spherical coordinates  <a name="___sec346"></a></h2>

\( r_{1,2} \in [0,\infty) \), here we use the mapping \( r_{1,2}=-ln(1-ran) \)
with \( ran \in [0,1] \), a uniform distribution point.

<p>
\( \theta_{1,2} \in [0,\pi] \), use mapping \( \theta_{1,2}=\pi*ran \)
with \( ran \in [0,1] \) a uniform distribution point.

<p>
\( \phi_{1,2} \in [0,2\pi] \), use mapping \( \phi_{1,2}=2\pi*ran \)
with \( ran \in [0,1] \) a uniform distribution point.

<p>
Be careful with the integrand

<p>&nbsp;<br>
$$
\frac{\exp{(-4(r_1+r_2))}r_1^2dr_1 r_2^2dr_2 dcos(\theta_1)dcos(\theta_2)d\phi_1d\phi_2}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Example, six-dimensional integral (project 3 2012)  <a name="___sec347"></a></h2>

The ansatz for the wave function for two electrons is then given by the product of two
\( 1s \) wave functions as

<p>&nbsp;<br>
$$
   \Psi({\bf r}_1,{\bf r}_2)  =   e^{-\alpha (r_1+r_2)}.
$$
<p>&nbsp;<br>

Note that it is not possible to find an analytic solution to Schr\"odinger's equation for
two interacting electrons in the helium atom.

<p>
The integral we need to solve is the quantum mechanical expectation value of the correlation
energy between two electrons, namely

<p>&nbsp;<br>
$$
\begin{equation}label{eq:correlationenergy}
   \langle \frac{1}{|{\bf r}_1-{\bf r}_2|} \rangle =
   \int_{-\infty}^{\infty} d{\bf r}_1d{\bf r}_2  e^{-2\alpha (r_1+r_2)}\frac{1}{|{\bf r}_1-{\bf r}_2|}=\frac{5\pi^2}{16^2}=0.192765711.
\end{equation}
$$
<p>&nbsp;<br>

Note that our wave function is not normalized. There is a normalization factor missing, but for this project
we don't need to worry about that.

<p>

</section>


<section>

<h2>Example, six-dimensional integral (project 3 2012)  <a name="___sec348"></a></h2>

<ul>
  <p><li> a-b) Use Gaussian  quadrature and compute the integral by integrating</li>
</ul>
<p>

for each variable \( x_1,y_1,z_1,x_2,y_2,z_2 \) from \( -\infty \) to \( \infty \).
How many mesh points do you need before the results converges at the level of the fourth leading digit?  Hint:  the single-particle wave function \( e^{-\alpha r_i} \)  is more or less zero at \( r_i \approx ? \).  You can therefore replace the integration limits \( -\infty \) and \( \infty \) with \( -\Lambda \) and \( \Lambda \), respectively.  You need to check that this approximation is satisfactory.

<ul>
  <p><li> c) Compute the same integral but now with brute force Monte Carlo</li>
</ul>
<p>

and compare your results with those from the previous point. Discuss the differences. With bruce force we mean that you should use the uniform distribution.

<ul>
  <p><li> d) Improve your brute force Monte Carlo calculation by using importance sampling. Hint: use the exponential distribution.</li>
</ul>
<p>

Does the variance decrease? Does the CPU time used compared with the brute force
Monte Carlo decrease in order to achieve the same accuracy? Comment your results.

<p>

</section>


<section>

<h2>Example, six-dimensional integral, Gauss-Legendre  <a name="___sec349"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">     <span style="color: #a7a7a7; font-weight: bold">double</span> *x = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [N];
     <span style="color: #a7a7a7; font-weight: bold">double</span> *w = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span> [N];
<span style="color: #228B22">//   set up the mesh points and weights</span>
     gauleg(a,b,x,w, N);

<span style="color: #228B22">//   evaluate the integral with the Gauss-Legendre method</span>
<span style="color: #228B22">//   Note that we initialize the sum</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> int_gauss = <span style="color: #B452CD">0.</span>;
     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">0</span>;i&lt;N;i++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>;j&lt;N;j++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> k = <span style="color: #B452CD">0</span>;k&lt;N;k++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> l = <span style="color: #B452CD">0</span>;l&lt;N;l++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> m = <span style="color: #B452CD">0</span>;m&lt;N;m++){
	     <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> n = <span style="color: #B452CD">0</span>;n&lt;N;n++){
        int_gauss+=w[i]*w[j]*w[k]*w[l]*w[m]*w[n]
       *int_function(x[i],x[j],x[k],x[l],x[m],x[n]);
     		}}}}}
	}
</pre></div>
<p>

</section>


<section>

<h2>Example, six-dimensional integral, Gauss-Legendre  <a name="___sec350"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  this function defines the function to integrate</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">int_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> x1, <span style="color: #a7a7a7; font-weight: bold">double</span> y1, <span style="color: #a7a7a7; font-weight: bold">double</span> z1,
                    <span style="color: #a7a7a7; font-weight: bold">double</span> x2, <span style="color: #a7a7a7; font-weight: bold">double</span> y2, <span style="color: #a7a7a7; font-weight: bold">double</span> z2)
{
   <span style="color: #a7a7a7; font-weight: bold">double</span> alpha = <span style="color: #B452CD">2.</span>;
<span style="color: #228B22">// evaluate the different terms of the exponential</span>
   <span style="color: #a7a7a7; font-weight: bold">double</span> exp1=-<span style="color: #B452CD">2</span>*alpha*sqrt(x1*x1+y1*y1+z1*z1);
   <span style="color: #a7a7a7; font-weight: bold">double</span> exp2=-<span style="color: #B452CD">2</span>*alpha*sqrt(x2*x2+y2*y2+z2*z2);
   <span style="color: #a7a7a7; font-weight: bold">double</span> deno=sqrt(pow((x1-x2),<span style="color: #B452CD">2</span>)+pow((y1-y2),<span style="color: #B452CD">2</span>)+pow((z1-z2),<span style="color: #B452CD">2</span>));
  <span style="color: #8B008B; font-weight: bold">if</span>(deno &lt;pow(<span style="color: #B452CD">10.</span>,-<span style="color: #B452CD">6.</span>)) { <span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;}
  <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #8B008B; font-weight: bold">return</span> exp(exp1+exp2)/deno;
} <span style="color: #228B22">// end of function to evaluate</span>
</pre></div>
<p>

</section>


<section>

<h2>Example, six-dimensional integral, brute force MC  <a name="___sec351"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">     <span style="color: #a7a7a7; font-weight: bold">double</span> int_mc = <span style="color: #B452CD">0.</span>;  <span style="color: #a7a7a7; font-weight: bold">double</span> variance = <span style="color: #B452CD">0.</span>;
     <span style="color: #a7a7a7; font-weight: bold">double</span> sum_sigma= <span style="color: #B452CD">0.</span> ; <span style="color: #a7a7a7; font-weight: bold">long</span> idum=-<span style="color: #B452CD">1</span> ;
     <span style="color: #a7a7a7; font-weight: bold">double</span> length=<span style="color: #B452CD">1.5</span>; <span style="color: #228B22">// we fix the max size of the box to L=3</span>
     <span style="color: #a7a7a7; font-weight: bold">double</span> volume=pow((<span style="color: #B452CD">2</span>*length),<span style="color: #B452CD">6.</span>);

<span style="color: #228B22">//   evaluate the integral with importance sampling</span>
     <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
<span style="color: #228B22">//   x[] contains the random numbers for all dimensions</span>
       <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j&lt; <span style="color: #B452CD">6</span>; j++) {
           <span style="color: #228B22">// Maps U[0,1] to U[-L,L]</span>
           x[j]=-length+<span style="color: #B452CD">2</span>*length*ran0(&amp;idum);

       fx=brute_force_MC(x);
       int_mc += fx;
       sum_sigma += fx*fx;

     int_mc = int_mc/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     sum_sigma = sum_sigma/((<span style="color: #a7a7a7; font-weight: bold">double</span>) n );
     variance=sum_sigma-int_mc*int_mc;
     ....
</pre></div>
<p>

</section>


<section>

<h2>Example, six-dimensional integral, brute force MC  <a name="___sec352"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">brute_force_MC</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *x)
{
   <span style="color: #a7a7a7; font-weight: bold">double</span> alpha = <span style="color: #B452CD">2.</span>;
<span style="color: #228B22">// evaluate the different terms of the exponential</span>
   <span style="color: #a7a7a7; font-weight: bold">double</span> exp1=-<span style="color: #B452CD">2</span>*alpha*sqrt(x[<span style="color: #B452CD">0</span>]*x[<span style="color: #B452CD">0</span>]+x[<span style="color: #B452CD">1</span>]*x[<span style="color: #B452CD">1</span>]+x[<span style="color: #B452CD">2</span>]*x[<span style="color: #B452CD">2</span>]);
   <span style="color: #a7a7a7; font-weight: bold">double</span> exp2=-<span style="color: #B452CD">2</span>*alpha*sqrt(x[<span style="color: #B452CD">3</span>]*x[<span style="color: #B452CD">3</span>]+x[<span style="color: #B452CD">4</span>]*x[<span style="color: #B452CD">4</span>]+x[<span style="color: #B452CD">5</span>]*x[<span style="color: #B452CD">5</span>]);
   <span style="color: #a7a7a7; font-weight: bold">double</span> deno=sqrt(pow((x[<span style="color: #B452CD">0</span>]-x[<span style="color: #B452CD">3</span>]),<span style="color: #B452CD">2</span>)
          +pow((x[<span style="color: #B452CD">1</span>]-x[<span style="color: #B452CD">4</span>]),<span style="color: #B452CD">2</span>)+pow((x[<span style="color: #B452CD">2</span>]-x[<span style="color: #B452CD">5</span>]),<span style="color: #B452CD">2</span>));
   <span style="color: #a7a7a7; font-weight: bold">double</span> value=exp(exp1+exp2)/deno;
	<span style="color: #8B008B; font-weight: bold">return</span> value;
} <span style="color: #228B22">// end function for the integrand</span>
</pre></div>
<p>

</section>


<section>

<h2>Example, six-dimensional integral, importance sampling  <a name="___sec353"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">	<span style="color: #a7a7a7; font-weight: bold">double</span> int_mc = <span style="color: #B452CD">0.</span>;  <span style="color: #a7a7a7; font-weight: bold">double</span> variance = <span style="color: #B452CD">0.</span>;
	<span style="color: #a7a7a7; font-weight: bold">double</span> sum_sigma= <span style="color: #B452CD">0.</span> ; <span style="color: #a7a7a7; font-weight: bold">long</span> idum=-<span style="color: #B452CD">1</span> ;
<span style="color: #228B22">// The &#39;volume&#39; contains 4 jacobideterminants(pi,pi,2pi,2pi)</span>
<span style="color: #228B22">// and a scaling factor 1/16</span>
	<span style="color: #a7a7a7; font-weight: bold">double</span> volume=<span style="color: #B452CD">4</span>*pow(acos(-<span style="color: #B452CD">1.</span>),<span style="color: #B452CD">4.</span>)*<span style="color: #B452CD">1.</span>/<span style="color: #B452CD">16</span>;
<span style="color: #228B22">//   evaluate the integral with importance sampling</span>
	<span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">1</span>;  i &lt;= n; i++){
	   <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">0</span>; j &lt; <span style="color: #B452CD">2</span>; j++) {
		y=ran0(&amp;idum);
		x[j]=-<span style="color: #B452CD">0.25</span>*log(<span style="color: #B452CD">1.</span>-y);
 	   }
	   <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">2</span>; j &lt; <span style="color: #B452CD">4</span>; j++) {
		x[j] = <span style="color: #B452CD">2</span>*acos(-<span style="color: #B452CD">1.</span>)*ran0(&amp;idum);
	   }
	   <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">4</span>; j &lt; <span style="color: #B452CD">6</span>; j++) {
			 x[j] = acos(-<span style="color: #B452CD">1.</span>)*ran0(&amp;idum);
  	   }
	fx=integrand_MC(x);
        ....
</pre></div>
<p>

</section>


<section>

<h2>Example, six-dimensional integral, importance sampling  <a name="___sec354"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// this function defines the integrand to integrate</span>

<span style="color: #a7a7a7; font-weight: bold">double</span>  <span style="color: #008b45">integrand_MC</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> *x)
{
<span style="color: #a7a7a7; font-weight: bold">double</span> num=x[<span style="color: #B452CD">0</span>]*x[<span style="color: #B452CD">0</span>]*x[<span style="color: #B452CD">1</span>]*x[<span style="color: #B452CD">1</span>]*sin(x[<span style="color: #B452CD">4</span>])*sin(x[<span style="color: #B452CD">5</span>]);
<span style="color: #a7a7a7; font-weight: bold">double</span> deno=sqrt(x[<span style="color: #B452CD">0</span>]*x[<span style="color: #B452CD">0</span>]+x[<span style="color: #B452CD">1</span>]*x[<span style="color: #B452CD">1</span>]-<span style="color: #B452CD">2</span>*x[<span style="color: #B452CD">0</span>]*x[<span style="color: #B452CD">1</span>]*
(sin(x[<span style="color: #B452CD">4</span>])*sin(x[<span style="color: #B452CD">5</span>])*cos(x[<span style="color: #B452CD">2</span>]-x[<span style="color: #B452CD">3</span>])+cos(x[<span style="color: #B452CD">4</span>])*cos(x[<span style="color: #B452CD">5</span>])));
<span style="color: #8B008B; font-weight: bold">return</span> num/deno;
} <span style="color: #228B22">// end function for the integrand</span>
</pre></div>
<p>

</section>


<section>

<h2>Test Runs and Comparison with Brute Force and Gauss-Legendre  <a name="___sec355"></a></h2>

The suffix \( br \) stands for the brute force approach
while \( is \) stands for the use of importance sampling.

<p>
\begin{tabular}{rllllll}\hline
$N$&$I_{br}$ &$\sigma_{br}$ &time(s)  &$I_{is}$  &$\sigma_{is}$ &time(s)\\\hline
1E6 &amp; 0.19238 &3.85124E-4 &amp; 0.6  &0.19176 &amp; 1.01515E-4 &amp; 1.4 \\
10E6   &0.18607  &1.18053E-4 &amp; 6 &0.192254    &1.22430E-4  &14 \\
100E6   &0.18846  &4.37163E-4 &amp; 57 &0.192720    &1.03346E-4  &138 \\
1000E6   &0.18843  &1.35879E-4  &581 &0.192789   &3.28795E-5  &1372 \\
\hline
\end{tabular}

<p>
Gauss-Legendre results:

<p>
\begin{tabular}{rllll}\hline
\( N \) &time(s)  &$I_n$     &$|I_n-I$ \\\hline
20 &amp; 31          &0.18047 &amp; 1.123E-2                 \\
30 &amp; 354 &amp; 0.18501 &amp; 7.76E-3              \\
40 &amp; 1999 &amp; 0.18653 &amp; 6.24E-3                \\
50 &amp; 7578 &amp; 0.18722 &amp; 5.54E-3               \\
\hline
\end{tabular}

<p>

</section>


<section>

<h2>Variance, covariance, errors etc, chapter 11.2  <a name="___sec356"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Statistical analysis.</b>
<p>

<ul>
  <p><li> Monte Carlo simulations can be treated as <em>computer experiments</em></li>
  <p><li> The results can be analysed with the same statistics tools
      we would use in analysing laboraty experiments</li>
  <p><li> As in all other experiments, we are looking for expectation values and an estimate of how accurate they are, i.e., the error</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec357"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Statistical analysis.</b>
<p>

<ul>
  <p><li> As in other experiments, Monte Carlo experiments have two classes of errors:</li>
  <p><li> Statistical errors</li>
  <p><li> Systematic errors</li>
  <p><li> Statistical errors can be estimated using standard tools
      from statistics</li>
  <p><li> Systematic errors are method specific and must be treated differently from case to case.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec358"></a></h2>

A <em>stochastic process</em> is a process that produces sequentially a
chain of values:

<p>&nbsp;<br>
$$
\{x_1, x_2,\dots\,x_k,\dots\}.
$$
<p>&nbsp;<br>

We will call these
values our <em>measurements</em> and the entire set as our measured
<em>sample</em>.  The action of measuring all the elements of a sample
we will call a stochastic <em>experiment</em> (since, operationally,
they are often associated with results of empirical observation of
some physical or mathematical phenomena; precisely an experiment). We
assume that these values are distributed according to some
PDF \( p_X^{\phantom X}(x) \), where \( X \) is just the formal symbol for the
stochastic variable whose PDF is \( p_X^{\phantom X}(x) \). Instead of
trying to determine the full distribution \( p \) we are often only
interested in finding the few lowest moments, like the mean
\( \mu_X^{\phantom X} \) and the variance \( \sigma_X^{\phantom X} \).

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec359"></a></h2>

The <em>probability distribution function (PDF)</em> is a function
\( p(x) \) on the domain which, in the discrete case, gives us the
probability or relative frequency with which these values of \( X \)
occur:

<p>&nbsp;<br>
$$
p(x) = \prob(X=x)
$$
<p>&nbsp;<br>

In the continuous case, the PDF does not directly depict the
actual probability. Instead we define the probability for the
stochastic variable to assume any value on an infinitesimal interval
around \( x \) to be \( p(x)dx \). The continuous function \( p(x) \) then gives us
the <em>density</em> of the probability rather than the probability
itself. The probability for a stochastic variable to assume any value
on a non-infinitesimal interval \( [a,\,b] \) is then just the integral:

<p>&nbsp;<br>
$$
\prob(a\leq X\leq b) = \int_a^b p(x)dx
$$
<p>&nbsp;<br>

Qualitatively speaking, a stochastic variable represents the values of
numbers chosen as if by chance from some specified PDF so that the
selection of a large set of these numbers reproduces this PDF.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec360"></a></h2>

Also of interest to us is the <em>cumulative probability
distribution function (CDF)</em>, \( P(x) \), which is just the probability
for a stochastic variable \( X \) to assume any value less than \( x \):

<p>&nbsp;<br>
$$
P(x)=\mbox{Prob(}X\leq x\mbox{)} =
\int_{-\infty}^x p(x^{\prime})dx^{\prime}
$$
<p>&nbsp;<br>

The relation between a CDF and its corresponding PDF is then:

<p>&nbsp;<br>
$$
p(x) = \frac{d}{dx}P(x)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec361"></a></h2>

A particularly useful class of special expectation values are the
<em>moments</em>. The \( n \)-th moment of the PDF \( p \) is defined as
follows:

<p>&nbsp;<br>
$$
\mean{x^n} \equiv \int\! x^n p(x)\,dx
$$
<p>&nbsp;<br>

The zero-th moment \( \mean{1} \) is just the normalization condition of
\( p \). The first moment, \( \mean{x} \), is called the <em>mean</em> of \( p \)
and often denoted by the letter \( \mu \):

<p>&nbsp;<br>
$$
\mean{x} = \mu \equiv \int\! x p(x)\,dx
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec362"></a></h2>

A special version of the moments is the set of <em>central moments</em>,
the n-th central moment defined as:

<p>&nbsp;<br>
$$
\mean{(x-\mean{x})^n} \equiv \int\! (x-\mean{x})^n p(x)\,dx
$$
<p>&nbsp;<br>

The zero-th and first central moments are both trivial, equal \( 1 \) and
\( 0 \), respectively. But the second central moment, known as the
<em>variance</em> of \( p \), is of particular interest. For the stochastic
variable \( X \), the variance is denoted as \( \sigma^2_X \) or \( \var(X) \):

<p>&nbsp;<br>
$$
\begin{align*}
\sigma^2_X\ \ =\ \ \var(X) &= \mean{(x-\mean{x})^2} =
\int\! (x-\mean{x})^2 p(x)\,dx\\
&= \int\! \left(x^2 - 2 x \mean{x}^{\phantom{2}} +
  \mean{x}^2\right)p(x)\,dx\\
&= \mean{x^2} - 2 \mean{x}\mean{x} + \mean{x}^2\\
&= \mean{x^2} - \mean{x}^2
\end{align*}
$$
<p>&nbsp;<br>


<p>
The square root of the variance, \( \sigma =
\sqrt{\mean{(x-\mean{x})^2}} \) is called the <em>standard
  deviation</em> of \( p \). It is clearly just the RMS (root-mean-square)
value of the deviation of the PDF from its mean value, interpreted
qualitatively as the &quot;spread&quot; of \( p \) around its mean.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec363"></a></h2>

Another important quantity is the so called covariance, a variant of
the above defined variance. Consider again the set \( \{X_i\} \) of \( n \)
stochastic variables (not necessarily uncorrelated) with the
multivariate PDF \( P(x_1,\dots,x_n) \). The <em>covariance</em> of two
of the stochastic variables, \( X_i \) and \( X_j \), is defined as follows:

<p>&nbsp;<br>
$$
\begin{align}
\cov(X_i,\,X_j) &\equiv \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\nonumber\\
&=
\int\!\cdots\!\int\!(x_i-\mean{x_i})(x_j-\mean{x_j})\,
P(x_1,\dots,x_n)\,dx_1\dots dx_n
\tag{28}
\end{align}
$$
<p>&nbsp;<br>

with

<p>&nbsp;<br>
$$
\mean{x_i} =
\int\!\cdots\!\int\!x_i\,P(x_1,\dots,x_n)\,dx_1\dots dx_n
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec364"></a></h2>

If we consider the above covariance as a matrix \( C_{ij} =
\cov(X_i,\,X_j) \), then the diagonal elements are just the familiar
variances, \( C_{ii} = \cov(X_i,\,X_i) = \var(X_i) \). It turns out that
all the off-diagonal elements are zero if the stochastic variables are
uncorrelated. This is easy to show, keeping in mind the linearity of
the expectation value. Consider the stochastic variables \( X_i \) and
\( X_j \), (\( i\neq j \)):

<p>&nbsp;<br>
$$
\begin{align*}
\cov(X_i,\,X_j) &= \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}\\
&=\mean{x_i x_j - x_i\mean{x_j} - \mean{x_i}x_j + \mean{x_i}\mean{x_j}}\\
&=\mean{x_i x_j} - \mean{x_i\mean{x_j}} - \mean{\mean{x_i}x_j} +
\mean{\mean{x_i}\mean{x_j}}\\
&=\mean{x_i x_j} - \mean{x_i}\mean{x_j} - \mean{x_i}\mean{x_j} +
\mean{x_i}\mean{x_j}\\
&=\mean{x_i x_j} - \mean{x_i}\mean{x_j}
\end{align*}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec365"></a></h2>

If \( X_i \) and \( X_j \) are independent, we get \( \mean{x_i x_j} =
\mean{x_i}\mean{x_j} \), resulting in \( \cov(X_i, X_j) = 0\ \ (i\neq j) \).

<p>
Also useful for us is the covariance of linear combinations of
stochastic variables. Let \( \{X_i\} \) and \( \{Y_i\} \) be two sets of
stochastic variables. Let also \( \{a_i\} \) and \( \{b_i\} \) be two sets of
scalars. Consider the linear combination:

<p>&nbsp;<br>
$$
U = \sum_i a_i X_i \qquad V = \sum_j b_j Y_j
$$
<p>&nbsp;<br>

By the linearity of the expectation value

<p>&nbsp;<br>
$$
\cov(U, V) = \sum_{i,j}a_i b_j \cov(X_i, Y_j)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec366"></a></h2>

Now, since the variance is just \( \var(X_i) = \cov(X_i, X_i) \), we get
the variance of the linear combination \( U = \sum_i a_i X_i \):
<p>&nbsp;<br>
$$
\var(U) = \sum_{i,j}a_i a_j \cov(X_i, X_j)
\tag{29}
$$
<p>&nbsp;<br>

And in the special case when the stochastic variables are
uncorrelated, the off-diagonal elements of the covariance are as we
know< zero, resulting in:

<p>&nbsp;<br>
$$
\var(U) = \sum_i a_i^2 \cov(X_i, X_i) = \sum_i a_i^2 \var(X_i)
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\var(\sum_i a_i X_i) = \sum_i a_i^2 \var(X_i)
$$
<p>&nbsp;<br>

which will become very useful in our study of the error in the mean
value of a set of measurements.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec367"></a></h2>

In practical situations a sample is always of finite size. Let that
size be \( n \). The expectation value of a sample, the <em>sample
mean</em>, is then defined as follows:

<p>&nbsp;<br>
$$
\bar x_n \equiv \frac{1}{n}\sum_{k=1}^n x_k
$$
<p>&nbsp;<br>

The <em>sample variance</em> is:

<p>&nbsp;<br>
$$
\var(x) \equiv \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2
$$
<p>&nbsp;<br>

its square root being the <em>standard deviation of the sample</em>. The
<em>sample covariance</em> is:

<p>&nbsp;<br>
$$
\cov(x)\equiv\frac{1}{n}\sum_{kl}(x_k - \bar x_n)(x_l - \bar x_n)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec368"></a></h2>

Note that the sample variance is the sample covariance without the
cross terms. In a similar manner as the covariance in
eq. <a href="#mjx-eqn-28">(28)</a> is a measure of the correlation between
two stochastic variables, the above defined sample covariance is a
measure of the sequential correlation between succeeding measurements
of a sample.

<p>
These quantities, being known experimental values, differ
significantly from and must not be confused with the similarly named
quantities for stochastic variables, mean \( \mu_X \), variance \( \var(X) \)
and covariance \( \cov(X,Y) \).

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec369"></a></h2>

The law of large numbers
states that as the size of our sample grows to infinity, the sample
mean approaches the true mean \( \mu_X^{\phantom X} \) of the chosen PDF:

<p>&nbsp;<br>
$$
\lim_{n\to\infty}\bar x_n = \mu_X^{\phantom X}
$$
<p>&nbsp;<br>

The sample mean \( \bar x_n \) works therefore as an estimate of the true
mean \( \mu_X^{\phantom X} \).

<p>
What we need to find out is how good an approximation \( \bar x_n \) is to
\( \mu_X^{\phantom X} \). In any stochastic measurement, an estimated
mean is of no use to us without a measure of its error. A quantity
that tells us how well we can reproduce it in another experiment. We
are therefore interested in the PDF of the sample mean itself. Its
standard deviation will be a measure of the spread of sample means,
and we will simply call it the <em>error</em> of the sample mean, or
just sample error, and denote it by \( \mbox{err}_X^{\phantom X} \). In
practice, we will only be able to produce an <em>estimate</em> of the
sample error since the exact value would require the knowledge of the
true PDFs behind, which we usually do not have.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec370"></a></h2>

The straight forward brute force way of estimating the sample error is
simply by producing a number of samples, and treating the mean of each
as a measurement. The standard deviation of these means will then be
an estimate of the original sample error. If we are unable to produce
more than one sample, we can split it up sequentially into smaller
ones, treating each in the same way as above. This procedure is known
as <em>blocking</em> and will be given more attention shortly. At this
point it is worth while exploring more indirect methods of estimation
that will help us understand some important underlying principles of
correlational effects.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec371"></a></h2>

Let us first take a look at what happens to the sample error as the
size of the sample grows. In a sample, each of the measurements \( x_i \)
can be associated with its own stochastic variable \( X_i \). The
stochastic variable \( \overline X_n \) for the sample mean \( \bar x_n \) is
then just a linear combination, already familiar to us:

<p>&nbsp;<br>
$$
\overline X_n = \frac{1}{n}\sum_{i=1}^n X_i
$$
<p>&nbsp;<br>

All the coefficients are just equal \( 1/n \). The PDF of \( \overline X_n \),
denoted by \( p_{\overline X_n}(x) \) is the desired PDF of the sample
means.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec372"></a></h2>

The probability density of obtaining a sample mean \( \bar x_n \)
is the product of probabilities of obtaining arbitrary values \( x_1,
x_2,\dots,x_n \) with the constraint that the mean of the set \( \{x_i\} \)
is \( \bar x_n \):

<p>&nbsp;<br>
$$
p_{\overline X_n}(x) = \int p_X^{\phantom X}(x_1)\cdots
\int p_X^{\phantom X}(x_n)\
\delta\!\left(x - \frac{x_1+x_2+\dots+x_n}{n}\right)dx_n \cdots dx_1
$$
<p>&nbsp;<br>

And in particular we are interested in its variance \( \var(\overline
X_n) \).

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec373"></a></h2>

It is generally not possible to express \( p_{\overline X_n}(x) \) in a
closed form given an arbitrary PDF \( p_X^{\phantom X} \) and a number
\( n \). But for the limit \( n\to\infty \) it is possible to make an
approximation. The very important result is called <em>the central
  limit theorem</em>. It tells us that as \( n \) goes to infinity,
\( p_{\overline X_n}(x) \) approaches a Gaussian distribution whose mean
and variance equal the true mean and variance, \( \mu_{X}^{\phantom X} \)
and \( \sigma_{X}^{2} \), respectively:
<p>&nbsp;<br>
$$
\lim_{n\to\infty} p_{\overline X_n}(x) =
\left(\frac{n}{2\pi\var(X)}\right)^{1/2}
e^{-\frac{n(x-\bar x_n)^2}{2\var(X)}}
\tag{30}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec374"></a></h2>

The desired variance
\( \var(\overline X_n) \), i.e. the sample error squared
\( \mbox{err}_X^2 \), is given by:
<p>&nbsp;<br>
$$
\begin{equation}
\mbox{err}_X^2 = \var(\overline X_n) = \frac{1}{n^2}
\sum_{ij} \cov(X_i, X_j)
\tag{31}
\end{equation}
$$
<p>&nbsp;<br>

We see now that in order to calculate the exact error of the sample
with the above expression, we would need the true means
\( \mu_{X_i}^{\phantom X} \) of the stochastic variables \( X_i \). To
calculate these requires that we know the true multivariate PDF of all
the \( X_i \). But this PDF is unknown to us, we have only got the measurements of
one sample. The best we can do is to let the sample itself be an
estimate of the PDF of each of the \( X_i \), estimating all properties of
\( X_i \) through the measurements of the sample.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec375"></a></h2>

Our estimate of \( \mu_{X_i}^{\phantom X} \) is then the sample mean \( \bar x \)
itself, in accordance with the the central limit theorem:

<p>&nbsp;<br>
$$
\mu_{X_i}^{\phantom X} = \mean{x_i} \approx \frac{1}{n}\sum_{k=1}^n
x_k = \bar x
$$
<p>&nbsp;<br>

Using \( \bar x \) in place of \( \mu_{X_i}^{\phantom X} \) we can give an
<em>estimate</em> of the covariance in eq. <a href="#mjx-eqn-31">(31)</a>:

<p>&nbsp;<br>
$$
\begin{align*}
\cov(X_i, X_j) &= \mean{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\approx\mean{(x_i - \bar x)(x_j - \bar{x})}\\
&\approx&\frac{1}{n} \sum_{l}^n \left(\frac{1}{n}\sum_{k}^n (x_k -
\bar x_n)(x_l - \bar x_n)\right)
=\frac{1}{n}\frac{1}{n} \sum_{kl} (x_k -
\bar x_n)(x_l - \bar x_n)\\
&=\frac{1}{n}\cov(x)
\end{align*}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec376"></a></h2>

By the same procedure we can use the sample variance as an
estimate of the variance of any of the stochastic variables \( X_i \):

<p>&nbsp;<br>
$$
\begin{align}
\var(X_i)
&=\mean{x_i - \mean{x_i}} \approx \mean{x_i - \bar x_n}\nonumber\\
&\approx&\frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)\nonumber\\
&=\var(x)
\tag{32}
\end{align}
$$
<p>&nbsp;<br>


<p>
Now we can calculate an estimate of the error
\( \mbox{err}_X^{\phantom X} \) of the sample mean \( \bar x_n \):

<p>&nbsp;<br>
$$
\begin{align}
\mbox{err}_X^2
&=\frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) \nonumber \\
&\approx&\frac{1}{n^2}\sum_{ij}\frac{1}{n}\cov(x) =
\frac{1}{n^2}n^2\frac{1}{n}\cov(x)\nonumber\\
&=\frac{1}{n}\cov(x)
\tag{33}
\end{align}
$$
<p>&nbsp;<br>

which is nothing but the sample covariance divided by the number of
measurements in the sample.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec377"></a></h2>

In the special case that the measurements of the sample are
uncorrelated (equivalently the stochastic variables \( X_i \) are
uncorrelated) we have that the off-diagonal elements of the covariance
are zero. This gives the following estimate of the sample error:

<p>&nbsp;<br>
$$
\begin{align}
\mbox{err}_X^2 &= \frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) =
\frac{1}{n^2} \sum_i \var(X_i)\nonumber\\
&\approx&
\frac{1}{n^2} \sum_i \var(x)\nonumber\\ &= \frac{1}{n}\var(x)
\tag{34}
\end{align}
$$
<p>&nbsp;<br>

where in the second step we have used eq. <a href="#mjx-eqn-32">(32)</a>.
The error of the sample is then just its standard deviation divided by
the square root of the number of measurements the sample contains.
This is a very useful formula which is easy to compute. It acts as a
first approximation to the error, but in numerical experiments, we
cannot overlook the always present correlations.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec378"></a></h2>

For computational purposes one usually splits up the estimate of
\( \mbox{err}_X^2 \), given by eq. <a href="#mjx-eqn-33">(33)</a>, into two
parts:

<p>&nbsp;<br>
$$
\begin{align}
\mbox{err}_X^2 &=
\frac{1}{n}\var(x) + \frac{1}{n}(\cov(x)-\var(x))\nonumber\\&=
\frac{1}{n^2}\sum_{k=1}^n (x_k - \bar x_n)^2 +
\frac{2}{n^2}\sum_{k < l} (x_k - \bar x_n)(x_l - \bar x_n)
\tag{35}
\end{align}
$$
<p>&nbsp;<br>

The first term is the same as the error in the uncorrelated case,
Eq. <a href="#mjx-eqn-34">(34)</a>. This means that the second
term accounts for the error correction due to correlation between the
measurements. For uncorrelated measurements this second term is zero.

<p>

</section>


<section>

<h2>Variance, covariance, errors etc  <a name="___sec379"></a></h2>

Computationally the uncorrelated first term is much easier to treat
efficiently than the second.

<p>&nbsp;<br>
$$
\var(x) = \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2 =
\left(\frac{1}{n}\sum_{k=1}^n x_k^2\right) - \bar x_n^2
$$
<p>&nbsp;<br>


<p>
We just accumulate separately the values \( x^2 \) and \( x \) for every
measurement \( x \) we receive. The correlation term, though, has to be
calculated at the end of the experiment since we need all the
measurements to calculate the cross terms. Therefore, all measurements
have to be stored throughout the experiment.

<p>

</section>


<section>

<h2>Random Numbers, chapter 11.3  <a name="___sec380"></a></h2>

<!-- 2DOFIGURE: [random.jpg, width=500 frac=0.5] -->

<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec381"></a></h2>

Most used are so-called 'Linear congruential'

<p>&nbsp;<br>
$$
  N_i=(aN_{i-1}+c) MOD (M),
$$
<p>&nbsp;<br>

and to find a number in \( x\in [0,1] \)

<p>&nbsp;<br>
$$
  x_i=N_i/M
$$
<p>&nbsp;<br>

\( M \) is called the period and should be as big as possible.
The start value is \( N_0 \) and is called the seed.

<ul>
  <p><li> The random variables should result in the uniform distribution</li>
  <p><li> No correlations between numbers (zero covariance)</li>
  <p><li> As big as possible period \( M \)</li>
  <p><li> Fast algo</li>
</ul>
<p>


</section>


<section>

<h2>Random Numbers  <a name="___sec382"></a></h2>

The problem with such generators is that their outputs are periodic;
they
will start to repeat themselves with a period that is at most \( M \). If however
the parameters \( a \) and \( c \) are badly chosen, the period may be even shorter.

<p>
Consider the following example

<p>&nbsp;<br>
$$
  N_i=(6N_{i-1}+7) \mbox{MOD} (5),
$$
<p>&nbsp;<br>

with a seed \( N_0=2 \). This generator produces the sequence
\( 4,1,3,0,2,4,1,3,0,2,...\dots \), i.e., a sequence with period \( 5 \).
However, increasing \( M \) may not guarantee a larger period as the following
example shows

<p>&nbsp;<br>
$$
  N_i=(27N_{i-1}+11) \mbox{MOD} (54),
$$
<p>&nbsp;<br>

which still, with \( N_0=2 \), results in \( 11,38,11,38,11,38,\dots \), a period of
just \( 2 \).

<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec383"></a></h2>

Typical periods for the random generators provided in the program library
are of the order of \( \sim 10^9 \) or larger. Other random number generators which have
become increasingly popular are so-called shift-register generators.
In these generators each successive number depends on many preceding
values (rather than the last values as in the linear congruential
generator).
For example, you could make a shift register generator whose $l$th
number is the sum of the $l-i$th and $l-j$th values with modulo \( M \),
<p>&nbsp;<br>
$$
   N_l=(aN_{l-i}+cN_{l-j})\mbox{MOD}(M).
$$
<p>&nbsp;<br>

Such a generator again produces a sequence of pseudorandom numbers
but this time with a period much larger than \( M \).
It is also possible to construct more elaborate algorithms by including
more than two past terms in the sum of each iteration.
One example is the generator of Marsaglia and Zaman (Computers in Physics {\bf 8} (1994) 117)
which consists of two congruential relations

<p>&nbsp;<br>
$$
   N_l=(N_{l-3}-N_{l-1})\mbox{MOD}(2^{31}-69),
   \tag{36}
$$
<p>&nbsp;<br>

followed by

<p>&nbsp;<br>
$$
   N_l=(69069N_{l-1}+1013904243)\mbox{MOD}(2^{32}),
   \tag{37}
$$
<p>&nbsp;<br>

which according to the authors has a period larger than \( 2^{94} \).

<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec384"></a></h2>

Using modular addition, we could use the bitwise
exclusive-OR (\( \oplus \)) operation so that
<p>&nbsp;<br>
$$
   N_l=(N_{l-i})\oplus (N_{l-j})
$$
<p>&nbsp;<br>

where the bitwise action of \( \oplus \) means that if \( N_{l-i}=N_{l-j} \) the result is
\( 0 \) whereas if \( N_{l-i}\ne N_{l-j} \) the result is
\( 1 \). As an example, consider the case where  \( N_{l-i}=6 \) and \( N_{l-j}=11 \). The first
one has a bit representation (using 4 bits only) which reads \( 0110 \) whereas the
second number is \( 1011 \). Employing the \( \oplus \) operator yields
\( 1101 \), or \( 2^3+2^2+2^0=13 \).

<p>
In Fortran90, the bitwise \( \oplus \) operation is coded through the intrinsic
function \( \mbox{IEOR}(m,n) \) where \( m \) and \( n \) are the input numbers, while in \( C \)
it is given by \( m\wedge n \).

<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec385"></a></h2>

The function \( ran0 \) implements

<p>&nbsp;<br>
$$
  N_i=(aN_{i-1}) \mbox{MOD} (M).
$$
<p>&nbsp;<br>

Note that \( c=0 \) and that it cannot be initialized with \( N_0=0 \).
However, since \( a \) and \( N_{i-1} \) are integers and their multiplication
could become greater than the standard 32 bit integer, there is a trick via
Schrage's algorithm which approximates the multiplication
of large integers through the factorization

<p>&nbsp;<br>
$$
  M=aq+r,
$$
<p>&nbsp;<br>

where we have defined

<p>&nbsp;<br>
$$
   q=[M/a],
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
  r = M\quad\mbox{MOD} \quad a.
$$
<p>&nbsp;<br>

where the brackets denote integer division. In the code below the numbers
\( q \) and \( r \) are chosen so that \( r < q \).

<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec386"></a></h2>

To see how this works we note first that

<p>&nbsp;<br>
$$
(aN_{i-1}) \mbox{MOD} (M)= (aN_{i-1}-[N_{i-1}/q]M)\mbox{MOD} (M),
\tag{38}
$$
<p>&nbsp;<br>

since we can add or subtract any integer multiple of \( M \) from \( aN_{i-1} \).
The last term $ [N_{i-1}/q]M \mbox{MOD} (M)$ is zero since the integer division
\( [N_{i-1}/q] \) just yields a constant which is multiplied with \( M \).
Rewrite as

<p>&nbsp;<br>
$$
(aN_{i-1}) \mbox{MOD} (M)= (aN_{i-1}-[N_{i-1}/q](aq+r))\mbox{MOD} (M),
\tag{39}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec387"></a></h2>

It gives

<p>&nbsp;<br>
$$
(aN_{i-1}) \mbox{MOD} (M)= \left(a(N_{i-1}-[N_{i-1}/q]q)-[N_{i-1}/q]r)\right)\mbox{MOD} (M),
\tag{40}
$$
<p>&nbsp;<br>

yielding

<p>&nbsp;<br>
$$
(aN_{i-1}) \mbox{MOD} (M)= \left(a(N_{i-1}\mbox{MOD} (q)) -[N_{i-1}/q]r)\right)\mbox{MOD} (M).
\tag{41}
$$
<p>&nbsp;<br>


<ul>
  <p><li> \( [N_{i-1}/q]r \) is always smaller or equal \( N_{i-1}(r/q) \) and with \( r < q \) we obtain always a number smaller than \( N_{i-1} \), which is smaller than \( M \).</li>
  <p><li> \( N_{i-1}\mbox{MOD} (q) \) is between zero and \( q-1 \) then \( a(N_{i-1}\mbox{MOD} (q)) < aq \).</li>
  <p><li> Our definition of \( q=[M/a] \) ensures that this term is also smaller than \( M \) meaning that both terms fit into a 32-bit signed integer. None of these two terms can be negative, but their difference could.</li>
</ul>
<p>


</section>


<section>

<h2>Random Numbers  <a name="___sec388"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">/*  ran0() is an &quot;Minimal&quot; random number generator of Park and Miller</span>
<span style="color: #228B22">** Set or reset the input value</span>
<span style="color: #228B22">** idum to any integer value (except the unlikely value MASK)</span>
<span style="color: #228B22">** to initialize the sequence; idum must not be altered between</span>
<span style="color: #228B22">** calls for sucessive deviates in a sequence.</span>
<span style="color: #228B22">** The function returns a uniform deviate between 0.0 and 1.0.</span>
<span style="color: #228B22">*/</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> <span style="color: #008b45">ran0</span>(<span style="color: #a7a7a7; font-weight: bold">long</span> &amp;idum)
{
   <span style="color: #8B008B; font-weight: bold">const</span> <span style="color: #a7a7a7; font-weight: bold">int</span> a = <span style="color: #B452CD">16807</span>, m = <span style="color: #B452CD">2147483647</span>, q = <span style="color: #B452CD">127773</span>;
   <span style="color: #8B008B; font-weight: bold">const</span> <span style="color: #a7a7a7; font-weight: bold">int</span> r = <span style="color: #B452CD">2836</span>, MASK = <span style="color: #B452CD">123459876</span>;
   <span style="color: #8B008B; font-weight: bold">const</span> <span style="color: #a7a7a7; font-weight: bold">double</span> am = <span style="color: #B452CD">1.</span>/m;
   <span style="color: #a7a7a7; font-weight: bold">long</span>     k;
   <span style="color: #a7a7a7; font-weight: bold">double</span>   ans;
   idum ^= MASK;
   k = (*idum)/q;
   idum = a*(idum - k*q) - r*k;
   <span style="color: #228B22">// add m if negative difference</span>
   <span style="color: #8B008B; font-weight: bold">if</span>(idum &lt; <span style="color: #B452CD">0</span>) idum += m;
   ans=am*(idum);
   idum ^= MASK;
   <span style="color: #8B008B; font-weight: bold">return</span> ans;
} <span style="color: #228B22">// End: function ran0()</span>
</pre></div>
<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec389"></a></h2>

Important tests  of random numbers are the standard deviation \( \sigma \) and the mean
\( \mu=\langle x\rangle \).

<p>
For the uniform distribution we have

<p>&nbsp;<br>
$$
   \langle x^k\rangle=\int_0^1dxp(x)x^k=\int_0^1dxx^k=\frac{1}{k+1},
$$
<p>&nbsp;<br>

since \( p(x)=1 \).
The mean value \( \mu \) is then

<p>&nbsp;<br>
$$
  \mu=\langle x\rangle=\frac{1}{2}
$$
<p>&nbsp;<br>

while the standard deviation is

<p>&nbsp;<br>
$$
   \sigma=\sqrt{\langle x^2\rangle-\mu^2}=\frac{1}{\sqrt{12}}=0.2886.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec390"></a></h2>

Number of \( x \)-values for various intervals
generated by 4 random number generators,
their corresponding mean values and standard deviations. All calculations
have been initialized with the variable \( idum=-1 \).
\begin{tabular}{crrrr}\hline
\( x \)-bin &ran0&ran1&ran2&ran3\\\hline
0.0-0.1 &1013 &991 &938 &1047 \\
0.1-0.2 &1002 &1009 &1040 &1030 \\
0.2-0.3 &989 &999 &1030 &993 \\
0.3-0.4 &939 &960 &1023 &937 \\
0.4-0.5 &1038 &1001 &1002 &992 \\
0.5-0.6 &1037 &1047 &1009 &amp; 1009\\
0.6-0.7 &1005 &989 &1003 &989 \\
0.7-0.8 &986 &962 &985 &954 \\
0.8-0.9 &1000 &1027 &1009 &1023 \\
0.9-1.0 &991 &1015 &961 &1026 \\ \hline
\( \mu \) &0.4997 &0.5018 &0.4992 &amp; 0.4990\\
\( \sigma \) &0.2882 &0.2892 &0.2861 &0.2915 \\
\hline
\end{tabular}

<p>

</section>


<section>

<h2>Random Number  <a name="___sec391"></a></h2>

<!-- 2DOFIGURE: [../Cartoons/skann0001.jpg, width=500 frac=0.25] -->

<p>

</section>


<section>

<h2>Random Numbers  <a name="___sec392"></a></h2>

<!-- 2DOFIGURE: [../Cartoons/skann0003.jpg, width=500 frac=0.4] -->

<p>

</section>


<section>

<h1>Overview of week 46  <a name="___sec393"></a></h1>


</section>


<section>

<h2>Overview of week 46  <a name="___sec394"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Monte Carlo methods, chapter 12.</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Brownian motion and Markov chains, chapters 12.2 and 12.3 of lecture notes</li>
  <p><li> Tuesday:</li>
  <p><li> More on Markov chains (chapter 12.4 and 12.5)</li>
  <p><li> The Metropolis algorithm (chapter 12.5) and simple implementations of it</li>
  <p><li> Discussion of project 5 , there are three versions, available from Tuesday the 12th. Next week we will continue to discuss the various projects. For the quantum mechanical version we will also discuss how to perform quantum mechanical calculations. We will also discuss how to parallelize the diffusion equation.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Why Markov Chains?  <a name="___sec395"></a></h2>

<ul>
  <p><li> We want to study a physical system which evolves towards equilibrium, from given initial conditions.</li>
  <p><li> We start with a PDF \( w(x_0,t_0) \) and we want to understand how it evolves with time.</li>
  <p><li> We want to reach a situation where after a given number of time steps we obtain a steady state. This means that the system reaches its most likely state (equilibrium situation)</li>
  <p><li> Our PDF is normally a multidimensional object whose normalization constant is impossible to find.</li>
  <p><li> Analytical calculations from \( w(x,t) \) are not possible.</li>
  <p><li> To sample directly from from \( w(x,t) \) is not possible/difficult.</li>
  <p><li> The transition probability \( W \) is also not known.</li>
  <p><li> How can we establish that we have reached a steady state? Sounds impossible! Use Markov chain Monte Carlo</li>
</ul>
<p>


</section>


<section>

<h2>Brownian Motion and Markov Processes  <a name="___sec396"></a></h2>

A Markov process is a random walk with a selected probability for making a
move. The new move is independent of the previous history of the system.
The Markov process is used repeatedly in Monte Carlo simulations in order to generate
new random states.
The reason for choosing a Markov process is that when it is run for a
long enough time starting with a random state,
we will eventually reach the most likely state of the system.
In thermodynamics, this means that after a certain number of Markov processes
we reach an equilibrium distribution.

<p>
This mimicks the way a real system reaches
its most likely state at a given temperature of the surroundings.

<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes  <a name="___sec397"></a></h2>

To reach this distribution, the Markov process needs to obey two important conditions, that of
{\bf ergodicity} and {\bf detailed balance}. These conditions impose then constraints on our algorithms
for accepting or rejecting new random states.
The Metropolis algorithm discussed here
abides to both these constraints.
The Metropolis algorithm is widely used in Monte Carlo
simulations and the understanding of it rests within
the interpretation of random walks and Markov processes.

<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes  <a name="___sec398"></a></h2>

In a random walk one defines a mathematical entity called a {\bf walker}, whose attributes
completely define the state of the system in question. The state of the system can refer to any physical quantities,
from the vibrational state of a molecule specified by a set of quantum numbers, to the brands of coffee
in your favourite supermarket.
The walker moves in an appropriate state space by a combination of deterministic and random displacements from its previous
position.

<p>
This sequence of steps forms a {\bf chain}.

<p>

</section>


<section>

<h2>Sequence of ingredients  <a name="___sec399"></a></h2>

<ul>
  <p><li> We want to study a physical system which evolves towards equilibrium, from given initial conditions.</li>
  <p><li> Markov chains are intimately linked with the physical process of diffusion. Proof in lecture notes.</li>
  <p><li> From a Markov chain we can then derive the conditions for detailed balance and ergodicity. These are the conditions needed for obtaining a steady state.</li>
  <p><li> The widely used algorithm for doing this is the so-called Metropolis algorithm, in its refined form the Metropolis-Hastings algorithm.</li>
</ul>
<p>


</section>


<section>

<h2>Applications: almost every field  <a name="___sec400"></a></h2>

Financial engineering, see for example Patriarca <em>et al</em>, Physica {\bf 340}, page 334 (2004).

<p>
Neuroscience, see for example Lipinski, Physics Medical Biology {\bf 35}, page 441 (1990) or
Farnell and Gibson, Journal of Computational Physics {\bf 208}, page 253 (2005)

<p>
Tons of applications in physics

<p>
and chemistry

<p>
and biology, medicine

<p>
Nobel prize in economy to Black and Scholes

<p>&nbsp;<br>
$$
\frac{\partial V}{\partial t}+\frac{1}{2}\sigma^{2}S^{2}\frac{\partial^{2} V}{\partial S^{2}}+rS\frac{\partial V}{\partial S}-rV=0.
$$
<p>&nbsp;<br>

The
Black and Scholes equation is a partial differential equation, which describes the price
of the option over time. The derivation is based on Brownian motion (Langevin and Fokker-Planck, 12.6)

<p>
...the list is almost infinite

<p>

</section>


<section>

<h2>A simple Example  <a name="___sec401"></a></h2>

The obvious case is that of a random walker on a one-, or two- or three-dimensional lattice
(dubbed coordinate space hereafter)

<p>
Consider a system whose energy is defined by the orientation of single spins.
Consider the state \( i \), with given energy \( E_i \) represented by the following \( N \) spins

<p>&nbsp;<br>
$$
\begin{array}{cccccccccc}
\uparrow&\uparrow&\uparrow&\dots&\uparrow&\downarrow&\uparrow&\dots&\uparrow&\downarrow\\
1&2&3&\dots& k-1&k&k+1&\dots&N-1&N\end{array}
$$
<p>&nbsp;<br>

We may be  interested in the transition with one single  spinflip to a new state \( j \) with energy \( E_j \)

<p>&nbsp;<br>
$$
\begin{array}{cccccccccc}
\uparrow&\uparrow&\uparrow&\dots&\uparrow&\uparrow&\uparrow&\dots&\uparrow&\downarrow\\
1&2&3&\dots& k-1&k&k+1&\dots&N-1&N\end{array}
$$
<p>&nbsp;<br>

This change from one microstate \( i \) (or spin configuration)  to another microstate \( j \) is the
{\bf configuration space}  analogue to a random walk on a lattice. Instead of jumping from
one place to another in space, we 'jump' from one microstate to another.

<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec402"></a></h2>

From experiment there are strong indications that the flux of particles \( j(x,t) \), viz., the number of particles passing \( x \) at a time \( t \) is proportional to the
gradient of \( w(x,t) \). This proportionality is expressed mathematically through

<p>&nbsp;<br>
$$
\begin{equation}
    j(x,t) = -D\frac{\partial w(x,t)}{\partial x},
\end{equation}
$$
<p>&nbsp;<br>

where \( D \) is the so-called diffusion constant, with dimensionality length$^2$ per time.
If the number of particles is conserved, we have the continuity equation

<p>&nbsp;<br>
$$
\begin{equation}
    \frac{\partial j(x,t)}{\partial x} = -\frac{\partial w(x,t)}{\partial t},
\end{equation}
$$
<p>&nbsp;<br>

which leads to

<p>&nbsp;<br>
$$
\begin{equation}
    \frac{\partial w(x,t)}{\partial t} =
    D\frac{\partial^2w(x,t)}{\partial x^2},
\tag{42}
\end{equation}
$$
<p>&nbsp;<br>

which is the diffusion equation in one dimension. Solved as a partial differential equation
in chapter 10.

<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec403"></a></h2>

With the probability distribution function \( w(x,t)dx \) we can
compute expectation values such as  the mean distance

<p>&nbsp;<br>
$$
\begin{equation}
   \langle x(t)\rangle = \int_{-\infty}^{\infty}xw(x,t)dx,
\end{equation}
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
\begin{equation}
   \langle x^2(t)\rangle = \int_{-\infty}^{\infty}x^2w(x,t)dx,
\end{equation}
$$
<p>&nbsp;<br>

which allows for the computation of the variance
\( \sigma^2=\langle x^2(t)\rangle-\langle x(t)\rangle^2 \). Note well that
these expectation values are time-dependent. In a similar way we can also
define expectation values of functions \( f(x,t) \) as

<p>&nbsp;<br>
$$
\begin{equation}
   \langle f(x,t)\rangle = \int_{-\infty}^{\infty}f(x,t)w(x,t)dx.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec404"></a></h2>

Since \( w(x,t) \) is now treated as a PDF, it needs to obey the same criteria
as discussed in the previous chapter. However, the normalization condition

<p>&nbsp;<br>
$$
\begin{equation}
   \int_{-\infty}^{\infty}w(x,t)dx=1
\end{equation}
$$
<p>&nbsp;<br>

imposes significant constraints on \( w(x,t) \). These are

<p>&nbsp;<br>
$$
\begin{equation}
   w(x=\pm \infty,t)=0 \quad
   \frac{\partial^{n}w(x,t)}{\partial x^n}|_{x=\pm\infty} = 0,
\end{equation}
$$
<p>&nbsp;<br>

implying that when we study the time-derivative
\( {\partial\langle x(t)\rangle}/{\partial t} \), we obtain after integration by parts and using
Eq. <a href="#mjx-eqn-42">(42)</a>

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{\partial \langle x\rangle}{\partial t} =
   \int_{-\infty}^{\infty}x\frac{\partial w(x,t)}{\partial t}dx=
   D\int_{-\infty}^{\infty}x\frac{\partial^2w(x,t)}{\partial x^2}dx,
\end{equation}
$$
<p>&nbsp;<br>

leading to

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{\partial \langle x\rangle}{\partial t} =
   Dx\frac{\partial w(x,t)}{\partial x}|_{x=\pm\infty}-
   D\int_{-\infty}^{\infty}\frac{\partial w(x,t)}{\partial x}dx,
\end{equation}
$$
<p>&nbsp;<br>

implying that

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{\partial \langle x\rangle}{\partial t} = 0.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec405"></a></h2>

This means in turn that \( \langle x\rangle \) is independent of time.
If we choose the initial position \( x(t=0)=0 \),
the average displacement \( \langle x\rangle= 0 \).
If we link this discussion to a random walk in one dimension with equal probability
of jumping to the left or right and with an initial position \( x=0 \), then our probability
distribution remains centered around \( \langle x\rangle= 0 \) as function of time.
However, the variance is not necessarily 0. Consider first

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{\partial \langle x^2\rangle}{\partial t} =
   Dx^2\frac{\partial w(x,t)}{\partial x}|_{x=\pm\infty}-
   2D\int_{-\infty}^{\infty}x\frac{\partial w(x,t)}{\partial x}dx,
\end{equation}
$$
<p>&nbsp;<br>

where we have performed an integration by parts as we did
for \( \frac{\partial \langle x\rangle}{\partial t} \). A further integration by parts
results in

<p>&nbsp;<br>
$$
\begin{equation}
   \frac{\partial \langle x^2\rangle}{\partial t} =
   -Dxw(x,t)|_{x=\pm\infty}+
   2D\int_{-\infty}^{\infty}w(x,t)dx=2D,
\end{equation}
$$
<p>&nbsp;<br>

leading to

<p>&nbsp;<br>
$$
\begin{equation}
   \langle x^2\rangle = 2Dt,
\end{equation}
$$
<p>&nbsp;<br>

and the variance as

<p>&nbsp;<br>
$$
\begin{equation}label{eq:variancediffeq}
   \langle x^2\rangle-\langle x\rangle^2 = 2Dt.
\end{equation}
$$
<p>&nbsp;<br>

The root mean square displacement after a time \( t \) is then

<p>&nbsp;<br>
$$
\begin{equation}
   \sqrt{\langle x^2\rangle-\langle x\rangle^2} = \sqrt{2Dt}.
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Random walks, chapter 12.2  <a name="___sec406"></a></h2>

Consider now a random walker in one dimension, with probability \( R \) of moving to the right
and \( L \) for moving to the left.
At \( t=0 \) we place the walker at \( x=0 \).
The walker can then jump, with the above probabilities, either to the left or to the
right for each time step. Note that in principle we could also have the possibility that the
walker remains in the same position. This is not implemented in this example.
Every step has length \( \Delta x = l \). Time is discretized and we have a jump either to the left or
to the right at every time step.

<p>

</section>


<section>

<h2>Random walks  <a name="___sec407"></a></h2>

Let us now assume that we have
equal probabilities for jumping to the left or to the right, i.e.,
\( L=R=1/2 \).
The average displacement
after \( n \) time steps is

<p>&nbsp;<br>
$$
   \langle x(n)\rangle = \sum_{i}^{n} \Delta x_i = 0 \quad \Delta x_i=\pm l,
$$
<p>&nbsp;<br>

since we have an equal probability of jumping either to the left or to right.
The value of \( \langle x(n)^2\rangle \) is

<p>&nbsp;<br>
$$
   \langle x(n)^2\rangle = \left(\sum_{i}^{n} \Delta x_i\right)^2=\sum_{i}^{n} \Delta x_i^2+
\sum_{i\ne j}^{n} \Delta x_i\Delta x_j=l^2n.
$$
<p>&nbsp;<br>

For many enough steps the non-diagonal contribution is

<p>&nbsp;<br>
$$
   \sum_{i\ne j}^{N} \Delta x_i\Delta x_j=0,
$$
<p>&nbsp;<br>

since \( \Delta x_{i,j} = \pm l \).

<p>

</section>


<section>

<h2>Random walks  <a name="___sec408"></a></h2>

The variance is then

<p>&nbsp;<br>
$$
   \langle x(n)^2\rangle - \langle x(n)\rangle^2 = l^2n.
   \tag{43}
$$
<p>&nbsp;<br>

It is also rather straightforward to compute the variance for \( L\ne R \). The result is

<p>&nbsp;<br>
$$
   \langle x(n)^2\rangle - \langle x(n)\rangle^2 = 4LRl^2n.
$$
<p>&nbsp;<br>

The variable \( n \) represents the number of time
steps. If we define \( n=t/\Delta t \), we can then couple the variance result
from a random walk
in one dimension with the variance  from diffusion
by defining the diffusion constant as

<p>&nbsp;<br>
$$
   D = \frac{l^2}{\Delta t}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec409"></a></h2>

When solving partial differential equations such as the diffusion equation numerically,
the derivatives are always discretized.
We can rewrite the time derivative as

<p>&nbsp;<br>
$$
\begin{equation}
    \frac{\partial w(x,t)}{\partial t} \approx
    \frac{w(i,n+1)-w(i,n)}{\Delta t},
\end{equation}
$$
<p>&nbsp;<br>

whereas the gradient is approximated as

<p>&nbsp;<br>
$$
\begin{equation}
    D\frac{\partial^2w(x,t)}{\partial x^2}\approx
    D\frac{w(i+1,n)+w(i-1,n)-2w(i,n)}{(\Delta x)^2},
\end{equation}
$$
<p>&nbsp;<br>

resulting in the discretized diffusion equation
<p>&nbsp;<br>
$$
   \frac{w(i,n+1)-w(i,n)}{\Delta t}=D\frac{w(i+1,n)+w(i-1,n)-2w(i,n)}{(\Delta x)^2},
$$
<p>&nbsp;<br>

where \( n \) represents a given time step and \( i \) a step in the \( x \)-direction.

<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec410"></a></h2>

A Markov process allows in principle for a microscopic description of Brownian motion.
As with the random walk, we consider a particle
which moves along the  \( x \)-axis in the form of a series of jumps with step length
\( \Delta x = l \). Time and space are discretized and the subsequent moves are
statistically independent, i.e., the new move depends only on the previous step
and not on the results from earlier trials.
We start at a position \( x=jl=j\Delta x \) and move to
a new position \( x =i\Delta x \) during a step \( \Delta t=\epsilon \), where
\( i\ge  0 \) and \( j\ge 0 \) are integers.
The original probability distribution function (PDF) of the particles is given by
\( w_i(t=0) \) where \( i \) refers to a specific position on a grid, with \( i=0 \) representing \( x=0 \).
The function \( w_i(t=0) \) is now the discretized version of \( w(x,t) \).
We can regard the discretized PDF as a vector.

<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec411"></a></h2>

For the Markov process we have a transition probability from a position
\( x=jl \) to a position \( x=il \) given by
<p>&nbsp;<br>
$$
   W_{ij}(\epsilon)=W(il-jl,\epsilon)=\left\{\begin{array}{cc}\frac{1}{2} & |i-j| = 1\\
                                             0 & \mbox{else} \end{array} \right.
$$
<p>&nbsp;<br>

We call \( W_{ij} \) for the transition probability and we can represent it, see below,
as a matrix. Our new PDF \( w_i(t=\epsilon) \) is now related to the PDF at
\( t=0 \) through the relation
<p>&nbsp;<br>
$$
   w_i(t=\epsilon) = W(j\rightarrow i)w_j(t=0).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec412"></a></h2>

This equation represents the discretized time-development of an original
PDF.
Since both \( W \) and \( w \) represent probabilities, they have to be normalized, i.e., we require
that at each time step we have
<p>&nbsp;<br>
$$
   \sum_i w_i(t) = 1,
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
   \sum_j W(j\rightarrow i) = 1.
$$
<p>&nbsp;<br>

The further constraints are
\( 0 \le W_{ij} \le 1 \)  and  \( 0 \le w_{j} \le 1 \).
Note that the probability for remaining at the same place is in general
not necessarily equal zero. In our Markov process we allow only for jumps to the left or to
the right.

<p>

</section>


<section>

<h2>Diffusion from Markov Chain  <a name="___sec413"></a></h2>

The time development of our initial PDF can now be represented through the action of
the transition probability matrix applied \( n \) times. At a
time  \( t_n=n\epsilon \) our initial distribution has developed into
<p>&nbsp;<br>
$$
   w_i(t_n) = \sum_jW_{ij}(t_n)w_j(0),
$$
<p>&nbsp;<br>

and defining
<p>&nbsp;<br>
$$
   W(il-jl,n\epsilon)=(W^n(\epsilon))_{ij}
$$
<p>&nbsp;<br>

we obtain
<p>&nbsp;<br>
$$
   w_i(n\epsilon) = \sum_j(W^n(\epsilon))_{ij}w_j(0),
$$
<p>&nbsp;<br>

or in matrix form

<p>&nbsp;<br>
$$
\begin{equation}
\tag{44}
   \hat{w(n\epsilon)} = \hat{W}^n(\epsilon)\hat{w}(0).
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes  <a name="___sec414"></a></h2>

We wish to study the time-development of a PDF after a given number of time steps.
We define our PDF by the function \( w(t) \). In addition we define a transition probability
\( W \).
The time development of our PDF \( w(t) \), after one time-step from \( t=0 \) is given by

<p>&nbsp;<br>
$$
   w_i(t=\epsilon) = W(j\rightarrow i)w_j(t=0).
$$
<p>&nbsp;<br>

Normally we don't know the form of \( W \)!!
This equation represents the discretized time-development of an original
PDF.  We can rewrite this as a

<p>&nbsp;<br>
$$
   w_i(t=\epsilon) = W_{ij}w_j(t=0).
$$
<p>&nbsp;<br>

with the transition matrix \( W \) for a random walk left or right (cannot stay in the same position) given by

<p>&nbsp;<br>
$$
   W_{ij}(\epsilon)=W(il-jl,\epsilon)=\left\{\begin{array}{cc}\frac{1}{2} & |i-j| = 1\\
                                             0 & \mbox{else} \end{array} \right.
$$
<p>&nbsp;<br>

We call \( W_{ij} \) for the transition probability and we represent it
as a matrix.

<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes  <a name="___sec415"></a></h2>

Both  \( W \) and \( w \) represent probabilities and they have to be normalized, meaning that
that at each time step we have

<p>&nbsp;<br>
$$
   \sum_i w_i(t) = 1,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   \sum_j W(j\rightarrow i) = 1.
$$
<p>&nbsp;<br>

Further constraints are
\( 0 \le W_{ij} \le 1 \)  and  \( 0 \le w_{j} \le 1 \).
We can thus write the action of \( W \) as

<p>&nbsp;<br>
$$
   w_i(t+1) = \sum_jW_{ij}w_j(t),
$$
<p>&nbsp;<br>

or as vector-matrix relation

<p>&nbsp;<br>
$$
   {\bf \hat{w}}(t+1) = {\bf \hat{W}\hat{w}}(t),
$$
<p>&nbsp;<br>

and if we have that \( ||{\bf \hat{w}}(t+1)-{\bf \hat{w}}(t)||\rightarrow 0 \), we say that
we have reached the most likely state of the system, the so-called steady state or equilibrium state.
Another way of phrasing this is

<p>&nbsp;<br>
$$ {\bf w}(t=\infty) = {\bf Ww}(t=\infty). $$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, a simple Example  <a name="___sec416"></a></h2>

Consider the simple \( 3\times 3 \) matrix \( \hat{W} \)

<p>&nbsp;<br>
$$
   \hat{W} = \left(\begin{array}{ccc} 1/4 & 1/8 & 2/3\\
                                 3/4 & 5/8 & 0\\
                                 0 & 1/4 & 1/3\\   \end{array} \right),
$$
<p>&nbsp;<br>

and we choose our initial state as

<p>&nbsp;<br>
$$
\hat{w}(t=0)=  \left(\begin{array}{c} 1\\
                                 0\\
                                 0 \end{array} \right).
$$
<p>&nbsp;<br>

The first iteration is

<p>&nbsp;<br>
$$
   w_i(t=\epsilon) = W(j\rightarrow i)w_j(t=0),
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
\hat{w}(t=\epsilon)=  \left(\begin{array}{c} 1/4\\
                                3/4 \\
                                 0 \end{array} \right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, a simple Example  <a name="___sec417"></a></h2>

The next iteration results in

<p>&nbsp;<br>
$$
   w_i(t=2\epsilon) = W(j\rightarrow i)w_j(t=\epsilon),
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
\hat{w}(t=2\epsilon)=  \left(\begin{array}{c} 5/32\\
                                21/32 \\
                                6/32 \end{array} \right).
$$
<p>&nbsp;<br>

Note that the vector \( \hat{w} \) is always normalized to \( 1 \). We find the steady state of the system by solving the linear set of equations

<p>&nbsp;<br>
$$ {\bf w}(t=\infty) = {\bf Ww}(t=\infty). $$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, a simple Example  <a name="___sec418"></a></h2>

This linear set of equations reads

<p>&nbsp;<br>
$$
\begin{align}
 W_{11}w_1(t=\infty) +W_{12}w_2(t=\infty) +W_{13}w_3(t=\infty)=&w_1(t=\infty) \nonumber \\
W_{21}w_1(t=\infty) + W_{22}w_2(t=\infty) + W_{23}w_3(t=\infty)=&w_2(t=\infty) \nonumber \\
W_{31}w_1(t=\infty) + W_{32}w_2(t=\infty) + W_{33}w_3(t=\infty)=&w_3(t=\infty) \nonumber \\
\end{align}
$$
<p>&nbsp;<br>

with the constraint that

<p>&nbsp;<br>
$$
   \sum_i w_i(t=\infty) = 1,
$$
<p>&nbsp;<br>

yielding as solution

<p>&nbsp;<br>
$$
\hat{w}(t=\infty)=  \left(\begin{array}{c} 4/15\\
                                8/15 \\
                                3/15 \end{array} \right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, a simple Example  <a name="___sec419"></a></h2>

Convergence of the simple example

<p>
\begin{tabular}{rllll}\hline
Iteration &$w_1$   &$w_2$  &$w_3$\\\hline
0 &amp; 1.00000 &0.00000  &0.00000 \\
1 &amp; 0.25000 &0.75000  &0.00000 \\
2 &amp; 0.15625 &0.62625  &0.18750 \\
3 &amp; 0.24609 &0.52734  &0.22656 \\
4   &0.27848 &0.51416 &0.20736 \\
5   &0.27213 &0.53021 &0.19766 \\
6   &0.26608 &0.53548 &0.19844 \\
7  &0.26575 &0.53424 &0.20002 \\
8  &0.26656 &0.53321 &0.20023 \\
9  &0.26678 &0.53318 &0.20005 \\
10  &0.26671 &0.53332 &0.19998 \\
11  &0.26666 &0.53335 &0.20000 \\
12  &0.26666 &0.53334 &0.20000 \\
13  &0.26667 &0.53333 &0.20000 \\
\( \hat{w}(t=\infty) \) &0.26667 &0.53333 &0.20000 \\
\hline
\end{tabular}

<p>
{\bf In a Markov chain Monte Carlo  \( w \) is normally given, we need to find \( W \)!}

<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, what is happening?  <a name="___sec420"></a></h2>

We have after \( t \)-steps

<p>&nbsp;<br>
$$
   {\bf \hat{w}}(t) = {\bf \hat{W}^t\hat{w}}(0),
$$
<p>&nbsp;<br>

with \( {\bf \hat{w}}(0) \) the distribution at \( t=0 \) and \( {\bf \hat{W}} \) representing the
transition probability matrix.
We can always expand \( {\bf \hat{w}}(0) \) in terms of the right eigenvectors
\( {\bf \hat{v}} \) of \( {\bf \hat{W}} \) as

<p>&nbsp;<br>
$$
    {\bf \hat{w}}(0)  = \sum_i\alpha_i{\bf \hat{v}}_i,
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
   {\bf \hat{w}}(t) = {\bf \hat{W}}^t{\bf \hat{w}}(0)={\bf \hat{W}}^t\sum_i\alpha_i{\bf \hat{v}}_i=
\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i,
$$
<p>&nbsp;<br>

with \( \lambda_i \) the \( i^{\mbox{th}} \) eigenvalue corresponding to
the eigenvector \( {\bf \hat{v}}_i \).

<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, what is happening?  <a name="___sec421"></a></h2>

If we assume that \( \lambda_0 \) is the largest eigenvector we see that in the limit \( t\rightarrow \infty \),
\( {\bf \hat{w}}(t) \) becomes proportional to the corresponding eigenvector
\( {\bf \hat{v}}_0 \). This is our steady state or final distribution.

<p>
In our discussion below in connection with the entropy of a system and later statistical physics and
quantum physics
applications, we will relate these properties to correlation functions such as
the time-correlation function.

<p>
That will allow us to define the so-called <em>equilibration time</em>,viz the time needed for the system
to reach its most likely state. From that state and on we can can compute contributions to various statistical
variables.

<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, what is happening?  <a name="___sec422"></a></h2>

We anticipate parts of the discussion on statistical physics.

<p>
We can relate this property to an observable like the mean magnetization of say a magnetic material.
With the probabilty \( {\bf \hat{w}}(t) \) we
can write the mean magnetization as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \sum_{\mu} {\bf \hat{w}}(t)_{\mu}{\cal M}_{\mu},
$$
<p>&nbsp;<br>

or as the scalar of a  vector product

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m},
$$
<p>&nbsp;<br>

with \( {\bf m} \) being the vector whose elements are the values of \( {\cal M}_{\mu} \) in its
various microstates \( \mu \).

<p>
Recall our definition of an expectation value with a discrete PDF \( p(x_i) \):

<p>&nbsp;<br>
$$
    E[x^k]= \langle x^k\rangle=\frac{1}{N}\sum_{i=1}^{N}x_i^kp(x_i),
$$
<p>&nbsp;<br>

provided that the sums (or integrals) $ \sum_{i=1}^{N}p(x_i)$ converge absolutely (viz ,
$ \sum_{i=1}^{N}|p(x_i)|$ converges)

<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, what is happening?  <a name="___sec423"></a></h2>

We rewrite the last relation as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m}=\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i{\bf m}_i.
$$
<p>&nbsp;<br>

If we define \( m_i={\bf \hat{v}}_i{\bf m}_i \) as the expectation value of
\( {\cal M} \) in the \( i^{\mbox{th}} \) eigenstate we can rewrite the last equation as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
$$
<p>&nbsp;<br>

Since we have that in the limit \( t\rightarrow \infty \) the mean magnetization is dominated by the
largest eigenvalue \( \lambda_0 \), we can rewrite the last equation as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Brownian Motion and Markov Processes, what is happening?  <a name="___sec424"></a></h2>

We define the quantity

<p>&nbsp;<br>
$$
   \tau_i=-\frac{1}{log\lambda_i},
$$
<p>&nbsp;<br>

and rewrite the last expectation value as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
$$
<p>&nbsp;<br>

The quantities \( \tau_i \) are the correlation times for the system. They control also the time-correlation functions.

<p>
The longest correlation time is obviously given by the second largest
eigenvalue \( \tau_1 \), which normally defines the correlation time discussed above. For large times, this is the
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from
\( \lambda_1 \) and we simulate long enough,  \( \tau_1 \) may well define the correlation time.
In other cases we may not be able to extract a reliable result for \( \tau_1 \).

<p>

</section>


<section>

<h2>Entropy and Equilibrium, section 12.4  <a name="___sec425"></a></h2>

The definition of the entropy \( S \) (as a dimensionless quantity here) is
<p>&nbsp;<br>
$$
   S = -\sum_i w_i ln(w_i),
$$
<p>&nbsp;<br>

where \( w_i \) is the probability of finding our system in a state \( i \). For our one-dimensional randow walk
it represents the probability for being at position \( i=i\Delta x \) after a given number of time steps.
Assume now that we have  \( N \) random walkers at
\( i=0 \) and \( t=0 \) and let these random walkers diffuse as function of time.

<p>

</section>


<section>

<h2>Entropy and Equilibrium, section 12.4  <a name="___sec426"></a></h2>

We compute then the probability distribution for \( N \) walkers after a given number of steps \( i \) along \( x \) and
time steps \( j \).
We can then compute an entropy \( S_j \) for a given number of time steps by summing over all probabilities \( i \).
The code used to compute these results is in programs/chapter12/program4.cpp.
Here we have used
100 walkers on a lattice of length from \( L=-50 \) to \( L=50 \) employing periodic boundary conditions meaning
that if a walker reaches the point \( x=L \) it is shifted to \( x=-L \) and if \( x=-L \) it is shifted to \( x=L \).

<p>

</section>


<section>

<h2>Entropy  <a name="___sec427"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// loop over all time steps</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> step=<span style="color: #B452CD">1</span>; step &lt;= time_steps; step++){
    <span style="color: #228B22">// move all walkers with periodic boundary conditions</span>
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> walks = <span style="color: #B452CD">1</span>; walks &lt;= walkers; walks++){
      <span style="color: #8B008B; font-weight: bold">if</span> (ran0(&amp;idum) &lt;= move_probability) {
	<span style="color: #8B008B; font-weight: bold">if</span> ( x[walks] +<span style="color: #B452CD">1</span> &gt; length) {
	  x[walks] = -length;
	}
	<span style="color: #8B008B; font-weight: bold">else</span>{
	  x[walks] += <span style="color: #B452CD">1</span>;
	}
</pre></div>
<p>

</section>


<section>

<h2>Entropy  <a name="___sec428"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">      <span style="color: #8B008B; font-weight: bold">else</span> {
	<span style="color: #8B008B; font-weight: bold">if</span> ( x[walks] -<span style="color: #B452CD">1</span> &lt; -length) {
	  x[walks] = length;

	<span style="color: #8B008B; font-weight: bold">else</span>{
	  x[walks] -= <span style="color: #B452CD">1</span>;
	}

    }  <span style="color: #228B22">// end of loop over walks</span>
  } <span style="color: #228B22">// end of loop over trials</span>
</pre></div>
<p>

</section>


<section>

<h2>Entropy  <a name="___sec429"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// at the final time step we compute the probability</span>
  <span style="color: #228B22">// by counting the number of walkers at every position</span>
  <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">int</span> i = -length; i &lt;= length; i++){
    <span style="color: #a7a7a7; font-weight: bold">int</span> count = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>( <span style="color: #a7a7a7; font-weight: bold">int</span> j = <span style="color: #B452CD">1</span>; j &lt;= walkers; j++){
      <span style="color: #8B008B; font-weight: bold">if</span> ( x[j] == i ) {
        count += <span style="color: #B452CD">1</span>;

    probability[i+length] = count;
</pre></div>
<p>

</section>


<section>

<h2>Entropy  <a name="___sec430"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// Writes the results to screen</span>
<span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">output</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> length, <span style="color: #a7a7a7; font-weight: bold">int</span> time_steps, <span style="color: #a7a7a7; font-weight: bold">int</span> walkers, <span style="color: #a7a7a7; font-weight: bold">int</span> *probability)
{
  <span style="color: #a7a7a7; font-weight: bold">double</span> entropy, histogram;
  <span style="color: #228B22">// find norm of probability</span>
  <span style="color: #a7a7a7; font-weight: bold">double</span> norm = <span style="color: #B452CD">1.0</span>/walkers;
  <span style="color: #228B22">// compute the entropy</span>
  entropy = <span style="color: #B452CD">0.</span>; histogram = <span style="color: #B452CD">0.</span>;
  <span style="color: #8B008B; font-weight: bold">for</span>( <span style="color: #a7a7a7; font-weight: bold">int</span>  i = -length; i &lt;=  length; i++){
    histogram = (<span style="color: #a7a7a7; font-weight: bold">double</span>) probability[i+length]*norm;
    <span style="color: #8B008B; font-weight: bold">if</span> ( histogram &gt; <span style="color: #B452CD">0.0</span>) {
    entropy -= histogram*log(histogram);

<span style="color: #228B22">// then write entropy to file</span>
</pre></div>
<p>

</section>


<section>

<h2>Entropy  <a name="___sec431"></a></h2>

At small time steps
the entropy is very small, reflecting the fact that we have an ordered state. As time elapses, the random walkers spread
out in space (here in one dimension) and the entropy increases as there are more states, that is positions accesible
to the system. We say that the system shows an increased degree of disorder.
After several time steps, we see that the entropy  reaches a constant value, a situation called a steady state.
This signals that the system has reached its equilibrium situation and that the random walkers spread out to
occupy all possible available states. At equilibrium it means thus that all states
are equally probable and this is not baked into any dynamical equations such as Newton's law of motion.

<p>

</section>


<section>

<h2>Entropy  <a name="___sec432"></a></h2>

It occurs because the system is allowed to explore all possibilities. An important hypothesis is the ergodic
hypothesis which states that in equilibrium all available states of a closed
system have equal probability.
This hypothesis states also that if we are able to simulate long enough, then one should be able to trace through all
possible paths in the space of available states to reach the equilibrium situation.
Our Markov process should be able to reach any state of the system from any other state if we run for long enough.

<p>

</section>


<section>

<h2>Detailed Balance  <a name="___sec433"></a></h2>

{\bf In a Markov Monte Carlo  \( w \) is normally given, we need to find \( W \)!}
But we need to find which distribution we obtain when the steady state has been achieved.

<p>
Markov process with transition probability from a state \( j \) to another
       state \( i \)

<p>&nbsp;<br>
$$ \sum_j W(j\rightarrow i) = 1 $$
<p>&nbsp;<br>

Note that the probability for remaining at the same place is not necessarily equal zero.

<p>
PDF \( w_i \) at time  \( t=n\epsilon \)

<p>&nbsp;<br>
$$ w_i(t) = \sum_j W(j\rightarrow i)^nw_j(t=0) $$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$ \sum_i w_i(t) = 1 $$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Detailed Balance  <a name="___sec434"></a></h2>

Detailed balance condition

<p>&nbsp;<br>
$$ \sum_i W(j\rightarrow i)w_j= \sum_i W(i\rightarrow j)w_i  $$
<p>&nbsp;<br>

Ensures that it is the correct  distribution which is achieved when equilibrium
is reached.

<p>
When a Markow process reaches equilibrium we have

<p>&nbsp;<br>
$$ {\bf w}(t=\infty) = {\bf Ww}(t=\infty) $$
<p>&nbsp;<br>


<p>
General condition at equilibrium

<p>&nbsp;<br>
$$ W(j\rightarrow i)w_j= W(i\rightarrow j)w_i  $$
<p>&nbsp;<br>

which is the detailed balance condition.  Proof is simple.

<p>

</section>


<section>

<h2>Detailed Balance  <a name="___sec435"></a></h2>

To derive the conditions for equilibrium, we start from the so-called Master equation, which relates the temporal development of a PDF \( w_i(t) \). The equation is given

<p>&nbsp;<br>
$$
\frac{d w_i(t)}{dt} = \sum_j\left[ W(j\rightarrow i)w_j-W(i\rightarrow j)w_i\right],
$$
<p>&nbsp;<br>

which simply states that the rate at which the systems moves from a state \( j \)
to a final state \( i \) (the first term on the right-hand side of the last equation) is balanced by the rate at which the systems undergoes transitions from the state \( i \) to a state \( j \) (the second term). If we have reached the so-called steady state, then the temporal dependence is zero. This  means that in equilibrium we have
<p>&nbsp;<br>
$$
\frac{d w_i(t)}{dt} = 0.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Ergodicity  <a name="___sec436"></a></h2>

It should be possible for any Markov process to reach every possible state of the system
from any starting point if the simulations is carried out for a long enough time.

<p>
Any state in a Boltzmann distribution has a probability different from zero and if such
a state cannot be reached from a given starting point, then the system is not ergodic.

<p>

</section>


<section>

<h2>Example:  Boltzmann Distribution  <a name="___sec437"></a></h2>

At equilibrium detailed balance gives

<p>&nbsp;<br>
$$ \frac{W(j\rightarrow i)}{W(i\rightarrow j)}=\frac{w_i}{w_j}  $$
<p>&nbsp;<br>


<p>
Boltzmann distribution

<p>&nbsp;<br>
$$ \frac{w_i}{w_j}= \exp{(-\beta(E_i-E_j))}  $$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Selection Rule  <a name="___sec438"></a></h2>

In general

<p>&nbsp;<br>
$$ W(i\rightarrow j)=g(i\rightarrow j)A(i\rightarrow j)  $$
<p>&nbsp;<br>

where \( g \) is a selection probability while  \( A \) is the probability for accepting a
move. It is also called the acceptance ratio.

<p>
With detailed balance this gives

<p>&nbsp;<br>
$$ \frac{g(j\rightarrow i)A(j\rightarrow i)}{g(i\rightarrow j)A(i\rightarrow j)}= \exp{(-\beta(E_i-E_j))}  $$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Metropolis Algorithm  <a name="___sec439"></a></h2>

For a system which follows the Boltzmann distribution the Metropolis algorithm reads

<p>&nbsp;<br>
$$ A(j\rightarrow i)=\left\{\begin{array}{cc} \exp{(-\beta(E_i-E_j))} & E_i-E_j > 0 \\ 1 & else \end{array} \right.  $$
<p>&nbsp;<br>

This algorithm satisfies the condition for detailed balance and ergodicity.

<p>

</section>


<section>

<h2>Implementation  <a name="___sec440"></a></h2>

<ul>
  <p><li> Establish an initial energy \( E_b \)</li>
  <p><li> Do a random change of this initial state by e.g., flipping an individual spin. This new state has energy \( E_t \). Compute then \( \Delta E=E_t-E_b \)</li>
  <p><li> If \( \Delta E \le 0 \) accept the new configuration.</li>
  <p><li> If \( \Delta E > 0 \), compute \( w=e^{-(\beta \Delta E)} \).</li>
  <p><li> Compare \( w \) with a random number \( r \). If \( r \le w \) accept, else keep the old configuration.</li>
  <p><li> Compute the terms in the sums \( \sum A_s P_s \).</li>
  <p><li> Repeat the above steps in order to have a large enough number of microstates</li>
  <p><li> For a given number of MC cycles, compute then expectation values.</li>
</ul>
<p>


</section>


<section>

<h2>Test of the Metropolis Algorithm  <a name="___sec441"></a></h2>

Want  to show that the Metropolis algorithm
generates the Boltzmann distribution
<p>&nbsp;<br>
$$
   P(\beta)=\frac{e^{-\beta E}}{Z},
$$
<p>&nbsp;<br>

with \( \beta=1/kT \) being the inverse temperature, \( E \) is the energy of
the system and
\( Z \) is the partition function. The only functions you will need are those
to generate random numbers.

<p>
We are going to study one single particle in equilibrium with
its surroundings, the latter modeled via a large heat bath
with temperature \( T \).

<p>
The model used to describe this particle is that of an ideal gas
in {\bf one} dimension and with velocity \( -v \) or \( v \).
We are interested in finding  \( P(v)dv \), which expresses the probability
for finding the system with a given velocity \( v\in [v,v+dv] \).
The energy for this one-dimensional system is

<p>&nbsp;<br>
$$
  E=\frac{1}{2}kT=\frac{1}{2}v^2,
$$
<p>&nbsp;<br>

with mass \( m=1 \).

<p>

</section>


<section>

<h2>Test of the Metropolis Algorithm  <a name="___sec442"></a></h2>

Want  to show that the Metropolis algorithm
generates the Boltzmann distribution
<p>&nbsp;<br>
$$
   P(\beta)=\frac{e^{-\beta E}}{Z},
$$
<p>&nbsp;<br>

with \( \beta=1/kT \) being the inverse temperature, \( E \) is the energy of
the system and
\( Z \) is the partition function. The only functions you will need are those
to generate random numbers.

<p>
We are going to study one single particle in equilibrium with
its surroundings, the latter modeled via a large heat bath
with temperature \( T \).

<p>
The model used to describe this particle is that of an ideal gas
in {\bf one} dimension and with velocity \( -v \) or \( v \).
We are interested in finding  \( P(v)dv \), which expresses the probability
for finding the system with a given velocity \( v\in [v,v+dv] \).
The energy for this one-dimensional system is
<p>&nbsp;<br>
$$
  E=\frac{1}{2}kT=\frac{1}{2}v^2,
$$
<p>&nbsp;<br>

with mass \( m=1 \).

<p>

</section>


<section>

<h2>Test of the Metropolis Algorithm, closed form results  <a name="___sec443"></a></h2>

  The partition function of the system of interest is:

<p>&nbsp;<br>
$$
\begin{equation}
    Z = \int_{-\infty}^{+\infty} e^{-\beta v^2 /2} dv = \sqrt{2\pi} \beta^{-1/2} \nonumber
\end{equation}
$$
<p>&nbsp;<br>

  The mean velocity

<p>&nbsp;<br>
$$
\begin{equation}
    \langle v \rangle =  \int_{-\infty}^{+\infty} v e^{-\beta v^2 /2} dv = 0 \nonumber
\end{equation}
$$
<p>&nbsp;<br>

The  expressions for \( \langle E \rangle \) and \( \sigma_E \) assume the following form:

<p>&nbsp;<br>
$$
\begin{equation}
    \langle E \rangle =  \int_{-\infty}^{+\infty} \frac{v^2}{2}\, e^{-\beta v^2 /2} dv = -\frac{1}{Z} \frac{\partial Z}{\partial \beta} =
    \frac{1}{2} \beta^{-1} =  \frac{1}{2} T \nonumber
\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}
    \langle E^2 \rangle =  \int_{-\infty}^{+\infty} \frac{v^4}{4}\, e^{-\beta v^2 /2} dv = \frac{1}{Z} \frac{\partial^2 Z}{\partial \beta^2} =
    \frac{3}{4} \beta^{-2} =  \frac{3}{4} T^2 \nonumber
\end{equation}
$$
<p>&nbsp;<br>

  and

<p>&nbsp;<br>
$$
\begin{equation}
    \sigma_E = \langle E^2 \rangle - \langle E \rangle^2 = \frac{1}{2} T^2 \nonumber
\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Test of the Metropolis Algorithm  <a name="___sec444"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   <span style="color: #8B008B; font-weight: bold">for</span>( montecarlo_cycles=<span style="color: #B452CD">1</span>; Max_cycles; montecarlo_cycles++) {
      ...
      <span style="color: #228B22">// change speed as function of delta v</span>
      v_change = (<span style="color: #B452CD">2</span>*ran1(&amp;idum) -<span style="color: #B452CD">1</span> )* delta_v;
      v_new = v_old+v_change;
      <span style="color: #228B22">// energy change</span>
      delta_E = <span style="color: #B452CD">0.5</span>*(v_new*v_new - v_old*v_old) ;
      ......
      <span style="color: #228B22">// Metropolis algorithm begins here</span>
        <span style="color: #8B008B; font-weight: bold">if</span> ( ran1(&amp;idum) &lt;= exp(-beta*delta_E)  ) {
            accept_step = accept_step + <span style="color: #B452CD">1</span> ;
            v_old = v_new ;

      <span style="color: #228B22">// thereafter we must fill in  P[N] as a function of</span>
      <span style="color: #228B22">// the new speed</span>
      <span style="color: #228B22">// upgrade mean velocity, energy and variance</span>
</pre></div>
<p>

</section>


<section>

<h2>Test of the Metropolis Algorithm  <a name="___sec445"></a></h2>

Analytical vs numerical results. \( T=4 \), \( 10^8 \) MC tries, \( \Delta v = 0.2 \)
    \begin{tabular}{ccc}
      \\
      \hline
      Observable &amp; Analytical value &amp; Numerical value \\
      \hline
      \\
      \( \langle v \rangle  \) &   0.00000  & -0.00679  \\
      \( \langle E \rangle  \) &   2.00000 &amp; 1.99855  \\
      \( \sigma_E           \) &   8.00000 &amp; 8.06669  \\
      \\
      \hline
    \end{tabular}

<p>

</section>


<section>

<h2>Code for Metropolis test  <a name="___sec446"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  v_current = v0;

  <span style="color: #228B22">// start simulation</span>
  ofile.open(<span style="color: #CD5555">&quot;evsmc.dat&quot;</span>);
  <span style="color: #8B008B; font-weight: bold">for</span> (tries = <span style="color: #B452CD">1</span>; tries &lt;= MC; tries++){

    v_change = (<span style="color: #B452CD">2.</span>*ran0(&amp;idum) - <span style="color: #B452CD">1.</span>) * dv;
    v_trial  = v_current + v_change;

    <span style="color: #228B22">// evaluate dE</span>
    delta_E = <span style="color: #B452CD">0.5</span> * ( v_trial * v_trial - v_current * v_current );
</pre></div>
<p>

</section>


<section>

<h2>Code for Metropolis test  <a name="___sec447"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    <span style="color: #228B22">// Metropolis test</span>
    <span style="color: #8B008B; font-weight: bold">if</span> (delta_E &lt;= <span style="color: #B452CD">0</span>) {
      acceptance++; v_current = v_trial;

    <span style="color: #8B008B; font-weight: bold">else</span> <span style="color: #008b45">if</span> (ran0(&amp;idum) &lt;= exp( -beta * delta_E )){
      acceptance++; v_current = v_trial;


    <span style="color: #228B22">// check if velocity value lies within given limits</span>
    <span style="color: #8B008B; font-weight: bold">if</span> (abs(v_current) &gt; v_max) {
      cout&lt;&lt;<span style="color: #CD5555">&quot;Velocity out of range.&quot;</span>; exit(<span style="color: #B452CD">1</span>);
</pre></div>
<p>

</section>


<section>

<h2>Keep or scratch?  <a name="___sec448"></a></h2>

<!-- 2DOFIGURE: [skann0002.jpg, width=500 frac=0.2] -->

<p>

</section>


<section>

<h2>Code for Metropolis test  <a name="___sec449"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    <span style="color: #228B22">// save event in P array</span>
    address = (<span style="color: #a7a7a7; font-weight: bold">int</span>) floor( v_current / dv ) + N/<span style="color: #B452CD">2</span> + <span style="color: #B452CD">1</span>;
    P[address]++;

    <span style="color: #228B22">// update mean velocity, mean energy and energy variance values</span>
    mean_v += v_current;
    mean_E += <span style="color: #B452CD">0.5</span> * v_current * v_current;
    E_variance += <span style="color: #B452CD">0.25</span> * v_current * v_current * v_current * v_current;
</pre></div>
<p>

</section>


<section>

<h2>Code for Metropolis test  <a name="___sec450"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// initialize model parameters</span>
  beta = <span style="color: #B452CD">1.</span>/T; v_max = <span style="color: #B452CD">10.</span> * sqrt (T);
  <span style="color: #228B22">// calculate amount of P-array elements</span>
  N = <span style="color: #B452CD">2</span> * (<span style="color: #a7a7a7; font-weight: bold">int</span>)(v_max/dv) + <span style="color: #B452CD">1</span>;
  <span style="color: #228B22">// initialize P-array</span>
  P = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">int</span> [N];

  <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> i=<span style="color: #B452CD">0</span>; i &lt; N; i++) P[i] = <span style="color: #B452CD">0</span>;

  mean_v = <span style="color: #B452CD">0.</span>; mean_E = <span style="color: #B452CD">0.</span>; E_variance = <span style="color: #B452CD">0.</span>;
  acceptance = <span style="color: #B452CD">0</span>;

}<span style="color: #228B22">// initialize</span>
</pre></div>
<p>

</section>


<section>

<h1>Overview of week 47  <a name="___sec451"></a></h1>


</section>


<section>

<h2>Overview of week 47  <a name="___sec452"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Summary and exam discussion.</b>
<p>

<ul>
  <p><li> Monday:</li>
  <p><li> Brief repetition from last week and discussion of projects</li>
  <p><li> Metropolis algorithm and Variational Monte Carlo</li>
  <p><li> Emphasis on the Monte Carlo project (integration and Variational Monte Carlo)</li>
  <p><li> Tuesday:</li>
  <p><li> Discussion of projects and parallelization (all projects)</li>
  <p><li> Discussion of Monte Carlo simulation of Markov chains (linked with the diffusion project)</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Pros and Cons of Quantum Monte Carlo  <a name="___sec453"></a></h2>

<ul>
  <p><li> Is physically intuitive.</li>
  <p><li> Allows one to study systems with many degrees of freedom. Diffusion Monte Carlo (DMC) and Green's function Monte Carlo (GFMC) yield in principle the exact solution to Schr\"odinger's equation.</li>
  <p><li> Variational Monte Carlo (VMC) is easy to implement but needs a reliable trial wave function, can be difficult to obtain.</li>
  <p><li> DMC/GFMC for fermions (spin with half-integer values, electrons, baryons, neutrinos, quarks) has a sign problem. Nature prefers an anti-symmetric wave function. PDF in this case given distribution of random walkers (\( p\ge 0 \)).</li>
  <p><li> The solution has a statistical error, which can be large.</li>
  <p><li> There is a limit for how large systems one can study, DMC needs a huge number of random walkers in order to achieve stable results.</li>
  <p><li> Obtain only the lowest-lying states with a given symmetry. Can get excited states.</li>
</ul>
<p>


</section>


<section>

<h2>Where and why do we use Monte Carlo Methods in Quantum Physics  <a name="___sec454"></a></h2>

<ul>
  <p><li> Quantum systems with many particles at finite temperature: Path Integral Monte Carlo with applications to dense matter and quantum liquids (phase transitions from normal fluid to superfluid). Strong correlations.</li>
  <p><li> Bose-Einstein condensation of dilute gases, method transition from non-linear PDE to Diffusion Monte Carlo as density increases.</li>
  <p><li> Light atoms, molecules, solids and nuclei.</li>
  <p><li> Lattice Quantum-Chromo Dynamics. Impossible to solve without MC calculations.</li>
  <p><li> Simulations of systems in solid state physics, from semiconductors to spin systems. Many electrons active and possibly strong correlations. Chapter 13 on statistical physics simulations will not be discussed this year.</li>
</ul>
<p>


</section>


<section>

<h2>Bose-Einstein Condensation of atoms, thousands of Atoms in one State, Possible project 5  <a name="___sec455"></a></h2>

<!-- 2DOFIGURE: [BEC_three_peaks, width=500 frac=0.3] -->

<p>

</section>


<section>

<h2>Quantum Monte Carlo and Schr\"odinger's equation  <a name="___sec456"></a></h2>

For one-body problems (one dimension)

<p>&nbsp;<br>
$$
    -\frac{\hbar^2}{2m}\nabla^2\Psi(x,t)+
V(x,t)\Psi(x,t)=
    \imath\hbar\frac{\partial \Psi(x,t)}{\partial t},
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
   P(x,t)=\Psi(x,t)^*\Psi(x,t)
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
   P(x,t)dx=\Psi(x,t)^*\Psi(x,t)dx
$$
<p>&nbsp;<br>

Interpretation: probability of finding the system in a region between
\( x \) and \( x+dx \).
Always real

<p>&nbsp;<br>
$$
   \Psi(x,t)=R(x,t)+\imath I(x,t)
$$
<p>&nbsp;<br>

yielding

<p>&nbsp;<br>
$$
   \Psi(x,t)^*\Psi(x,t)=(R-\imath I)(R+\imath I)=R^2+I^2
$$
<p>&nbsp;<br>

Variational Monte Carlo uses only \( P(x,t) \)!!

<p>

</section>


<section>

<h2>Quantum Monte Carlo and Schr\"odinger's equation  <a name="___sec457"></a></h2>

Petit digression

<p>
Choose \( \tau = it/\hbar \).

<p>
The time-dependent  (1-dim) Schr\"odinger equation becomes then

<p>&nbsp;<br>
$$
  \frac{\partial \Psi(x,\tau)}{\partial \tau} =
  \frac{\hbar^2}{2m} \frac{\partial^2\Psi(x,\tau)}{\partial x^2}
   -V(x,\tau)\Psi(x,\tau).
$$
<p>&nbsp;<br>

With \( V=0 \) we have a diffusion equation in complex time with
diffusion constant

<p>&nbsp;<br>
$$
   D= \frac{\hbar^2}{2m}.
$$
<p>&nbsp;<br>

Used in diffusion Monte Carlo calculations. Topic for FYS4411, Computational Physics II

<p>

</section>


<section>

<h2>Quantum Monte Carlo and Schr\"odinger's equation  <a name="___sec458"></a></h2>

Conditions which \( \Psi \) has to satisfy:

<p>
Normalization

<p>&nbsp;<br>
$$ \int_{-\infty}^{\infty}P(x,t)dx=
\int_{-\infty}^{\infty}\Psi(x,t)^*\Psi(x,t)dx=1 $$
<p>&nbsp;<br>

meaning that

<p>&nbsp;<br>
$$ \int_{-\infty}^{\infty}\Psi(x,t)^*\Psi(x,t)dx < \infty $$
<p>&nbsp;<br>


<p>
And

<ul>
  <p><li> \( \Psi(x,t) \) and \( \partial \Psi(x,t)/\partial x \) must be finite,</li>
  <p><li> \( \Psi(x,t) \) and \( \partial \Psi(x,t)/\partial x \) must be continuous,</li>
  <p><li> \( \Psi(x,t) \) and \( \partial \Psi(x,t)/\partial x \) must be single valued square integrable functions.</li>
</ul>
<p>


</section>


<section>

<h2>First Postulate  <a name="___sec459"></a></h2>

Any physical quantity \( A(\vec{r},\vec{p}) \) which depends on position
\( \vec{r} \) and momentum
\( \vec{p} \) has a corresponding quantum mechanical operator by replacing
\( \vec{p} \)
\( -i\hbar \vec{\bigtriangledown} \), yielding the quantum mechanical operator

<p>&nbsp;<br>
$$
\OP{A} = A(\vec{r},-i\hbar \vec{\bigtriangledown)}.
$$
<p>&nbsp;<br>

\begin{tabular}{|l|l|l|}  \hline
Quantity &amp; Classical definition &amp; QM operator\\
\hline
Position            & \( \vec{r} \)           & $\OP{\vec{r}} = \vec{r}$\\
Momentum    & \( \vec{p} \)
						  & $\OP{\vec{p}} = -i \hbar \vec{\bigtriangledown}$\\
Orbital momentum           & \( \vec{L} = \vec{r} \times \vec{p} \)
		  & $\OP{\vec{L}} = \vec{r} \times (-i\hbar \vec{\bigtriangledown})$\\
Kinetic energy     & \( T = (\vec{p})^2 / 2 m \)
						  & $\OP{T} = - (\hbar^2 / 2 m) (\vec{\bigtriangledown})^2$\\
Total energy 		  & \( H = (p^2 / 2 m) + V(\vec{r}) \)
						  & $\OP{H} = - ( \hbar^2 / 2 m )(\vec{\bigtriangledown})^2
										  + V(\vec{r})$\\
\hline
\end{tabular}

<p>

</section>


<section>

<h2>Second Postulate  <a name="___sec460"></a></h2>

{\sl The only possible outcome of  an ideal measurement of the physical
quantity \( A \) are the eigenvalues of the corresponding quantum mechanical
operator \( \OP{A} \).}

<p>&nbsp;<br>
$$
\OP{A} \psi_{\nu}
	 = a_{\nu} \psi_{\nu},
$$
<p>&nbsp;<br>


<p>
resulting in the eigenvalues  $ a_1, a_2, a_3,\cdots$
as the only outcomes of a measurement. The corresponding
eigenstates
$ \psi_1, \psi_2, \psi_3 \cdots$
contain all relevant information about the system.

<p>

</section>


<section>

<h2>Third Postulate  <a name="___sec461"></a></h2>

Assume \( \Phi \) is
a linear combination of the eigenfunctions
\( \psi_{\nu} \)
for \( \OP{A} \),

<p>&nbsp;<br>
$$
\Phi = c_1 \psi_1 + c_2 \psi_2 + \cdots
  = \sum_{\nu} c_{\nu} \psi_{\nu}.
$$
<p>&nbsp;<br>


<p>
The eigenfunctions are orthogonal
and we get

<p>&nbsp;<br>
$$
c_{\nu} = \int (\Phi)^{\ast} \psi_{\nu} d\tau.
$$
<p>&nbsp;<br>


<p>
From this we can formulate the third postulate:
\sl
When the eigenfunction is  \( \Phi \), the probability of
obtaining the value \( a_{\nu} \) as the outcome of a measurement of the
physical quantity
\( A \) is given by \( |c_{\nu}|^2 \) and \( \psi_{\nu} \) is an eigenfunction of
\( \OP{A} \) with eigenvalue  \( a_{\nu} \).

<p>

</section>


<section>

<h2>Third Postulate  <a name="___sec462"></a></h2>

As a consequence one can show that:
{\sl when a quantal system is in the state \( \Phi \),
the mean value or expectation value of a physical quantity
\( A(\vec{r}, \vec{p}) \)
is given by}

<p>&nbsp;<br>
$$
\langle A \rangle
	= \int (\Phi)^{\ast} \OP{A}(\vec{r}, -i \hbar\vec{\bigtriangledown})
		 \Phi d\tau.
$$
<p>&nbsp;<br>

We  have assumed that
\( \Phi \) has been normalized, viz., \( \int (\Phi)^{\ast} \Phi d\tau = 1 \).
Else

<p>&nbsp;<br>
$$
 \langle A \rangle = \frac{\int (\Phi)^{\ast} \OP{A} \Phi d\tau}
			  {\int (\Phi)^{\ast} \Phi d\tau}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Fourth Postulate  <a name="___sec463"></a></h2>

The time development of of a quantal system is given by

<p>&nbsp;<br>
$$
i \hbar \frac{\partial \Psi}{\partial t} = \OP{H} \Psi,
$$
<p>&nbsp;<br>


<p>
with \( \OP{H} \) the quantal Hamiltonian operator for the system.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec464"></a></h2>

Most quantum mechanical
problems of interest in e.g., atomic, molecular, nuclear and solid state
physics consist of a large number of
interacting electrons and ions or nucleons.
The total number of particles \( N \) is usually sufficiently large
that an exact solution cannot be found.
Typically,
the expectation value for a chosen hamiltonian for a system of
\( N \) particles is

<p>&nbsp;<br>
$$
   \langle H \rangle =
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
   \frac{\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
         \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
          H({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
          \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)}
        {\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
        \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
        \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)},
$$
<p>&nbsp;<br>


<p>
an in general intractable problem.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec465"></a></h2>

Given a hamiltonian \( H \) and a trial
wave function \( \Psi_T \), the variational principle states that
the expectation value of \( \langle H \rangle \), defined through

<p>&nbsp;<br>
$$
   E[H]= \langle H \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})},
$$
<p>&nbsp;<br>

is an upper bound to the ground state energy \( E_0 \) of the hamiltonian \( H \), that
is

<p>&nbsp;<br>
$$
    E_0 \le \langle H \rangle .
$$
<p>&nbsp;<br>

In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones. Traditional integration methods
such as the Gauss-Legendre will not be adequate for say the
computation of the energy of a many-body system.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec466"></a></h2>

The trial wave function can be expanded
in the eigenstates of the hamiltonian since they form a complete set, viz.,

<p>&nbsp;<br>
$$
   \Psi_T({\bf R})=\sum_i a_i\Psi_i({\bf R}),
$$
<p>&nbsp;<br>

and assuming the set of eigenfunctions to be normalized one obtains

<p>&nbsp;<br>
$$
     \frac{\sum_{nm}a^*_ma_n \int d{\bf R}\Psi^{\ast}_m({\bf R})H({\bf R})\Psi_n({\bf R})}
        {\sum_{nm}a^*_ma_n \int d{\bf R}\Psi^{\ast}_m({\bf R})\Psi_n({\bf R})} =\frac{\sum_{n}a^2_n E_n}
        {\sum_{n}a^2_n} \ge E_0,
$$
<p>&nbsp;<br>

where we used that \( H({\bf R})\Psi_n({\bf R})=E_n\Psi_n({\bf R}) \).
In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones.
The variational principle yields the lowest state of a given symmetry.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec467"></a></h2>

In most cases, a wave function has only small values in large parts of
configuration space, and a straightforward procedure which uses
homogenously distributed random points in configuration space
will most likely lead to poor results. This may suggest that some kind
of importance sampling combined with e.g., the Metropolis algorithm
may be  a more efficient way of obtaining the ground state energy.
The hope is then that those regions of configurations space where
the wave function assumes appreciable values are sampled more
efficiently.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec468"></a></h2>

The tedious part in a VMC calculation is the search for the variational
minimum. A good knowledge of the system is required in order to carry out
reasonable VMC calculations. This is not always the case,
and often VMC calculations
serve rather as the starting
point for so-called diffusion Monte Carlo calculations (DMC). DMC is a way of
solving exactly the many-body Schr\"odinger equation by means of
a stochastic procedure. A good guess on the binding energy
and its wave function is however necessary.
A carefully performed VMC calculation can aid in this context.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec469"></a></h2>

Construct first a trial wave function \( \psi_T^{\alpha}({\bf R}) \),
for a many-body
system consisting of \( N \) particles located at positions
\( {\bf R=(R_1,\dots ,R_N)} \). The trial wave function depends
on \( \alpha \) variational parameters
\( {\bf \alpha}=(\alpha_1,\dots ,\alpha_N) \).

<p>
Then we evaluate the expectation value of the hamiltonian \( H \)

<p>&nbsp;<br>
$$
   E[H]=\langle H \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_{T_{\alpha}}({\bf R})H({\bf R})
         \Psi_{T_{\alpha}}({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_{T_{\alpha}}({\bf R})\Psi_{T_{\alpha}}({\bf R})}.
$$
<p>&nbsp;<br>


<p>
Thereafter we vary \( \alpha \) according to some minimization
algorithm and return to the first step.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec470"></a></h2>

Choose a trial wave function
\( \psi_T({\bf R}) \).

<p>&nbsp;<br>
$$
   P({\bf R})= \frac{\left|\psi_T({\bf R})\right|^2}{\int \left|\psi_T({\bf R})\right|^2d{\bf R}}.
$$
<p>&nbsp;<br>

This is our new probability distribution function  (PDF).
The approximation to the expectation value of the Hamiltonian is now

<p>&nbsp;<br>
$$
   E[H]\approx
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}.
$$
<p>&nbsp;<br>

Define a new quantity

<p>&nbsp;<br>
$$
   E_L({\bf R})=\frac{1}{\psi_T({\bf R})}H\psi_T({\bf R}),
   \tag{45}
$$
<p>&nbsp;<br>

called the local energy, which, together with our trial PDF yields

<p>&nbsp;<br>
$$
  E[H]=\langle H \rangle \approx \int P({\bf R})E_L({\bf R}) d{\bf R}\approx \frac{1}{N}\sum_{i=1}^NE_L({\bf R_i}
  \tag{46}
$$
<p>&nbsp;<br>

with \( N \) being the number of Monte Carlo samples.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec471"></a></h2>

Algo:

<ul>
  <p><li> Initialisation: Fix the number of Monte Carlo steps. Choose an initial \( {\bf R} \) and variational parameters \( \alpha \) and calculate \( \left|\psi_T^{\alpha}({\bf R})\right|^2 \).</li>
  <p><li> Initialise the energy and the variance and start the Monte Carlo calculation (thermalize)</li>

<ol>
   <p><li> Calculate a trial position \( {\bf R}_p={\bf R}+r*step \) where \( r \) is a random variable \( r \in [0,1] \).</li>
   <p><li> Metropolis algorithm to accept or reject this move \( w = P({\bf R}_p)/P({\bf R}) \).</li>
   <p><li> If the step is accepted, then we set \( {\bf R}={\bf R}_p \). Update averages</li>
</ol>
<p>

  <p><li> Finish and compute final averages.</li>
</ul>
<p>

Observe that the jumping in space is governed by the variable
\( step \). Called brute-force sampling.  Need importance sampling to get
more relevant sampling.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec472"></a></h2>

The radial Schrodinger equation for the hydrogen atom can be
written as

<p>&nbsp;<br>
$$
-\frac{\hbar^2}{2m}\frac{\partial^2 u(r)}{\partial r^2}-
\left(\frac{ke^2}{r}-\frac{\hbar^2l(l+1)}{2mr^2}\right)u(r)=Eu(r),
$$
<p>&nbsp;<br>

or with dimensionless variables

<p>&nbsp;<br>
$$
-\frac{1}{2}\frac{\partial^2 u(\rho)}{\partial \rho^2}-
\frac{u(\rho)}{\rho}+\frac{l(l+1)}{2\rho^2}u(\rho)-\lambda u(\rho)=0,
\tag{47}
$$
<p>&nbsp;<br>

with the hamiltonian

<p>&nbsp;<br>
$$
H=-\frac{1}{2}\frac{\partial^2 }{\partial \rho^2}-
\frac{1}{\rho}+\frac{l(l+1)}{2\rho^2}.
$$
<p>&nbsp;<br>

Use variational parameter \( \alpha \) in the trial
wave function

<p>&nbsp;<br>
$$
   u_T^{\alpha}(\rho)=\alpha\rho e^{-\alpha\rho}.
   \tag{48}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec473"></a></h2>

Inserting this wave function into the expression for the
local energy \( E_L \) gives

<p>&nbsp;<br>
$$
   E_L(\rho)=-\frac{1}{\rho}-
              \frac{\alpha}{2}\left(\alpha-\frac{2}{\rho}\right).
$$
<p>&nbsp;<br>


<p>
\begin{tabular}{rrrr}\hline
$\alpha$&$\langle H \rangle $&$\sigma^2$&$\sigma/\sqrt{N}$ \\\hline
 7.00000E-01 & -4.57759E-01 &amp; 4.51201E-02 &amp; 6.71715E-04 \\
 8.00000E-01 & -4.81461E-01 &amp; 3.05736E-02 &amp; 5.52934E-04 \\
 9.00000E-01 & -4.95899E-01 &amp; 8.20497E-03 &amp; 2.86443E-04 \\
 1.00000E-00 & -5.00000E-01 &amp; 0.00000E+00 &amp; 0.00000E+00 \\
 1.10000E+00 & -4.93738E-01 &amp; 1.16989E-02 &amp; 3.42036E-04 \\
 1.20000E+00 & -4.75563E-01 &amp; 8.85899E-02 &amp; 9.41222E-04 \\
 1.30000E+00 & -4.54341E-01 &amp; 1.45171E-01 &amp; 1.20487E-03 \\
\end{tabular}

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec474"></a></h2>

We note that at \( \alpha=1 \) we obtain the exact
result, and the variance is zero, as it should. The reason is that
we then have the exact wave function, and the action of the hamiltionan
on the wave function

<p>&nbsp;<br>
$$
   H\psi = \mbox{constant}\times \psi,
$$
<p>&nbsp;<br>

yields just a constant. The integral which defines various
expectation values involving moments of the hamiltonian becomes then

<p>&nbsp;<br>
$$
   \langle H^n \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H^n({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}=
\mbox{constant}\times\frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}=
\mbox{constant}.
$$
<p>&nbsp;<br>

{\bf This gives an important information: the exact wave function leads to zero variance!}
Variation is then performed by minimizing both the energy and the variance.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec475"></a></h2>

The helium atom consists of two electrons and a nucleus with
charge \( Z=2 \).
The contribution
to the potential energy due to the attraction from the nucleus is

<p>&nbsp;<br>
$$
   -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2},
$$
<p>&nbsp;<br>

and if we add the repulsion arising from the two
interacting electrons, we obtain the potential energy

<p>&nbsp;<br>
$$
 V(r_1, r_2)=-\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
$$
<p>&nbsp;<br>

with the electrons separated at a distance
\( r_{12}=|{\bf r}_1-{\bf r}_2| \).

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec476"></a></h2>

The hamiltonian becomes then

<p>&nbsp;<br>
$$
   \OP{H}=-\frac{\hbar^2\nabla_1^2}{2m}-\frac{\hbar^2\nabla_2^2}{2m}
          -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
$$
<p>&nbsp;<br>

and  Schr\"odingers equation reads

<p>&nbsp;<br>
$$
   \OP{H}\psi=E\psi.
$$
<p>&nbsp;<br>

All observables are evaluated with respect to the probability distribution

<p>&nbsp;<br>
$$
   P({\bf R})= \frac{\left|\psi_T({\bf R})\right|^2}{\int \left|\psi_T({\bf R})\right|^2d{\bf R}}.
$$
<p>&nbsp;<br>

generated by the trial wave function.
The trial wave function must approximate an exact
eigenstate in order that accurate results are to be obtained.
Improved trial
wave functions also improve the importance sampling,
reducing the cost of obtaining a certain statistical accuracy.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec477"></a></h2>

Choice of trial wave function for Helium:
Assume \( r_1 \rightarrow 0 \).

<p>&nbsp;<br>
$$
   E_L({\bf R})=\frac{1}{\psi_T({\bf R})}H\psi_T({\bf R})=
     \frac{1}{\psi_T({\bf R})}\left(-\frac{1}{2}\nabla^2_1
     -\frac{Z}{r_1}\right)\psi_T({\bf R}) + \mbox{finite terms}.
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
    E_L(R)=
    \frac{1}{{\cal R}_T(r_1)}\left(-\frac{1}{2}\frac{d^2}{dr_1^2}-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right){\cal R}_T(r_1) + \mbox{finite terms}
$$
<p>&nbsp;<br>

For small values of \( r_1 \), the terms which dominate are

<p>&nbsp;<br>
$$
    \lim_{r_1 \rightarrow 0}E_L(R)=
    \frac{1}{{\cal R}_T(r_1)}\left(-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right){\cal R}_T(r_1),
$$
<p>&nbsp;<br>

since the second derivative does not diverge due to the finiteness of
\( \Psi \) at the origin.

<p>

</section>


<section>

<h2>Quantum Monte Carlo  <a name="___sec478"></a></h2>

This results in

<p>&nbsp;<br>
$$
     \frac{1}{{\cal R}_T(r_1)}\frac{d {\cal R}_T(r_1)}{dr_1}=-Z,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   {\cal R}_T(r_1)\propto e^{-Zr_1}.
$$
<p>&nbsp;<br>

A similar condition applies to electron 2 as well.
For orbital momenta \( l > 0 \) we have

<p>&nbsp;<br>
$$
     \frac{1}{{\cal R}_T(r)}\frac{d {\cal R}_T(r)}{dr}=-\frac{Z}{l+1}.
$$
<p>&nbsp;<br>

Similalry, studying the case \( r_{12}\rightarrow 0 \) we can write
a possible trial wave function as

<p>&nbsp;<br>
$$
   \psi_T({\bf R})=e^{-\alpha(r_1+r_2)}e^{\beta r_{12}}.
    \tag{49}
$$
<p>&nbsp;<br>

The last equation can be generalized to

<p>&nbsp;<br>
$$
   \psi_T({\bf R})=\phi({\bf r}_1)\phi({\bf r}_2)\dots\phi({\bf r}_N)
                   \prod_{i < j}f(r_{ij}),
$$
<p>&nbsp;<br>

for a system with \( N \) electrons or particles.

<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>  <a name="___sec479"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">//  Here we define global variables  used in various functions</span>
<span style="color: #228B22">//  These can be changed by reading from file the different parameters</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> dimension = <span style="color: #B452CD">3</span>; <span style="color: #228B22">// three-dimensional system</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> charge = <span style="color: #B452CD">2</span>;  <span style="color: #228B22">//  we fix the charge to be that of the helium atom</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> my_rank, numprocs;  <span style="color: #228B22">//  these are the parameters used by MPI  to</span>
                        <span style="color: #228B22">//    define which node and how many</span>
<span style="color: #a7a7a7; font-weight: bold">double</span> step_length = <span style="color: #B452CD">1.0</span>;  <span style="color: #228B22">//  we fix the brute force jump to 1 Bohr radius</span>
<span style="color: #a7a7a7; font-weight: bold">int</span> number_particles  = <span style="color: #B452CD">2</span>;  <span style="color: #228B22">//  we fix also the number of electrons to be 2</span>
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, main part  <a name="___sec480"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">//  MPI initializations</span>
  MPI_Init (&amp;argc, &amp;argv);
  MPI_Comm_size (MPI_COMM_WORLD, &amp;numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &amp;my_rank);
  time_start = MPI_Wtime();

  <span style="color: #8B008B; font-weight: bold">if</span> (my_rank == <span style="color: #B452CD">0</span> &amp;&amp; argc &lt;= <span style="color: #B452CD">2</span>) {
    cout &lt;&lt; <span style="color: #CD5555">&quot;Bad Usage: &quot;</span> &lt;&lt; argv[<span style="color: #B452CD">0</span>] &lt;&lt;
      <span style="color: #CD5555">&quot; read also output file on same line&quot;</span> &lt;&lt; endl;
    exit(<span style="color: #B452CD">1</span>);

  <span style="color: #8B008B; font-weight: bold">if</span> (my_rank == <span style="color: #B452CD">0</span> &amp;&amp; argc &gt; <span style="color: #B452CD">2</span>) {
    outfilename=argv[<span style="color: #B452CD">1</span>];
    ofile.open(outfilename);
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, main part  <a name="___sec481"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// Setting output file name for this rank:</span>
ostringstream ost;
ost &lt;&lt; <span style="color: #CD5555">&quot;blocks_rank&quot;</span> &lt;&lt; my_rank &lt;&lt; <span style="color: #CD5555">&quot;.dat&quot;</span>;
<span style="color: #228B22">// Open file for writing:</span>
blockofile.open(ost.str().c_str(), ios::out | ios::binary);

total_cumulative_e = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[max_variations+<span style="color: #B452CD">1</span>];
total_cumulative_e2 = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[max_variations+<span style="color: #B452CD">1</span>];
cumulative_e = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[max_variations+<span style="color: #B452CD">1</span>];
cumulative_e2 = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[max_variations+<span style="color: #B452CD">1</span>];

<span style="color: #228B22">//  initialize the arrays  by zeroing them</span>
<span style="color: #8B008B; font-weight: bold">for</span>( i=<span style="color: #B452CD">1</span>; i &lt;= max_variations; i++){
   cumulative_e[i] = cumulative_e2[i]  = <span style="color: #B452CD">0.0</span>;
   total_cumulative_e[i] = total_cumulative_e2[i]  = <span style="color: #B452CD">0.0</span>;
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, main part  <a name="___sec482"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// broadcast the total number of  variations</span>
MPI_Bcast (&amp;max_variations, <span style="color: #B452CD">1</span>, MPI_INT, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
MPI_Bcast (&amp;number_cycles, <span style="color: #B452CD">1</span>, MPI_INT, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
total_number_cycles = number_cycles*numprocs;
<span style="color: #228B22">// array to store all energies for last variation of alpha</span>
all_energies = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[number_cycles+<span style="color: #B452CD">1</span>];
<span style="color: #228B22">//  Do the mc sampling  and accumulate data with MPI_Reduce</span>
mc_sampling(max_variations, number_cycles, cumulative_e,
              cumulative_e2, all_energies);
<span style="color: #228B22">//  Collect data in total averages</span>
<span style="color: #8B008B; font-weight: bold">for</span>( i=<span style="color: #B452CD">1</span>; i &lt;= max_variations; i++){
  MPI_Reduce(&amp;cumulative_e[i], &amp;total_cumulative_e[i], <span style="color: #B452CD">1</span>, MPI_DOUBLE,
                                        MPI_SUM, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
  MPI_Reduce(&amp;cumulative_e2[i], &amp;total_cumulative_e2[i], <span style="color: #B452CD">1</span>, MPI_DOUBLE,
                               MPI_SUM, <span style="color: #B452CD">0</span>, MPI_COMM_WORLD);
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, main part  <a name="___sec483"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">blockofile.write((<span style="color: #a7a7a7; font-weight: bold">char</span>*)(all_energies+<span style="color: #B452CD">1</span>),
		   number_cycles*<span style="color: #8B008B; font-weight: bold">sizeof</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>));
blockofile.close();
<span style="color: #8B008B; font-weight: bold">delete</span> [] total_cumulative_e; <span style="color: #8B008B; font-weight: bold">delete</span> [] total_cumulative_e2;
<span style="color: #8B008B; font-weight: bold">delete</span> [] cumulative_e; <span style="color: #8B008B; font-weight: bold">delete</span> [] cumulative_e2; <span style="color: #8B008B; font-weight: bold">delete</span> [] all_energies;
<span style="color: #228B22">// End MPI</span>
MPI_Finalize ();
<span style="color: #8B008B; font-weight: bold">return</span> <span style="color: #B452CD">0</span>;
}  <span style="color: #228B22">//  end of main function</span>
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, sampling  <a name="___sec484"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"> alpha = <span style="color: #B452CD">0.5</span>*charge;
 <span style="color: #228B22">// every node has its own seed for the random numbers</span>
 idum = -<span style="color: #B452CD">1</span>-my_rank;
 <span style="color: #228B22">// allocate matrices which contain the position of the particles</span>
 r_old =(<span style="color: #a7a7a7; font-weight: bold">double</span> **)matrix(number_particles,dimension,<span style="color: #8B008B; font-weight: bold">sizeof</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>));
 r_new =(<span style="color: #a7a7a7; font-weight: bold">double</span> **)matrix(number_particles,dimension,<span style="color: #8B008B; font-weight: bold">sizeof</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>));
 <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles; i++) {
    <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
      r_old[i][j] = r_new[i][j] = <span style="color: #B452CD">0</span>;

 <span style="color: #228B22">// loop over variational parameters</span>
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, sampling  <a name="___sec485"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">for</span> (variate=<span style="color: #B452CD">1</span>; variate &lt;= max_variations; variate++){
    <span style="color: #228B22">// initialisations of variational parameters and energies</span>
    alpha += <span style="color: #B452CD">0.1</span>;
    energy = energy2 = <span style="color: #B452CD">0</span>; accept =<span style="color: #B452CD">0</span>; delta_e=<span style="color: #B452CD">0</span>;
    <span style="color: #228B22">//  initial trial position, note calling with alpha</span>
    <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles; i++) {
      <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
	  r_old[i][j] = step_length*(ran2(&amp;idum)-<span style="color: #B452CD">0.5</span>);

    wfold = wave_function(r_old, alpha);
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, sampling  <a name="___sec486"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// loop over monte carlo cycles</span>
<span style="color: #8B008B; font-weight: bold">for</span> (cycles = <span style="color: #B452CD">1</span>; cycles &lt;= number_cycles; cycles++){
   <span style="color: #228B22">// new position</span>
   <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles; i++) {
       <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
  	   r_new[i][j] = r_old[i][j]+step_length*(ran2(&amp;idum)-<span style="color: #B452CD">0.5</span>);

<span style="color: #228B22">//  for the other particles we need to set the position to the old position since</span>
<span style="color: #228B22">//  we move only one particle at the time</span>
     <span style="color: #8B008B; font-weight: bold">for</span> (k = <span style="color: #B452CD">0</span>; k &lt; number_particles; k++) {
	 <span style="color: #8B008B; font-weight: bold">if</span> ( k != i) {
	    <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
	      r_new[k][j] = r_old[k][j];
	    }
	  }
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, sampling  <a name="___sec487"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">      wfnew = wave_function(r_new, alpha);
<span style="color: #228B22">// The Metropolis test is performed by moving one particle at the time</span>
      <span style="color: #8B008B; font-weight: bold">if</span>(ran2(&amp;idum) &lt;= wfnew*wfnew/wfold/wfold ) {
	  <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
	    r_old[i][j]=r_new[i][j];
	  }
	  wfold = wfnew;
	}
      }  <span style="color: #228B22">//  end of loop over particles</span>
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, sampling  <a name="___sec488"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">      <span style="color: #228B22">// compute local energy</span>
      delta_e = local_energy(r_old, alpha, wfold);
      <span style="color: #228B22">// save all energies on last variate</span>
      <span style="color: #8B008B; font-weight: bold">if</span>(variate==max_variations){
	   all_energies[cycles] = delta_e;

      <span style="color: #228B22">// update energies</span>
      energy += delta_e;
      energy2 += delta_e*delta_e;
    }   <span style="color: #228B22">// end of loop over MC trials</span>
    <span style="color: #228B22">// update the energy average and its squared</span>
    cumulative_e[variate] = energy;
    cumulative_e2[variate] = energy2;
  }    <span style="color: #228B22">// end of loop over variational  steps</span>
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, wave function  <a name="___sec489"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// Function to compute the squared wave function, simplest form</span>

<span style="color: #a7a7a7; font-weight: bold">double</span>  <span style="color: #008b45">wave_function</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> **r, <span style="color: #a7a7a7; font-weight: bold">double</span> alpha)
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> i, j, k;
  <span style="color: #a7a7a7; font-weight: bold">double</span> wf, argument, r_single_particle, r_12;

  argument = wf = <span style="color: #B452CD">0</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles; i++) {
    r_single_particle = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span> (j = <span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
      r_single_particle  += r[i][j]*r[i][j];

    argument += sqrt(r_single_particle);

  wf = exp(-argument*alpha) ;
  <span style="color: #8B008B; font-weight: bold">return</span> wf;
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, local energy  <a name="___sec490"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// Function to calculate the local energy with num derivative</span>

<span style="color: #a7a7a7; font-weight: bold">double</span>  <span style="color: #008b45">local_energy</span>(<span style="color: #a7a7a7; font-weight: bold">double</span> **r, <span style="color: #a7a7a7; font-weight: bold">double</span> alpha, <span style="color: #a7a7a7; font-weight: bold">double</span> wfold)
{
  <span style="color: #a7a7a7; font-weight: bold">int</span> i, j , k;
  <span style="color: #a7a7a7; font-weight: bold">double</span> e_local, wfminus, wfplus, e_kinetic, e_potential, r_12,
    r_single_particle;
  <span style="color: #a7a7a7; font-weight: bold">double</span> **r_plus, **r_minus;
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, local energy  <a name="___sec491"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// allocate matrices which contain the position of the particles</span>
  <span style="color: #228B22">// the function matrix is defined in the progam library</span>
  r_plus =(<span style="color: #a7a7a7; font-weight: bold">double</span> **)matrix(number_particles,dimension,<span style="color: #8B008B; font-weight: bold">sizeof</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>));
  r_minus =(<span style="color: #a7a7a7; font-weight: bold">double</span> **)matrix(number_particles,dimension,<span style="color: #8B008B; font-weight: bold">sizeof</span>(<span style="color: #a7a7a7; font-weight: bold">double</span>));
  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles; i++) {
    <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
      r_plus[i][j] = r_minus[i][j] = r[i][j];
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, local energy  <a name="___sec492"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// compute the kinetic energy</span>
  e_kinetic = <span style="color: #B452CD">0</span>;
  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles; i++) {
    <span style="color: #8B008B; font-weight: bold">for</span> (j = <span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
      r_plus[i][j] = r[i][j]+h;
      r_minus[i][j] = r[i][j]-h;
      wfminus = wave_function(r_minus, alpha);
      wfplus  = wave_function(r_plus, alpha);
      e_kinetic -= (wfminus+wfplus-<span style="color: #B452CD">2</span>*wfold);
      r_plus[i][j] = r[i][j];
      r_minus[i][j] = r[i][j];

<span style="color: #228B22">// include electron mass and hbar squared and divide by wave function</span>
  e_kinetic = <span style="color: #B452CD">0.5</span>*h2*e_kinetic/wfold;
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, local energy  <a name="___sec493"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// compute the potential energy</span>
  e_potential = <span style="color: #B452CD">0</span>;
  <span style="color: #228B22">// contribution from electron-proton potential</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles; i++) {
    r_single_particle = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span> (j = <span style="color: #B452CD">0</span>; j &lt; dimension; j++) {
      r_single_particle += r[i][j]*r[i][j];

    e_potential -= charge/sqrt(r_single_particle);
</pre></div>
<p>

</section>


<section>

<h2>VMC code for helium, <code>vmc_para.cpp</code>, local energy  <a name="___sec494"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// contribution from electron-electron potential</span>
  <span style="color: #8B008B; font-weight: bold">for</span> (i = <span style="color: #B452CD">0</span>; i &lt; number_particles-<span style="color: #B452CD">1</span>; i++) {
    <span style="color: #8B008B; font-weight: bold">for</span> (j = i+<span style="color: #B452CD">1</span>; j &lt; number_particles; j++) {
      r_12 = <span style="color: #B452CD">0</span>;
      <span style="color: #8B008B; font-weight: bold">for</span> (k = <span style="color: #B452CD">0</span>; k &lt; dimension; k++) {
	r_12 += (r[i][k]-r[j][k])*(r[i][k]-r[j][k]);

      e_potential += <span style="color: #B452CD">1</span>/sqrt(r_12);
</pre></div>
<p>

</section>


<section>

<h2>Structuring the code  <a name="___sec495"></a></h2>

During the development of our code we need to make several checks. It is also very instructive to compute a closed form expression for the local energy. Since our wave function is rather simple  it is straightforward
to find an analytic expressions.  Consider first the case of the simple helium function

<p>&nbsp;<br>
$$
   \Psi_T(\mathbf{r}_1,\mathbf{r}_2) = e^{-\alpha(r_1+r_2)}
$$
<p>&nbsp;<br>

The local energy is for this case

<p>&nbsp;<br>
$$
E_{L1} = \left(\alpha-Z\right)\left(\frac{1}{r_1}+\frac{1}{r_2}\right)+\frac{1}{r_{12}}-\alpha^2
$$
<p>&nbsp;<br>

which gives an expectation value for the local energy given by

<p>&nbsp;<br>
$$
\langle E_{L1} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Structuring the code  <a name="___sec496"></a></h2>

With analytic formulae we  can speed up the computation of the correlation. In our case
we write it as

<p>&nbsp;<br>
$$
\Psi_C= \exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$
<p>&nbsp;<br>

which means that the gradient needed for the local energy
can be calculated analytically.
This will speed up your code since the computation of the correlation part and the Slater determinant are the most
time consuming parts in your code.

<p>
We will refer to this correlation function as \( \Psi_C \) or the <em>linear Pad\'e-Jastrow</em>.

<p>

</section>


<section>

<h2>Structuring the code  <a name="___sec497"></a></h2>

We can test this by computing the local energy for our helium wave function

<p>&nbsp;<br>
$$
   \psi_{T}({\bf r}_1,{\bf r}_2) =
   \exp{\left(-\alpha(r_1+r_2)\right)}
   \exp{\left(\frac{r_{12}}{2(1+\beta r_{12})}\right)},
$$
<p>&nbsp;<br>


<p>
with \( \alpha \) and \( \beta \) as variational parameters.

<p>
The local energy is for this case

<p>&nbsp;<br>
$$
E_{L2} = E_{L1}+\frac{1}{2(1+\beta r_{12})^2}\left\{\frac{\alpha(r_1+r_2)}{r_{12}}(1-\frac{\mathbf{r}_1\mathbf{r}_2}{r_1r_2})-\frac{1}{2(1+\beta r_{12})^2}-\frac{2}{r_{12}}+\frac{2\beta}{1+\beta r_{12}}\right\}
$$
<p>&nbsp;<br>

Getting a closed form expression for the local energy means that you don't need to
compute derivatives numerically.

<p>

</section>


<section>

<h2>Structuring the code, simple task  <a name="___sec498"></a></h2>

<ul>
  <p><li> Make another copy of your code.</li>
  <p><li> Implement the closed form expression for the local energy</li>
  <p><li> Compile the new and old codes with the -pg option for profiling.</li>
  <p><li> Run both codes and profile them afterwards using <code>gprof <executable> > outprofile</code></li>
  <p><li> Study the time usage in the file <code>outprofile</code></li>
</ul>
<p>


</section>


<section>

<h2>Structuring the code, simple task  <a name="___sec499"></a></h2>

<ul>
  <p><li> Use also better compiler options like <code>c++ -O3</code></li>
  <p><li> Can speed up considerably your code</li>
</ul>
<p>


</section>


<section>

<h1>Overview of week 48  <a name="___sec500"></a></h1>


</section>


<section>

<h2>Overview of week 48  <a name="___sec501"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Summary and exam discussion.</b>
<p>

<ul>
  <p><li> Monday:</li>
  <p><li> Brief repetition from last week and discussion of projects</li>
  <p><li> Project discussions and structure of report</li>
  <p><li> Summary and exam discussion</li>
</ul>
</div>

No lecture on Tuesday. Monday is the last lecture this semester.

<p>

</section>


<section>

<h2>The report  <a name="___sec502"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should it contain? A possible structure.</b>
<p>

<ul>
  <p><li> An introduction where you explain the rational for the physics case and what you have done. At the end of the introduction you should give a brief summary of the structure of the report</li>
  <p><li> Theoretical models and technicalities. This is the methods section.</li>
  <p><li> Results and discussion</li>
  <p><li> Conclusions and perspectives</li>
  <p><li> Appendix with extra material, include your program!!</li>
  <p><li> Bibliography</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec503"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Introduction.</b>
<p>
You don't need to answer all questions in a chronological order.  When you write the introduction you could focus on the following aspects

<ul>
  <p><li> Motivate the reader, the first part of the introduction gives always a motivation and tries to give the overarching ideas</li>
  <p><li> What I have done</li>
  <p><li> The structure of the report, how it is organized etc</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec504"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Methods sections.</b>
<p>

<ul>
  <p><li> Describe the methods and algorithms</li>
  <p><li> You need to explain how you implemented the methods and also say something about the structure of your algorithm and present some parts of your code</li>
  <p><li> You should plug in some calculations to demonstrate your code, such as selected runs used to validate and verify your results. The latter is extremely important!! A reader needs to understand that your code reproduces selected benchmarks and reproduces previous results, either numerical and/or well-known closed form expressions.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec505"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Results.</b>
<p>

<ul>
  <p><li> Present your results</li>
  <p><li> Give a critical discussion of your work and place it in the correct context.</li>
  <p><li> Relate your work to other calculations/studies</li>
  <p><li> An eventual reader should be able to reproduce your calculations if she/he wants to do so. All input variables should be properly explained.</li>
  <p><li> Make sure that figures and tables should contain enough information in their captions, axis labels etc so that an eventual reader can gain a first impression of your work by studying figures and tables only.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec506"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? Conclusions.</b>
<p>

<ul>
  <p><li> State your main findings and interpretations</li>
  <p><li> Try as far as possible to present perspectives for future work</li>
  <p><li> Try to discuss the pros and cons of the methods and possible improvements</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec507"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? additional material.</b>
<p>

<ul>
  <p><li> Additional calculations used to validate the codes</li>
  <p><li> Selected calculations, these can be listed with few comments</li>
  <p><li> Listing of the code if you feel this is necessary You can consider moving parts of the material from the methods section to the appendix. You can also place additional material on your webpage.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>The report  <a name="___sec508"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>What should I focus on? References.</b>
<p>

<ul>
  <p><li> Give always references to material you base your work on, either scientific articles/reports or books.</li>
  <p><li> <em>Wikipedia is not accepted as a scientific reference</em>. Under no circumstances.</li>
  <p><li> Refer to articles as: name(s) of author(s), journal, volume (boldfaced), page and year in parenthesis.</li>
  <p><li> Refer to books as: name(s) of author(s), title of book, publisher, place and year, eventual page numbers</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Exam FYS3150/FYS4150  <a name="___sec509"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Pensum/syllabus.</b>
<p>
Go to <a href="http://www.uio.no/studier/emner/matnat/fys/FYS3150/h13/" target="_blank"><tt>http://www.uio.no/studier/emner/matnat/fys/FYS3150/h13/</tt></a>
and click on syllabus.

<p>
For the written exam, you can bring with you 2 pages (written on both sides) with own notes. See also the exams
from last years.
</div>


<p>

</section>


<section>

<h2>Exam FYS3150/FYS4150  <a name="___sec510"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Topics.</b>
<p>

<ul>
  <p><li> Linear algebra and eigenvalue problems. (Lecture notes chapters 6.1-6.5 and 7.1-7.5 and projects 1 and 2).</li>
  <p><li> Numerical integration, standard methods and Monte Carlo methods (Lecture notes chapters 5.1-5.5 and 11 and project 5).</li>
  <p><li> Ordinary differential equations (Lecture notes chapter 8 and projects 3 and 5 )</li>
  <p><li> Partial differential equations (Lecture notes chapter 10 and projects 4 and 5)</li>
  <p><li> Monte Carlo methods in physics (Lecture notes chapters 11, 12 and 14, project 5)</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Exam FYS3150/FYS4150  <a name="___sec511"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Linear algebra and eigenvalue problems, chapters 6.1-6.5 and 7.1-7.5.</b>
<p>

<ul>
  <p><li> Know Gaussian elimination and LU decomposition (project 1)</li>
  <p><li> How to solve linear equations (project 1)</li>
  <p><li> How to obtain the inverse and the determinant of a real symmetric matrix</li>
  <p><li> Cubic spline</li>
  <p><li> Tridiagonal matrix decomposition (project 1)</li>
  <p><li> Householder's tridiagonalization technique and finding eigenvalues based on this</li>
  <p><li> Jacobi's method for finding eigenvalues (project 2)</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Exam FYS3150/FYS4150  <a name="___sec512"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Numerical integration, standard methods and Monte Carlo methods (5.1-5.5 and 11).</b>
<p>

<ul>
  <p><li> Trapezoidal, rectangle and Simpson's rules</li>
  <p><li> Gaussian quadrature, emphasis on Legendre polynomials, but you need to know about other polynomials as well (project 5).</li>
  <p><li> Brute force Monte Carlo integration (project 5)</li>
  <p><li> Random numbers (simplest algo, ran0) and probability distribution functions, expectation values</li>
  <p><li> Improved Monte Carlo integration and importance sampling (project 5).</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Exam FYS3150/FYS4150  <a name="___sec513"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Monte Carlo methods in physics (12 and 14).</b>
<p>

<ul>
  <p><li> Random walks and Markov chains and relation with diffusion equation (project 5)</li>
  <p><li> Metropolis algorithm, detailed balance and ergodicity (project 5)</li>
  <p><li> Applications to quantum mechanical systems (project 5)</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Exam FYS3150/FYS4150  <a name="___sec514"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Ordinary differential equations (Chapter 8).</b>
<p>

<ul>
  <p><li> Euler's method and improved Euler's method, truncation errors (project 3)</li>
  <p><li> Runge Kutta methods, 2nd and 4th order, truncation errors (project 3)</li>
  <p><li> Leap-frog and Verlet algoritm (project 5)</li>
  <p><li> How to implement a second-order differential equation, both linear and non-linear. How to make your equations dimensionless.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Exam FYS3150/FYS4150  <a name="___sec515"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Partial differential equations, chapter 10.</b>
<p>

<ul>
  <p><li> Set up diffusion, Poisson and wave equations up to 2 spatial dimensions and time</li>
  <p><li> Set up the mathematical model and algorithms for these equations, with boundary and initial conditions. The stability conditions for the diffusion equation.</li>
  <p><li> Explicit, implicit and Crank-Nicolson schemes, and how to solve them. Remember that they result in triangular matrices (project 4).</li>
  <p><li> Diffusion equation in two dimensions (project 5)</li>
  <p><li> How to compute the Laplacian in Poisson's equation (project 5).</li>
  <p><li> How to solve the wave equation in one and two dimensions using an explicit scheme.</li>
</ul>
</div>


<p>
x

<p>

</section>


<section>

<h2>Other courses in Computational Science at UiO  <a name="___sec516"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Bachelor/Master/PhD Courses.</b>
<p>

<ul>
  <p><li> INF-MAT4350 Numerical linear algebra</li>
  <p><li> MAT-INF3300/3310/4300/4310, PDEs and Sobolev spaces I and II</li>
  <p><li> INF-MAT3360 Partial differential equation</li>
  <p><li> INF3380 Parallell programming for scientific problems</li>
  <p><li> INF5620 Numerical methods for PDEs, finite element method</li>
  <p><li> FYS4411 Computational physics II, computational quantum mechanics.</li>
  <p><li> FYS4460: Computational statistical mechanics</li>
</ul>
</div>


<p>

</section>


<section>

<h2>AND, GOOD LUCK TO YOU ALL!  <a name="___sec517"></a></h2>

<center><p><img src="fig-slides1/Nebbdyr2.png" align="bottom" width=500></p></center>

<p>

</section>


<section>

<h2>Version control, Git and dropbox  <a name="___sec518"></a></h2>

Why version control?

<ul>
  <p><li> It allows you to keep track of different change made</li>
  <p><li> It allows people you collaborate with to see the recent changes</li>
  <p><li> It becomes an excellent logbook and allows people to verify your results</li>
  <p><li> It can aso be used to build up a large directory of codes, with validation examples as well Git is a very popular version control software, and easy to use. To install on ubuntu just write <code>cppcod sudo apt-get install git-core</code>.</li>
</ul>
<p>

This applies to Dropbox as well.

<p>

</section>


<section>

<h2>Version control, Git  <a name="___sec519"></a></h2>

When you want to start tracking a project just go to the project's directory and type

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">git init
</pre></div>
<p>
This creates a subdirectory .git tat contains all of your necessary repository files. At this point nothing in your project is tracked yet.  To start, there are a few commands you need in the beginning. As an example

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">git add *.cpp  *.hpp
git add ANOTHERFILE
git commit -m <span style="color: #a61717; background-color: #e3d2d2">&#39;</span>This is first setup of my superduper project<span style="color: #a61717; background-color: #e3d2d2">&#39;</span>
</pre></div>
<p>
using

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">git status
</pre></div>
<p>
Tells you about the modifcations made. You can also standard unix commands like \( mv \), \( rm \) etc

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">git mv file_from file_to
</pre></div>
<p>

</section>


<section>

<h2>Version control, Git  <a name="___sec520"></a></h2>

If you want to get a copy of an existing Git repository for example,
a project you would like to contribute to, the command you need is

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">git clone
</pre></div>
<p>
If you are familiar with other VCS systems such as Subversion, you will
notice that the command is clone and not checkout. This is an important distinction.
Git receives a copy of nearly all data that the server has.
Every version of every file for the history of the project is pulled down when you run git clone.
In fact, if your server disk gets corrupted, you can use any of the clones on any client to set the server back to the state it was in when it was cloned.

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">git status
</pre></div>
<p>

</section>


<section>

<h2>Optimization and profiling, useful for project 3  <a name="___sec521"></a></h2>

Till now we have not paid much attention to speed and possible optimization possibilities
inherent in the various compilers. We have compiled and linked as

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">c++  -c  mycode.cpp
c++  -o  mycode.exe  mycode.o
</pre></div>
<p>
This is what we call a flat compiler option and should be used when we develop the code.
It produces normally a very large and slow code when translated to machine instructions.
We use this option for debugging and for establishing the correct program output because
every operation is done precisely as the user specified it.

<p>
It is instructive to look up the compiler manual for further instructions

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">man c++  &gt;  out_to_file
</pre></div>
<p>

</section>


<section>

<h2>Optimization and profiling  <a name="___sec522"></a></h2>

We have additional compiler options for optimization. These may include procedure inlining where
performance may be improved, moving constants inside loops outside the loop,
identify potential parallelism, include automatic vectorization or replace a division with a reciprocal
and a multiplication if this speeds up the code.

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">c++  -O3 -c  mycode.cpp
c++  -O3 -o  mycode.exe  mycode.o
</pre></div>
<p>
This is the recommended option. {\bf But you must check that you get the same results as previously}.

<p>

</section>


<section>

<h2>Optimization and profiling  <a name="___sec523"></a></h2>

It is also useful to profile your program under the development stage.
You would then compile with (Mac and unix/linux)

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">c++  -pg -O3 -c  mycode.cpp
c++  -pg -O3 -o  mycode.exe  mycode.o
</pre></div>
<p>
After you have run the code you can obtain the profiling information via

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">gprof mycode.exe &gt;  out_to_profile
</pre></div>
<p>
When you have profiled properly your code, you must take out this option as it
increases your CPU expenditure.

<p>

</section>


<section>

<h2>Optimization and profiling  <a name="___sec524"></a></h2>

Other hints

<ul>
  <p><li> avoid if tests or call to functions inside loops, if possible.</li>
  <p><li> avoid multiplication with constants inside loops if possible.</li>
</ul>
<p>

Bad code

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #8B008B; font-weight: bold">for</span> i = <span style="color: #B452CD">1</span>:n a(i) = b(i) +c*d e = g(k) end
</pre></div>
<p>
Better code

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">temp = c*d <span style="color: #8B008B; font-weight: bold">for</span> i = <span style="color: #B452CD">1</span>:n a(i) = b(i) + temp end e = g(k)
</pre></div>
<p>

</section>


<section>

<h2>What do we have then?  <a name="___sec525"></a></h2>

<ul>
  <p><li> Several functions under the program link, see the file OOcodes.tar.gz</li>
  <p><li> Matrix and array manipulations (similar to Blitz++/Armadillo)</li>
  <p><li> Random numbers and numerical integration</li>
  <p><li> Functions to convert arrays from C++ to Numpy and viceversa</li>
  <p><li> Numerical derivatives and differential equation solvers. These files can serve as a help when you want to start to write your own classes. We will discuss some of these classes during the course.</li>
</ul>
<p>


</section>


<section>

<h2>But what should I do else????? Some hints.  <a name="___sec526"></a></h2>

If your needs (common in most problems) include handling of large arrays and linear
algebra problem, I would not recommend to write your own vector-matrix or more general array handling class.
It is easyto make error.

<ul>
  <p><li> Old-fashioned allocation of arrays and explicit handling of all loops in for example matrix-matrix multiplication.</li>
  <p><li> You can use what we have developed for Blitz++ or the Array class (but more limited than Blitz++)</li>
  <p><li> or (recommended) you can use armadillo, a great C++ library for handling arrays and doing linear algebra.</li>
  <p><li> Armadillo provides a user friendly interface to lapack and blas functions. Here an example of using the Blas function {\bf DGEMM} for matrix-matrix multiplication.</li>
  <p><li> After having installed armadillo, compile with {\bf c++ -O3 -o test.x test.cpp -lblas -lamardillo -llapack}.</li>
</ul>
<p>


</section>


<section>

<h2>Matrix-matrix multiplication  <a name="___sec527"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #1e889b">#include &lt;cstdlib&gt;</span>
<span style="color: #1e889b">#include &lt;ios&gt;</span>
<span style="color: #1e889b">#include &lt;iostream&gt;</span>
<span style="color: #1e889b">#include &lt;armadillo&gt;</span>
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> std;
<span style="color: #8B008B; font-weight: bold">using</span> <span style="color: #8B008B; font-weight: bold">namespace</span> arma;

<span style="color: #228B22">/*Because fortran files don&#39;t have any header files,</span>
<span style="color: #228B22"> *we need to declare the functions ourself.*/</span>
<span style="color: #8B008B; font-weight: bold">extern</span> <span style="color: #CD5555">&quot;C&quot;</span>
{
    <span style="color: #a7a7a7; font-weight: bold">void</span> dgemm_(<span style="color: #a7a7a7; font-weight: bold">char</span>*, <span style="color: #a7a7a7; font-weight: bold">char</span>*, <span style="color: #a7a7a7; font-weight: bold">int</span>*, <span style="color: #a7a7a7; font-weight: bold">int</span>*, <span style="color: #a7a7a7; font-weight: bold">int</span>*, <span style="color: #a7a7a7; font-weight: bold">double</span>*,
            <span style="color: #a7a7a7; font-weight: bold">double</span>*, <span style="color: #a7a7a7; font-weight: bold">int</span>*, <span style="color: #a7a7a7; font-weight: bold">double</span>*, <span style="color: #a7a7a7; font-weight: bold">int</span>*, <span style="color: #a7a7a7; font-weight: bold">double</span>*, <span style="color: #a7a7a7; font-weight: bold">double</span>*, <span style="color: #a7a7a7; font-weight: bold">int</span>*);
</pre></div>
<p>

</section>


<section>

<h2>Matrix-matrix multiplication  <a name="___sec528"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">main</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> argc, <span style="color: #a7a7a7; font-weight: bold">char</span>** argv)
{
    <span style="color: #228B22">//Dimensions</span>
    <span style="color: #a7a7a7; font-weight: bold">int</span> n = atoi(argv[<span style="color: #B452CD">1</span>]);
    <span style="color: #a7a7a7; font-weight: bold">int</span> m = n;
    <span style="color: #a7a7a7; font-weight: bold">int</span> p = m;

    <span style="color: #228B22">/*Create random matrices</span>
<span style="color: #228B22">     * (note that older versions of armadillo uses &quot;rand&quot; instead of &quot;randu&quot;) */</span>
    srand(time(<span style="color: #658b00">NULL</span>));
    mat A(n, p);
    A.randu();
</pre></div>
<p>

</section>


<section>

<h2>Matrix-matrix multiplication  <a name="___sec529"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    <span style="color: #228B22">// Pretty print, and pretty save, are as easy as the two following lines.</span>
    <span style="color: #228B22">//    cout &lt;&lt; A &lt;&lt; endl;</span>
    <span style="color: #228B22">//    A.save(&quot;A.mat&quot;, raw_ascii);</span>
    mat A_trans = trans(A);
    mat <span style="color: #008b45">B</span>(p, m);
    B.randu();
    mat <span style="color: #008b45">C</span>(n, m);
    <span style="color: #228B22">//    cout &lt;&lt; B &lt;&lt; endl;</span>
    <span style="color: #228B22">//    B.save(&quot;B.mat&quot;, raw_ascii);</span>
</pre></div>
<p>

</section>


<section>

<h2>Matrix-matrix multiplication  <a name="___sec530"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    <span style="color: #228B22">//   ARMADILLO  TEST</span>
    cout &lt;&lt; <span style="color: #CD5555">&quot;Starting armadillo multiplication\n&quot;</span>;
    <span style="color: #228B22">//Simple wall_clock timer is a part of armadillo.</span>
    wall_clock timer;
    timer.tic();
    C = A*B;
    <span style="color: #a7a7a7; font-weight: bold">double</span> num_sec = timer.toc();
    cout &lt;&lt; <span style="color: #CD5555">&quot;-- Finished in &quot;</span> &lt;&lt; num_sec &lt;&lt; <span style="color: #CD5555">&quot; seconds.\n\n&quot;</span>;
</pre></div>
<p>

</section>


<section>

<h2>Matrix-matrix multiplication  <a name="___sec531"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    C = zeros&lt;mat&gt; (n, m);
    cout &lt;&lt; <span style="color: #CD5555">&quot;Starting blas multiplication.\n&quot;</span>;

        <span style="color: #a7a7a7; font-weight: bold">char</span> trans = <span style="color: #CD5555">&#39;N&#39;</span>;
        <span style="color: #a7a7a7; font-weight: bold">double</span> alpha = <span style="color: #B452CD">1.0</span>;
        <span style="color: #a7a7a7; font-weight: bold">double</span> beta = <span style="color: #B452CD">0.0</span>;
        <span style="color: #a7a7a7; font-weight: bold">int</span> _numRowA = A.n_rows;
        <span style="color: #a7a7a7; font-weight: bold">int</span> _numColA = A.n_cols;
        <span style="color: #a7a7a7; font-weight: bold">int</span> _numRowB = B.n_rows;
        <span style="color: #a7a7a7; font-weight: bold">int</span> _numColB = B.n_cols;
        <span style="color: #a7a7a7; font-weight: bold">int</span> _numRowC = C.n_rows;
        <span style="color: #a7a7a7; font-weight: bold">int</span> _numColC = C.n_cols;
        <span style="color: #a7a7a7; font-weight: bold">int</span> lda = (A.n_rows &gt;= A.n_cols) ? A.n_rows : A.n_cols;
        <span style="color: #a7a7a7; font-weight: bold">int</span> ldb = (B.n_rows &gt;= B.n_cols) ? B.n_rows : B.n_cols;
        <span style="color: #a7a7a7; font-weight: bold">int</span> ldc = (C.n_rows &gt;= C.n_cols) ? C.n_rows : C.n_cols;
</pre></div>
<p>

</section>


<section>

<h2>Matrix-matrix multiplication, calling DGEMM  <a name="___sec532"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">        dgemm_(&amp;trans, &amp;trans, &amp;_numRowA, &amp;_numColB, &amp;_numColA, &amp;alpha,
                A.memptr(), &amp;lda, B.memptr(), &amp;ldb, &amp;beta, C.memptr(), &amp;ldc);
</pre></div>
<p>

</section>


<section>

<h1>Overview of week 44  <a name="___sec533"></a></h1>


</section>


<section>

<h2>Overview of week 44  <a name="___sec534"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Monte Carlo methods, chapter 14.</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Statistical physics and the Metropolis algorithm (covered by chapter 13)</li>
  <p><li> Discussion of project 4.</li>
  <p><li> Wednesday:</li>
  <p><li> Statistical physics and the Metropolis algorithm (covered by chapter 13)</li>
  <p><li> Discussion of project 4.</li>
  <p><li> Introduction to differential equations</li>
  <p><li> Differential equations, general properties.</li>
  <p><li> Runge-Kutta methods Chapter 14 on quantum mechanical Monte Carlo is not part of this year's curriculum.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Most Common Ensembles in Statistical Physics  <a name="___sec535"></a></h2>

\begin{tabular}{|l|c|c|c|c|}
\hline
&&&&\\
&\multicolumn{1}{|c}{Microcanonical}&\multicolumn{1}{|c}{Canonical}&
\multicolumn{1}{|c}{Grand can.}&\multicolumn{1}{|c|}{Pressure can.}\\
&&&&\\
\hline
&&&&\\
Exchange of heat  &no&yes&yes&yes\\
with the environment&&&&\\
&&&&\\
Exchange of particles&no&no&yes&no\\
with the environemt&&&&\\
&&&&\\
Thermodynamical&$V, \cal M, \cal D$&$V, \cal M, \cal D$&$V, \cal M, \cal D$
&$P, \cal H, \cal E$\\
parameters&     $E$&$T$&$T$&$T$\\
	    &$N$&$N$&$\mu$&$N$\\
&&&&\\
Potential& Entropy& Helmholtz & \( PV \) & Gibbs\\
&&&&\\
Energy &Internal& Internal &amp; Internal &amp; Enthalpy\\
&&&&\\

<p>
&&&&\\
&&&&\\ \hline
\end{tabular}

<p>

</section>


<section>

<h2>Microcanonical Ensemble  <a name="___sec536"></a></h2>

Entropy

<p>&nbsp;<br>
$$
\begin{equation}S=k_{B}ln\Omega\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}dS=\frac{1}{T}dE+\frac{p}{T}dV-\frac{\mu}{T}dN\end{equation}
$$
<p>&nbsp;<br>


<p>
Temperature

<p>&nbsp;<br>
$$
\begin{equation}\frac{1}{k_{B}T}=\left(\frac{\partial ln\Omega}{\partial E}\right)_{N, V}\end{equation}
$$
<p>&nbsp;<br>


<p>
Pressure

<p>&nbsp;<br>
$$
\begin{equation}\frac{p}{k_{B}T}=\left(\frac{\partial ln\Omega}{\partial V}\right)_{N, E}\end{equation}
$$
<p>&nbsp;<br>


<p>
Chemical potential

<p>&nbsp;<br>
$$
\begin{equation}\frac{\mu}{k_{B}T}=-\left(\frac{\partial ln\Omega}{\partial N}\right)_{V, E}\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Canonical Ensemble  <a name="___sec537"></a></h2>

Helmholtz Free Energy

<p>&nbsp;<br>
$$
\begin{equation}F=-k_{B}TlnZ\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}dF=-SdT-pdV+\mu dN\end{equation}
$$
<p>&nbsp;<br>


<p>
Entropy

<p>&nbsp;<br>
$$
\begin{equation}S =k_{B}lnZ
+k_{B}T\left(\frac{\partial lnZ}{\partial T}\right)_{N, V}\end{equation}
$$
<p>&nbsp;<br>


<p>
Pressure

<p>&nbsp;<br>
$$
\begin{equation}p=k_{B}T\left(\frac{\partial lnZ}{\partial V}\right)_{N, T}\end{equation}
$$
<p>&nbsp;<br>


<p>
Chemical Potential

<p>&nbsp;<br>
$$
\begin{equation}\mu =-k_{B}T\left(\frac{\partial lnZ}{\partial N}\right)_{V, T}\end{equation}
$$
<p>&nbsp;<br>


<p>
Energy (internal only)

<p>&nbsp;<br>
$$
\begin{equation}E =k_{B}T^{2}\left(\frac{\partial lnZ}{\partial T}\right)_{V, N}\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Grand Canonical Ensemble  <a name="___sec538"></a></h2>

Potential

<p>&nbsp;<br>
$$
\begin{equation}pV=k_{B}Tln\Xi\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}d(pV)=SdT+Nd\mu +pdV\end{equation}
$$
<p>&nbsp;<br>


<p>
Entropy

<p>&nbsp;<br>
$$
\begin{equation}S =k_{B}ln\Xi
+k_{B}T\left(\frac{\partial ln\Xi}{\partial T}\right)_{V, \mu}\end{equation}
$$
<p>&nbsp;<br>


<p>
Particles

<p>&nbsp;<br>
$$
\begin{equation}N =k_{B}T\left(\frac{\partial ln\Xi}{\partial \mu}\right)_{V, T}\end{equation}
$$
<p>&nbsp;<br>


<p>
Pressure

<p>&nbsp;<br>
$$
\begin{equation}p =k_{B}T\left(\frac{\partial ln\Xi}{\partial V}\right)_{\mu, T}\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Pressure Canonical Ensemble  <a name="___sec539"></a></h2>

Gibbs Free Energy

<p>&nbsp;<br>
$$
\begin{equation}G=-k_{B}Tln\Delta\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}dG=-SdT+Vdp+\mu dN\end{equation}
$$
<p>&nbsp;<br>


<p>
Entropy

<p>&nbsp;<br>
$$
\begin{equation}S =k_{B}ln\Delta
+k_{B}T\left(\frac{\partial ln\Delta}{\partial T}\right)_{p, N}\end{equation}
$$
<p>&nbsp;<br>


<p>
Volume

<p>&nbsp;<br>
$$
\begin{equation}V =-k_{B}T\left(\frac{\partial ln\Delta}{\partial p}\right)_{N, T}\end{equation}
$$
<p>&nbsp;<br>


<p>
Chemical potential

<p>&nbsp;<br>
$$
\begin{equation}\mu =-k_{B}T\left(\frac{\partial ln\Delta}{\partial N}\right)_{p, T}\end{equation}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Expectation Values  <a name="___sec540"></a></h2>

At a given temperature we have the probability distribution
<p>&nbsp;<br>
$$
  P_i(\beta) = \frac{e^{-\beta E_i}}{Z}
$$
<p>&nbsp;<br>

with \( \beta=1/kT \) being the inverse temperature, \( k \) the
Boltzmann constant, \( E_i \) is the energy of a state \( i \) while
\( Z \) is the partition function for the canonical ensemble
defined as
<p>&nbsp;<br>
$$
Z=\sum_{i=1}^{M}e^{-\beta E_i},
$$
<p>&nbsp;<br>

where the sum extends over all states
\( M \).
\( P_i \) expresses the probability of finding the system in a given
configuration \( i \).

<p>

</section>


<section>

<h2>Expectation Values  <a name="___sec541"></a></h2>

For a system described by the canonical ensemble, the energy is an
expectation value since we allow energy to be exchanged with the surroundings
(a heat bath with temperature \( T \)). This expectation value, the mean energy,
can be calculated using the probability distribution
\( P_i \) as
<p>&nbsp;<br>
$$
  \langle E \rangle = \sum_{i=1}^M E_i P_i(\beta)=
  \frac{1}{Z}\sum_{i=1}^M E_ie^{-\beta E_i},
$$
<p>&nbsp;<br>

with a corresponding variance defined as
<p>&nbsp;<br>
$$
\sigma_E^2=\langle E^2 \rangle-\langle E \rangle^2=
         \frac{1}{Z}\sum_{i=1}^M E_i^2e^{-\beta E_i}-
          \left(\frac{1}{Z}\sum_{i=1}^M E_ie^{-\beta E_i}\right)^2.
$$
<p>&nbsp;<br>

If we divide the latter quantity with
\( kT^2 \) we obtain the specific heat at constant volume
<p>&nbsp;<br>
$$
   C_V= \frac{1}{kT^2}\left(\langle E^2 \rangle-\langle E \rangle^2\right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Expectation Values  <a name="___sec542"></a></h2>

We can also write
<p>&nbsp;<br>
$$
   \langle E \rangle=-\frac{\partial lnZ}{\partial \beta}.
$$
<p>&nbsp;<br>

The specific heat is
<p>&nbsp;<br>
$$
   C_V=\frac{1}{kT^2}\frac{\partial^2 lnZ}{\partial\beta^2}
$$
<p>&nbsp;<br>

These expressions link a physical quantity (in thermodynamics) with the microphysics
given by the partition function. Statistical physics is the field where one relates
microscopic quantities to observables at finite temperature.

<p>

</section>


<section>

<h2>Expectation Values  <a name="___sec543"></a></h2>

<p>&nbsp;<br>
$$
  \langle {\cal M} \rangle = \sum_i^M {\cal M}_i P_i(\beta)=
  \frac{1}{Z}\sum_i^M {\cal M}_ie^{-\beta E_i},
$$
<p>&nbsp;<br>

and the corresponding variance
<p>&nbsp;<br>
$$
\sigma_{{\cal M}}^2=\langle {\cal M}^2 \rangle-\langle {\cal M} \rangle^2=
         \frac{1}{Z}\sum_{i=1}^M {\cal M}_i^2e^{-\beta E_i}-
          \left(\frac{1}{Z}\sum_{i=1}^M {\cal M}_ie^{-\beta E_i}\right)^2.
$$
<p>&nbsp;<br>

This quantity defines also the susceptibility
\( \chi \)
<p>&nbsp;<br>
$$
  \chi=\frac{1}{kT}\left(\langle {\cal M}^2 \rangle-\langle {\cal M} \rangle^2\right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Phase Transitions  <a name="___sec544"></a></h2>

NOTE: Helmholtz free energy and canonical ensemble

<p>&nbsp;<br>
$$
   F= \langle E\rangle -TS  = -kTln Z
$$
<p>&nbsp;<br>

meaning $ lnZ = -F/kT  = -F\beta$
and

<p>&nbsp;<br>
$$
   \langle E \rangle=-\frac{\partial lnZ}{\partial \beta} =\frac{\partial (\beta F)}{\partial \beta}.
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   C_V=-\frac{1}{kT^2}\frac{\partial^2 (\beta F)}{\partial\beta^2}.
$$
<p>&nbsp;<br>

We can relate observables to various derivatives of the partition
function and the free energy. When a given derivative of the free energy or the partition function is discontinuous
or diverges (logarithmic divergence for the heat capacity from the Ising model) we talk of a phase transition
of order of the derivative.

<p>

</section>


<section>

<h2>Phase Transitions  <a name="___sec545"></a></h2>

<ul>
  <p><li> An important quantity is the correlation length. The correlation length defines the length scale at which the overall properties of a material start to differ from its bulk properties. It is the distance over which the fluctuations of the microscopic degrees of freedom (for example the position of atoms) are significantly correlated with each other. Usually it is of the order of few interatomic spacings for a solid.</li>
  <p><li> The correlation length \( \xi \) depends however on external conditions such as pressure and temperature.</li>
  <p><li> A phase transition is marked by abrupt macroscopic changes as external parameters are changed, such as an increase of temperature.</li>
  <p><li> The point where a phase transition takes place is called a critical point.</li>
</ul>
<p>


</section>


<section>

<h2>Two Scenarios for Phase Transitions  <a name="___sec546"></a></h2>

<ul>
  <p><li> First order/discontinuous phase transitions: Two or more states on either side of the critical point also coexist exactly at the critical point. As we pass through the critical point we observe a discontinuous behavior of thermodynamical functions. The correlation length is normally finite at the critical point. Phenomena such as hysteris occur, viz. there is a continuation of state below the critical point into one above the critical point. This continuation is metastable so that the system may take a macroscopically long time to readjust. Classical example, melting of ice.</li>
  <p><li> Second order or continuous transitions: The correlation length diverges at the critical point, fluctuations are correlated over all distance scales, which forces the system to be in a unique critical phase. The two phases on either side of the critical point become identical. Smooth behavior of first derivatives of the partition function, while second derivatives diverge. Strong correlations make a perturbative treatment impossible. Renormalization group theory.</li>
</ul>
<p>


</section>


<section>

<h2>Examples of  Phase Transitions  <a name="___sec547"></a></h2>

\begin{tabular}{c|c|c|}
\hline
&&\\
\multicolumn{1}{|c}{System}&\multicolumn{1}{|c}{Transition}&\multicolumn{1}{|c}{Order Parameter}\\
&&\\
\hline
&&\\
Liquid-gas &amp; Condensation/evaporation&  Density difference \( \Delta\rho=\rho_{liquid}-\rho_{gas} \) \\
Binary liquid&  mixture/Unmixing &amp; Composition difference \\
Quantum liquid &amp; Normal fluid/superfluid &  \( < \phi > \), \( \psi \) = wavefunction  \\
Liquid-solid &amp; Melting/crystallisation &amp; Reciprocal lattice vector \\
Magnetic solid &amp; Ferromagnetic&  Spontaneous magnetisation $M$\\
       &               Antiferromagnetic &amp; Sublattice magnetisation $M$\\
Dielectric solid &amp; Ferroelectric&  Polarization \( P \) \\
                  &    Antiferroelectric &amp; Sublattice polarisation $P$\\
&&\\
&&\\ \hline
\end{tabular}

<p>

</section>


<section>

<h2>Ising Model  <a name="___sec548"></a></h2>

The model we will employ in our studies of phase transitions at finite temperature for
magnetic systems is the so-called Ising model. In its simplest form
the energy is expressed as
<p>&nbsp;<br>
$$
  E=-J\sum_{< kl >}^{N}s_ks_l-B\sum_k^Ns_k,
$$
<p>&nbsp;<br>

with  \( s_k=\pm 1 \), \( N \) is the total number of spins,
\( J \) is a coupling constant expressing the strength of the interaction
between neighboring spins and
\( B \) is an external magnetic field interacting with the magnetic
moment set up by the spins.
The symbol \( < kl > \) indicates that we sum over nearest
neighbors only.

<p>

</section>


<section>

<h2>Ising Model  <a name="___sec549"></a></h2>

Notice that for \( J>0 \) it is energetically favorable for neighboring spins
to be aligned. This feature leads to, at low enough temperatures,
to a cooperative phenomenon called spontaneous magnetization. That is,
through interactions between nearest neighbors, a given magnetic
moment can influence the alignment of spins  that are separated
from the given spin by a macroscopic distance. These long range correlations
between spins are associated with a long-range order in which
the lattice has a net magnetization in the absence of a magnetic field.
This phase is normally called the ferromagnetic phase. With \( J < 0 \), we have a
so-called antiferromagnetic case.
At a critical temperature we have a phase transition to a disordered phase, a so-called
paramagnetic phase.

<p>

</section>


<section>

<h2>Treatment of Boundaries  <a name="___sec550"></a></h2>

With two spins, since each spin takes two values only, it means that in total
we have \( 2^2=4 \) possible arrangements of the two spins.
These four possibilities are

<p>&nbsp;<br>
$$
   1= \uparrow\uparrow\quad
    2= \uparrow\downarrow\quad
   3= \downarrow\uparrow\quad
      4=\downarrow\downarrow
$$
<p>&nbsp;<br>


<p>
What is the energy of each of these configurations?

<p>
For small systems, the way we treat the ends matters.
In the first case we employ what is called
free ends. For the one-dimensional case, the energy is then written as
a sum over a single index

<p>&nbsp;<br>
$$
   E_i =-J\sum_{j=1}^{N-1}s_js_{j+1},
$$
<p>&nbsp;<br>

If we  label the first spin as \( s_1 \) and the second as \( s_2 \)
we obtain the following
expression for the energy

<p>&nbsp;<br>
$$
   E=-Js_1s_2.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Treatment of Boundaries  <a name="___sec551"></a></h2>

The calculation of the energy for the one-dimensional lattice
with free ends for one specific spin-configuration
can easily be implemented in the following lines

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">1</span>; j &lt; N; j++) {
        energy += spin[j]*spin[j+<span style="color: #B452CD">1</span>];
</pre></div>
<p>
where the vector \( spin[] \) contains the spin value \( s_k=\pm 1 \).
For the specific state \( E_1 \), we have chosen all spins up. The energy of
this configuration becomes then

<p>&nbsp;<br>
$$
   E_1=E_{\uparrow\uparrow}=-J.
$$
<p>&nbsp;<br>

The other configurations give

<p>&nbsp;<br>
$$
   E_2=E_{\uparrow\downarrow}=+J,
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
   E_3=E_{\downarrow\uparrow}=+J,
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   E_4=E_{\downarrow\downarrow}=-J.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Treatment of Boundaries  <a name="___sec552"></a></h2>

We can also choose so-called periodic boundary conditions.
This means that if \( i=N \), we set the spin number to \( i=1 \).
In this case the energy for the one-dimensional lattice
reads

<p>&nbsp;<br>
$$
   E_i =-J\sum_{j=1}^{N}s_js_{j+1},
$$
<p>&nbsp;<br>

and we obtain the following expression for the two-spin case

<p>&nbsp;<br>
$$
   E=-J(s_1s_2+s_2s_1).
$$
<p>&nbsp;<br>

If we choose to use periodic boundary conditions we can code the above
expression as

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    jm=N;
    <span style="color: #8B008B; font-weight: bold">for</span> ( j=<span style="color: #B452CD">1</span>; j &lt;=N ; j++) {
        energy += spin[j]*spin[jm];
        jm = j ;
</pre></div>
<p>

</section>


<section>

<h2>Treatment of Boundaries  <a name="___sec553"></a></h2>

\begin{tabular}{crrr}\\\hline
State& Energy (FE) & Energy (PBC) & Magnetization \\ \hline
$1= \uparrow\uparrow$& \( -J \) & \( -2J \) & 2\\
$2=\uparrow\downarrow$&  \( J \) & \( 2J \)  & 0\\
$   3= \downarrow\uparrow$&  \( J \) & \( 2J \) &  0\\
$      4=\downarrow\downarrow$&  \( -J \) & \( -2J \) & -2\\ \hline
\end{tabular}
\begin{tabular}{llrrr}\\\hline
Number spins up& Degeneracy& Energy (FE)& Energy (PBC) & Magnetization \\ \hline
2& 1&$-J$ &$-2J$ & 2\\
1& 2 & \( J \) &$2J$ & 0\\
0& 1 & \( -J \) &$-2J$ & -2 \\ \hline
\end{tabular}

<p>

</section>


<section>

<h2>Treatment of Boundaries  <a name="___sec554"></a></h2>

It is worth noting that for small dimensions of the lattice,
the energy differs depending on whether we use
periodic boundary conditions or free ends. This means also
that the partition functions will be different, as discussed
below. In the thermodynamic limit however, \( N\rightarrow \infty \),
the final results do not depend on the kind of boundary conditions
we choose.
The magnetization is however the same, defined as

<p>&nbsp;<br>
$$
   {\cal M}_i=\sum_{j=1}^N s_j,
$$
<p>&nbsp;<br>

where we  sum over all spins for a given configuration \( i \).

<p>

</section>


<section>

<h2>Treatment of Boundaries  <a name="___sec555"></a></h2>

In a similar way, we could enumerate the number of states for
a two-dimensional system consisting of two spins, i.e.,
a \( 2\times 2 \) Ising model on a square lattice with <em>periodic
boundary conditions</em>. In this case we have a total of
\( 2^4=16 \) states.
Some
examples of configurations with their respective energies are
listed here

<p>&nbsp;<br>
$$
  E=-8J\quad\begin{array}{cc}\uparrow & \uparrow \\
                                    \uparrow & \uparrow\end{array}
\quad
  E=0\quad\begin{array}{cc}\uparrow & \uparrow \\
                                    \uparrow & \downarrow\end{array}
\quad
  E=0\quad\begin{array}{cc}\downarrow & \downarrow \\
                                    \uparrow & \downarrow\end{array}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Treatment of Boundaries  <a name="___sec556"></a></h2>

We can group these configurations
according to their total energy and magnetization.
\begin{tabular}{llrr}\\\hline
Number spins up& Degeneracy& Energy &amp; Magnetization \\ \hline
4&1 & \( -8J \) & 4\\
3& 4 & \( 0 \) & 2\\
2& 4 & \( 0 \) & 0 \\
2& 2 & \( 8J \) & 0 \\
1& 4 & \( 0 \) & -2 \\
0& 1 & \( -8J \) & -4\\ \hline
\end{tabular}

<p>

</section>


<section>

<h2>Modelling the Ising Model  <a name="___sec557"></a></h2>

The code uses periodic boundary conditions with energy

<p>&nbsp;<br>
$$
   E_i =-J\sum_{j=1}^{N}s_js_{j+1},
$$
<p>&nbsp;<br>

In our case we have as the Monte Carlo sampling function the probability
for finding the system in a state \( s \) given by

<p>&nbsp;<br>
$$
P_s=\frac{e^{-(\beta E_s)}}{Z},
$$
<p>&nbsp;<br>

with energy \( E_s \), \( \beta=1/kT \) and \( Z \) is a normalization constant which
defines the partition function in the canonical ensemble

<p>&nbsp;<br>
$$
  Z(\beta)=\sum_se^{-(\beta E_s)}
$$
<p>&nbsp;<br>

This is difficult to compute since we need all states. In a calculation
of the Ising model in two dimensions, the number of configurations is
given by \( 2^N \) with \( N=L\times L \) the number of spins for a lattice
of length \( L \). Fortunately, the Metropolis algorithm considers
only ratios between probabilities and we do not need to
compute the partition function at all.

<p>

</section>


<section>

<h2>Metropolis Algorithm  <a name="___sec558"></a></h2>

<ul>
  <p><li> Establish an initial state with energy \( E_b \) by positioning yourself at a random position in the lattice</li>
  <p><li> Change the initial configuration by flipping e.g., one spin only. Compute the energy of this trial state \( E_t \).</li>
  <p><li> Calculate \( \Delta E=E_t-E_b \). The number of values \( \Delta E \) is limited to five for the Ising model in two dimensions, see the discussion below.</li>
  <p><li> If \( \Delta E \le 0 \) we accept the new configuration, meaning that the energy is lowered and we are hopefully moving towards the energy minimum at a given temperature. Go to step 7.</li>
  <p><li> If \( \Delta E > 0 \), calculate \( w=e^{-(\beta \Delta E)} \).</li>
  <p><li> Compare \( w \) with a random number \( r \). If \( r \le w \) then accept the new configuration, else we keep the old configuration and its values.</li>
  <p><li> The next step is to update various expectations values.</li>
  <p><li> The steps (2)-(7) are then repeated in order to obtain a sufficently good representation of states.</li>
</ul>
<p>


</section>


<section>

<h2>Modelling the Ising Model  <a name="___sec559"></a></h2>

In the calculation of the energy difference from one spin configuration
to the other, we will limit the change to the flipping of one
spin only. For the Ising model in two dimensions it means that there
will only be a limited set of values for \( \Delta E \). Actually, there are
only five  possible values. To see this,
select first a random spin position \( x,y \)
and assume that this spin
and its nearest neighbors are all pointing up.
The energy for this
configuration is \( E=-4J \). Now we flip this spin as shown below.
The energy of the new configuration is \( E=4J \), yielding \( \Delta E=8J \).

<p>&nbsp;<br>
$$
  E=-4J\quad\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \uparrow & \uparrow\\
                                  & \uparrow & \end{array}
\quad\Longrightarrow\quad
  E=4J\quad\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \downarrow & \uparrow\\
                                  & \uparrow & \end{array}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Modelling the Ising Model  <a name="___sec560"></a></h2>

The four other possibilities are as follows

<p>&nbsp;<br>
$$
  E=-2J\quad\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \uparrow & \uparrow\\
                                  & \uparrow & \end{array}
\quad\Longrightarrow\quad
  E=2J\quad\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \downarrow & \uparrow\\
                                  & \uparrow & \end{array}
$$
<p>&nbsp;<br>

with \( \Delta E=4J \),

<p>&nbsp;<br>
$$
  E=0\quad\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \uparrow & \uparrow\\
                                  & \downarrow & \end{array}
\quad\Longrightarrow\quad
  E=0\quad\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \downarrow & \uparrow\\
                                  & \downarrow & \end{array}
$$
<p>&nbsp;<br>

with \( \Delta E=0 \)

<p>

</section>


<section>

<h2>Modelling the Ising Model  <a name="___sec561"></a></h2>

<p>&nbsp;<br>
$$
  E=2J\quad\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \uparrow & \uparrow\\
                                  & \downarrow & \end{array}
\quad\Longrightarrow\quad
  E=-2J\quad\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \downarrow & \uparrow\\
                                  & \downarrow & \end{array}
$$
<p>&nbsp;<br>

with \( \Delta E=-4J \) and finally

<p>&nbsp;<br>
$$
  E=4J\quad\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \uparrow & \downarrow\\
                                  & \downarrow & \end{array}
\quad\Longrightarrow\quad
  E=-4J\quad\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \downarrow & \downarrow\\
                                  & \downarrow & \end{array}
$$
<p>&nbsp;<br>

with \( \Delta E=-8J \).
This means in turn that we could construct an array which contains all values
of \( e^{\beta \Delta E} \) before doing the Metropolis sampling. Else, we
would have to evaluate the exponential  at each Monte Carlo sampling.

<p>

</section>


<section>

<h2>The loop over \( T \) in main   <a name="___sec562"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">for</span> ( <span style="color: #a7a7a7; font-weight: bold">double</span> temp = initial_temp; temp &lt;= final_temp; temp+=temp_step){
    <span style="color: #228B22">//    initialise energy and magnetization</span>
    E = M = <span style="color: #B452CD">0.</span>;
    <span style="color: #228B22">// setup array for possible energy changes</span>
    <span style="color: #8B008B; font-weight: bold">for</span>( <span style="color: #a7a7a7; font-weight: bold">int</span> de =-<span style="color: #B452CD">8</span>; de &lt;= <span style="color: #B452CD">8</span>; de++) w[de+<span style="color: #B452CD">8</span>] = <span style="color: #B452CD">0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>( <span style="color: #a7a7a7; font-weight: bold">int</span> de =-<span style="color: #B452CD">8</span>; de &lt;= <span style="color: #B452CD">8</span>; de+=<span style="color: #B452CD">4</span>) w[de+<span style="color: #B452CD">8</span>] = exp(-de/temp);
    <span style="color: #228B22">// initialise array for expectation values</span>
    <span style="color: #8B008B; font-weight: bold">for</span>( <span style="color: #a7a7a7; font-weight: bold">int</span> i = <span style="color: #B452CD">0</span>; i &lt; <span style="color: #B452CD">5</span>; i++) average[i] = <span style="color: #B452CD">0.</span>;
    initialize(n_spins, temp, spin_matrix, E, M);
    <span style="color: #228B22">// start Monte Carlo computation</span>
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> cycles = <span style="color: #B452CD">1</span>; cycles &lt;= mcs; cycles++){
      Metropolis(n_spins, idum, spin_matrix, E, M, w);
      <span style="color: #228B22">// update expectation values</span>
      average[<span style="color: #B452CD">0</span>] += E;    average[<span style="color: #B452CD">1</span>] += E*E;
      average[<span style="color: #B452CD">2</span>] += M;    average[<span style="color: #B452CD">3</span>] += M*M; average[<span style="color: #B452CD">4</span>] += fabs(M);

    <span style="color: #228B22">// print results</span>
    output(n_spins, mcs, temp, average);
</pre></div>
<p>

</section>


<section>

<h2>The Initialise function  <a name="___sec563"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">void</span> <span style="color: #008b45">initialize</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> n_spins, <span style="color: #a7a7a7; font-weight: bold">double</span> temp, <span style="color: #a7a7a7; font-weight: bold">int</span> **spin_matrix,
		<span style="color: #a7a7a7; font-weight: bold">double</span>&amp; E, <span style="color: #a7a7a7; font-weight: bold">double</span>&amp; M)
{
  <span style="color: #228B22">// setup spin matrix and intial magnetization</span>
  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> y =<span style="color: #B452CD">0</span>; y &lt; n_spins; y++) {
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> x= <span style="color: #B452CD">0</span>; x &lt; n_spins; x++){
      spin_matrix[y][x] = <span style="color: #B452CD">1</span>; <span style="color: #228B22">// spin orientation for the ground state</span>
      M +=  (<span style="color: #a7a7a7; font-weight: bold">double</span>) spin_matrix[y][x];

  <span style="color: #228B22">// setup initial energy</span>
  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> y =<span style="color: #B452CD">0</span>; y &lt; n_spins; y++) {
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> x= <span style="color: #B452CD">0</span>; x &lt; n_spins; x++){
      E -=  (<span style="color: #a7a7a7; font-weight: bold">double</span>) spin_matrix[y][x]*
	(spin_matrix[periodic(y,n_spins,-<span style="color: #B452CD">1</span>)][x] +
	 spin_matrix[y][periodic(x,n_spins,-<span style="color: #B452CD">1</span>)]);

}<span style="color: #228B22">// end function initialise</span>
</pre></div>
<p>

</section>


<section>

<h2>The periodic function  <a name="___sec564"></a></h2>

A compact way of dealing with periodic boundary conditions is given as follows:

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// inline function for periodic boundary conditions</span>
<span style="color: #8B008B; font-weight: bold">inline</span> <span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">periodic</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> i, <span style="color: #a7a7a7; font-weight: bold">int</span> limit, <span style="color: #a7a7a7; font-weight: bold">int</span> add) {
  <span style="color: #8B008B; font-weight: bold">return</span> (i+limit+add) % (limit);
</pre></div>
<p>
with the following example from the function initialise

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">      E -=  (<span style="color: #a7a7a7; font-weight: bold">double</span>) spin_matrix[y][x]*
	(spin_matrix[periodic(y,n_spins,-<span style="color: #B452CD">1</span>)][x] +
	 spin_matrix[y][periodic(x,n_spins,-<span style="color: #B452CD">1</span>)]);
</pre></div>
<p>

</section>


<section>

<h2>Alternative way for periodic boundary conditions  <a name="___sec565"></a></h2>

A more pedagogical way is given by the (here Fortran as example)  program

<p>

<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">   DO y = <span style="color: #B452CD">1</span>,lattice_y
       DO x = <span style="color: #B452CD">1</span>,lattice_x
          right = x+<span style="color: #B452CD">1</span> ; IF(x == lattice_x  ) right = <span style="color: #B452CD">1</span>
          left = x-<span style="color: #B452CD">1</span> ; IF(x == <span style="color: #B452CD">1</span>  ) left = lattice_x
          up = y+<span style="color: #B452CD">1</span> ; IF(y == lattice_y  ) up = <span style="color: #B452CD">1</span>
          down = y-<span style="color: #B452CD">1</span> ; IF(y == <span style="color: #B452CD">1</span>  ) down = lattice_y
          energy=energy - spin_matrix(x,y)*(spin_matrix(right,y)+&amp;
               spin_matrix(left,y)+spin_matrix(x,up)+ &amp;
               spin_matrix(x,down) )
          magnetization = magnetization + spin_matrix(x,y)
       ENDDO
    ENDDO
    energy = energy*<span style="color: #B452CD">0.5</span>
</pre></div>
<p>

</section>


<section>

<h2>Computing \( \Delta E \) and \( \Delta M \)  <a name="___sec566"></a></h2>

The energy difference between a state \( E_1 \) and a state \( E_2 \) with zero magnetic field is

<p>&nbsp;<br>
$$
   \Delta E = E_2-E_1 =J\sum_{< kl >}^{N}s_k^1s_{l}^1-J\sum_{< kl >}^{N}s_k^2s_{l}^2,
$$
<p>&nbsp;<br>

which we can rewrite as

<p>&nbsp;<br>
$$
   \Delta E  = -J \sum_{< kl >}^{N}s_k^2(s_l^2-s_{l}^1),
$$
<p>&nbsp;<br>

where the sum now runs only over the nearest neighbors \( k \) of the spin.
Since the spin to be flipped takes only two values, \( s_l^1=\pm 1 \) and \( s_l^2=\pm 1 \), it means that if
\( s_l^1= 1 \), then \( s_l^2=-1 \) and if \( s_l^1= -1 \), then \( s_l^2=1 \).
The other spins keep their values, meaning that
\( s_k^1=s_k^2 \).
If \( s_l^1= 1 \) we must have \( s_l^1-s_{l}^2=2 \), and
if \( s_l^1= -1 \) we must have \( s_l^1-s_{l}^2=-2 \). From these results we see that the energy difference
can be coded efficiently as

<p>&nbsp;<br>
$$
   \Delta E  = 2Js_l^1\sum_{< k >}^{N}s_k,
$$
<p>&nbsp;<br>

where the sum runs only over the nearest neighbors \( k \) of spin \( l \)

<p>
The difference in magnetisation is given by the difference
\( s_l^1-s_{l}^2=\pm 2 \), or in a more compact way as

<p>&nbsp;<br>
$$
M_2 = M_1+2s_l^2,
$$
<p>&nbsp;<br>

where \( M_1 \) and \( M_2 \) are the magnetizations before and after the spin flip, respectively.

<p>

</section>


<section>

<h2>The Metropolis function  <a name="___sec567"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">// loop over all spins</span>
  <span style="color: #8B008B; font-weight: bold">for</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> y =<span style="color: #B452CD">0</span>; y &lt; n_spins; y++) {
    <span style="color: #8B008B; font-weight: bold">for</span> (<span style="color: #a7a7a7; font-weight: bold">int</span> x= <span style="color: #B452CD">0</span>; x &lt; n_spins; x++){
      <span style="color: #a7a7a7; font-weight: bold">int</span> ix = (<span style="color: #a7a7a7; font-weight: bold">int</span>) (ran1(&amp;idum)*(<span style="color: #a7a7a7; font-weight: bold">double</span>)n_spins);   <span style="color: #228B22">// RANDOM SPIN</span>
      <span style="color: #a7a7a7; font-weight: bold">int</span> iy = (<span style="color: #a7a7a7; font-weight: bold">int</span>) (ran1(&amp;idum)*(<span style="color: #a7a7a7; font-weight: bold">double</span>)n_spins);  <span style="color: #228B22">// RANDOM SPIN</span>
      <span style="color: #a7a7a7; font-weight: bold">int</span> deltaE =  <span style="color: #B452CD">2</span>*spin_matrix[iy][ix]*
	(spin_matrix[iy][periodic(ix,n_spins,-<span style="color: #B452CD">1</span>)]+
	 spin_matrix[periodic(iy,n_spins,-<span style="color: #B452CD">1</span>)][ix] +
	 spin_matrix[iy][periodic(ix,n_spins,<span style="color: #B452CD">1</span>)] +
	 spin_matrix[periodic(iy,n_spins,<span style="color: #B452CD">1</span>)][ix]);
      <span style="color: #8B008B; font-weight: bold">if</span> ( ran1(&amp;idum) &lt;= w[deltaE+<span style="color: #B452CD">8</span>] ) {
	spin_matrix[iy][ix] *= -<span style="color: #B452CD">1</span>;  <span style="color: #228B22">// flip one spin and accept new spin config</span>
        M += (<span style="color: #a7a7a7; font-weight: bold">double</span>) <span style="color: #B452CD">2</span>*spin_matrix[iy][ix];
        E += (<span style="color: #a7a7a7; font-weight: bold">double</span>) deltaE;
</pre></div>
<p>

</section>


<section>

<h2>Expectation Values  <a name="___sec568"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #a7a7a7; font-weight: bold">double</span> norm = <span style="color: #B452CD">1</span>/((<span style="color: #a7a7a7; font-weight: bold">double</span>) (mcs));<span style="color: #228B22">// divided by total number of cycles</span>
  <span style="color: #a7a7a7; font-weight: bold">double</span> Eaverage = average[<span style="color: #B452CD">0</span>]*norm;
  <span style="color: #a7a7a7; font-weight: bold">double</span> E2average = average[<span style="color: #B452CD">1</span>]*norm;
  <span style="color: #a7a7a7; font-weight: bold">double</span> Maverage = average[<span style="color: #B452CD">2</span>]*norm;
  <span style="color: #a7a7a7; font-weight: bold">double</span> M2average = average[<span style="color: #B452CD">3</span>]*norm;
  <span style="color: #a7a7a7; font-weight: bold">double</span> Mabsaverage = average[<span style="color: #B452CD">4</span>]*norm;
  <span style="color: #228B22">// all expectation values are per spin, divide by 1/n_spins/n_spins</span>
  <span style="color: #a7a7a7; font-weight: bold">double</span> Evariance = (E2average- Eaverage*Eaverage)/n_spins/n_spins;
  <span style="color: #a7a7a7; font-weight: bold">double</span> Mvariance = (M2average - Mabsaverage*Mabsaverage)/n_spins/n_spins;
  ofile &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
  ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; temp;
  ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; Eaverage/n_spins/n_spins;
  ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; Evariance/temp/temp;
  ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; Maverage/n_spins/n_spins;
  ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; Mvariance/temp;
  ofile &lt;&lt; setw(<span style="color: #B452CD">15</span>) &lt;&lt; setprecision(<span style="color: #B452CD">8</span>) &lt;&lt; Mabsaverage/n_spins/n_spins &lt;&lt; endl;
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec569"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #a7a7a7; font-weight: bold">int</span> <span style="color: #008b45">Jacobi_P</span>(<span style="color: #a7a7a7; font-weight: bold">int</span> mynode, <span style="color: #a7a7a7; font-weight: bold">int</span> numnodes, <span style="color: #a7a7a7; font-weight: bold">int</span> N, <span style="color: #a7a7a7; font-weight: bold">double</span> **A, <span style="color: #a7a7a7; font-weight: bold">double</span> *x, <span style="color: #a7a7a7; font-weight: bold">double</span> *b, <span style="color: #a7a7a7; font-weight: bold">double</span> abstol){
  <span style="color: #a7a7a7; font-weight: bold">int</span> i,j,k,i_global;
  <span style="color: #a7a7a7; font-weight: bold">int</span> maxit = <span style="color: #B452CD">100000</span>;
  <span style="color: #a7a7a7; font-weight: bold">int</span> rows_local,local_offset,last_rows_local,*count,*displacements;
  <span style="color: #a7a7a7; font-weight: bold">double</span> sum1,sum2,*xold;
  <span style="color: #a7a7a7; font-weight: bold">double</span> error_sum_local, error_sum_global;
  MPI_Status status;

  rows_local = (<span style="color: #a7a7a7; font-weight: bold">int</span>) floor((<span style="color: #a7a7a7; font-weight: bold">double</span>)N/numnodes);
  local_offset = mynode*rows_local;
  <span style="color: #8B008B; font-weight: bold">if</span>(mynode == (numnodes-<span style="color: #B452CD">1</span>))
    rows_local = N - rows_local*(numnodes-<span style="color: #B452CD">1</span>);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec570"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #228B22">/*Distribute the Matrix and R.H.S. among the processors */</span>
  <span style="color: #8B008B; font-weight: bold">if</span>(mynode == <span style="color: #B452CD">0</span>){
    <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">1</span>;i&lt;numnodes-<span style="color: #B452CD">1</span>;i++){
      <span style="color: #8B008B; font-weight: bold">for</span>(j=<span style="color: #B452CD">0</span>;j&lt;rows_local;j++)
	MPI_Send(A[i*rows_local+j],N,MPI_DOUBLE,i,j,MPI_COMM_WORLD);
      MPI_Send(b+i*rows_local,rows_local,MPI_DOUBLE,i,rows_local,
	       MPI_COMM_WORLD);

    last_rows_local = N-rows_local*(numnodes-<span style="color: #B452CD">1</span>);
    <span style="color: #8B008B; font-weight: bold">for</span>(j=<span style="color: #B452CD">0</span>;j&lt;last_rows_local;j++)
      MPI_Send(A[(numnodes-<span style="color: #B452CD">1</span>)*rows_local+j],N,MPI_DOUBLE,numnodes-<span style="color: #B452CD">1</span>,j,
	       MPI_COMM_WORLD);
    MPI_Send(b+(numnodes-<span style="color: #B452CD">1</span>)*rows_local,last_rows_local,MPI_DOUBLE,numnodes-<span style="color: #B452CD">1</span>,
	     last_rows_local,MPI_COMM_WORLD);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec571"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">else</span>{
    A = CreateMatrix(rows_local,N);
    x = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[rows_local];
    b = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[rows_local];
    <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">0</span>;i&lt;rows_local;i++)
      MPI_Recv(A[i],N,MPI_DOUBLE,<span style="color: #B452CD">0</span>,i,MPI_COMM_WORLD,&amp;status);
    MPI_Recv(b,rows_local,MPI_DOUBLE,<span style="color: #B452CD">0</span>,rows_local,MPI_COMM_WORLD,&amp;status);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec572"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  xold = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">double</span>[N];
  count = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">int</span>[numnodes];
  displacements = <span style="color: #8B008B; font-weight: bold">new</span> <span style="color: #a7a7a7; font-weight: bold">int</span>[numnodes];
  <span style="color: #228B22">//set initial guess to all 1.0</span>
  <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">0</span>; i&lt;N; i++){
    xold[i] = <span style="color: #B452CD">1.0</span>;

  <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">0</span>;i&lt;numnodes;i++){
    count[i] = (<span style="color: #a7a7a7; font-weight: bold">int</span>) floor((<span style="color: #a7a7a7; font-weight: bold">double</span>)N/numnodes);
    displacements[i] = i*count[i];

  count[numnodes-<span style="color: #B452CD">1</span>] = N - ((<span style="color: #a7a7a7; font-weight: bold">int</span>)floor((<span style="color: #a7a7a7; font-weight: bold">double</span>)N/numnodes))*(numnodes-<span style="color: #B452CD">1</span>);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec573"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  <span style="color: #8B008B; font-weight: bold">for</span>(k=<span style="color: #B452CD">0</span>; k&lt;maxit; k++){
    error_sum_local = <span style="color: #B452CD">0.0</span>;
    <span style="color: #8B008B; font-weight: bold">for</span>(i = <span style="color: #B452CD">0</span>; i&lt;rows_local; i++){
      i_global = local_offset+i;
      sum1 = <span style="color: #B452CD">0.0</span>; sum2 = <span style="color: #B452CD">0.0</span>;
      <span style="color: #8B008B; font-weight: bold">for</span>(j=<span style="color: #B452CD">0</span>; j &lt; i_global; j++)
	sum1 = sum1 + A[i][j]*xold[j];
      <span style="color: #8B008B; font-weight: bold">for</span>(j=i_global+<span style="color: #B452CD">1</span>; j &lt; N; j++)
	sum2 = sum2 + A[i][j]*xold[j];

      x[i] = (-sum1 - sum2 + b[i])/A[i][i_global];
      error_sum_local += (x[i]-xold[i_global])*(x[i]-xold[i_global]);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec574"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    MPI_Allreduce(&amp;error_sum_local,&amp;error_sum_global,<span style="color: #B452CD">1</span>,MPI_DOUBLE,
		  MPI_SUM,MPI_COMM_WORLD);
    MPI_Allgatherv(x,rows_local,MPI_DOUBLE,xold,count,displacements,
		   MPI_DOUBLE,MPI_COMM_WORLD);
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec575"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">    <span style="color: #8B008B; font-weight: bold">if</span>(sqrt(error_sum_global)&lt;abstol){
      <span style="color: #8B008B; font-weight: bold">if</span>(mynode == <span style="color: #B452CD">0</span>){
	<span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">0</span>;i&lt;N;i++)
	  x[i] = xold[i];

      <span style="color: #8B008B; font-weight: bold">else</span>{
	DestroyMatrix(A,rows_local,N);
	<span style="color: #8B008B; font-weight: bold">delete</span>[] x;
        <span style="color: #8B008B; font-weight: bold">delete</span>[] b;

      <span style="color: #8B008B; font-weight: bold">delete</span>[] xold;
      <span style="color: #8B008B; font-weight: bold">delete</span>[] count;
      <span style="color: #8B008B; font-weight: bold">delete</span>[] displacements;
      <span style="color: #8B008B; font-weight: bold">return</span> k;
</pre></div>
<p>

</section>


<section>

<h2>Code example for the parallel two-dimensional diff equation  <a name="___sec576"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%">  cerr &lt;&lt; <span style="color: #CD5555">&quot;Jacobi: Maximum Number of Interations Reached Without Convergence\n&quot;</span>;
  <span style="color: #8B008B; font-weight: bold">if</span>(mynode == <span style="color: #B452CD">0</span>){
    <span style="color: #8B008B; font-weight: bold">for</span>(i=<span style="color: #B452CD">0</span>;i&lt;N;i++)
      x[i] = xold[i];

  <span style="color: #8B008B; font-weight: bold">else</span>{
    DestroyMatrix(A,rows_local,N);
    <span style="color: #8B008B; font-weight: bold">delete</span>[] x;
    <span style="color: #8B008B; font-weight: bold">delete</span>[] b;

  <span style="color: #8B008B; font-weight: bold">delete</span>[] xold;
  <span style="color: #8B008B; font-weight: bold">delete</span>[] count;
  <span style="color: #8B008B; font-weight: bold">delete</span>[] displacements;

  <span style="color: #8B008B; font-weight: bold">return</span> maxit;
</pre></div>
<p>

</section>


<section>

<h2>How do I use the titan.uio.no cluster?  <a name="___sec577"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>hpc@usit.uio.no.</b>
<p>

<ul>
  <p><li> Computational Physics requires High Performance Computing (HPC) resources</li>
  <p><li> USIT and the Research Computing Services (RCS) provides
      HPC resources and HPC support</li>
  <p><li> Resources: <a href="titan.uio.no" target="_blank"><tt>titan.uio.no</tt></a></li>
  <p><li> Support: 14 people</li>
  <p><li> Contact: <a href="hpc@usit.uio.no" target="_blank"><tt>hpc@usit.uio.no</tt></a></li>
</ul>
</div>


<p>

</section>


<section>

<h2>Titan  <a name="___sec578"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Hardware.</b>
<p>

<ul>
  <p><li> 304 dual-cpu quad-core SUN X2200 Opteron nodes (total 2432 cores), 2.2 Ghz, and 8 - 16 GB RAM and 250 - 1000 GB disk on each node</li>
  <p><li> 3 eight-cpu quad-core Sun X4600 AMD Opteron nodes (total 96 cores), 2.5 Ghz, and 128, 128 and 256 GB memory, respectively</li>
  <p><li> Infiniband interconnect</li>
  <p><li> Heterogenous cluster!</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Titan  <a name="___sec579"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Software.</b>
<p>

<ul>
  <p><li> Batch system: SLURM and MAUI</li>
  <p><li> Message Passing Interface (MPI):</li>
  <p><li> OpenMPI</li>
  <p><li> Scampi</li>
  <p><li> MPICH2</li>
  <p><li> Compilers: GCC, Intel, Portland and Pathscale</li>
  <p><li> Optimized math libraries and scientific applications</li>
  <p><li> All you need may be found under \texttt{/site}</li>
  <p><li> Available software: <a href="http://www.hpc.uio.no/index.php/Titan_software" target="_blank"><tt>http://www.hpc.uio.no/index.php/Titan_software</tt></a></li>
</ul>
</div>


<p>

</section>


<section>

<h2>Getting started  <a name="___sec580"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Batch systems.</b>
<p>

<ul>
  <p><li> A batch system controls the use of the cluster resources</li>
  <p><li> Submits the job to the right resource</li>
  <p><li> Monitors the job while executing</li>
  <p><li> Restarts the job in case of failure</li>
  <p><li> Takes care of priorities and queues to control execution order of unrelated jobs</li>
</ul>
</div>

<div class="alert alert-block alert-block alert-text-normal">
<b>Sun Grid Engine.</b>
<p>

<ul>
  <p><li> SGE is the batch system used on Titan</li>
  <p><li> Jobs are executed either interactively or through job scripts</li>
  <p><li> Useful commands: \texttt{showq}, \texttt{qlogin}, \texttt{sbatch}</li>
  <p><li> <a href="http://hpc.uio.no/index.php/Titan_User_Guide" target="_blank"><tt>http://hpc.uio.no/index.php/Titan_User_Guide</tt></a></li>
</ul>
</div>


<p>

</section>


<section>

<h2>Getting started  <a name="___sec581"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Modules.</b>
<p>

<ul>
  <p><li> Different compilers, MPI-versions and applications need different sets of user environment variables</li>
  <p><li> The \texttt{modules} package lets you load and remove the
      different variable sets</li>
  <p><li> Useful commands:</li>
  <p><li> List available modules: \texttt{module avail}</li>
  <p><li> Load module: \texttt{module load <environment>}</li>
  <p><li> Unload module: \texttt{module unload <environment>}</li>
  <p><li> Currently loaded: \texttt{module list}</li>
  <p><li> <a href="http://hpc.uio.no/index.php/Titan_User_Guide" target="_blank"><tt>http://hpc.uio.no/index.php/Titan_User_Guide</tt></a></li>
</ul>
</div>


<p>

</section>


<section>

<h2>Example  <a name="___sec582"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Interactively.</b>
<p>
<!-- @@@CODE interactive.sh -->
</div>


<p>

</section>


<section>

<h2>The job script  <a name="___sec583"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>job.sge.</b>
<p>
<!-- @@@CODE job.sge -->
</div>


<p>

</section>


<section>

<h2>Example  <a name="___sec584"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Submitting.</b>
<p>
<!-- @@@CODE submitting.sh -->
</div>


<p>

</section>


<section>

<h2>Example  <a name="___sec585"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Checking execution.</b>
<p>
<!-- @@@CODE checking.sh -->
</div>


<p>

</section>


<section>

<h2>Tips and admonitions  <a name="___sec586"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Tips.</b>
<p>

<ul>
  <p><li> Titan FAQ: <a href="http://www.hpc.uio.no/index.php/FAQ" target="_blank"><tt>http://www.hpc.uio.no/index.php/FAQ</tt></a></li>
  <p><li> man-pages, e.g. \texttt{man sbatch}</li>
  <p><li> Ask us</li>
</ul>
</div>

<div class="alert alert-block alert-block alert-text-normal">
<b>Admonitions.</b>
<p>

<ul>
  <p><li> Remember to exit from qlogin-sessions; the resource is reserved for you untill you exit</li>
  <p><li> Don't run jobs on login-nodes; these are only for compiling
    and editing files</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Projects: quantum mechanics  <a name="___sec587"></a></h2>

The expectation value of the kinetic energy expressed in atomic units for electron \( i \) is

<p>&nbsp;<br>
$$
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{align}
\frac{\nabla^2 \Psi}{\Psi}
&= \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_J}{ \Psi_J} + 2 \frac{\Grad \Psi_{D}}{\Psi_{D}}\cdot\frac{\Grad  \Psi_J}{ \Psi_J}
\end{align}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Quantum mechanical project  <a name="___sec588"></a></h2>

We define the correlated function as

<p>&nbsp;<br>
$$
\Psi_J=\prod_{i < j}g(r_{ij})=\prod_{i < j}^Ng(r_{ij})= \prod_{i=1}^N\prod_{j=i+1}^Ng(r_{ij}),
$$
<p>&nbsp;<br>

with
\( r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2} \) for three dimensions and
\( r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2} \) for two dimensions.

<p>
In our particular case we have

<p>&nbsp;<br>
$$
\Psi_J=\prod_{i < j}g(r_{ij})=\exp{\left\{\sum_{i < j}f(r_{ij})\right\}}=
\exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Quantum mechanical project  <a name="___sec589"></a></h2>

The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is

<p>&nbsp;<br>
$$
\left[\frac{\nabla^2 \Psi_J}{\Psi_J}\right]_x =\
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
$$
<p>&nbsp;<br>

But we have a simple form for the function, namely

<p>&nbsp;<br>
$$
\Psi_{J}=\prod_{i < j}\exp{f(r_{ij})}= \exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$
<p>&nbsp;<br>

and it is easy to see that for particle \( k \)
we have

<p>&nbsp;<br>
$$
  \frac{\nabla^2_k \Psi_J}{\Psi_J }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Jastrow gradient  <a name="___sec590"></a></h2>

We have

<p>&nbsp;<br>
$$
\Psi_J=\prod_{i < j}g(r_{ij})= \exp{\left\{\sum_{i < j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
$$
<p>&nbsp;<br>

the gradient needed for the local energy is easy to compute.
We get for particle \( k \)

<p>&nbsp;<br>
$$
\frac{ \nabla_k \Psi_J}{ \Psi_J }= \sum_{j\ne k}\frac{{\bf r}_{kj}}{r_{kj}}\frac{a}{(1+\beta r_{kj})^2},
$$
<p>&nbsp;<br>

which is rather easy to code.  Remember to sum over all particles  when you compute the local energy.

<p>

</section>


<section>

<h2>Two-dimensional wave equation, Tsunami model  <a name="___sec591"></a></h2>

We assume that we can approximate the coastline with a quadratic grid.
As boundary condition at the coastline we will employ

<p>&nbsp;<br>
$$
\frac{\partial u}{\partial n} = \nabla u\cdot {\bf n} = 0,
$$
<p>&nbsp;<br>

where \( \partial u/\partial n \) is the derivative in the direction normal to the boundary.

<p>
Here you must pay particular attention to the endpoints.

<p>

</section>


<section>

<h2>Two-dimensional wave equation  <a name="___sec592"></a></h2>

We are going to model the impact of an earthquake on sea water. This is normally modelled
via an elevation of the sea bottom. We will assume that the movement of the sea bottom
is very rapid compared with the period of the propagating waves.  This means that we can
approximate the bottom elevation with an initial surface elevation.
The initial conditions are then given by (with \( L \) the length of the grid)

<p>&nbsp;<br>
$$
   u(x,y,0) = f(x,y)\quad x,y\in (0,L),
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
   \partial u/\partial t|_{t=0}=0 \quad x,y\in (0,L).
$$
<p>&nbsp;<br>


<p>
We will approximate the initial elevation with the function

<p>&nbsp;<br>
$$
 f(x,y) =  A_0 \exp{\left(-\left[\frac{x-x_c}{\sigma_x}\right]^2-\left[\frac{y-y_c}{\sigma_y}\right]^2\right)},
$$
<p>&nbsp;<br>

where \( A_0 \) is the elevation of the surface and is typically \( 1-2 \) m.  The variables \( \sigma_x \) and
\( \sigma_y \) represent the extensions of the surface elevation. Here we  let \( \sigma_x=80 \) km
and \( \sigma_y=200 \) km. The 2004 tsunami had extensions of approximately 200 and 1000 km, respectively.

<p>
The variables \( x_c \) and \( y_c \) represent the epicentre of the earthquake.

<p>

</section>


<section>

<h2>Exam FYS3150 place FV329, Lab  <a name="___sec593"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Place and duration.</b>
<p>

<ul>
  <p><li> 13/12-17/12.</li>
  <p><li> duration: \( \sim 45 \) min</li>
  <p><li> ca 20-25 min for discussion of the project, your presentation, reproduction of results, test runs etc</li>
  <p><li> 20-25 min for questions from one of the five topics listed below.</li>
  <p><li> Check final exam list by the end of this week, see webpage. Changes must be communicated to me at mhjensen@fys.uio.no.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Exam FYS3150  <a name="___sec594"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>How to prepare your Talk.</b>
<p>

<ul>
  <p><li> You can use slides, ps, pdf, ppt etc files (projector and slide projector at room FV329). Latex example on webpage. Choose among one of the two versions of project 5.</li>
  <p><li> 10-15 mins (5-15 slides) for your presentation, rest of 10-15 mins questions and test of code from classfronter You should discuss</li>
  <p><li> The mathematical model and the physics</li>
  <p><li> Your algorithm and how you implemented it, with selected results</li>
  <p><li> How you dealt with eventual comments and your corrections of these</li>
  <p><li> Any improvement you can think of</li>
</ul>
</div>


<p>

</section>


<section>

<h1>Week 44, 26-30 Oct  <a name="___sec595"></a></h1>


</section>


<section>

<h2>Week 44, October 26-30  <a name="___sec596"></a></h2>

<div class="alert alert-block alert-block alert-text-normal">
<b>Ising model, phase transitions and project 4.</b>
<p>

<ul>
  <p><li> Monday: Repetition from last week</li>
  <p><li> Discussion of project 4, results near the phase transition</li>
  <p><li> Definition of correlation time and equilibration time (optional part of project 4).</li>
  <p><li> Wednesday:</li>
  <p><li> End Monte Carlo discussion and summary of chapters 8-10. Chapter 11 on quantum Monte Carlo will not be discussed and is not part of the curriculum.</li>
</ul>
</div>


<p>

</section>


<section>

<h2>Mean Field Theory and the Ising Model  <a name="___sec597"></a></h2>

In studies of phase transitions we are interested in minimizing the free energy
by varying the average magnetization, which is the order parameter (disappears at
\( T_C \)).

<p>
In mean field theory the local magnetization is a treated as a constant, all effects from fluctuations
are neglected. A way to achieve this is to rewrite by adding and subtracting the mean magnetization
\( \langle s \rangle \)
<p>&nbsp;<br>
$$
s_is_j=(s_i-\langle s \rangle+\langle s \rangle)(s_i-\langle s \rangle+\langle s \rangle)
\approx \langle s \rangle^2+\langle s \rangle(s_i-\langle s \rangle)+\langle s \rangle(s_j-\langle s \rangle),
$$
<p>&nbsp;<br>

where we have ignored terms of the order
\( (s_i-\langle s \rangle)(s_i-\langle s \rangle) \), which leads to correlations between neighbouring
spins. In mean field theory we ignore correlations.

<p>

</section>


<section>

<h2>Mean Field Theory and the Ising Model  <a name="___sec598"></a></h2>

This means that we can rewrite the  Hamiltonian
<p>&nbsp;<br>
$$
  E=-J\sum_{< ij >}^{N}s_ks_l-B\sum_i^Ns_i,
$$
<p>&nbsp;<br>

as
<p>&nbsp;<br>
$$
  E=-J\sum_{< ij >}\langle s \rangle^2+\langle s \rangle(s_i-\langle s \rangle)+\langle s \rangle(s_j-\langle s \rangle)    -B\sum_i s_i,
$$
<p>&nbsp;<br>

resulting in
<p>&nbsp;<br>
$$
  E=-(B+zJ\langle s \rangle) \sum_i s_i +zJ\langle s \rangle^2,
$$
<p>&nbsp;<br>

with \( z \) the number of nearest neighbours for a given site \( i \).
We can define an effective field which all spins see, namely
<p>&nbsp;<br>
$$
  B_{\mbox{eff}}=(B+zJ\langle s \rangle).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Mean Field Theory and the Ising Model  <a name="___sec599"></a></h2>

How do we get \( \langle s \rangle) \)?

<p>
Here we use the canonical ensemble. The partition function reads in this case
<p>&nbsp;<br>
$$
Z=e^{-NzJ\langle s \rangle^2/kT}\left(2cosh(B_{\mbox{eff}}/kT)\right)^N,
$$
<p>&nbsp;<br>

with a free energy
<p>&nbsp;<br>
$$
F=-kTlnZ=-NkTln(2)+NzJ\langle s \rangle^2-NkTln\left(cosh(B_{\mbox{eff}}/kT)\right)
$$
<p>&nbsp;<br>

and minimizing \( F \) wrt \( \langle s \rangle \) we arrive at
<p>&nbsp;<br>
$$
\langle s \rangle = tanh(2cosh\left(B_{\mbox{eff}}/kT)\right).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Connecting to Landau Theory  <a name="___sec600"></a></h2>

Close to the phase transition we expect \( \langle s \rangle \) to become small
and eventually vanish. We can then expand \( F \) in powers of \( \langle s \rangle \) as
<p>&nbsp;<br>
$$
F=-NkTln(2)+NzJ\langle s \rangle^2-NkT-BN\langle s \rangle+NkT\left(\frac{1}{2}\langle s \rangle^2+
\frac{1}{12}\langle s \rangle^4+\dots\right),
$$
<p>&nbsp;<br>

and using \( \langle M\rangle = N\langle s \rangle \) we can rewrite as
<p>&nbsp;<br>
$$
F=F_0-B\langle M\rangle +\frac{1}{2}a\langle M \rangle^2+
\frac{1}{4}b\langle M \rangle^4+\dots
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Connecting to Landau Theory  <a name="___sec601"></a></h2>

Let \( \langle M \rangle = m \) and
<p>&nbsp;<br>
$$
F=F_0+\frac{1}{2}am^2+
\frac{1}{4}bm^4+\frac{1}{6}cm^6
$$
<p>&nbsp;<br>

\( F \) has a minimum at equilibrium \( F'(m) =0 \) and \( F''(m) > 0 \)

<p>&nbsp;<br>
$$
F'(m)=0=m(a+bm^2+cm^4),
$$
<p>&nbsp;<br>

and if we assume that \( m \) is real we have two solutions

<p>&nbsp;<br>
$$
   m=0,
$$
<p>&nbsp;<br>

or

<p>&nbsp;<br>
$$
  m^2 = \frac{b}{2c}\left(-1\pm\sqrt{1-4ac/b^2}\right)
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Second Order Phase Transition  <a name="___sec602"></a></h2>

Can describe both first and second-order phase transitions. Here we consider the second case.
Assume \( b > 0 \) and \( a\ll 1 \) small since we want to study a perturbation around \( m=0 \). We reach the critical point when
\( a=0 \).

<p>&nbsp;<br>
$$
  m^2 = \frac{b}{2c}\left((-1\pm\sqrt{1-4ac/b^2}\right)\approx -a/b
$$
<p>&nbsp;<br>

Assume that

<p>&nbsp;<br>
$$ a(T) = \alpha (T-T_C), $$
<p>&nbsp;<br>

with \( \alpha > 0 \) and \( T_C \) the critical temperature where the magnetization vanishes. If \( a \) is negative we have two solutions

<p>&nbsp;<br>
$$    m = \pm \sqrt{-a/b}  = \pm \sqrt{\frac{\alpha (T_C-T)}{b}}$$
<p>&nbsp;<br>

\( m \) evolves continuously to the critical temperature where \( F=0 \) for \( T \le T_C \) (see separate graph).

<p>

</section>


<section>

<h2>Entropy and Specific Heat  <a name="___sec603"></a></h2>

We can now compute the entropy

<p>&nbsp;<br>
$$ S= -\left(\frac{\partial F}{\partial T}\right)$$
<p>&nbsp;<br>

For \( T \ge T_C \) we have \( m=0 \) and

<p>&nbsp;<br>
$$ S= -\left(\frac{\partial F_0}{\partial T}\right)$$
<p>&nbsp;<br>

and for \( T \le T_C \)

<p>&nbsp;<br>
$$ S= -\left(\frac{\partial F_0}{\partial T}\right)-\alpha^2(T_C-T)/2b,$$
<p>&nbsp;<br>

and we see that there is a smooth  crossover at \( T_C \).

<p>

</section>


<section>

<h2>Entropy and Specific Heat  <a name="___sec604"></a></h2>

We can now compute the specific heat

<p>&nbsp;<br>
$$ C_V= T\left(\frac{\partial S}{\partial T}\right)$$
<p>&nbsp;<br>

and \( T_C \) we get a discontinuity of

<p>&nbsp;<br>
$$\Delta C_V  = -\alpha^2/2b,$$
<p>&nbsp;<br>

signalling a second-order phase transition.
Landau theory gives irrespective of dimension critical exponents

<p>&nbsp;<br>
$$ m \sim (T_C-T)^{\beta},$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$ C_V \sim (T_C-T)^{\alpha},$$
<p>&nbsp;<br>

with \( \beta =1/2 \) and \( \alpha = 1 \).
It predicts a phase transition for one dimension as well.
For the Ising model there is no phase transition for \( d=1 \).
In two dimensions we have \( \beta=1/8 \) and \( \alpha =0 \).

<p>

</section>


<section>

<h2>Scaling Results  <a name="___sec605"></a></h2>

Near \( T_C \) we can characterize the behavior of many physical quantities
by a power law behavior.
As an example, the mean magnetization is given by

<p>&nbsp;<br>
$$
\begin{equation}
  \langle {\cal M}(T) \rangle \sim \left(T-T_C\right)^{\beta},
\end{equation}
$$
<p>&nbsp;<br>

where \( \beta \) is a so-called critical exponent. A similar relation
applies to the heat capacity

<p>&nbsp;<br>
$$
\begin{equation}
  C_V(T) \sim \left|T_C-T\right|^{-\alpha},
\end{equation}
$$
<p>&nbsp;<br>

the susceptibility

<p>&nbsp;<br>
$$
\begin{equation}
  \chi(T) \sim \left|T_C-T\right|^{\gamma}.
\end{equation}
$$
<p>&nbsp;<br>

and the correlation length

<p>&nbsp;<br>
$$
\begin{equation}
  \xi(T) \sim \left|T_C-T\right|^{-\nu}.
\end{equation}
$$
<p>&nbsp;<br>

\( \alpha = 0 \), \( \beta = 1/8 \), \( \gamma = 7/4 \) and \( \nu = 1 \). Below we will derive these coefficients
from finite size scaling theories.

<p>

</section>


<section>

<h2>Scaling Results  <a name="___sec606"></a></h2>

Through finite size scaling relations
it is possible to relate the behavior at finite lattices with the
results for an infinitely large lattice.
The critical temperature scales then as

<p>&nbsp;<br>
$$
\begin{equation}
 T_C(L)-T_C(L=\infty) \sim aL^{-1/\nu},
 \tag{50}
\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}
  \langle {\cal M}(T) \rangle \sim \left(T-T_C\right)^{\beta}
  \rightarrow L^{-\beta/\nu},
  \tag{51}
\end{equation}
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\begin{equation}
  C_V(T) \sim \left|T_C-T\right|^{-\gamma} \rightarrow L^{\gamma/\nu},
  \tag{52}
\end{equation}
$$
<p>&nbsp;<br>

and

<p>&nbsp;<br>
$$
\begin{equation}
  \chi(T) \sim \left|T_C-T\right|^{-\alpha} \rightarrow L^{\alpha/\nu}.
  \tag{53}
\end{equation}
$$
<p>&nbsp;<br>

We can compute the slope of the curves for \( M \), \( C_V \) and \( \chi \) as function of lattice sites and extract
the exponent \( \nu \).

<p>

</section>


<section>

<h2>Analytic Results: two-dimensional Ising model  <a name="___sec607"></a></h2>

The analytic expression for the Ising model in two dimensions was obtained
in 1944 by the Norwegian chemist Lars Onsager (Nobel prize in chemistry).
The exact partition function
for \( N \) spins in two dimensions and with zero magnetic field \( \cal H \) is given by
<p>&nbsp;<br>
$$
   Z_N=\left[2cosh(\beta J)e^I\right]^N,
$$
<p>&nbsp;<br>

with
<p>&nbsp;<br>
$$
I=\frac{1}{2\pi}\int_0^{\pi}d\phi ln
        \left[\frac{1}{2}\left(1+(1-\kappa^2sin^2\phi)^{1/2}\right)\right],
$$
<p>&nbsp;<br>

and
<p>&nbsp;<br>
$$
   \kappa=2sinh(2\beta J)/cosh^2(2\beta J).
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analytic Results: two-dimensional Ising model  <a name="___sec608"></a></h2>

The resulting energy is given by
<p>&nbsp;<br>
$$
  \langle E\rangle =  -Jcoth(2\beta J)\left[1+\frac{2}{\pi}(2tanh^2(2\beta J)-1)K_1(q)\right],
$$
<p>&nbsp;<br>

with
\( q=2sinh(2\beta J)/cosh^2(2\beta J) \) and the complete elliptic integral of the first kind
<p>&nbsp;<br>
$$
K_1(q) = \int_0^{\pi/2}\frac{d\phi}{\sqrt{1-q^2sin^2\phi}}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analytic Results: two-dimensional Ising model  <a name="___sec609"></a></h2>

Differentiating once more with respect to temperature we obtain the specific heat given by

<p>&nbsp;<br>
$$
C_V=\frac{4k_B}{\pi}(\beta Jcoth(2\beta J))^2
$$
<p>&nbsp;<br>


<p>&nbsp;<br>
$$
\left\{K_1(q)-K_2(q)-(1-tanh^2(2\beta J))\left[\frac{\pi}{2}+(2tanh^2(2\beta J)-1)K_1(q)\right]\right\},
$$
<p>&nbsp;<br>

with
<p>&nbsp;<br>
$$
   K_2(q) = \int_0^{\pi/2}d\phi\sqrt{1-q^2sin^2\phi}.
$$
<p>&nbsp;<br>

is the complete elliptic integral of the second kind.
Near the critical temperature \( T_C \) the specific heat behaves as
<p>&nbsp;<br>
$$
C_V \approx -\frac{2}{k_B\pi}\left(\frac{J}{T_C}\right)^2ln\left|1-\frac{T}{T_C}\right|+\mbox{const}.
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Analytic Results: two-dimensional Ising model  <a name="___sec610"></a></h2>

In theories of critical phenomena one has that
<p>&nbsp;<br>
$$
C_V\sim \left|1-\frac{T}{T_C}\right|^{-\alpha},
$$
<p>&nbsp;<br>

and Onsager's result is a special case of this power law behavior. The limiting form of the function
<p>&nbsp;<br>
$$
lim_{\alpha\rightarrow 0} \frac{1}{\alpha}(Y^{-\alpha}-1)=-lnY,
$$
<p>&nbsp;<br>

meaning that the analytic result is a special case of the power law singularity with
\( \alpha =0 \).

<p>

</section>


<section>

<h2>Analytic Results: two-dimensional Ising model  <a name="___sec611"></a></h2>

One can also show that the mean magnetization per spin is

<p>&nbsp;<br>
$$
    \left[1-\frac{(1-tanh^2 (\beta J))^4}{16tanh^4(\beta J)}\right]^{1/8}
$$
<p>&nbsp;<br>

for \( T < T_C \) and \( 0 \) for \( T> T_C \). The behavior is thus as \( T\rightarrow T_C \) from below

<p>&nbsp;<br>
$$
M(T) \sim (T_C-T)^{1/8}
$$
<p>&nbsp;<br>

The susceptibility behaves as

<p>&nbsp;<br>
$$
\chi(T) \sim |T_C-T|^{-7/4}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec612"></a></h2>

The so-called time-displacement autocorrelation \( \phi(t) \) for the magnetization is given by

<p>&nbsp;<br>
$$
\phi(t) = \int dt' \left[{\cal M}(t')-\langle {\cal M} \rangle\right]\left[{\cal M}(t'+t)-\langle {\cal M} \rangle\right],
$$
<p>&nbsp;<br>

which can be rewritten as

<p>&nbsp;<br>
$$
\phi(t) = \int dt' \left[{\cal M}(t'){\cal M}(t'+t)-\langle {\cal M} \rangle^2\right],
$$
<p>&nbsp;<br>

where \( \langle {\cal M} \rangle \) is the average value of the magnetization and
\( {\cal M}(t) \) its instantaneous value. We can discretize this function as follows, where we used our
set of computed values \( {\cal M}(t) \) for a set of discretized times (our Monte Carlo cycles corresponding
to a sweep over the lattice)

<p>&nbsp;<br>
$$
\phi(t)  = \frac{1}{t_{\mbox{max}}-t}\sum_{t'=0}^{t_{\mbox{max}}-t}{\cal M}(t'){\cal M}(t'+t)
-\frac{1}{t_{\mbox{max}}-t}\sum_{t'=0}^{t_{\mbox{max}}-t}{\cal M}(t')\times
\frac{1}{t_{\mbox{max}}-t}\sum_{t'=0}^{t_{\mbox{max}}-t}{\cal M}(t'+t).label{eq:phitf}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Correlation Time, how to compute  <a name="___sec613"></a></h2>


<!-- code=c++ (!bc cppcod) typeset with pygments style "perldoc" -->
<div class="highlight" style="background: #eeeedd"><pre style="font-size: 80%; line-height: 125%"><span style="color: #228B22">// define m-value at each cycle within loop over cycles</span>
m_matrix[cycles] = Maverage/(n_spins**<span style="color: #B452CD">2</span>)/cycles
<span style="color: #228B22">//  Then compute phi(i)  as (in pseudocode)</span>
    <span style="color: #8B008B; font-weight: bold">for</span>  i = <span style="color: #B452CD">1</span>, mcs
         r = <span style="color: #B452CD">1.0</span>/(mcs-i)
         s = <span style="color: #B452CD">0.0</span>; v = <span style="color: #B452CD">0.0</span>; p= <span style="color: #B452CD">0.0</span>
         <span style="color: #8B008B; font-weight: bold">for</span> j = <span style="color: #B452CD">1</span>, mcs-i
             p = p+ m_matrix(j)*m_matrix(j+i)
             s = s+ m_matrix(j)
             v = v+ m_matrix(j+i)
         end <span style="color: #8B008B; font-weight: bold">for</span>
         phi(i)  = r*p-r*r*s*v
    end <span style="color: #8B008B; font-weight: bold">for</span>
</pre></div>
<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec614"></a></h2>

One should be careful with times close to \( t_{\mbox{max}} \), the upper limit of the sums
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in \( \phi(t) \) due to the random nature of the fluctuations in \( {\cal M}(t) \) can become large.

<p>
One should therefore choose \( t \ll t_{\mbox{max}} \).

<p>
Note also that we could replace the magnetization with the mean energy, or any other expectation values of interest.

<p>
The time-correlation function for the magnetization gives a measure of the correlation between the magnetization
at a time \( t' \) and a time \( t'+t \). If we multiply the magnetizations at these two different times,
we will get a positive contribution if the magnetizations are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function \( \phi(t) \) should take a non-zero value if the fluctuations are
correlated, else it should gradually go to zero. For times a long way apart the magnetizations are most likely
uncorrelated and \( \phi(t) \) should be zero.

<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec615"></a></h2>

We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations.
Our probability
distribution function \( {\bf \hat{w}}(t) \) after a given number of time steps \( t \) could be written as

<p>&nbsp;<br>
$$
   {\bf \hat{w}}(t) = {\bf \hat{W}^t\hat{w}}(0),
$$
<p>&nbsp;<br>

with \( {\bf \hat{w}}(0) \) the distribution at \( t=0 \) and \( {\bf \hat{W}} \) representing the
transition probability matrix.
We can always expand \( {\bf \hat{w}}(0) \) in terms of the right eigenvectors of
\( {\bf \hat{v}} \) of \( {\bf \hat{W}} \) as

<p>&nbsp;<br>
$$
    {\bf \hat{w}}(0)  = \sum_i\alpha_i{\bf \hat{v}}_i,
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
   {\bf \hat{w}}(t) = {\bf \hat{W}}^t{\bf \hat{w}}(0)={\bf \hat{W}}^t\sum_i\alpha_i{\bf \hat{v}}_i=
\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i,
$$
<p>&nbsp;<br>

with \( \lambda_i \) the \( i^{\mbox{th}} \) eigenvalue corresponding to
the eigenvector \( {\bf \hat{v}}_i \).

<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec616"></a></h2>

If we assume that \( \lambda_0 \) is the largest eigenvector we see that in the limit \( t\rightarrow \infty \),
\( {\bf \hat{w}}(t) \) becomes proportional to the corresponding eigenvector
\( {\bf \hat{v}}_0 \). This is our steady state or final distribution.

<p>
We can relate this property to an observable like the mean magnetization.
With the probabilty \( {\bf \hat{w}}(t) \) (which in our case is the Boltzmann distribution) we
can write the mean magnetization as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \sum_{\mu} {\bf \hat{w}}(t)_{\mu}{\cal M}_{\mu},
$$
<p>&nbsp;<br>

or as the scalar of a  vector product

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m},
$$
<p>&nbsp;<br>

with \( {\bf m} \) being the vector whose elements are the values of \( {\cal M}_{\mu} \) in its
various microstates \( \mu \).

<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec617"></a></h2>

We rewrite this relation  as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m}=\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i{\bf m}_i.
$$
<p>&nbsp;<br>

If we define \( m_i={\bf \hat{v}}_i{\bf m}_i \) as the expectation value of
\( {\cal M} \) in the \( i^{\mbox{th}} \) eigenstate we can rewrite the last equation as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
$$
<p>&nbsp;<br>

Since we have that in the limit \( t\rightarrow \infty \) the mean magnetization is dominated by the
the largest eigenvalue \( \lambda_0 \), we can rewrite the last equation as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
$$
<p>&nbsp;<br>

We define the quantity

<p>&nbsp;<br>
$$
   \tau_i=-\frac{1}{log\lambda_i},
$$
<p>&nbsp;<br>

and rewrite the last expectation value as

<p>&nbsp;<br>
$$
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\tag{54}
$$
<p>&nbsp;<br>


<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec618"></a></h2>

The quantities \( \tau_i \) are the correlation times for the system. They control also the auto-correlation function
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue \( \tau_1 \), which normally defines the correlation time discussed above. For large times, this is the
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from
\( \lambda_1 \) and we simulate long enough,  \( \tau_1 \) may well define the correlation time.
In other cases we may not be able to extract a reliable result for \( \tau_1 \).
Coming back to the time correlation function \( \phi(t) \) we can present a more general definition in terms
of the mean magnetizations $ \langle {\cal M}(t) \rangle$. Recalling that the mean value is equal
to $ \langle {\cal M}(\infty) \rangle$ we arrive at the expectation values

<p>&nbsp;<br>
$$
\phi(t) =\langle {\cal M}(0)-{\cal M}(\infty)\rangle \langle {\cal M}(t)-{\cal M}(\infty)\rangle,
$$
<p>&nbsp;<br>

resulting in

<p>&nbsp;<br>
$$
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
$$
<p>&nbsp;<br>

which is appropriate for all times.

<p>

</section>


<section>

<h2>Correlation Time  <a name="___sec619"></a></h2>

If the correlation function decays exponentially

<p>&nbsp;<br>
$$ \phi (t) \sim \exp{(-t/\tau)}$$
<p>&nbsp;<br>

then the exponential correlation time can be computed as the average

<p>&nbsp;<br>
$$   \tau_{\mbox{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle. $$
<p>&nbsp;<br>


<p>
If the decay is exponential, then

<p>&nbsp;<br>
$$  \int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),$$
<p>&nbsp;<br>

which  suggests another measure of correlation

<p>&nbsp;<br>
$$   \tau_{\mbox{int}} = \sum_k \frac{\phi(k)}{\phi(0)}, $$
<p>&nbsp;<br>

called the integrated correlation time.

<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec620"></a></h2>

Here we mention that
one can show, using scaling relations, that at the critical temperature the
correlation time \( \tau \) relates to the lattice size \( L \) as

<p>&nbsp;<br>
$$
    \tau \sim L^{d+z},
$$
<p>&nbsp;<br>

with \( d \) the dimensionality of the system.
For the Metropolis algorithm based on a single spin-flip process, Nightingale and Bl\"ote obtained
\( z=2.1665\pm 0.0012 \). This is a rather high value, meaning that our algorithm is not the best
choice when studying properties of the Ising model near \( T_C \).

<p>
We can understand this behavior by studying the development of the two-dimensional
Ising model as function of temperature.

<p>

</section>


<section>

<h2>Time Auto-correlation Function  <a name="___sec621"></a></h2>

Cooling the system down to the critical temperature we observe clusters pervading larger areas of the lattice.
The reason for the large correlation time (and the parameter \( z \)) for the single-spin flip Metropolis
algorithm is the development of these large domains or clusters with all spins pointing in one direction.
It is quite difficult for the algorithm to flip over one of these large domains because it has to do it spin by spin,
with each move having a high probability of being rejected due to the ferromagnetic interaction
between spins.
Since all spins point in the same direction, the chance of performing the flip

<p>&nbsp;<br>
$$
  E=-4J\quad\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \uparrow & \uparrow\\
                                  & \uparrow & \end{array}
\quad\Longrightarrow\quad
  E=4J\quad\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \downarrow & \uparrow\\
                                  & \uparrow & \end{array}
$$
<p>&nbsp;<br>

leads to an energy difference of \( \Delta E = 8J \). Using the exact critical temperature \( k_BT_C/J\approx 2.2.69 \),
we obtain a probability \( \exp{-(8/2.269)}=0.029429 \) which is rather small.
The increase in large correlation times due to increasing lattices can be diminished by using  so-called
cluster algorithms, such as that
introduced by Ulli Wolff in 1989 and the Swendsen-Wang algorithm from 1987.
The two-dimensional Ising model with the Wolff or Swendsen-Wang algorithms exhibits a much smaller
correlation time, with the variable \( z=0.25\pm 001 \). Here, instead of flipping a single spin, one flips an entire cluster
of spins pointing in the same direction.

<p>

</section>


<section>

<h2>Better  Algorithm needed  <a name="___sec622"></a></h2>

Monte Carlo simulations close to a phase transition are affected by critical slowing down. In the 2-D Ising system,
the correlation length  becomes very large, and the correlation time, which measures the number of steps between
independent Monte Carlo configurations behaves like

<p>&nbsp;<br>
$$  \tau \sim \xi^z, $$
<p>&nbsp;<br>

with \( z\approx 2.1 \) for the Metropolis algorithm.
The exponent \( z \) is  called the dynamic critical exponent.
The maximum possible value for \( \xi \)  in a finite system of \( N = L \times L \) spins is   \( \xi\sim L \), because  \( \xi \)
cannot be larger than the
lattice size!
This implies that   \( \tau\sim L^z \approx N \). This makes simulations difficult because the Metropolis algorithm time
scales like \( N \), so the time to generate independent Metropolis configurations scales like

<p>&nbsp;<br>
$$ N\tau\sim   N^2 = L^4.$$
<p>&nbsp;<br>

If the lattice
size

<p>&nbsp;<br>
$$
L \rightarrow  \sqrt{10}L\approx 3.2L,
$$
<p>&nbsp;<br>

the simulation time increases by a factor of \( 100 \).

<p>

</section>


<section>

<h2>Better Algorithm  needed  <a name="___sec623"></a></h2>

There is a simple physical argument which helps understand why \( z =
2 \), The Metropolis algorithm is a local algorithm, i.e., one spin is
tested and flipped at a time. Near \( T_C \) the system develops large
domains of correlated spins which are difficult to break up. So the
most likely change in configuration is the movement of a whole domain
of spins. But one Metropolis sweep of the lattice can move a domain at
most by approximately one lattice spacing in each time step.  This
motion is stochastic, i.e., like a random walk.  The distance traveled
in a random walk scales like \( \sqrt{\mbox{time}} \), so to move a domain
a distance of order \( \xi \) takes \( \tau\sim \xi^2 \) Monte Carlo steps.
This argument suggests that the way to speed up a Monte Carlo
simulation near \( T_C \) is to use a non-local algorithm.


</section>



</div> <!-- class="slides" -->
</div> <!-- class="reveal" -->

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.min.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

    // Display navigation controls in the bottom right corner
    controls: true,

    // Display progress bar (below the horiz. slider)
    progress: true,

    // Display the page number of the current slide
    slideNumber: true,

    // Push each slide change to the browser history
    history: false,

    // Enable keyboard shortcuts for navigation
    keyboard: true,

    // Enable the slide overview mode
    overview: true,

    // Vertical centering of slides
    //center: true,
    center: false,

    // Enables touch navigation on devices with touch input
    touch: true,

    // Loop the presentation
    loop: false,

    // Change the presentation direction to be RTL
    rtl: false,

    // Turns fragments on and off globally
    fragments: true,

    // Flags if the presentation is running in an embedded mode,
    // i.e. contained within a limited portion of the screen
    embedded: false,

    // Number of milliseconds between automatically proceeding to the
    // next slide, disabled when set to 0, this value can be overwritten
    // by using a data-autoslide attribute on your slides
    autoSlide: 0,

    // Stop auto-sliding after user input
    autoSlideStoppable: true,

    // Enable slide navigation via mouse wheel
    mouseWheel: false,

    // Hides the address bar on mobile devices
    hideAddressBar: true,

    // Opens links in an iframe preview overlay
    previewLinks: false,

    // Transition style
    transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Transition speed
    transitionSpeed: 'default', // default/fast/slow

    // Transition style for full page slide backgrounds
    backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

    // Number of slides away from the current that are visible
    viewDistance: 3,

    // Parallax background image
    //parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

    // Parallax background size
    //parallaxBackgroundSize: '' // CSS syntax, e.g. "2100px 900px"

    theme: Reveal.getQueryHash().theme, // available themes are in reveal.js/css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/none

});

Reveal.initialize({
    dependencies: [
        // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
        { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },

        // Interpret Markdown in <section> elements
        { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

        // Syntax highlight for <code> elements
        { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

        // Zoom in and out with Alt+click
        { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

        // Speaker notes
        { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

        // Remote control your reveal.js presentation using a touch device
        //{ src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

        // MathJax
        //{ src: 'reveal.js/plugin/math/math.js', async: true }
    ]
});

Reveal.initialize({

    // The "normal" size of the presentation, aspect ratio will be preserved
    // when the presentation is scaled to fit different resolutions. Can be
    // specified using percentage units.
    width:  960,
    height: 700,

    // Factor of the display size that should remain empty around the content
    margin: 0.1,

    // Bounds for smallest/largest possible scale to apply to content
    minScale: 0.2,
    maxScale: 1.0

});
</script>

<!-- begin footer logo
<div style="position: absolute; bottom: 0px; left: 0; margin-left: 0px">
<img src="somelogo.png">
</div>
     end footer logo -->



</body>
</html>
