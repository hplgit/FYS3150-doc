 % Slides for FYS3150/4150

\documentclass[compress]{beamer}


% Try the class options [notes], [notes=only], [trans], [handout],
% [red], [compress], [draft], [class=article] and see what happens

% For a green structure color use:
%\colorlet{structure}{green!50!black}

\mode<article> % only for the article version
{
  \usepackage{beamerbasearticle}
  \usepackage{fullpage}
  \usepackage{hyperref}
}

\beamertemplateshadingbackground{red!10}{blue!10}

%\usetheme{Hannover}

\setbeamertemplate{footline}[page number]


%\usepackage{beamerthemeshadow}



\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{shadow}
\lstset{language=c++}
\lstset{alsolanguage=[90]Fortran}
\lstset{basicstyle=\small}
%\lstset{backgroundcolor=\color{white}}
%\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
%\lstset{keywordstyle=\color{red}\bfseries}
%\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\usepackage{times}

% Use some nice templates
\beamertemplatetransparentcovereddynamic

% own commands

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
%\newcommand{\bra}[1]{\left\langle #1 \right|}
%\newcommand{\ket}[1]{\left| # \right\rangle}
\newcommand{\braket}[2]{\left\langle #1 \right| #2 \right\rangle}
\newcommand{\OP}[1]{{\bf\widehat{#1}}}
\newcommand{\matr}[1]{{\bf \cal{#1}}}
\newcommand{\beN}{\begin{equation*}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\beaN}{\begin{eqnarray*}}
\newcommand{\eeN}{\end{equation*}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\eeaN}{\end{eqnarray*}}
\newcommand{\bdm}{\begin{displaymath}}
\newcommand{\edm}{\end{displaymath}}
\newcommand{\bsubeqs}{\begin{subequations}}
\newcommand{\esubeqs}{\end{subequations}}
\newcommand{\Grad}[1]{\boldsymbol{\nabla}{#1}}
\def\psii{\psi_{i}}
\def\psij{\psi_{j}}
\def\psiij{\psi_{ij}}
\def\psisq{\psi^2}
\def\psisqex{\langle \psi^2 \rangle}
\def\psiR{\psi({\bf R})}
\def\psiRk{\psi({\bf R}_k)}
\def\psiiRk{\psi_{i}(\Rveck)}
\def\psijRk{\psi_{j}(\Rveck)}
\def\psiijRk{\psi_{ij}(\Rveck)}
\def\ranglep{\rangle_{\psisq}}
\def\Hpsibypsi{{H \psi \over \psi}}
\def\Hpsiibypsi{{H \psii \over \psi}}
\def\HmEpsibypsi{{(H-E) \psi \over \psi}}
\def\HmEpsiibypsi{{(H-E) \psii \over \psi}}
\def\HmEpsijbypsi{{(H-E) \psij \over \psi}}
\def\psiibypsi{{\psii \over \psi}}
\def\psijbypsi{{\psij \over \psi}}
\def\psiijbypsi{{\psiij \over \psi}}
\def\psiibypsiRk{{\psii(\Rveck) \over \psi(\Rveck)}}
\def\psijbypsiRk{{\psij(\Rveck) \over \psi(\Rveck)}}
\def\psiijbypsiRk{{\psiij(\Rveck) \over \psi(\Rveck)}}
\def\EL{E_{\rm L}}
\def\ELi{E_{{\rm L},i}}
\def\ELj{E_{{\rm L},j}}
\def\ELRk{E_{\rm L}(\Rveck)}
\def\ELiRk{E_{{\rm L},i}(\Rveck)}
\def\ELjRk{E_{{\rm L},j}(\Rveck)}
\def\Ebar{\bar{E}}
\def\Ei{\Ebar_{i}}
\def\Ej{\Ebar_{j}}
\def\Ebar{\bar{E}}
\def\Rvec{{\bf R}}
\def\Rveck{{\bf R}_k}
\def\Rvecl{{\bf R}_l}
\def\NMC{N_{\rm MC}}
\def\sumMC{\sum_{k=1}^{\NMC}}
\def\MC{Monte Carlo}
\def\adiag{a_{\rm diag}}
\def\tcorr{T_{\rm corr}}
\def\intR{{\int {\rm d}^{3N}\!\!R\;}}

\def\ul{\underline}
\def\beq{\begin{eqnarray}}
\def\eeq{\end{eqnarray}}

\newcommand{\eqbrace}[4]{\left\{
\begin{array}{ll}
#1 & #2 \\[0.5cm]
#3 & #4
\end{array}\right.}
\newcommand{\eqbraced}[4]{\left\{
\begin{array}{ll}
#1 & #2 \\[0.5cm]
#3 & #4
\end{array}\right\}}
\newcommand{\eqbracetriple}[6]{\left\{
\begin{array}{ll}
#1 & #2 \\
#3 & #4 \\
#5 & #6
\end{array}\right.}
\newcommand{\eqbracedtriple}[6]{\left\{
\begin{array}{ll}
#1 & #2 \\
#3 & #4 \\
#5 & #6
\end{array}\right\}}

\newcommand{\mybox}[3]{\mbox{\makebox[#1][#2]{$#3$}}}
\newcommand{\myframedbox}[3]{\mbox{\framebox[#1][#2]{$#3$}}}

%% Infinitesimal (and double infinitesimal), useful at end of integrals
%\newcommand{\ud}[1]{\mathrm d#1}
\newcommand{\ud}[1]{d#1}
\newcommand{\udd}[1]{d^2\!#1}

%% Operators, algebraic matrices, algebraic vectors

%% Operator (hat, bold or bold symbol, whichever you like best):
\newcommand{\op}[1]{\widehat{#1}}
%\newcommand{\op}[1]{\mathbf{#1}}
%\newcommand{\op}[1]{\boldsymbol{#1}}

%% Vector:
\renewcommand{\vec}[1]{\boldsymbol{#1}}

%% Matrix symbol:
%\newcommand{\matr}[1]{\boldsymbol{#1}}
%\newcommand{\bb}[1]{\mathbb{#1}}

%% Determinant symbol:
\renewcommand{\det}[1]{|#1|}

%% Means (expectation values) of varius sizes
\newcommand{\mean}[1]{\langle #1 \rangle}
\newcommand{\meanb}[1]{\big\langle #1 \big\rangle}
\newcommand{\meanbb}[1]{\Big\langle #1 \Big\rangle}
\newcommand{\meanbbb}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\meanbbbb}[1]{\Bigg\langle #1 \Bigg\rangle}

%% Shorthands for text set in roman font
\newcommand{\prob}[0]{\mathrm{Prob}} %probability
\newcommand{\cov}[0]{\mathrm{Cov}}   %covariance
\newcommand{\var}[0]{\mathrm{Var}}   %variancd

%% Big-O (typically for specifying the speed scaling of an algorithm)
\newcommand{\bigO}{\mathcal{O}}

%% Real value of a complex number
\newcommand{\real}[1]{\mathrm{Re}\!\left\{#1\right\}}

%% Quantum mechanical state vectors and matrix elements (of different sizes)
%\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\brab}[1]{\big\langle #1 \big|}
\newcommand{\brabb}[1]{\Big\langle #1 \Big|}
\newcommand{\brabbb}[1]{\bigg\langle #1 \bigg|}
\newcommand{\brabbbb}[1]{\Bigg\langle #1 \Bigg|}
%\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\ketb}[1]{\big| #1 \big\rangle}
\newcommand{\ketbb}[1]{\Big| #1 \Big\rangle}
\newcommand{\ketbbb}[1]{\bigg| #1 \bigg\rangle}
\newcommand{\ketbbbb}[1]{\Bigg| #1 \Bigg\rangle}
\newcommand{\overlap}[2]{\langle #1 | #2 \rangle}
\newcommand{\overlapb}[2]{\big\langle #1 \big| #2 \big\rangle}
\newcommand{\overlapbb}[2]{\Big\langle #1 \Big| #2 \Big\rangle}
\newcommand{\overlapbbb}[2]{\bigg\langle #1 \bigg| #2 \bigg\rangle}
\newcommand{\overlapbbbb}[2]{\Bigg\langle #1 \Bigg| #2 \Bigg\rangle}
\newcommand{\bracket}[3]{\langle #1 | #2 | #3 \rangle}
\newcommand{\bracketb}[3]{\big\langle #1 \big| #2 \big| #3 \big\rangle}
\newcommand{\bracketbb}[3]{\Big\langle #1 \Big| #2 \Big| #3 \Big\rangle}
\newcommand{\bracketbbb}[3]{\bigg\langle #1 \bigg| #2 \bigg| #3 \bigg\rangle}
\newcommand{\bracketbbbb}[3]{\Bigg\langle #1 \Bigg| #2 \Bigg| #3 \Bigg\rangle}
\newcommand{\projection}[2]
{| #1 \rangle \langle  #2 |}
\newcommand{\projectionb}[2]
{\big| #1 \big\rangle \big\langle #2 \big|}
\newcommand{\projectionbb}[2]
{ \Big| #1 \Big\rangle \Big\langle #2 \Big|}
\newcommand{\projectionbbb}[2]
{ \bigg| #1 \bigg\rangle \bigg\langle #2 \bigg|}
\newcommand{\projectionbbbb}[2]
{ \Bigg| #1 \Bigg\rangle \Bigg\langle #2 \Bigg|}


%% If you run out of greek symbols, here's another one you haven't
%% thought of:
\newcommand{\Feta}{\hspace{0.6ex}\begin{turn}{180}
        {\raisebox{-\height}{\parbox[c]{1mm}{F}}}\end{turn}}
\newcommand{\feta}{\hspace{-1.6ex}\begin{turn}{180}
        {\raisebox{-\height}{\parbox[b]{4mm}{f}}}\end{turn}}



\title[FYS3150]{Slides from FYS3150/4150 Lectures}
\author[Computational Physics I, FYS3150]{%
  Morten Hjorth-Jensen}
\institute[ORNL, University of Oslo and MSU]{
%  \inst{1}
  Department of Physics and Center of Mathematics for Applications\\
  University of Oslo, N-0316 Oslo, Norway  and \\
National Superconducting Cyclotron Laboratory, Michigan State University, East Lansing, MI 48824, USA}
%  \and
%  \inst{2}
%  Department of Physics and Astronomy, Michigan State University \\ 
%East Lansing, Michigan, USA}
  
\date[UiO]{Fall 2014}
\subject{Computational Physics I}

%\pgfdeclareimage[width=10cm,angle=270]{acceptreject}{acceptreject}
%\pgfdeclareimage[width=10cm,angle=270]{random}{random}
%\pgfdeclareimage[width=10cm,angle=270]{r_rho}{r_rho}
%\pgfdeclareimage[width=6cm,angle=270]{pi}{pi}

%\pgfdeclareimage[width=10cm,angle=270]{uwm}{uwm}
%\pgfdeclareimage[width=10cm,angle=270]{exp}{exp}

\begin{document}

\frame{\titlepage}

%\section<presentation>*{Outline}

%\frame
%{
%  \frametitle{Outline}
%  \tableofcontents[part=1,pausesections]
%}

%\AtBeginSubsection[]
%{
%  \frame<handout:0>
%  {
%    \frametitle{Outline}
%    \tableofcontents[current,currentsubsection]
%  }
%}

\part<presentation>{Main Talk}

%\section[Introduction]{Introduction to FYS3150/4150}

%\subsection[Presentation]{Plan for this week}

\section{Week 34}

\frame
{
  \frametitle{Week 34}
  \begin{block}{}
\begin{itemize}
\item Monday: First lecture: Presentation of the course, aims and content 
\item Monday: Second Lecture: Introduction to C++ programming and numerical
precision.
\item Tuesday: Numerical precision and C++ programming, continued and exercises for first week
\item Numerical differentiation and loss of numerical precision (chapter 3 lecture notes)
\item Computer-Lab: Thursday and Friday. First time: Thursday and
  Friday this week, Presentation of hardware and software  at room
  FV329 first hour of every labgroup and solution of first simple exercises.
\end{itemize}
  \end{block}
} 


%\subsection[Format]{Format of the Course}

\frame
{
  \frametitle{Lectures and ComputerLab}
  \begin{block}{}
\begin{itemize}
\item Lectures: Monday (4.15pm-6pm) and Tuesday (12.15pm-2pm) only this and next week. Thereafter weeks 38 and 39,
and weeks 43 and 44 and finally weeks 47 and 48.
\item Weekly reading assignments needed to solve projects.
\item First hour of each lab session used to discuss technicalities, address questions etc linked with projects.  
       \item Detailed lecture notes, exercises, all programs presented, projects etc 
can be found at the homepage of the course.
       \item Computerlab: Thursday (9am-7pm) and Friday (9am-7pm) room FV329. 
       \item Weekly plans and all other information are on the official webpage.
\item Final written exam December 12, 9am (four hours). 
\end{itemize}
  \end{block}
}


\frame
{
  \frametitle{Course Format}
  \begin{block}{}
\begin{itemize}
\item Several computer exercises, 5 compulsory projects. Electronic reports
only.
       \item Evaluation and grading: The last project (50\% of final grade) and a  final written exam (50\% of final grade). Final written exam December 12.

       \item The computer lab (room FV329)consists of  16 Linux PCs, but many prefer own laptops. C/C++ is the default
programming language, but Fortran2008 and Python are also used. All source codes discussed
during the lectures can be found at the webpage of the course. 
We recommend either C/C++, Fortran2008
or Python as languages.  
\end{itemize}
  \end{block}
}





\frame
{
  \frametitle{ComputerLab}
  \begin{block}{}
%\begin{small}
%{\scriptsize
\begin{tabular} {ll} \\ \hline
                day & teacher \\ \hline
                Thursday 9am-1pm& Anders, Morten L., H\aa vard, MHJ\\
                Thursday 1pm-5pm&  Anders, Morten L., H\aa vard, MHJ\\
                Friday 9am-1pm&  Anders, Morten L., H\aa vard, MHJ\\
                Friday 1pm-5pm&  Anders, Morten L., H\aa vard, MHJ\\ \hline

\end{tabular}\newline\newline
%}
%\end{small}
\end{block}
}










\frame
{
  \frametitle{Topics covered in this course}
  \begin{block}{}
\begin{small}
{\scriptsize
        \begin{itemize}
          \item Numerical precision and  intro to C++ programming
          \item Numerical derivation and integration
          \item Random numbers and Monte Carlo integration
          \item Monte Carlo methods in statistical physics
          \item Quantum Monte Carlo methods
          \item Linear algebra and eigenvalue problems
          \item Non-linear equations and roots of polynomials
          \item Ordinary differential equations
          \item Partial differential equations
          \item Parallelization of codes
          \item Programming av GPUs (optional)
         \end{itemize}
}
\end{small}
  \end{block}
}

\frame
{ 
  \frametitle{Syllabus FYS3150}
  \begin{block}{Linear algebra and eigenvalue problems, chapters 6 and 7}
\begin{itemize}
\item Know Gaussian elimination and LU decomposition 
\item How to solve linear equations   
\item How to obtain the inverse and the determinant of a real symmetric matrix
\item Cholesky and tridiagonal matrix decomposition 
\end{itemize}
  \end{block}
} 




\frame
{ 
  \frametitle{Syllabus FYS3150}
  \begin{block}{Linear algebra and eigenvalue problems, chapters 6 and 7}
\begin{itemize}
\item Householder's tridiagonalization technique and finding eigenvalues based on this
\item Jacobi's method for finding eigenvalues
\item Singular value decomposition
\item Qubic Spline interpolation 
\end{itemize}
  \end{block}
} 




\frame
{ 
  \frametitle{Syllabus FYS3150}
  \begin{block}{Numerical integration, standard methods and Monte Carlo methods (chapters 4 and 11)}
\begin{itemize}
\item Trapezoidal, rectangle  and Simpson's rules
\item Gaussian quadrature, emphasis on Legendre polynomials, but you need
to know about other polynomials as well.  
\item Brute force Monte Carlo integration 
\item Random numbers (simplest algo, ran0) and probability distribution functions, expectation values
\item Improved Monte Carlo integration and importance sampling. 
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Syllabus FYS3150}
  \begin{block}{Monte Carlo methods in physics (chapters 12, 13, and 14)}
\begin{itemize}
\item Random walks and Markov chains and relation with diffusion equation 
\item Metropolis algorithm, detailed balance and ergodicity
\item Simple spin systems and phase transitions
\item Variational Monte Carlo
\item How to construct trial wave functions for quantum systems 
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Syllabus FYS3150}
  \begin{block}{Ordinary differential equations (chapters 8 and 9)}
\begin{itemize}
\item Euler's method and improved Euler's method, truncation errors
\item Runge Kutta methods, 2nd and 4th order, truncation errors
\item How to implement a second-order differential equation, 
both linear and non-linear. How to make your equations dimensionless.
\item Boundary value problems, shooting and matching method (chap 9).
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Syllabus FYS3150}
  \begin{block}{Partial differential equations, chapter 10}
\begin{itemize}
\item Set up diffusion, Poisson and wave equations up to 2
spatial dimensions and time   
\item Set up the mathematical model and algorithms for these equations,
with boundary and initial conditions. Their stability conditions.
\item Explicit, implicit and Crank-Nicolson schemes, and how to solve them.
Remember that they result in triangular matrices. 
\item How to compute the Laplacian in Poisson's equation.
\item How to solve the wave equation in one and two dimensions.
\end{itemize}
  \end{block}
} 



\frame
{ 
  \frametitle{Overarching aims of this course}
\begin{itemize}
\item Develop a  critical approach to all steps in  a project, which methods are 
most relevant, which natural laws and physical processes are important. Sort out initial conditions and boundary conditions etc.
\item This means to teach you structured scientific computing, learn to structure a project.
\item A critical understanding of central mathematical algorithms and methods
from numerical analysis. In particular their limits and stability criteria.
\item Always try to find good checks of your codes (like solutions on closed form)
\item To enable you to develop a  critical view on the mathematical model and the physics.
\end{itemize}
} 


\frame
{ 
  \frametitle{And, there is nothing like a code which gives correct results!!}
\vspace{-10cm}
\begin{figure}
\includegraphics[scale=0.6]{Nebbdyr2.pdf}
\end{figure}
} 


\frame
{
  \frametitle<presentation>{Selected Texts and lectures on C/C++}
 \begin{small}
 {\scriptsize

  \beamertemplatebookbibitems

  \begin{thebibliography}{10}
   \bibitem{ref1} J.~J.~Barton and L.~R.~Nackman,{\em Scientific and Engineering C++}, Addison Wesley, 3rd edition 2000.
   \bibitem{ref2} B.~Stoustrup, {\em The C++ programming language}, Pearson, 1997. 
   \bibitem{ref3} H.~P.~Langtangen INF-VERK3830 \url{http://heim.ifi.uio.no/~hpl/INF-VERK4830/}
   \bibitem{ref4} D.~Yang, {\em C++ and Object-oriented Numeric Computing for
Scientists and Engineers}, Springer 2000.
\bibitem{ref5} More books reviewed at \url{http:://www.accu.org/} and 
\url{http://www.comeaucomputing.com/booklist/}
\end{thebibliography}
 }
 \end{small}
}



\frame
{ 
  \frametitle{Other courses in Computational Science at UiO}
  \begin{block}{Bachelor/Master/PhD Courses}
\begin{itemize}
\item INF-MAT4350 Numerical linear algebra
\item MAT-INF3300/3310, PDEs and Sobolev spaces I and II
\item INF-MAT3360 PDEs
\item INF5620 Numerical methods for PDEs, finite element method
\item FYS4411 Computational physics II (Parallelization (MPI), object orientation, 
quantum mechanical
systems with many interacting particles), spring semester
\item FYS4460 Computational physics III (Parallelization (MPI), object orientation, 
classical statistical physics, simulation of phase transitions, spring semester
\item INF3331 Problem solving with high-level languages (Python), fall semester
\item INF3380 Parallel computing for problems in the Natural Sciences (mostly PDEs), spring semester
\end{itemize}
  \end{block}
} 

\frame
{ 
  \frametitle{Extremely useful tools, strongly recommended}
  \begin{block}{and discussed at the lab sessions the first week}
\begin{itemize}
\item GIT  for version control (see webpage) 
\item ipython notebook
\item QTcreator for editing and mastering computational projects (for C++ codes, see webpage of course)
\item Armadillo as a useful numerical library for C++, highly recommended
\item Unit tests, see also webpage
\item Devilry for handing in projects
\end{itemize}
  \end{block}
} 

%\section[C/C++]{Introduction to C/C++ and Numerical Precision}

%\subsection[Style]{Structured Programming}

\frame
{
  \frametitle{A structured programming approach}
\begin{small}
{\scriptsize
        \begin{itemize}
\item Before writing a single line, have the algorithm clarified and
understood. It is crucial to have a logical structure of e.g., the flow
and organization of data before one starts writing.
%
\item Always try  to choose the simplest algorithm. Computational speed
can be improved upon later.
%
\item Try to write a as clear program as possible. Such programs are
easier to debug, and although it may take more time, in the long run
it may save you time. If you collaborate with other people, it
reduces spending time on debuging and
trying to understand what the codes do. A clear program will also allow
you to remember better what the program really does!

         \end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{A structured programming approach}
\begin{small}
{\scriptsize
        \begin{itemize}
\item The planning of the program should be from top down to bottom,
trying to keep the flow as linear as possible. Avoid jumping back and
forth in the program. First you need to arrange the major tasks to be
achieved. Then try to break the major tasks into subtasks. These can be
represented by functions or subprograms. They should accomplish limited tasks
and as far as possible be independent of each other.  That will allow
you to use them in other programs as well.
%
\item Try always to find some cases where an analytical solution
exists or where simple test cases can be applied.
If possible, devise different algorithms for solving
the same problem. If you get the same answers, you may have
coded things correctly or made the same error twice or more.

         \end{itemize}
}
\end{small}
}



%\subsection[C/C++]{C/C++ Basics}

\frame[containsverbatim]
{
  \frametitle{Getting Started}
  \begin{block}{Compiling and linking, without QTcreator}
\begin{small}
{\scriptsize
In order to obtain an executable file for a C++ program, the following 
instructions under Linux/Unix can be used
\begin{verbatim}
c++ -c -Wall myprogram.cpp
c++ -o myprogram myprogram.o
\end{verbatim}
where the compiler is called through the command c++/g++. The compiler
option -Wall means that a warning is issued in case of non-standard
language. The executable file is in this case $myprogram$. The option
$-c$ is for compilation only, where the program is translated into machine code,
while the $-o$ option links the produced object file $myprogram.o$ 
and produces the executable $myprogram$ .

For Fortran2008 we use the Intel compiler, replace c++ with ifort.
Also, to speed up the code use compile options like
\begin{verbatim}
c++ -O3 -c -Wall myprogram.cpp
\end{verbatim}
}
\end{small}
  \end{block}
}

\frame[containsverbatim]
{
  \frametitle{Makefiles and simple scripts}
\begin{small}
{\scriptsize
Under Linux/Unix it is often convenient to create a
so-called makefile, which is a script which includes possible
compiling commands.
\begin{verbatim}
# Comment lines
# General makefile for c - choose PROG =   name of given program
# Here we define compiler option, libraries and the  target
CC= g++ -Wall
PROG= myprogram
# this is the math library in C, not necessary for C++
LIB = -lm
# Here we make the executable file
${PROG} :          ${PROG}.o
                   ${CC} ${PROG}.o ${LIB} -o ${PROG}
# whereas here we create the object file
${PROG}.o :       ${PROG}.c
                  ${CC} -c ${PROG}.c
\end{verbatim}
If you name your file for 'makefile', simply type the command
{\bf make} and Linux/Unix executes all of the statements in the above
makefile. Note that C++ files have the extension .cpp
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Hello world}
  \begin{block}{The C encounter}
\begin{small}
{\scriptsize
Here we present first the C version.

\begin{verbatim}
/* comments in C begin like this and end with */
#include <stdlib.h> /* atof function */
#include <math.h>   /* sine function */
#include <stdio.h>  /* printf function */
int main (int argc, char* argv[])
{
  double r, s;        /* declare variables */
  r = atof(argv[1]);  /* convert the text argv[1] to double */
  s = sin(r);
  printf("Hello, World! sin(%g)=%g\n", r, s);
  return 0;           /* success execution of the program */
}
\end{verbatim}
}
\end{small}
  \end{block}
}

\frame[containsverbatim]
{
  \frametitle{Hello World}
  \begin{block}{Dissection I}
\begin{small}
{\scriptsize
The compiler must see a declaration of a function before you can 
call it (the compiler checks the argument and return types). 
The declaration of library functions appears 
in so-called ``header files'' that must be included in the program, e.g.,
\begin{verbatim}
   #include <stdlib.h> /* atof function */
\end{verbatim}
We call three functions (atof, sin, printf) 
and these are declared in three different header files. 
The main program is a function called main 
with a return value set to an integer, int (0 if success). 
The operating system stores the return value, 
and other programs/utilities can check whether 
the execution was successful or not. 
The command-line arguments are transferred to the main function through
\begin{verbatim}
   int main (int argc, char* argv[])
\end{verbatim}
}
\end{small}
  \end{block}
}

\frame[containsverbatim]
{
  \frametitle{Hello World}
  \begin{block}{Dissection II}

\begin{small}
{\scriptsize
The command-line arguments are transferred to the main function through
\begin{verbatim}
   int main (int argc, char* argv[])
\end{verbatim}
The integer $argc$ is the no of command-line arguments, set to
one in our case, while  
$argv$ is a vector of strings containing the command-line arguments 
with    $argv[0]$ containing  the name of the program 
and $argv[1]$, $argv[2]$, ... are the command-line args, i.e., the number of 
lines of input to the program.                       
Here we define floating points, see also below, 
through the keywords $float$ for single precision real numbers and  
$double$ for double precision. The function  
$atof$ transforms a text $(argv[1])$ to a float. 
The sine function is declared in math.h, a library which 
is not automatically included and needs to be linked when computing
an executable file.

With the command $printf$ we obtain a formatted printout.
The $printf$ syntax is used for formatting output 
in many C-inspired languages (Perl, Python, awk, partly C++). 
}
\end{small}
  \end{block}
}

\frame[containsverbatim]
{
  \frametitle{Hello World}
  \begin{block}{Now in C++}
\begin{small}
{\scriptsize
Here we present first the C++ version.
\begin{verbatim}
// A comment line begins like this in C++ programs
// Standard ANSI-C++ include files 
using namespace std
#include <iostream>  // input and output 
int main (int argc, char* argv[])
{
//  convert the text argv[1] to double using atof: 
  double r = atof(argv[1]); 
  double s = sin(r);
  cout << "Hello, World! sin(" << r << ")=" << s << '\n';
// success 
  return 0;  
}
\end{verbatim}
}
\end{small}
  \end{block}
}

\frame[containsverbatim]
{
  \frametitle{C++ Hello World}
  \begin{block}{Dissection I}
\begin{small}
{\scriptsize
We have replaced the call to $printf$ with the standard C++ function
$cout$. The header file $<iostream.h>$ is then needed.
In addition, we don't need to 
declare variables like $r$ and $s$  at the beginning of the program. 
I personally prefer
however to declare all variables at the beginning of a function, as this
gives {\bf me} a feeling of greater readability.
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Brief summary}
  \begin{block}{C/C++ program}
\begin{itemize}
\item A C/C++ program begins with include statements of header files  (libraries,intrinsic functions etc) 
       \item Functions which are used are normally defined at top (details next week)
\item The main program is set up as an integer, it returns 0 (everything correct) or 1  (something went wrong)
\item Standard {\bf if}, {\bf while} and {\bf for} statements as in Java, Fortran, Python...
\item Integers have a very limited range.  
\end{itemize}
  \end{block}
}



\frame
{
  \frametitle{Brief summary}
  \begin{block}{Arrays}
\begin{itemize}
\item A C/C++ array begins by indexing at 0!
       \item Array allocations are done by size, not by the final index value.If you allocate an array with 10
elements, you should index them from $0,1,\dots, 9$.
\item Initialize always an array before a computation.
\end{itemize}
  \end{block}
}



\frame
{
  \frametitle{Serious problems and representation of numbers}
  \begin{block}{Integer and Real Numbers}
\begin{itemize} 
\item {\bf Overflow }
\item {\bf Underflow }
\item {\bf Roundoff errors}
\item {\bf Loss of precision}
\end{itemize}
  \end{block}
}




\frame
{
  \frametitle{Limits, you must declare variables}
  \begin{block}{C++ and Fortran declarations}
\begin{small}
{\scriptsize
\begin{tabular}{lcl}\hline \hline
\hspace*{\fill} type in C/C++ and Fortran2008 \hspace*{\fill}
&\hspace*{\fill} bits \hspace*{\fill}
&\hspace*{\fill} range \hspace*{\fill} \\ \hline
& & \\[-2mm]
int/INTEGER (2) & 16 & $-32768$ to 32767\\
unsigned int & 16 & 0 to 65535\\
signed int & 16 & $-32768$ to 32767\\
short int & 16 & $-32768$ to 32767\\
unsigned short int & 16 & 0 to 65535\\
signed short int & 16 & $-32768$ to 32767\\
int/long int/INTEGER(4) & 32 & $-2147483648$ to 2147483647\\
signed long int & 32 & $-2147483648$ to 2147483647\\
float/REAL(4) & 32 & $3.4\times 10^{-44}$ to $3.4\times 10^{+38}$\\
double/REAL(8) & 64 & $1.7\times 10^{-322}$ to $1.7\times 10^{+308}$\\
long double & 64 & $1.7\times 10^{-322}$ to $1.7\times 10^{+308}$\\\hline
\end{tabular}
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{From decimal to binary representation}
  \begin{block}{How to do it}
\begin{small}
{\scriptsize
\[
  a_n2^n+a_{n-1}2^{n-1}  +a_{n-2}2^{n-2}  +\dots +a_{0}2^{0}.
\]
%
In binary notation we have thus $(417)_{10} =(110110001)_2$
since we have 
\[
(110100001)_2
=1\times2^8+1\times 2^{7}+0\times 2^{6}+1\times 2^{5}+0\times 2^{4}+0\times 2^{3}+0\times 2^{2}+0\times 2^{2}+0\times 2^{1}+1\times 2^{0}.
\]
To see this, we have performed the following divisions by 2
\begin{center}
%\begin{table}[hbtp]
\begin{tabular}{lcc}\hline
417/2=208  & remainder 1& coefficient of $2^{0}$ is 1\\
208/2=104  & remainder 0& coefficient of $2^{1}$ is 0\\
104/2=52  & remainder 0& coefficient of $2^{2}$ is 0\\
52/2=26  & remainder 0& coefficient of $2^{3}$ is 0\\
26/2=13  & remainder 1& coefficient of $2^{4}$ is 0\\
13/2= 6 & remainder 1& coefficient of $2^{5}$ is 1\\
6/2= 3 & remainder 0& coefficient of $2^{6}$ is 0\\
3/2= 1 & remainder 1& coefficient of $2^{7}$ is 1\\
1/2= 0 & remainder 1& coefficient of $2^{8}$ is 1\\
\hline\end{tabular}%\end{table}
\end{center}
}
\end{small}
  \end{block}
}

\frame[containsverbatim]
{
  \frametitle{From decimal to binary representation}
  \begin{block}{Integer numbers}
\begin{small}
{\scriptsize
\begin{verbatim}
using namespace std;
#include <iostream>
int main (int argc, char* argv[])
{
  int i;
  int terms[32]; // storage of a0, a1, etc, up to 32 bits
  int number = atoi(argv[1]);
  // initialise the term a0, a1 etc
  for (i=0; i < 32 ; i++){ terms[i] = 0;}
  for (i=0; i < 32 ; i++){
    terms[i] = number%2;
    number /= 2;
  }
  // write out results
  cout << "Number of bytes used= " << sizeof(number) << endl;
  for (i=0; i < 32 ; i++){
    cout << " Term nr: " << i << "Value= " << terms[i];
    cout << endl;
  }
  return 0;
}
\end{verbatim}
}
\end{small}
  \end{block}
}

\frame[containsverbatim]
{
  \frametitle{From decimal to binary representation}
  \begin{block}{Integer numbers, Fortran}
\begin{small}
{\scriptsize
\begin{verbatim}
PROGRAM binary_integer
IMPLICIT NONE
  INTEGER  i, number, terms(0:31) ! storage of a0, a1, etc, up to 32 bits

  WRITE(*,*) 'Give a number to transform to binary notation'
  READ(*,*) number
! Initialise the terms a0, a1 etc
  terms = 0
! Fortran takes only integer loop variables
  DO i=0, 31
     terms(i) = MOD(number,2)
     number = number/2
  ENDDO
! write out results
  WRITE(*,*) 'Binary representation '
  DO i=0, 31
    WRITE(*,*)' Term nr and value', i, terms(i)
  ENDDO

END PROGRAM binary_integer
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Integer Numbers}
  \begin{block}{Possible Overflow for Integers}
\begin{small}
{\scriptsize
\begin{verbatim}
// A comment line begins like this in C++ programs
// Program to calculate 2**n
// Standard ANSI-C++ include files */
using namespace std
#include <iostream>
#include <cmath>
int main()
{
   int  int1, int2, int3;
// print to screen
   cout << "Read in the exponential N for 2^N =\n";    
// read from screen
   cin >> int2; 
   int1 = (int) pow(2., (double) int2);
   cout << " 2^N * 2^N = " << int1*int1 << "\n";
   int3 = int1 - 1;
   cout << " 2^N*(2^N - 1) = " << int1 * int3  << "\n";
   cout << " 2^N- 1 = " << int3  << "\n";
   return 0;
} 
// End: program main() 
\end{verbatim}
}
\end{small}
  \end{block}
}

\frame
{
  \frametitle{Loss of Precision}
  \begin{block}{Machine Numbers}
\begin{small}
{\scriptsize
In the decimal system we would write a number like $9.90625$ 
in what is called the normalized scientific notation. 
\[
  9.90625=0.990625\times 10^{1},
\]
and a real non-zero number could be generalized as
\begin{equation}
    x=\pm r\times 10^{{\mathrm{n}}},
\end{equation}
with $r$ a number in the range $1/10 \le r < 1$.
In a similar way we can use represent a binary number in  
scientific notation as 
\begin{equation}
    x=\pm q\times 2^{{\mathrm{m}}},
\end{equation}
with $q$ a number in the range $1/2 \le q < 1$. 
This means that the mantissa of a binary number would be represented by
the general formula
\begin{equation}
(0.a_{-1}a_{-2}\dots a_{-n})_2=a_{-1}\times 2^{-1}
+a_{-2}\times 2^{-2}+\dots+a_{-n}\times 2^{-n}.
\end{equation}
}
\end{small}
  \end{block}
} 


\frame
{
  \frametitle{Loss of Precision}
  \begin{block}{Machine Numbers}
\begin{small}
{\scriptsize


In a typical computer, floating-point numbers are represented
in the way described above, but with certain restrictions
on $q$ and $m$ imposed by the available word length. 
In the machine, our
number $x$ is represented as
%
\begin{equation}
    x=(-1)^s\times {\mathrm{mantissa}}\times 2^{{\mathrm{exponent}}},
\end{equation}
%
where $s$ is the sign bit, and the exponent gives the available range.
With a single-precision word, 32 bits, 8 bits would typically be reserved
for the exponent,  1 bit for the sign and 23 for the mantissa. 
}
\end{small}
  \end{block}
} 



\frame
{
  \frametitle{Loss of Precision}
  \begin{block}{Machine Numbers}
\begin{small}
{\scriptsize
A modification of the scientific notation for binary numbers is to
require that the leading binary digit 1 appears to the left of the binary point. 
In this case the representation of the mantissa $q$ would be
$(1.f)_2$ and $ 1 \le q < 2$. This form is rather useful when storing
binary numbers in a computer word, since we can always assume that the leading 
bit 1 is there. One bit of space can then be saved meaning that a 23 bits
mantissa has actually 24 bits. This means explicitely that a binary number with 23 bits 
for the mantissa reads
\begin{equation}
(1.a_{-1}a_{-2}\dots a_{-23})_2=1\times 2^0+a_{-1}\times 2^{-1}+
+a_{-2}\times 2^{-2}+\dots+a_{-23}\times 2^{-23}.
\end{equation}
As an example, consider the 32 bits binary number
\[
(10111110111101000000000000000000)_2,
\]
where the first bit is reserved for the sign, 1 in this case yielding a
negative sign. The exponent $m$ is given by the next 8 binary numbers
$01111101$ resulting in 125 in the decimal system. 
}
\end{small}
  \end{block}
} 


\frame
{
  \frametitle{Loss of Precision}
  \begin{block}{Machine Numbers}
\begin{small}
{\scriptsize
However, since the 
exponent has eight bits, this means it has  $2^8-1=255$ possible numbers in the interval
$-128 \le m \le 127$, our final
exponent is $125-127=-2$ resulting in $2^{-2}$.
Inserting the sign and the mantissa yields the final number in the decimal representation as
\[
 -2^{-2}\left(1\times 2^0+1\times 2^{-1}+
1\times 2^{-2}+1\times 2^{-3}+0\times 2^{-4}+1\times 2^{-5}\right)=\]
\[
(-0.4765625)_{10}.
\]
In this case we have an exact machine representation with 32 bits (actually, we need less than
23 bits for the mantissa).


If our number $x$ can be exactly represented in the machine, we call
$x$ a machine number. Unfortunately, most numbers cannot  and are thereby
only approximated in the machine. When such a number occurs as the result
of reading some input data or of a computation, an inevitable error
will arise in representing it as accurately as possible by
a machine number.
}
\end{small}
  \end{block}
} 


\frame
{
  \frametitle{Loss of Precision}
  \begin{block}{Machine Numbers}
\begin{small}
{\scriptsize
A floating number x, labelled $fl(x)$ will therefore always be represented as
\begin{equation}
  fl(x) = x(1\pm \epsilon_x),
\end{equation}
with $x$ the exact number and the error $|\epsilon_x| \le |\epsilon_M|$, where
$\epsilon_M$ is the precision assigned. A number like $1/10$ has no exact binary representation
with single or double precision. Since the mantissa 
\[
\left(1.a_{-1}a_{-2}\dots a_{-n}\right)_2
\]
is always truncated at some stage $n$ due to its limited number of bits, there is only a 
limited number of real binary numbers. The spacing between every real binary number is given by the 
chosen machine precision.
For a 32 bit words this number is approximately
$ \epsilon_M \sim 10^{-7}$ and for double precision (64 bits) we have
$ \epsilon_M \sim 10^{-16}$, or in terms of a binary base
as $2^{-23}$ and $2^{-52}$ for single and double precision, respectively. 
}
\end{small}
  \end{block}
} 




\frame
{
  \frametitle{Loss of Precision}
  \begin{block}{Machine Numbers}
\begin{small}
{\scriptsize
In the machine a number is represented as
\begin{equation}
  fl(x)= x(1+\epsilon)
\end{equation}  
%
where $|\epsilon| \leq \epsilon_M$ and $\epsilon$ is given by the
specified precision, $10^{-7}$ for single and $10^{-16}$ for double
precision, respectively.  
$\epsilon_M$ is the given precision.
In case of a subtraction $a=b-c$, we have  
\begin{equation}
   fl(a)=fl(b)-fl(c) = a(1+\epsilon_a),
\end{equation}
or
%
\begin{equation} 
   fl(a)=b(1+\epsilon_b)-c(1+\epsilon_c),
\end{equation} 
%
meaning that
%
\begin{equation} 
   fl(a)/a=1+\epsilon_b\frac{b}{a}- \epsilon_c\frac{c}{a},
\end{equation}
%
and if $b\approx c$ we see that there is a potential for an increased
error in $fl(a)$. 
}
\end{small}
  \end{block}
} 


\frame
{
  \frametitle{Loss of Precision}
  \begin{block}{Machine Numbers}
\begin{small}
{\scriptsize
Define
the absolute error as 
\begin{equation}
   |fl(a)-a|,
\end{equation}
whereas the relative error is 
\begin{equation}
   \frac{ |fl(a)-a|}{a} \le \epsilon_a.
\end{equation}
The above subraction is thus
\begin{equation}
   \frac{ |fl(a)-a|}{a}=\frac{ |fl(b)-fl(c)-(b-c)|}{a},
\end{equation}
yielding
\begin{equation}
   \frac{ |fl(a)-a|}{a}=\frac{ |b\epsilon_b- c\epsilon_c|}{a}.
\end{equation}
The relative error
is the quantity of interest in scientific work. Information about the 
absolute error is normally of little use in the absence of the magnitude
of the quantity being measured.
}
\end{small}
  \end{block}
} 

%\subsection[Numerical Precision]{Loss of Numerical Precision, a simple case}
\frame
{
  \frametitle{Loss of numerical precision}
\begin{small}
{\scriptsize
Suppose we wish to evaluate the function
\[
   f(x)=\frac{1-\cos(x)}{\sin(x)},
\]
for small values of $x$. Five leading digits. If we multiply the denominator and numerator
with $1+\cos(x)$ we obtain the equivalent expression
\[
   f(x)=\frac{\sin(x)}{1+\cos(x)}.
\]

If we now choose $x=0.007$ (in radians) our choice of precision results in
\[
   \sin(0.007)\approx 0.69999\times 10^{-2},
\]
and
\[
   \cos(0.007)\approx 0.99998.
\]

}
\end{small}
}


\frame
{
  \frametitle{Loss of numerical precision}
\begin{small}
{\scriptsize
The first expression for $f(x)$ results in
\[
   f(x)=\frac{1-0.99998}{0.69999\times 10^{-2}}=\frac{0.2\times 10^{-4}}{0.69999\times 10^{-2}}=0.28572\times 10^{-2},
\]
while the second expression results in
\[
   f(x)=\frac{0.69999\times 10^{-2}}{1+0.99998}=
\frac{0.69999\times 10^{-2}}{1.99998}=0.35000\times 10^{-2},
\]
which is also the exact result. In the first expression, due to our
choice of precision, we have  
only one relevant digit in the numerator, after the 
subtraction. This leads to a loss of precision and a wrong result due to
a cancellation of two nearly equal numbers. 
If we had chosen a precision of six leading digits, both expressions
yield the same answer.
}
\end{small}
}


\frame
{
  \frametitle{Loss of numerical precision}
\begin{small}
{\scriptsize
If we were to evaluate $x\sim \pi$, then the second expression for $f(x)$ 
can lead to potential losses of precision due to cancellations of nearly
equal numbers. 

This simple example demonstrates  the loss of numerical precision due
to roundoff errors, where the number of leading digits is lost 
in a subtraction of two near equal numbers. 
The lesson to be drawn is that we cannot blindly compute a function.
We will always need to carefully analyze our algorithm in the search for
potential pitfalls. There is no magic recipe however, the only guideline
is an understanding of the fact that a machine cannot represent
correctly {\bf all} numbers. 
}
\end{small}
}









\frame
{
  \frametitle{Loss of precision can cuae serious problems}
  \begin{block}{Real Numbers}
\begin{small}
{\scriptsize
\begin{itemize} 
\item {\bf Overflow :} When the positive exponent exceeds the 
max value, e.g., 308 for DOUBLE PRECISION (64 bits).
Under such circumstances the program will terminate and some
compilers may give you the warning 'OVERFLOW'.
\item {\bf Underflow :} When the negative exponent becomes smaller
than  the 
min  value, e.g., -308 for DOUBLE PRECISION. Normally, the variable
is then set to zero and the program continues. Other compilers
(or compiler options) may warn you with the 'UNDERFLOW' message and
the program terminates.
\end{itemize}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Loss of precision, real numbers}
\begin{small}
{\scriptsize
\begin{itemize} 
\item {\bf Roundoff errors}
A floating point number like 
\begin{equation}
   x= 1.234567891112131468 = 0.1234567891112131468\times 10^{1} 
\end{equation}
may be stored in the following way. The exponent is  small
and is stored in full precision. However,
the mantissa is not stored fully. In double precision (64 bits), digits 
beyond the 
15th are lost since the mantissa is normally stored in two words,
one which is the most significant one representing 
123456 and the least significant one containing 789111213. The digits
beyond 3 are lost. Clearly, if we are summing alternating series
with large numbers, subtractions between two large numbers may lead
to roundoff errors, since not all relevant digits are kept.
This leads eventually to the next problem, namely
\end{itemize}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{More on loss of precision}
  \begin{block}{Real Numbers}
\begin{small}
{\scriptsize
\begin{itemize} 
\item {\bf Loss of precision}
%
When one has to e.g., multiply two large numbers where one suspects
that the outcome may be beyond the bonds imposed by the 
variable declaration, one could represent the numbers by logarithms,
or rewrite the equations to be solved in terms of 
dimensionless variables. When dealing with problems in e.g.,
particle physics or nuclear physics where distance is measured
in fm ($10^{-15}$m), it can be quite convenient to redefine the
variables for distance in terms of a dimensionless variable of the order of unity. To give an example, suppose you work with single precision and wish to
perform the addition $1+10^{-8}$. In this case, the information containing
in $10^{-8}$ is simply lost in the addition. Typically, when performing the addition, the computer equates first the exponents of the two numbers to be added.
For  $10^{-8}$ this has however catastrophic consequences since in order to
obtain an exponent equal to $10^0$, bits in the mantissa are shifted to the right.
At the end, all bits in the mantissa are zeros.
\end{itemize}
}
\end{small}
  \end{block}
}




\frame
{
  \frametitle{A problematic case}
  \begin{block}{Three ways of computing $e^{-x}$}
\begin{small}
{\scriptsize
\begin{enumerate}
\item Brute force \[\exp{(-x)}=\sum_{n=0}^{\infty}(-1)^n\frac{x^n}{n!}\]
\item recursion relation for
\[
\exp{(-x)}=\sum_{n=0}^{\infty}s_n=\sum_{n=0}^{\infty}(-1)^n\frac{x^n}{n!}
\]
\[
s_n=-s_{n-1}\frac{x}{n},
\]
\item 
\[ 
\exp{(x)}=\sum_{n=0}^{\infty}s_n
\]
\[
   \exp{(-x)}=\frac{1}{\exp{(x)}}
\]
%
\end{enumerate}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Program to compute $\exp{(-x)}$}
  \begin{block}{Brute Force}
\begin{small}
{\scriptsize
\begin{verbatim}
// Program to calculate function exp(-x)
// using straightforward summation with differing  precision
using namespace std
#include <iostream>
#include <cmath>
// type float:  32 bits precision
// type double: 64 bits precision
#define   TYPE          double
#define   PHASE(a)      (1 - 2 * (abs(a) % 2))
#define   TRUNCATION    1.0E-10
// function declaration 
TYPE factorial(int);
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Program to compute $\exp{(-x)}$}
  \begin{block}{Still Brute Force}
\begin{small}
{\scriptsize
\begin{verbatim}
int main()
{
   int   n;
   TYPE  x, term, sum;
   for(x = 0.0; x < 100.0; x += 10.0)  {
     sum  = 0.0;                //initialization
     n    = 0;
     term = 1;
     while(fabs(term) > TRUNCATION)  {
         term =  PHASE(n) * (TYPE) pow((TYPE) x,(TYPE) n) 
                / factorial(n);
         sum += term;
         n++;
     }  // end of while() loop 
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Program to compute $\exp{(-x)}$}
  \begin{block}{Oh, it never ends!}
\begin{small}
{\scriptsize
\begin{verbatim}
      printf("\nx = %4.1f   exp = %12.5E  series = %12.5E  
              number of terms = %d",
              x, exp(-x), sum, n);
   } // end of for() loop 

   printf("\n");           // a final line shift on output 
   return 0;
} // End: function main() 
//     The function factorial()
//     calculates and returns n!
TYPE factorial(int n)
{
   int  loop;
   TYPE fac;
   for(loop = 1, fac = 1.0; loop <= n; loop++)  {
      fac *= loop; 
   }
   return fac;
} // End: function factorial()
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Results $\exp{(-x)}$}
  \begin{block}{What is going on?}
\begin{small}
{\scriptsize
\begin{center}
\begin{tabular}{rlrc}\\\hline
$x$&$\exp{(-x)}$&Series&Number of terms in series\\\hline
  0.0& 0.100000E+01& 0.100000E+01&    1\\
 10.0& 0.453999E-04& 0.453999E-04 &  44\\
 20.0& 0.206115E-08& 0.487460E-08&   72\\
 30.0& 0.935762E-13& -0.342134E-04 & 100\\
 40.0& 0.424835E-17& -0.221033E+01&  127\\
 50.0& 0.192875E-21& -0.833851E+05&  155\\
 60.0& 0.875651E-26& -0.850381E+09&  171\\
 70.0& 0.397545E-30&         NaN&  171\\
 80.0& 0.180485E-34&         NaN&  171\\
 90.0& 0.819401E-39 &        NaN&  171\\
100.0& 0.372008E-43&         NaN&  171\\    \hline
\end{tabular} 
\end{center} 
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Program to compute $\exp{(-x)}$}
  \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
// program to compute exp(-x) without exponentials
using namespace std
#include <iostream>
#include <cmath>
#define  TRUNCATION     1.0E-10

int main()
{
   int       loop, n;
   double    x, term, sum;
   for(loop = 0; loop <= 100; loop += 10) 
   {
     x    = (double) loop;          // initialization 
     sum  = 1.0;
     term = 1;
     n    = 1;
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Program to compute $\exp{(-x)}$}
  \begin{block}{Last statements}
\begin{small}
{\scriptsize
\begin{verbatim}
     while(fabs(term) > TRUNCATION) 
       {
	 term *= -x/((double) n);
	 sum  += term;
	 n++;
       } // end while loop 
     cout << "x = " << x   << " exp = " << exp(-x) <<"series = " 
          << sum  << " number of terms =" << n << "\n";
   } // end of for() loop 

   cout << "\n";           // a final line shift on output 

}  /*    End: function main() */
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Results $\exp{(-x)}$}
  \begin{block}{More Problems}
\begin{small}
{\scriptsize
\begin{center}
\begin{tabular}{rllc}\\\hline
$x$&$\exp{(-x)}$&Series&Number of terms in series\\\hline
    0.000000&   0.10000000E+01&  0.10000000E+01&       1\\
   10.000000&   0.45399900E-04&  0.45399900E-04&      44\\
   20.000000&   0.20611536E-08&  0.56385075E-08&       72\\
   30.000000&   0.93576230E-13& -0.30668111E-04&      100\\
   40.000000&   0.42483543E-17& -0.31657319E+01&     127\\
   50.000000&   0.19287498E-21&  0.11072933E+05&     155\\
   60.000000&   0.87565108E-26& -0.33516811E+09&     182\\
   70.000000&   0.39754497E-30& -0.32979605E+14&     209\\
   80.000000&   0.18048514E-34&  0.91805682E+17&     237\\
   90.000000&   0.81940126E-39& -0.50516254E+22&     264\\
  100.000000&   0.37200760E-43& -0.29137556E+26&     291 \\\hline   
\end{tabular}  
\end{center}
}
\end{small}
  \end{block}
}








\frame
{
  \frametitle{Most used formula for derivatives}
  \begin{block}{3 point formulae}
\begin{small}
{\scriptsize
First derivative  ($f_0 = f(x_0)$, $f_{-h}=f(x_0-h)$ and $f_{+h}=f(x_0+h)$ 
\[
   \frac{f_h-f_{-h}}{2h}=f'_0+\sum_{j=1}^{\infty}\frac{f_0^{(2j+1)}}{(2j+1)!}h^{2j}.
\]
Second derivative
\[
 \frac{ f_h -2f_0 +f_{-h}}{h^2}=f_0''+2\sum_{j=1}^{\infty}\frac{f_0^{(2j+2)}}{(2j+2)!}h^{2j}. 
\]
}
\end{small}
  \end{block}
}



\frame
{
  \frametitle{Error Analysis}
  \begin{block}{}
\begin{small}
{\scriptsize
\[
   \epsilon=log_{10}\left(\left|\frac{f''_{\mathrm{computed}}-f''_{\mathrm{exact}}}
                 {f''_{\mathrm{exact}}}\right|\right),
\]

\[
   \epsilon_{\mathrm{tot}}=\epsilon_{\mathrm{approx}}+\epsilon_{\mathrm{ro}}.
\]

For the computed second derivative  we have 
\[
 f_0''=\frac{ f_h -2f_0 +f_{-h}}{h^2}-2\sum_{j=1}^{\infty}\frac{f_0^{(2j+2)}}{(2j+2)!}h^{2j},
\]
and the truncation or approximation error goes like
\[
  \epsilon_{\mathrm{approx}}\approx \frac{f_0^{(4)}}{12}h^{2}.
\]
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Error Analysis}
  \begin{block}{}
\begin{small}
{\scriptsize
If we were not to worry about loss of precision, we could in principle
make $h$ as small as possible. 
However, due to the computed expression in the above program example
\[
 f_0''=\frac{ f_h -2f_0 +f_{-h}}{h^2}=\frac{ (f_h -f_0) +(f_{-h}-f_0)}{h^2},
\]
we reach fairly quickly a limit for where loss of precision due to the subtraction
of two nearly equal numbers becomes crucial. 


If $(f_{\pm h} -f_0)$ are very close, we have
$(f_{\pm h} -f_0)\approx \epsilon_M$, where $|\epsilon_M|\le 10^{-7}$ for single and
$|\epsilon_M|\le 10^{-15}$ for double precision, respectively.

We have then
\[
 \left|f_0''\right|=
 \left|\frac{ (f_h -f_0) +(f_{-h}-f_0)}{h^2}\right|\le \frac{ 2 \epsilon_M}{h^2}.
\]
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Error Analysis}
  \begin{block}{}
\begin{small}
{\scriptsize
Our total error becomes 
\[
   \left|\epsilon_{\mathrm{tot}}\right|\le  \frac{2 \epsilon_M}{h^2} + 
                          \frac{f_0^{(4)}}{12}h^{2}. 
    \label{eq:experror}
\]
It is then natural to ask which value of $h$ yields the smallest
total error. Taking the derivative of $\left|\epsilon_{\mathrm{tot}}\right|$
with respect to $h$ results in
\[
   h= \left(\frac{ 24\epsilon_M}{f_0^{(4)}}\right)^{1/4}.
\]
With double precision and $x=10$ we obtain 
\[
   h\approx 10^{-4}.
\]
Beyond this value, it is essentially the loss of numerical precision
which takes over. 

}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Error Analysis}
  \begin{block}{}
\begin{small}
{\scriptsize
Due to the subtractive cancellation in the expression
for $f''$ there is a pronounced detoriation in accuracy as $h$ is made smaller
and smaller. 

It is instructive in this analysis to rewrite the numerator of
the computed derivative as
\[
   (f_h -f_0) +(f_{-h}-f_0)=(e^{x+h}-e^{x}) + (e^{x-h}-e^{x}),
\]
as
\[
   (f_h -f_0) +(f_{-h}-f_0)=e^x(e^{h}+e^{-h}-2),
\]
since it is the difference $(e^{h}+e^{-h}-2)$ which causes
the loss of precision.
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Error Analysis}
  \begin{block}{}
\begin{small}
{\scriptsize
\begin{tabular}{rrrrrr}\hline
$x$&$h=0.01$&$h=0.001$&$h=0.0001$&$h=0.0000001$ &Exact\\\hline
  0.0&   1.000008 &   1.000000 &   1.000000 &   1.010303 &   1.000000  \\ 
 1.0&   2.718304  &  2.718282  &  2.718282  &  2.753353  &  2.718282  \\
 2.0&  7.389118  &  7.389057  &  7.389056  &  7.283063  &  7.389056  \\
 3.0&  20.085704 &  20.085539 &  20.085537 &  20.250467 &  20.085537   \\
 4.0&  54.598605 &  54.598155  & 54.598151 &  54.711789  & 54.598150  \\
 5.0& 148.414396 & 148.413172 & 148.413161 & 150.635056 & 148.413159 \\\hline
\end{tabular} 
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Error Analysis}
  \begin{block}{}
\begin{small}
{\scriptsize
The results, for $x=10$ are shown in the Table
\begin{tabular}{lll}\hline
$h$&$e^{h}+e^{-h}$ & $e^{h}+e^{-h}-2$\\\hline
 $10^{-1}$ & 2.0100083361116070 &  1.0008336111607230$\times 10^{-2}$ \\
 $10^{-2}$ & 2.0001000008333358 &  1.0000083333605581$\times 10^{-4}$ \\
 $10^{-3}$ & 2.0000010000000836 &  1.0000000834065048$\times 10^{-6}$ \\
 $10^{-5}$ & 2.0000000099999999 &  1.0000000050247593$\times 10^{-8}$ \\
 $10^{-5}$ & 2.0000000001000000 &  9.9999897251734637$\times 10^{-11}$  \\
 $10^{-6}$ & 2.0000000000010001 &  9.9997787827987850$\times 10^{-13}$  \\
 $10^{-7}$ & 2.0000000000000098 &  9.9920072216264089$\times 10^{-15}$  \\
 $10^{-8}$ & 2.0000000000000000 &  0.0000000000000000$\times 10^{0}$ \\
 $10^{-9}$ & 2.0000000000000000 &  1.1102230246251565$\times 10^{-16}$  \\
 $10^{-10}$  & 2.0000000000000000 &  0.0000000000000000$\times 10^{0}$ \\
&&\\\hline
\end{tabular} 
}
\end{small}
  \end{block}
}



\section[Week 35]{Week 35}
\frame
{
  \frametitle{Week 35}
  \begin{block}{}
\begin{itemize}
\item Monday: Repetition from last week
\item Numerical differentiation
\item C/C++ programming details, pointers, read/write to/from file
\item Tuesday: Intro to linear Algebra  and
presentation of project 1. 
\item Matrices in C++ and Fortran2008 
\item Gaussian elimination and discussion of project 1.
\item Computer-Lab: start project 1.
\end{itemize}
Reading asssignments and preparation for project 1: sections 2.5 and 3.1 for general C++ and Fortran features.
Sections 6.1-6.4 (till page 182) are relevant for project 1.
  \end{block}
} 

\frame[containsverbatim]
{
  \frametitle{Technical Matter in C/C++: Pointers}
  \begin{block}{}
\begin{small}
{\scriptsize
{\bf A pointer specifies where a value resides in the computer's memory (like a house number specifies where a particular family resides on a street).} \newline
{\bf A pointer points to an address not to a data container of any kind!}\newline
Simple example declarations:
\begin{verbatim}
  using namespace std; // note use of namespace
  int main()
 {
// what are the differences?
   int var;
   cin >> var;
   int *p, q;
   int *s, *t;
   int * a new[var];   // dynamic memory allocation
   delete [] a;
 }
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{Technical Matter in C/C++: Pointer example I}
  \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
1  using namespace std; // note use of namespace
2  int main()
3 {
4   int var;
5   int *p;
6   p = &var;
7   var  = 421;
8   printf("Address of integer variable var : %p\n",&var);
9   printf("Its value: %d\n", var);
10  printf("Value of integer pointer p : %p\n",p);
11  printf("The value p points at :  %d\n",*p);
12  printf("Address of the pointer p : %p\n",&p);
13  return 0;
14 }
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Pointer example I}
  \begin{block}{Discussion}
{\small
\begin{center}
%
\begin{tabular}{|ll|}\hline
\hfill Line \hfill
&\hspace*{\fill} Comments \hspace*{\fill}\\ \hline
&  \\[-2mm]
4 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
Defines an integer variable var.
\end{minipage}\\
5 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
Define an integer pointer  -- reserves space in memory.
\end{minipage}\\
6 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
The content of the address of  pointer is the  address of var.
\end{minipage}\\
7 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
The value of  var is 421.
\end{minipage}\\
8 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
Writes the address of  var in hexadecimal notation for pointers \%p.
\end{minipage}\\
9 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
Writes the value of  var in decimal notation\%d.
\end{minipage}\\[1ex]
\hline
\end{tabular}
\end{center}
} % end small
%
  \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Pointer example II}
  \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
         ....
5  int matr[2];
6  int *p;
7  p = &matr[0];
8  matr[0] = 321;
9  matr[1] = 322;
   printf("\nAddress of matrix element matr[1]: %p",&matr[0]);
   printf("\nValue of the  matrix element  matr[1]; %d",matr[0]);
   printf("\nAddress of matrix element matr[2]: %p",&matr[1]);
   printf("\nValue of the matrix element  matr[2]: %d\n", matr[1]);
   printf("\nValue of the pointer p: %p",p);
   printf("\nThe value p points to: %d",*p);
   printf("\nThe value that (p+1) points to  %d\n",*(p+1));
   printf("\nAddress of pointer p : %p\n",&p);
         ...
\end{verbatim}
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Pointer example II}
  \begin{block}{Discussion}
{\small
\begin{center}
%
\begin{tabular}{|ll|}\hline
\hfill Line \hfill
&\hspace*{\fill}  \hspace*{\fill}\\ \hline
&  \\[-2mm]
5 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
Declaration of an integer array matr with two elements
\end{minipage}\\
6 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
Declaration of an integer pointer
\end{minipage}\\
7 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
The pointer is initialized to point at the first element of the 
array matr.
\end{minipage}\\
8--9 &$\bullet$
\begin{minipage}[t]{0.65\textwidth}
Values are assigned to the array matr.
\end{minipage}\\[1ex]  
\hline  
\end{tabular}  
\end{center}  
} % end small  
  \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Pointer example II}
  \begin{block}{Discussion}
{\small
The ouput of this example, compiled again with c++, is
\begin{verbatim}
Address of the matrix element matr[1]: 0xbfffef70
Value of the  matrix element  matr[1]; 321
Address of the matrix element matr[2]: 0xbfffef74
Value of the matrix element  matr[2]: 322
Value of the pointer: 0xbfffef70
The value pointer points at: 321
The value that (pointer+1) points at:  322
Address of the pointer variable : 0xbfffef6c
\end{verbatim}
}
  \end{block}
} 



\frame[containsverbatim]
{
  \frametitle{File handling, C-way}
%  \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
using namespace std;
#include <iostream>
int main(int argc, char *argv[])
{
  FILE *in_file, *out_file;
  if( argc < 3)  {
    printf("The programs has the following structure :\n");
    printf("write in the name of the input and output files \n");
    exit(0);
  }
  in_file = fopen( argv[1], "r");// returns pointer to the  input file
  if( in_file == NULL )  { // NULL means that the file is missing
    printf("Can't find the input file %s\n", argv[1]);
    exit(0);
}
\end{verbatim}
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{File handling, C way contn}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
 out_file = fopen( argv[2], "w"); // returns a pointer to the output file
 if( out_file == NULL )  {       // can't find the file
    printf("Can't find the output file%s\n", argv[2]);
    exit(0);
  }
  fclose(in_file);
  fclose(out_file);
  return 0;
}

\end{verbatim}
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{File handling, C++-way}
%  \begin{block}{}
\begin{small}
{\scriptsize
You must first declare input and output files
\begin{verbatim}
#include <fstream>


// input and output file as global variable
ofstream ofile;  
ifstream ifile;  
\end{verbatim}
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{File handling, C++-way}
%  \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
int main(int argc, char* argv[])
{
  char *outfilename;
  //Read in output file, abort if there are too 
  //few command-line arguments
  if( argc <= 1 ){
    cout << "Bad Usage: " << argv[0] << 
      " read also output file on same line" << endl;
    exit(1);
  }
  else{
    outfilename=argv[1];
  }
  ofile.open(outfilename); 
  .....
  ofile.close();  // close output file
\end{verbatim}
}
\end{small}
%  \end{block}
}


\frame[containsverbatim]
{
  \frametitle{File handling, C++-way}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
void output(double r_min , double r_max, int max_step, 
            double *d)
{
int i;
ofile << "RESULTS:" << endl;
ofile << setiosflags(ios::showpoint | ios::uppercase);
ofile <<"R_min = " << setw(15) << setprecision(8) <<r_min <<endl;  
ofile <<"R_max = " << setw(15) << setprecision(8) <<r_max <<endl;  
ofile <<"Number of steps = " << setw(15) << max_step << endl;  
ofile << "Five lowest eigenvalues:" << endl;
for(i = 0; i < 5; i++) {
    ofile << setw(15) << setprecision(8) << d[i] << endl;
}
}  // end of function output
\end{verbatim}
}
\end{small}
 % \end{block}
}


\frame[containsverbatim]
{
  \frametitle{File handling, C++-way}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
int main(int argc, char* argv[])
{
  char *infilename;
  // Read in input file, abort if there are too 
  // few command-line arguments
  if( argc <= 1 ){
    cout << "Bad Usage: " << argv[0] << 
      " read also input file on same line" << endl;
    exit(1);
  }
  else{
    infilename=argv[1];
  }
  ifile.open(infilename); 
  ....
  ifile.close();  // close input file
\end{verbatim}
}
\end{small}
 % \end{block}
}




\frame[containsverbatim]
{
  \frametitle{File handling, C++-way}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{verbatim}
const char* filename1 = "myfile";
ifstream ifile(filename1);
string filename2 = filename1 + ".out"
ofstream ofile(filename2);  // new output file
ofstream ofile(filename2, ios_base::app);  // append

      Read something from the file:

double a; int b; char c[200];
ifile >> a >> b >> c;  // skips white space in between

      Can test on success of reading:

if (!(ifile >> a >> b >> c)) ok = 0;

\end{verbatim}
}
\end{small}
 % \end{block}
}




\frame
{
  \frametitle{Call by value and reference}
 % \begin{block}{}
\begin{small}
{\scriptsize
{\small
\expandafter\ifx\csname indentation\endcsname\relax
\newlength{\indentation}\fi
\setlength{\indentation}{0.5em}
\begin{flushleft}
{\bf int\/} main({\bf int\/} argc, {\bf char\/} $\ast$argv[])\mbox{}\\
\{\mbox{}\\
\hspace*{3\indentation}{\bf int\/}     a:                                                          \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}1\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}{\bf int\/}     $\ast$b;                                                         \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}2\hspace*{1\indentation}}\mbox{}\\
\mbox{}\\
\hspace*{3\indentation}a = 10;                                                           \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}3\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}b = {\bf new int}[10];                             \hfill{\em /$/$\hspace*{1\indentation}line\hspace*{1\indentation}4\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}{\bf for\/}(i = 0; i $<$ 10; i++) \{\mbox{}\\
\hspace*{5\indentation}b[i] = i;                                                      \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}5\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}\}   \mbox{}\\
\hspace*{3\indentation}func( a,b);                                                      \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}6\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}{\bf delete []} b ;                                                      \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}7\hspace*{1\indentation}}\mbox{}\\

\hspace*{2\indentation}{\bf return\/} 0;\mbox{}\\
\}\hspace*{2\indentation}{\em $//$\hspace*{1\indentation}End:\hspace*{1\indentation}function\hspace*{1\indentation}main()\hspace*{1\indentation}}\mbox{}\\
\end{flushleft}}
}
\end{small}
 % \end{block}
}

\frame
{
  \frametitle{Call by value and reference}
 % \begin{block}{}
\begin{small}
{\scriptsize
{\small
\expandafter\ifx\csname indentation\endcsname\relax
\newlength{\indentation}\fi
\setlength{\indentation}{0.5em}
\begin{flushleft}
{\bf void\/} func( {\bf int\/} x, {\bf int\/} $\ast$y)                                                       \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}7\hspace*{1\indentation}}\mbox{}\\
\{\mbox{}\\
\hspace*{3\indentation}x += 7;                                                                   \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}8\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}$\ast$y += 10;                                                             \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}9\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}y[6] += 10;                                                          \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}10\hspace*{1\indentation}}\mbox{}\\
\hspace*{3\indentation}{\bf return\/};                                                \hfill{\em $//$\hspace*{1\indentation}line\hspace*{1\indentation}11\hspace*{1\indentation}}\mbox{}\\
\}\hspace*{2\indentation}{\em $//$\hspace*{1\indentation}End:\hspace*{1\indentation}function\hspace*{1\indentation}func()\hspace*{1\indentation}}\mbox{}\\
\end{flushleft}}
}
\end{small}
 % \end{block}
}

\frame
{
  \frametitle{Call by value and reference}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{itemize}
%
\item Lines 1,2: Declaration of two variables a and b. The
compiler reserves two locations in memory. The size of the location
depends on the type of variable. Two properties are important for
these locations -- the address in memory and the content in the
location.

The value of a: a. The address of a: \&a\\
The value of b: *b. The address of b: \&b.
%
\item Line 3: The value of a is now 10.
%
\item Line 4: Memory to store 10 integers is reserved. The
address to the first location is stored in b. Address to element
number 6 is given by the expression (b + 6). 
%
\item Line 5: All 10 elements of b are given values: b[0] = 0, b[1] =
1, ....., b[9] = 9;
\item line 7: here we deallocate the variable b.
\end{itemize}
}
\end{small}
 % \end{block}
}


\frame
{
  \frametitle{Call by value and reference}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{itemize}
\item Line 6: The main() function calls the function func() and the
program counter transfers to the first statement in func().
With respect to data the following happens. The content of a 
(= 10) and the content of b (a memory address) are copied to a stack
(new memory location) associated with the function func()
%
\item Line 7: The variable x and y are local variables in
func(). They have the values -- x = 10, y = address of the first
element in b in the main().
%
\item Line 8: The local variable x stored in the stack memory is
changed to 17. Nothing happens with the value a in main().
% 

\end{itemize}
}
\end{small}
 % \end{block}
}


\frame
{
  \frametitle{Call by value and reference}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{itemize}
\item Line 9: The value of y is an address and the symbol *y means
the position in memory which has this address. The value in this
location is now increased by 10. This means that the value of b[0] in
the main program is equal to 10. Thus func() has modified a value in main().
%
\item Line 10: This statement has the same effect as line 9 except
that it modifies the element b[6] in main() by adding a value of 10 to
what was there originally, namely 5.
% 
\item Line 11: The program counter returns to main(), the next
expression after {\sl func(a,b);}. All data on the stack associated
with func() are destroyed.
%
\end{itemize}
}
\end{small}
%  \end{block}
}


\frame
{
  \frametitle{Call by value and reference}
 % \begin{block}{}
\begin{small}
{\scriptsize
\begin{itemize}

\item The value of a is transferred to func() and stored
in a new memory location called x. Any modification of x in func()
does not affect in any way the value of a in main(). This is called {\bf
transfer of data by value}. On the other hand the next argument in
func() is an address which is transferred to func(). This address can
be used to modify the corresponding value in main(). In the C language
it is expressed as a modification of the value 
which y points to, namely the first element of b.
This is called {\bf transfer of data by reference} and is a method to
transfer data back to the calling function, in this case  main().

\end{itemize}
}
\end{small}
 % \end{block}
}

\frame[containsverbatim]
{
  \frametitle{Call by value and reference}
 % \begin{block}{}
\begin{small}
{\scriptsize
C++ allows however the programmer to use solely call by reference
(note that call by reference is implemented as pointers).
To see the difference between C and C++, consider the following simple
examples. In C we would write
\begin{verbatim}
   int n; n =8;
   func(&n); /* &n is a pointer to n */
   ....
   void func(int *i)
   {
     *i = 10; /* n is changed to 10 */
     ....
   }
\end{verbatim}
}
\end{small}
 % \end{block}
}

\frame[containsverbatim]
{
  \frametitle{}
 % \begin{block}{}
\begin{small}
{\scriptsize
whereas in C++ we would write
\begin{verbatim}
   int n; n =8;
   func(n); // just transfer n itself
   ....
   void func(int& i)
   {
     i = 10; // n is changed to 10
     ....
   }
\end{verbatim}
The reason why we emphasize the difference between call by value and call 
by reference is that it allows the programmer to avoid pitfalls
like unwanted changes of variables. However, many people feel that this
reduces the readability of the code. 
}
\end{small}
 % \end{block}
}



\frame[containsverbatim]
{
  \frametitle{Call by value and reference, F90/95}
 % \begin{block}{}
\begin{small}
{\scriptsize
In Fortran we can use INTENT(IN), INTENT(OUT), INTENT(INOUT) to let the
program know which values should or should not be changed.
\begin{verbatim}
SUBROUTINE coulomb_integral(np,lp,n,l,coulomb)
  USE effective_interaction_declar
  USE energy_variables
  USE wave_functions
  IMPLICIT NONE
  INTEGER, INTENT(IN)  :: n, l, np, lp
  INTEGER :: i
  REAL(KIND=8), INTENT(INOUT) :: coulomb
  REAL(KIND=8) :: z_rel, oscl_r, sum_coulomb
  ...
\end{verbatim}
This hinders unwanted changes and increases readability.
}
\end{small}
 % \end{block}
}



%\section{Matrices and linear algebra}




\frame
{
  \frametitle{Important Matrix and vector handling packages}
\begin{small}
{\scriptsize
The Numerical Recipes codes have been rewritten in Fortran 90/95 and C/C++ by us.
The original source codes are taken from the widely used software
package LAPACK, which follows two other popular packages developed in the 1970s, 
namely EISPACK
and LINPACK.
\begin{itemize}
\item LINPACK: package for linear equations 
and least square problems.
\item LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems.
From LAPACK's website \url{http://www.netlib.org}  it is 
possible to download for free all source codes from 
this library. Both C/C++ and Fortran versions are available.
\item BLAS (I, II and III): (Basic Linear Algebra Subprograms) 
are routines that provide standard building blocks for performing basic vector and matrix operations.  
Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations.
Highly parallelized and efficient codes, all available for download from 
\url{http://www.netlib.org}. 
\end{itemize}
}
\end{small}
} 







\frame
{
  \frametitle{Basic Matrix Features}
  \begin{block}{Matrix Properties Reminder}
\[
 {\bf A} =
      \left( \begin{array}{cccc} a_{11} & a_{12} & a_{13} & a_{14} \\
                                 a_{21} & a_{22} & a_{23} & a_{24} \\
                                   a_{31} & a_{32} & a_{33} & a_{34} \\
                                  a_{41} & a_{42} & a_{43} & a_{44} 
             \end{array} \right)
\hspace*{2cm} {\bf I} =
      \left( \begin{array}{cccc} 1 & 0 & 0 & 0 \\
                                 0 & 1 & 0 & 0 \\
                                 0 & 0 & 1 & 0 \\
                                 0 & 0 & 0 & 1 
             \end{array} \right)
\]
%
The inverse of a matrix is defined by 
% 
\[
{\bf A}^{-1} \cdot {\bf A} = I
\]
  \end{block}
} 


\frame
{
  \frametitle{Basic Matrix Features}
  \begin{block}{Matrix Properties Reminder}
\begin{center}
\begin{tabular}{|l|l|l|}\hline
Relations               & Name       & matrix elements\\ \hline
$ {\bf A} = {\bf A}^{T}$ & symmetric & $a_{ij} = a_{ji} $ \\ 
$ {\bf A} = \left ({\bf A}^{T} \right )^{-1} $ & real orthogonal&
                   $\sum_k a_{ik} a_{jk} = \sum_k a_{ki} a_{kj} = \delta_{ij}$ \\
$ {\bf A} = {\bf A}^{*}  $ & real matrix& $a_{ij} = a_{ij}^{*}$\\
$ {\bf A} = {\bf A}^{\dagger}  $ &  hermitian& $a_{ij} = a_{ji}^{*}$\\
$ {\bf A} = \left ({\bf A}^{\dagger} \right )^{-1} $ & unitary& 
             $\sum_k a_{ik} a_{jk}^{*} = \sum_k a_{ki}^{*} a_{kj}
                                                  = \delta_{ij}$ \\ \hline
\end{tabular}
\end{center} 
  \end{block}
} 

\frame
{
  \frametitle{Some famous Matrices}
\begin{enumerate}
\item Diagonal if $a_{ij}=0$ for $i\ne j$
\item Upper triangular if $a_{ij}=0$ for $i >j$
\item Lower triangular if $a_{ij}=0$ for $i <j$
\item Upper Hessenberg if $a_{ij}=0$ for $i >j+1$
\item Lower Hessenberg if $a_{ij}=0$ for $i <j+1$
\item Tridiagonal if $a_{ij}=0$ for $|i -j|>1$
\item Lower banded with bandwidth $p$ $a_{ij}=0$ for $i > j+p$
\item Upper banded with bandwidth $p$ $a_{ij}=0$ for $i < j+p$
\item Banded, block upper triangular, block lower triangular....
\end{enumerate}
} 

\frame
{
  \frametitle{Basic Matrix Features}
  \begin{block}{Some Equivalent Statements}
For an $N\times N$ matrix  ${\bf A}$ the following properties are all equivalent
\begin{enumerate}
\item If the inverse of   ${\bf A}$ exists, ${\bf A}$ is nonsingular.
\item The equation ${\bf Ax}=0$ implies ${\bf x}=0$.
\item The rows of ${\bf A}$ form a basis of $R^N$.
\item  The columns of ${\bf A}$ form a basis of $R^N$.
\item ${\bf A}$ is a product of elementary matrices.
\item $0$ is not eigenvalue of ${\bf A}$.
\end{enumerate}
  \end{block}
} 


\frame
{
  \frametitle{Important Mathematical Operations}
\begin{small}
{\scriptsize
The basic matrix operations that we will deal with are addition and subtraction
\begin{equation}
{\bf A}= {\bf B}\pm{\bf C}  \Longrightarrow a_{ij} = b_{ij}\pm c_{ij},
\label{eq:mtxadd}
\end{equation}
scalar-matrix multiplication
\begin{equation}
{\bf A}= \gamma{\bf B}  \Longrightarrow a_{ij} = \gamma b_{ij},
\end{equation}
vector-matrix multiplication 
\begin{equation}
{\bf y}={\bf Ax}   \Longrightarrow y_{i} = \sum_{j=1}^{n} a_{ij}x_j,
\label{eq:vecmtx}
\end{equation}
matrix-matrix multiplication 
\begin{equation}
{\bf A}={\bf BC}   \Longrightarrow a_{ij} = \sum_{k=1}^{n} b_{ik}c_{kj},
\label{eq:mtxmtx}
\end{equation}
and transposition
\begin{equation}
{\bf A}={\bf B}^T   \Longrightarrow a_{ij} = b_{ji}
\end{equation}
}
\end{small}
} 


\frame
{
  \frametitle{Important Mathematical Operations}
\begin{small}
{\scriptsize
Similarly, important vector operations that we will deal with are addition and subtraction
\begin{equation}
{\bf x}= {\bf y}\pm{\bf z}  \Longrightarrow x_{i} = y_{i}\pm z_{i},
\end{equation}
scalar-vector multiplication
\begin{equation}
{\bf x}= \gamma{\bf y}  \Longrightarrow x_{i} = \gamma y_{i},
\end{equation}
vector-vector multiplication (called Hadamard multiplication)
\begin{equation}
{\bf x}={\bf yz}   \Longrightarrow x_{i} = y_{i}z_i,
\end{equation}
the inner or so-called dot product  resulting in a constant
\begin{equation}
x={\bf y}^T{\bf z}   \Longrightarrow x = \sum_{j=1}^{n} y_{j}z_{j},
\label{eq:innerprod}
\end{equation}
and the outer product, which yields a matrix,
\begin{equation}
{\bf A}=  {\bf yz}^T \Longrightarrow  a_{ij} = y_{i}z_{j},
\label{eq:outerprod}
\end{equation}
}
\end{small}
} 





\frame[containsverbatim]
{
  \frametitle{Matrix Handling in C/C++, Static and Dynamical allocation}
  \begin{block}{Static}
We have  an $N\times N$ matrix A  with $N=100$
In C/C++ this would be  defined as 
\begin{verbatim}
   int N = 100;
   double A[100][100];
   //   initialize all elements to zero
   for(i=0 ; i < N ; i++) {  
      for(j=0 ; j < N ; j++) {
         A[i][j] = 0.0;
      }
   }  
\end{verbatim}
Note the way the matrix is organized, row-major order.  
  \end{block}
} 





\frame[containsverbatim]
{
  \frametitle{Matrix Handling in C/C++}
  \begin{block}{Row Major Order, Addition}
We have  $N\times N$ matrices A, B and C and we wish to 
evaluate $A=B+C$. 
\[
{\bf A}= {\bf B}\pm{\bf C}  \Longrightarrow a_{ij} = b_{ij}\pm c_{ij},
\]
In C/C++ this would be coded like
\begin{verbatim}
   for(i=0 ; i < N ; i++) {  
      for(j=0 ; j < N ; j++) {
         a[i][j] = b[i][j]+c[i][j]
      }
   }  
\end{verbatim}
  \end{block}
} 

\frame[containsverbatim]
{
  \frametitle{Matrix Handling in C/C++}
  \begin{block}{Row Major Order, Multiplication}
We have  $N\times N$ matrices A, B and C and we wish to 
evaluate $A=BC$. 
\[
{\bf A}={\bf BC}   \Longrightarrow a_{ij} = \sum_{k=1}^{n} b_{ik}c_{kj},
\]
In C/C++ this would be coded like
\begin{verbatim}
   for(i=0 ; i < N ; i++) {  
      for(j=0 ; j < N ; j++) {
         for(k=0 ; k < N ; k++) {
            a[i][j]+=b[i][k]*c[k][j];
         }
      }
   }  
\end{verbatim}
\end{block}
} 

\frame[containsverbatim]
{
  \frametitle{Matrix Handling in Fortran 90/95}
\begin{small}
{\scriptsize
  \begin{block}{Column Major Order}
\begin{verbatim}
   ALLOCATE (a(N,N), b(N,N), c(N,N))
   DO j=1,  N
      DO i=1, N
         a(i,j)=b(i,j)+c(i,j)
      ENDDO
   ENDDO
   ...
   DEALLOCATE(a,b,c)
\end{verbatim}
Fortran 90 writes the above statements in a much simpler way
\begin{verbatim}
   a=b+c
\end{verbatim}
Multiplication
\begin{verbatim}
   a=MATMUL(b,c)
\end{verbatim}
Fortran contains also the intrinsic functions TRANSPOSE and CONJUGATE.
  \end{block}
}
\end{small}
} 


\frame
{
  \frametitle{Dynamic memory allocation in C/C++}
At least three possibilities in this course
\begin{itemize}
\item Do it yourself
\item Use the functions provided in the library package lib.cpp
\item Use Armadillo \url{http://arma.sourceforgenet} (a C++ linear algebra library, discussion next two weeks, both here and at lab). 
\end{itemize}
} 

\frame[containsverbatim]
{
  \frametitle{Matrix Handling in C/C++, Dynamic Allocation}
  \begin{block}{Do it yourself}
\begin{verbatim}
int N;
double **  A;
A = new double*[N]
for ( i = 0; i < N; i++)
    A[i] = new double[N];
\end{verbatim}
Always free space when you don't need an array anymore.
\begin{verbatim}
for ( i = 0; i < N; i++)
    delete[] A[i];
delete[] A;
\end{verbatim}
  \end{block}
} 

\frame[containsverbatim]
{
  \frametitle{Armadillo, recommended!!}
\begin{small}
{\scriptsize
\begin{itemize}
\item  Armadillo is a C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. The syntax is deliberately similar to Matlab.
 
\item Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Various matrix decompositions are provided through optional integration with LAPACK, or one of its high performance drop-in replacements (such as the multi-threaded MKL or ACML libraries).
 
\item A delayed evaluation approach is employed (at compile-time) to combine several operations into one and reduce (or eliminate) the need for temporaries. This is accomplished through recursive templates and template meta-programming.
 
\item Useful for conversion of research code into production environments, or if C++ has been decided as the language of choice, due to speed and/or integration capabilities.
 
\item The library is open-source software, and is distributed under a license that is useful in both open-source and commercial/proprietary contexts. 
\end{itemize}
}
\end{small}
} 

\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
#include <iostream>
#include <armadillo>

using namespace std;
using namespace arma;

int main(int argc, char** argv)
  {
  mat A = randu<mat>(5,5);
  mat B = randu<mat>(5,5);
  
  cout << A*B << endl;
  
  return 0;
  }
\end{verbatim}
}
\end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Armadillo, how to compile and install}
\begin{small}
{\scriptsize
For people using Ubuntu, Debian, Linux Mint, simply go to the synaptic package manager and install
armadillo from there. 
You may have to install Lapack as well.
For Mac and Windows users, follow the instructions from the webpage 
\url{http://arma.sourceforge.net}.\newline
To compile, use for example
\begin{verbatim}
c++ -O2 -o program.x program.cpp  -larmadillo -llapack -lblas
\end{verbatim}
where the $-l$ option indicates the library you wish to link to. 
}
\end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
#include <iostream>
#include "armadillo"
using namespace arma;
using namespace std;

int main(int argc, char** argv)
  {
  // directly specify the matrix size (elements are uninitialised)
  mat A(2,3);
  // .n_rows = number of rows    (read only)
  // .n_cols = number of columns (read only)
  cout << "A.n_rows = " << A.n_rows << endl;
  cout << "A.n_cols = " << A.n_cols << endl;
  // directly access an element (indexing starts at 0)
  A(1,2) = 456.0;
  A.print("A:");
  // scalars are treated as a 1x1 matrix,
  // hence the code below will set A to have a size of 1x1
  A = 5.0;
  A.print("A:");
  // if you want a matrix with all elements set to a particular value
  // the .fill() member function can be used
  A.set_size(3,3);
  A.fill(5.0);  A.print("A:");
\end{verbatim}
}
\end{small}
} 






\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
  mat B;
  
  // endr indicates "end of row"
  B << 0.555950 << 0.274690 << 0.540605 << 0.798938 << endr
    << 0.108929 << 0.830123 << 0.891726 << 0.895283 << endr
    << 0.948014 << 0.973234 << 0.216504 << 0.883152 << endr
    << 0.023787 << 0.675382 << 0.231751 << 0.450332 << endr;
  
  // print to the cout stream
  // with an optional string before the contents of the matrix
  B.print("B:");
  
  // the << operator can also be used to print the matrix
  // to an arbitrary stream (cout in this case) 
  cout << "B:" << endl << B << endl;
  // save to disk
  B.save("B.txt", raw_ascii);
  // load from disk
  mat C;
  C.load("B.txt");
  C += 2.0 * B;
  C.print("C:");
\end{verbatim}  
}
\end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
  // submatrix types:
  //
  // .submat(first_row, first_column, last_row, last_column)
  // .row(row_number)
  // .col(column_number)
  // .cols(first_column, last_column)
  // .rows(first_row, last_row)
  
  cout << "C.submat(0,0,3,1) =" << endl;
  cout << C.submat(0,0,3,1) << endl;
  
  // generate the identity matrix
  mat D = eye<mat>(4,4);
  
  D.submat(0,0,3,1) = C.cols(1,2);
  D.print("D:");
  
  // transpose
  cout << "trans(B) =" << endl;
  cout << trans(B) << endl;
  
  // maximum from each column (traverse along rows)
  cout << "max(B) =" << endl;
  cout << max(B) << endl;
  
\end{verbatim}
}
\end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
  // maximum from each row (traverse along columns)
  cout << "max(B,1) =" << endl;
  cout << max(B,1) << endl;
  // maximum value in B
  cout << "max(max(B)) = " << max(max(B)) << endl;
  // sum of each column (traverse along rows)
  cout << "sum(B) =" << endl;
  cout << sum(B) << endl;
  // sum of each row (traverse along columns)
  cout << "sum(B,1) =" << endl;
  cout << sum(B,1) << endl;
  // sum of all elements
  cout << "sum(sum(B)) = " << sum(sum(B)) << endl;
  cout << "accu(B)     = " << accu(B) << endl;
  // trace = sum along diagonal
  cout << "trace(B)    = " << trace(B) << endl;
  // random matrix -- values are uniformly distributed in the [0,1] interval
  mat E = randu<mat>(4,4);
  E.print("E:");

\end{verbatim}
}
\end{small}
} 




\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
  // row vectors are treated like a matrix with one row
  rowvec r;
  r << 0.59499 << 0.88807 << 0.88532 << 0.19968;
  r.print("r:");
  
  // column vectors are treated like a matrix with one column
  colvec q;
  q << 0.81114 << 0.06256 << 0.95989 << 0.73628;
  q.print("q:");
  
  // dot or inner product
  cout << "as_scalar(r*q) = " << as_scalar(r*q) << endl;
  
    // outer product
  cout << "q*r =" << endl;
  cout << q*r << endl;
  
    
  // sum of three matrices (no temporary matrices are created)
  mat F = B + C + D;
  F.print("F:");
  
    return 0;
  }


\end{verbatim}
}
\end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
#include <iostream>
#include "armadillo"
using namespace arma;
using namespace std;

int main(int argc, char** argv)
  {
  cout << "Armadillo version: " << arma_version::as_string() << endl;
  
  mat A;
  
  A << 0.165300 << 0.454037 << 0.995795 << 0.124098 << 0.047084 << endr
    << 0.688782 << 0.036549 << 0.552848 << 0.937664 << 0.866401 << endr
    << 0.348740 << 0.479388 << 0.506228 << 0.145673 << 0.491547 << endr
    << 0.148678 << 0.682258 << 0.571154 << 0.874724 << 0.444632 << endr
    << 0.245726 << 0.595218 << 0.409327 << 0.367827 << 0.385736 << endr;
  
  A.print("A =");
  
  // determinant
  cout << "det(A) = " << det(A) << endl;
\end{verbatim}  
}
\end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Armadillo, simple examples}
\begin{small}
{\scriptsize
\begin{verbatim}
  // inverse
  cout << "inv(A) = " << endl << inv(A) << endl;
  double k = 1.23;
  
  mat    B = randu<mat>(5,5);
  mat    C = randu<mat>(5,5);
  
  rowvec r = randu<rowvec>(5);
  colvec q = randu<colvec>(5);
  
  
  // examples of some expressions
  // for which optimised implementations exist
  // optimised implementation of a trinary expression
  // that results in a scalar
  cout << "as_scalar( r*inv(diagmat(B))*q ) = ";
  cout << as_scalar( r*inv(diagmat(B))*q ) << endl;
  
  // example of an expression which is optimised 
  // as a call to the dgemm() function in BLAS:
  cout << "k*trans(B)*C = " << endl << k*trans(B)*C;
  
    return 0;
  }
\end{verbatim}
}
\end{small}
} 


\frame
{
  \frametitle{Gaussian Elimination}
\begin{small}
{\scriptsize
We start with the linear set of equations
\[
   {\bf A}{\bf x} = {\bf w}.
\]
We assume also that the matrix ${\bf A}$ is non-singular and that the 
matrix elements along the diagonal satisfy $a_{ii} \ne 0$. Simple $4\times 4 $ example
\[
\left(\begin{array}{cccc}
                           a_{11}& a_{12} &a_{13}& a_{14}\\
                           a_{21}& a_{22} &a_{23}& a_{24}\\
                           a_{31}& a_{32} &a_{33}& a_{34}\\
                           a_{41}& a_{42} &a_{43}& a_{44}\\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           w_1\\
                           w_2\\
                           w_3 \\
                           w_4\\
                      \end{array} \right).
\]
or
\begin{eqnarray}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4. \nonumber
\end{eqnarray}
}
\end{small}
}



\frame
{
  \frametitle{Gaussian Elimination}
\begin{small}
{\scriptsize
The basic idea of Gaussian elimination is to use the first equation to eliminate the first unknown $x_1$
from the remaining $n-1$ equations. Then we use the new second equation to eliminate the second unknown
$x_2$ from the remaining $n-2$ equations. With $n-1$ such eliminations
we obtain a so-called upper triangular set of equations of the form
\begin{eqnarray}\label{eq:gaussbacksub}
 b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
 b_{22}x_2 + b_{23}x_3 + b_{24}x_4=&y_2 \nonumber \\
b_{33}x_3 + b_{34}x_4=&y_3 \nonumber \\
b_{44}x_4=&y_4. \nonumber
\end{eqnarray}
We can solve this system of equations recursively starting from $x_n$ (in our case $x_4$) and proceed with 
what is called a backward substitution. This process can be expressed mathematically as
\begin{equation}
   x_m = \frac{1}{b_{mm}}\left(y_m-\sum_{k=m+1}^nb_{mk}x_k\right)\hspace{0.5cm} m=n-1,n-2,\dots,1.
\end{equation}
To arrive at such an upper triangular system of equations, we start by eliminating
the unknown $x_1$ for $j=2,n$. We achieve this by multiplying the first equation by $a_{j1}/a_{11}$ and then subtract
the result from the $j$th equation. We assume obviously that $a_{11}\ne 0$ and that
${\bf A}$ is not singular. 
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination}
\begin{small}
{\scriptsize
Our actual $4\times 4$ example reads after the first operation
\[
\left(\begin{array}{cccc}
                           a_{11}& a_{12} &a_{13}& a_{14}\\
                           0& (a_{22}-\frac{a_{21}a_{12}}{a_{11}}) &(a_{23}-\frac{a_{21}a_{13}}{a_{11}}) & (a_{24}-\frac{a_{21}a_{14}}{a_{11}})\\
0& (a_{32}-\frac{a_{31}a_{12}}{a_{11}})& (a_{33}-\frac{a_{31}a_{13}}{a_{11}})& (a_{34}-\frac{a_{31}a_{14}}{a_{11}})\\
0&(a_{42}-\frac{a_{41}a_{12}}{a_{11}}) &(a_{43}-\frac{a_{41}a_{13}}{a_{11}}) & (a_{44}-\frac{a_{41}a_{14}}{a_{11}}) \\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           y_1\\
                           w_2^{(2)}\\
                           w_3^{(2)} \\
                           w_4^{(2)}\\
                      \end{array} \right),
\]
or 
\begin{eqnarray}
 b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
 a^{(2)}_{22}x_2 + a^{(2)}_{23}x_3 + a^{(2)}_{24}x_4=&w^{(2)}_2 \nonumber \\
 a^{(2)}_{32}x_2 + a^{(2)}_{33}x_3 + a^{(2)}_{34}x_4=&w^{(2)}_3 \nonumber \\
 a^{(2)}_{42}x_2 + a^{(2)}_{43}x_3 + a^{(2)}_{44}x_4=&w^{(2)}_4, \nonumber \\
\end{eqnarray}
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination}
\begin{small}
{\scriptsize
The new coefficients are 
\begin{equation} 
   b_{1k} = a_{1k}^{(1)} \hspace{0.1cm} k=1,\dots,n,
\end{equation}
where each $a_{1k}^{(1)}$ is equal to the original $a_{1k}$ element. The other coefficients are
\begin{equation} 
   a_{jk}^{(2)} = a_{jk}^{(1)}-\frac{a_{j1}^{(1)}a_{1k}^{(1)}}{a_{11}^{(1)}} \hspace{0.1cm} j,k=2,\dots,n,
\end{equation}
with a new right-hand side given by 
\begin{equation} 
   y_{1}=w_1^{(1)}, \hspace{0.1cm} w_j^{(2)} =w_j^{(1)}-\frac{a_{j1}^{(1)}w_1^{(1)}}{a_{11}^{(1)}} \hspace{0.1cm} j=2,\dots,n.
\end{equation}
We have also set $w_1^{(1)}=w_1$, the original vector element. 
We see that the system of unknowns $x_1,\dots,x_n$ is transformed into an $(n-1)\times (n-1)$ problem.
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination}
\begin{small}
{\scriptsize
This step is called forward substitution.
Proceeding with these substitutions, we obtain the 
general expressions for the new coefficients 
\begin{equation} 
   a_{jk}^{(m+1)} = a_{jk}^{(m)}-\frac{a_{jm}^{(m)}a_{mk}^{(m)}}{a_{mm}^{(m)}} \hspace{0.1cm} j,k=m+1,\dots,n,
\end{equation}
with $m=1,\dots,n-1$ and a 
right-hand side given by 
\begin{equation} 
   w_j^{(m+1)} =w_j^{(m)}-\frac{a_{jm}^{(m)}w_m^{(m)}}{a_{mm}^{(m)}} \hspace{0.1cm} j=m+1,\dots,n.
\end{equation}
This set of $n-1$ elimations leads us to an equations which is solved by back substitution. 
If the arithmetics is exact and the matrix ${\bf A}$ is not singular, then the computed answer will be exact.

Even though the matrix elements along the diagonal are not zero,
numerically small numbers may appear and subsequent divisions may lead to large numbers, which, if added
to a small number may yield losses of precision. Suppose for example that our first division in $(a_{22}-a_{21}a_{12}/a_{11})$
results in $-10^{-7}$ and that $a_{22}$ is one.
one. We are then 
adding $10^7+1$. With single precision this results in $10^7$. 
}
\end{small}
}





\frame
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
Suppose we want to solve the following boundary value equation
\[
  -\frac{d^2u(x)}{dx^2} = f(x,u(x)),
\]
with $x\in (a,b)$ and with boundary conditions $u(a)=u(b) = 0$.
We assume that $f$ is a continuous function in the domain $x\in (a,b)$.
Since, except the few cases where it is possible to find analytic solutions, we
will seek after approximate solutions, we choose to represent the approximation to the second derivative 
from the previous chapter 
\[
  f''=\frac{f_h -2f_0 +f_{-h}}{h^2} +O(h^2).
\]
We subdivide our interval $x\in (a,b)$ into $n$ subintervals by setting $x_i = ih$, with $i=0,1,\dots,n+1$.
The step size is then given by $h=(b-a)/(n+1)$ with $n\in {\mathbb{N}}$.
For the internal grid points $i=1,2,\dots n$ we replace the differential operator with the above formula
resulting in
\[
u''(x_i) \approx  \frac{u(x_i+h) -2u(x_i) +u(x_i-h)}{h^2},
\]
which we rewrite as 
\[
u^{''}_i \approx  \frac{u_{i+1} -2u_i +u_{i-i}}{h^2}.
\]
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
We can rewrite our original differential equation in terms of a discretized equation with approximations to the 
derivatives as
\[
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
\]
with $i=1,2,\dots, n$. We need to add to this system the two boundary conditions $u(a) =u_0$ and $u(b) = u_{n+1}$.
If we define a matrix 
\[
    {\bf A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 & 2 & -1 & & & \\
                           & -1 & 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 & 2 \\
                      \end{array} \right)
\]
and the corresponding vectors ${\bf u} = (u_1, u_2, \dots,u_n)^T$ and 
${\bf f}({\bf u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T$  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns 
\[
   {\bf A}{\bf u} = {\bf f}({\bf u}).
 \]
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
We start with the linear set of equations
\[
   {\bf A}{\bf u} = {\bf f},
\]
where ${\bf A}$ is a tridiagonal matrix which we rewrite as 
\[
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_n & b_n \\
                      \end{array} \right)
\]
where $a,b,c$ are one-dimensional arrays of length $1:n$. 
In project 1 the arrays $a$ and $c$ are equal, namely $a_i=c_i=-1/h^2$.
The matrix is  also positive definite.
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
We can rewrite as
\[
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_n & b_n \\
                      \end{array} \right)\left(\begin{array}{c}
                           u_1\\
                           u_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           u_n\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           f_1\\
                           f_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           f_n\\
                      \end{array} \right).
\]
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
A tridiagonal matrix is a special form of banded matrix where all the elements are zero except for 
those on and immediately above and below the leading diagonal.
The above tridiagonal system   can be written as
\[
  a_iu_{i-1}+b_iu_i+c_iu_{i+1} = f_i,
\]
for $i=1,2,\dots,n$. We see that $u_{-1}$ and $u_{n+1}$ are not required and we can set $a_1=c_n=0$.
In many applications the matrix is symmetric and we have $a_i=c_i$.
The algorithm for solving this set of equations is rather simple and requires two steps only,
a forward substitution and a backward substitution. These steps are also 
common to the algorithms based on
Gaussian elimination that 
we discussed previously. However, due to its simplicity, the number of floating point operations  
is in this
case proportional with $O(n)$ while Gaussian elimination requires $2n^3/3+O(n^2)$ floating point operations.  
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
In case your system of equations leads to a tridiagonal matrix, it is clearly an overkill to employ
Gaussian elimination or the standard LU decomposition. 
You will encounter several applications involving tridiagonal matrices in our discussion of
partial differential equations in chapter 10.

Our algorithm starts with forward substitution with a loop over of the elements $i$ and can be expressed via the 
following piece of code
 
\begin{verbatim}
   btemp = b[1];
   u[1] = f[1]/btemp;
   for(i=2 ; i <= n ; i++) {  
      temp[i] = c[i-1]/btemp;
      btemp = b[i]-a[i]*temp[i];
      u[i] = (f[i] - a[i]*u[i-1])/btemp; 
   }  
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
Note that you should avoid cases with $b_1=0$. If that is the case, you should rewrite the equations
as a set of order $n-1$ with $u_2$ eliminated. 
Finally we perform the backsubstitution leading to the following code
\begin{verbatim}
   for(i=n-1 ; i >= 1 ; i--) {  
      u[i] -= temp[i+1]*u[i+1];
   }  
\end{verbatim}
}
\end{small}
}


\frame
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
Note that our sums start with $i=1$ and that one  should avoid cases with $b_1=0$. If that is the case, you should rewrite the equations
as a set of order $n-1$ with $u_2$ eliminated. However, a tridiagonal matrix problem is not a guarantee that we
can find a solution. The matrix ${\bf A}$ which rephrases a second derivative in a discretized form
\[
    {\bf A} = \left(\begin{array}{cccccc}
                          2 & -1 & 0 & 0  &0  & 0\\
                          -1 & 2 & -1 &0 &0 &0 \\
                          0 & -1 & 2 & -1 & 0& 0 \\
                          0 & \dots   & \dots & \dots   &\dots & \dots \\
                          0 &0   &0  &-1  &2& -1 \\
                          0 &  0  &0  &0   &-1 & 2 \\
                      \end{array} \right),
\]
fulfills the condition of a weak dominance of the diagonal, with
$|b_1| > |c_1|$, $|b_n| > |a_n|$ and  $|b_k| \ge |a_k|+|c_k|$ for $k=2,3,\dots,n-1$.   
This is a relevant but not sufficient condition to guarantee that the matrix ${\bf A}$ yields a solution to a linear
equation problem. The matrix needs also to be irreducible. A tridiagonal irreducible matrix means that all the elements $a_i$ and
$c_i$ are non-zero. If these two conditions are present, then ${\bf A}$ is nonsingular and has a unique LU decomposition.
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Project 1, hints}
\begin{small}
{\scriptsize
When setting up the algo it is useful to note that the different 
operations on the matrix (here as a $4\times 4$ case  with diagonals
$d_i$ and off-diagonals $e_i$
\[
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          e_1 & d_2 & e_2 & 0 \\
                          0 & e_2 & d_3 & e_3 \\
                          0 & 0 & e_3 & d_4 
                      \end{array} \right)\rightarrow
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & e_2 & d_3 & e_3 \\
                          0 & 0 & e_3 & d_4 
                      \end{array} \right)\rightarrow
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & 0 & \tilde{d}_3 & e_3 \\
                          0 & 0 & e_3 & d_4 
                      \end{array} \right)
\]
and finally
\[
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & 0 & \tilde{d}_3 & e_3 \\
                          0 & 0 & 0 & \tilde{d}_4 
                      \end{array} \right)
\]
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Project 1, hints}
\begin{small}
{\scriptsize
We notice the sub-blocks which get repeated
\[
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & 0 & \tilde{d}_3 & e_3 \\
                          0 & 0 & 0 & \tilde{d}_4 
                      \end{array} \right)
\]
The matrices we often end up with in rewriting for for example partial differential equations,
have the feature that all leading principal submatrices are non-singular. If the matrix
is symmetric as well it can be rewritten as $A=LDL^T$ with $D$ the diagonal and we have the 
following relations
$a_{11} = d_1$, $a_{k,k-1}=e_{k-1}d_{k-1}$ for $k=2,\dots,n$ and finally
\[
a_{kk} = d_k+e_{k-1}^2d_{k-1}=d_k+e_{k-1}a_{k,k-1}
\]
for $k=2,\dots,n$. 
}
\end{small}
}


\frame
{
  \frametitle{Linear Algebra Methods}
\begin{itemize}
\item Gaussian elimination, $O(2/3n^3)$ flops, general matrix
\item LU decomposition, upper triangular and lower tridiagonal
     matrices, $O(2/3n^3)$ flops, general matrix. Get easily the inverse,
     determinant and can solve linear equations with back-substitution only,
     $O(n^2)$ flops
\item Cholesky decomposition $A=LL^T$. Real symmetric or hermitian positive definite
matrix, $O(1/3n^3)$ flops.
\item Tridiagonal linear systems, important for differential equations. Normally 
positive definite and non-singular. $O(8n)$ flops for symmetric. $A=LDL^T$ with $D$ the diagonal.
Special case of banded matrices.
\item Singular value decomposition 
\item the QR method will be discussed in chapter 7 in connection with eigenvalue systems. $O(4/3n^3)$ flops.
\end{itemize}
} 



\frame
{
  \frametitle{LU Decomposition}
\begin{small}
{\scriptsize
The LU decomposition method means that we can rewrite
this matrix as the product of two matrices ${\bf L}$ and ${\bf U}$
where 
\[
\label{eq3}
   \left(\begin{array}{cccc}
                          a_{11} & a_{12} & a_{13} & a_{14} \\
                          a_{21} & a_{22} & a_{23} & a_{24} \\
                          a_{31} & a_{32} & a_{33} & a_{34} \\
                          a_{41} & a_{42} & a_{43} & a_{44} 
                      \end{array} \right)
                      = \left( \begin{array}{cccc}
                              1  & 0      & 0      & 0 \\
                          l_{21} & 1      & 0      & 0 \\
                          l_{31} & l_{32} & 1      & 0 \\
                          l_{41} & l_{42} & l_{43} & 1 
                      \end{array} \right) 
                        \left( \begin{array}{cccc}
                          u_{11} & u_{12} & u_{13} & u_{14} \\
                               0 & u_{22} & u_{23} & u_{24} \\
                               0 & 0      & u_{33} & u_{34} \\
                               0 & 0      &  0     & u_{44} 
             \end{array} \right).
\]
LU decomposition forms the backbone of other algorithms in linear algebra, such as the
solution of linear equations given by
\begin{eqnarray}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4.  \nonumber
\end{eqnarray}
The above set of equations is conveniently solved by using LU decomposition as an intermediate step.

The matrix ${\bf A}\in \mathbb{R}^{n\times n}$ has an LU factorization if the determinant 
is different from zero. If the LU factorization exists and ${\bf A}$ is non-singular, then the LU factorization
is unique and the determinant is given by 
\[
det\{{\bf A}\}=det\{{\bf LU}\}= det\{{\bf L}\}det\{{\bf U}\}=u_{11}u_{22}\dots u_{nn}.
\]
}
\end{small}
}


\frame
{
  \frametitle{LU Decomposition, why?}
\begin{small}
{\scriptsize
There are at least three main advantages with LU decomposition compared with standard Gaussian elimination:
\begin{itemize}
\item It is straightforward to compute the determinant of a matrix
\item If we have to solve sets of linear equations with the same matrix but with different vectors ${\bf y}$,
the number of FLOPS is of the order $n^3$.
\item The invers is such an operation
\end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{LU Decomposition, linear equations}
\begin{small}
{\scriptsize
With the LU decomposition it is rather 
simple to solve a system of linear equations
%
\begin{eqnarray}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4. \nonumber
\end{eqnarray}
%
This can be written in matrix form as 
\[
   {\bf Ax}={\bf w}.
\]
%
where ${\bf A}$ and ${\bf w}$ are known and we have to solve for
${\bf x}$. Using the LU dcomposition we write 
%
\[
%\label{eq4}
   {\bf A} {\bf x} \equiv {\bf L} {\bf U} {\bf x} ={\bf w}.
\]

}
\end{small}
}


\frame
{
  \frametitle{LU Decomposition, linear equations}
\begin{small}
{\scriptsize
The previous equation can be calculated in two steps
%
\[
  {\bf L} {\bf y} = {\bf w}; \hspace*{2cm} {\bf Ux}={\bf y}.
  \label{eq:byw}
\]
%
To show that this is correct we use to the LU decomposition
to rewrite our system of linear equations as
\[
   {\bf LUx}={\bf w},
\]
and since the determinat of ${\bf L}$ is equal to 1 (by construction
since the diagonals of ${\bf L}$ equal 1) we can use the inverse of
${\bf L}$ to obtain 
\[
   {\bf Ux}={\bf L^{-1}w}={\bf y},
\]
which yields the intermediate step 
\[
   {\bf L^{-1}w}={\bf y}
\]
and as soon as we have ${\bf y}$ we can obtain ${\bf x}$
through ${\bf Ux}={\bf y}$. 
}
\end{small}
}


\frame
{
  \frametitle{LU Decomposition, why?}
\begin{small}
{\scriptsize
For our four-dimentional example this takes the form 
%
\begin{eqnarray} 
 y_1=&w_1 \nonumber\\
l_{21}y_1 + y_2=&w_2\nonumber \\
l_{31}y_1 + l_{32}y_2 + y_3 =&w_3\nonumber \\
l_{41}y_1 + l_{42}y_2 + l_{43}y_3 + y_4=&w_4. \nonumber
\end{eqnarray}
%
and 
%
\begin{eqnarray} 
 u_{11}x_1 +u_{12}x_2 +u_{13}x_3 + u_{14}x_4=&y_1 \nonumber\\
u_{22}x_2 + u_{23}x_3 + u_{24}x_4=&y_2\nonumber \\
u_{33}x_3 + u_{34}x_4=&y_3\nonumber \\
u_{44}x_4=&y_4  \nonumber
\end{eqnarray}
%
This example shows the basis for the algorithm
needed to solve the set of $n$ linear equations. 
}
\end{small}
}


\frame
{
  \frametitle{LU Decomposition, linear equations}
\begin{small}
{\scriptsize
The algorithm goes as follows
%
\begin{itemize}
\item Set up the matrix {\bf A} and the vector {\bf w}
      with their correct dimensions. This determines the dimensionality
      of the unknown vector {\bf x}.
\item Then LU decompose the matrix {\bf A} through a call to
      the function
      % 
      \begin{center}
         ludcmp(double a, int n, int indx, double \&d)
      \end{center}
      %
      This functions returns the LU decomposed
      matrix {\bf A}, its determinant and the vector indx which keeps track 
     of the number of interchanges of  rows. If the determinant is zero, 
     the solution is malconditioned.
\item Thereafter you call the function
      %
      \begin{center}
       lubksb(double a, int n, int indx, double w)
      \end{center}
      %
      which uses the
      LU decomposed matrix {\bf A} and the vector {\bf w} and returns {\bf x}
      in the same place as {\bf w}. Upon exit the original content
      in {\bf w} is destroyed. If you wish to keep this information, you should make
      a backup of it in your calling function.
\end{itemize}
}
\end{small}
}




\frame
{
  \frametitle{LU Decomposition, the inverse of a matrix}
\begin{small}
{\scriptsize
If the inverse exists
then
\[
   {\bf A}^{-1}{\bf A}={\bf I},
\]
the identity matrix. With an LU decomposed matrix we can rewrite the last equation as
\[
   {\bf LU}{\bf A}^{-1}={\bf I}.
\]
If we assume that the first column (that is column 1) of the inverse matrix 
can be written as a vector with unknown entries
\[
    {\bf A}_1^{-1}= \left( \begin{array}{c}
  
                              a_{11}^{-1} \\
                              a_{21}^{-1} \\  
                              \dots \\  
                              a_{n1}^{-1} \\  
                    \end{array} \right), 
\]
then we have a linear set of equations
\[
    {\bf LU}\left( \begin{array}{c}
  
                              a_{11}^{-1} \\
                              a_{21}^{-1} \\  
                              \dots \\  
                              a_{n1}^{-1} \\  
                    \end{array} \right) =\left( \begin{array}{c}
                               1 \\
                              0 \\  
                              \dots \\  
                              0 \\  
                    \end{array} \right).
\]
}
\end{small}
}

\frame
{
  \frametitle{LU Decomposition, the inverse}
\begin{small}
{\scriptsize
In a similar way we can compute the unknow entries of the second column,
\[
    {\bf LU}\left( \begin{array}{c}
  
                              a_{12}^{-1} \\
                              a_{22}^{-1} \\  
                              \dots \\  
                              a_{n2}^{-1} \\  
                    \end{array} \right) =\left( \begin{array}{c}
                                0 \\
                              1 \\  
                              \dots \\  
                              0 \\  
                    \end{array} \right),
\]
and continue till we have solved all $n$ sets of linear equations.
}
\end{small}
}

%

\frame[containsverbatim]
{
  \frametitle{How to use the Library functions}
\begin{small}
{\scriptsize
Standard C/C++: fetch the files lib.cpp and lib.h. You can make a directory where you store
these files, and eventually its compiled version lib.o. The example here is program1.cpp from
chapter 6 and performs the matrix inversion.
\begin{verbatim}
/  Simple matrix inversion example
#include <iostream>
#include <new>
#include <cstdio>
#include <cstdlib>
#include <cmath>
#include <cstring>
#include    "lib.h"

using namespace std;

/* function declarations */

void inverse(double **, int);

\end{verbatim}
}
\end{small}
} 

\frame[containsverbatim]
{
  \frametitle{How to use the Library functions}
\begin{small}
{\scriptsize
\begin{verbatim}
void inverse(double **a, int n)
{
  int          i,j, *indx;
  double       d, *col, **y;
  // allocate space in memory
  indx = new int[n];
  col  = new double[n];
  y    = (double **) matrix(n, n, sizeof(double));
  ludcmp(a, n, indx, &d);   // LU decompose  a[][]
  printf("\n\nLU form of matrix of a[][]:\n");
  for(i = 0; i < n; i++) {
    printf("\n");
    for(j = 0; j < n; j++) {
      printf(" a[%2d][%2d] = %12.4E",i, j, a[i][j]);
    }
  }
\end{verbatim}
}
\end{small}
} 

\frame[containsverbatim]
{
  \frametitle{How to use the Library functions}
\begin{small}
{\scriptsize
\begin{verbatim}
  // find inverse of a[][] by columns
  for(j = 0; j < n; j++) {
    // initialize right-side of linear equations
    for(i = 0; i < n; i++) col[i] = 0.0;
    col[j] = 1.0;
    lubksb(a, n, indx, col);
    // save result in y[][]
    for(i = 0; i < n; i++) y[i][j] = col[i];
  }   //j-loop over columns
  // return the inverse matrix in a[][]
  for(i = 0; i < n; i++) {
    for(j = 0; j < n; j++) a[i][j] = y[i][j];
  }
  free_matrix((void **) y);     // release local memory
  delete [] col;
  delete []indx;
}  // End: function inverse()
\end{verbatim}
}
\end{small}
} 




\frame[containsverbatim]
{
  \frametitle{How to use the Library functions}
\begin{small}
{\scriptsize
For Fortran users:
\begin{verbatim}
PROGRAM matrix
  USE constants
  USE F90library
  IMPLICIT NONE
  !      The definition of the matrix, using dynamic allocation
  REAL(DP), ALLOCATABLE, DIMENSION(:,:) :: a, ainv, unity
  !      the determinant
  REAL(DP) :: d
  !      The size of the matrix
  INTEGER :: n
  ....
  !      Allocate now place in heap for a
  ALLOCATE ( a(n,n), ainv(n,n), unity(n,n) )
\end{verbatim}
}
\end{small}
} 



\frame[containsverbatim]
{
  \frametitle{How to use the Library functions}
\begin{small}
{\scriptsize
For Fortran users:
\begin{verbatim}
  WRITE(6,*) ' The matrix before inversion'
  WRITE(6,'(3F12.6)') a
  ainv=a
  CALL matinv (ainv, n, d)
  ....
  !      get the unity matrix
  unity=MATMUL(ainv,a)
  WRITE(6,*) ' The unity matrix'
  WRITE(6,'(3F12.6)') unity 
  !      deallocate all arrays
  DEALLOCATE (a, ainv, unity)
END PROGRAM matrix

\end{verbatim}
}
\end{small}
} 





\end{document}
\section[Week 36]{Week 36}

\frame
{
  \frametitle{Week 36}
  \begin{block}{Linear Algebra}
\begin{itemize}
%\item Monday: Repetition from last week
\item Discussion of Project 1
\item Object orientation and unit testing
%\item Gaussian elimination, LU decomposition and linear equations
%\item Inverse of  a matrix.
%\item Tuesday: Further discussion of linear algebra methods.
%\item Dynamic memory allocation in C/C++ and Fortran2008, use of the libraries like Armadillo for C++ users. How to use the C/C++ and Fortran 90/95 libraries. Programming classes in C++.
%\item Discussion of version control. Dropbox and GIT.
%\item Computer-Lab: Project 1.
\end{itemize}
  \end{block}
} 






\frame[containsverbatim]
{
  \frametitle{Object orientation}
\begin{small}
{\scriptsize
Why object orientation? 
\begin{itemize}
\item Three main topics: objects, class hierarchies and polymorphism
\item The aim here is to  be to be able to write a more general code which can easily be tailored
to new situations. 
\item {\bf Polymorphism} is a term used in software development to describe a variety of 
techniques employed by programmers to create flexible and reusable software components. The term is Greek and it loosely translates to "many forms".
\end{itemize}
Strategy: try to single out the variables needed to describe a given system and those needed to describe a given solver.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Object orientation}
\begin{small}
{\scriptsize
In programming languages, a polymorphic object is an entity, such as a variable or a procedure, that can hold or operate on values of differing types during the program's execution. Because a polymorphic object can operate on a variety of values and types, it can also be used in a variety of programs, sometimes with little or no change by the programmer. The idea of write once, run many, also known as code reusability, is an important characteristic to the programming paradigm known as Object-Oriented Programming (OOP).

OOP describes an approach to programming where a program is viewed as a collection of interacting, but mostly independent software components. These software components are known as objects in OOP and they are typically implemented in a programming language as an entity that encapsulates both data and procedures.

}
\end{small}
}



%\section{Programming classes and templates}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
In Fortran a vector or matrix start with $1$, but it is easy 
to change a vector so that it starts with zero or even a negative number.
If we have a double precision Fortran vector  which starts at $-10$ and ends at $10$, we could declare it as 
\lstinline{REAL(KIND=8) ::  vector(-10:10)}. Similarly, if we want to start at zero and end at 10 we could write
\lstinline{REAL(KIND=8) ::  vector(0:10)}.  
We have also seen that Fortran  allows us to write a matrix addition ${\bf A} = {\bf B}+{\bf C}$ as
\lstinline{A = B + C}.  This means that we have overloaded the addition operator so that it translates this operation into
two loops and an addition of two matrix elements $a_{ij} = b_{ij}+c_{ij}$.
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
The way the matrix addition is written is very close to the way we express this relation mathematically. The benefit for the 
programmer is that our code is easier to read. Furthermore, such a way of coding makes it  more likely  to spot eventual 
errors as well.  


In Ansi C and C++ arrays start by default from $i=0$.  Moreover, if we  wish to add two matrices we need to explicitely write out
the two loops as
\lstset{language=c++}  
\begin{lstlisting}
   for(i=0 ; i < n ; i++) {  
      for(j=0 ; j < n ; j++) {
         a[i][j]=b[i][j]+c[i][j]
      }
   }  
\end{lstlisting} 

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
However, 
the strength of C++ is the possibility 
to define new data types, tailored to some particular problem.
Via new data types and overloading of operations such as addition and subtraction, we can easily define 
sets of operations and data types which allow us to write a matrix addition in exactly the same
way as we would do in Fortran.  We could also change the way we declare a C++ matrix elements $a_{ij}$, from  $a[i][j]$ 
to say $a(i,j)$, as we would do in Fortran. Similarly, we could also change the default range from $0:n-1$ to $1:n$. 

To achieve this we need to introduce two important entities in C++ programming, classes and templates.        
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
The function and class declarations are fundamental concepts within C++.  Functions are abstractions
which encapsulate an algorithm or parts of it and perform specific tasks in a program. 
We have already met several examples on how to use  functions. 
Classes can be defined as abstractions which encapsulate
data and operations on these data. 
The data can be very complex data structures  and the class can contain particular functions
which operate on these data. Classes allow therefore for a higher level of abstraction in computing.
The elements (or components) of the data
type are the class data members, and the procedures are the class
member functions. 
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
Classes are user-defined tools used to create multi-purpose software which can be reused by other classes or functions.
These user-defined data types contain data (variables) and 
functions operating on the data.  

A simple example is that of a point in two dimensions.  
The data could be the $x$ and $y$ coordinates of a given  point. The functions
we define could be simple read and write functions or the possibility to compute the distance between two points.
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
C++ has a class complex in its standard
template library (STL). The standard usage in a given function could then look like 
\begin{lstlisting}
// Program to calculate addition and multiplication of two complex numbers
using namespace std;
#include <iostream>
#include <cmath>
#include <complex>
int main()
{
  complex<double> x(6.1,8.2), y(0.5,1.3);
  // write out x+y
  cout << x + y << x*y  << endl;
  return 0;
}
\end{lstlisting}
where we add and multiply two complex numbers $x=6.1+\imath 8.2$ and $y=0.5+\imath 1.3$ with the obvious results
$z=x+y=6.6+\imath 9.5$ and $z=x\cdot y= -7.61+\imath 12.03$. 
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
We proceed by  splitting our task in three files.  

We define first a header file complex.h  which contains the declarations of
the class. The header file contains the class declaration (data and
functions), declaration of stand-alone functions, and all inlined
functions, starting as follows
\begin{lstlisting}
#ifndef Complex_H
#define Complex_H
//   various include statements and definitions
#include <iostream>          // Standard ANSI-C++ include files
#include <new>
#include ....

class Complex
{...
definition of variables and their character
};
//   declarations of various functions used by the class
...
#endif
\end{lstlisting}

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
Next we provide a file complex.cpp where the code and algorithms of different functions  (except inlined functions) 
declared within the class are written.
The files complex.h and complex.cpp are normally placed in a directory with other classes and libraries we have 
defined.  

Finally,we discuss here an example of a main program which uses this particular class.
An example of a program which uses our complex class is given below. In particular we would like our class to
perform tasks like declaring complex variables, writing out the real and imaginary part and performing 
algebraic operations such as adding or multiplying two complex numbers.
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
\begin{lstlisting}
#include "Complex.h"
...  other include and declarations
int main ()
{
  Complex a(0.1,1.3);    // we declare a complex variable a
  Complex b(3.0), c(5.0,-2.3);  // we declare  complex variables b and c
  Complex d = b;         //  we declare  a new complex variable d 
  cout << "d=" << d << ", a=" << a << ", b=" << b << endl;
  d = a*c + b/a;  //   we add, multiply and divide two complex numbers 
  cout << "Re(d)=" << d.Re() << ", Im(d)=" << d.Im() << endl;  // write out of the real and imaginary parts
}
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
We include the header file complex.h and define four different complex variables. These
are $a=0.1+\imath 1.3$, $b=3.0+\imath 0$ (note that if you don't define a value for the imaginary part  this is set to
zero), $c=5.0-\imath 2.3$ and $d=b$.  Thereafter we have defined standard algebraic operations and the member functions
of the class which allows us to print out the real and imaginary part of a given variable.
}




\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
\begin{lstlisting}
class Complex
{
private:
   double re, im; // real and imaginary part
public:
   Complex ();                              // Complex c;
   Complex (double re, double im = 0.0); // Definition of a complex variable;
   Complex (const Complex& c);              // Usage: Complex c(a);   // equate two complex variables
   Complex& operator= (const Complex& c); // c = a;   //  equate two complex variables, same as previous
....

\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{small}
{\scriptsize
\begin{lstlisting}
  ~Complex () {}                        // destructor
   double   Re () const;        // double real_part = a.Re();
   double   Im () const;        // double imag_part = a.Im();
   double   abs () const;       // double m = a.abs(); // modulus
   friend Complex operator+ (const Complex&  a, const Complex& b);
   friend Complex operator- (const Complex&  a, const Complex& b);
   friend Complex operator* (const Complex&  a, const Complex& b);
   friend Complex operator/ (const Complex&  a, const Complex& b);
};
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Programming classes}
The class is defined via the statement \lstinline{class Complex}. We must first use the key word 
\lstinline{class}, which in turn is followed by the user-defined variable name  \lstinline{Complex}. 
The body of the class, data and functions, is encapsulated  within the parentheses $\{...\};$.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Data and specific functions can be private, which means that they cannot be accessed from outside the class.
This means also that access cannot be inherited by other functions outside the class. If we use \lstinline{protected}
instead of \lstinline{private}, then data and functions can be inherited outside the class.

}
\frame[containsverbatim]
{
  \frametitle{Programming classes}
The key word \lstinline{public} means  that data and functions can be accessed from outside the class.
Here we have defined several functions  which can be accessed by functions outside the class.
The declaration \lstinline{friend} means that stand-alone functions can work on privately declared  variables  of the type
\lstinline{(re, im)}.  Data members of a class should be declared as private variables.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The first public function we encounter is a so-called   
constructor, which  tells how we declare a variable of type \lstinline{Complex} 
and how this variable is initialized. We have chose  three possibilities in the example above:
\begin{itemize}
\item A declaration like \lstinline{Complex c;} calls the member function \lstinline{Complex()}
which can have the following implementation 
\begin{lstlisting}
Complex:: Complex ()   { re = im = 0.0; }
\end{lstlisting}
meaning that it sets the real and imaginary parts to zero.  Note the way a member function is defined.
The constructor is the first function that is called when an object is instantiated.
\end{itemize}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{itemize}
\item Another possibility  is 
\begin{lstlisting}
Complex:: Complex ()   {}
\end{lstlisting}
which means that there is no initialization of the real and imaginary parts.  The drawback is that a given compiler
can then assign random values to a given variable.
\item  A call like \lstinline{Complex a(0.1,1.3);} means that we could call the member function 
\lstinline{Complex(double, double)}as 
\begin{lstlisting}
Complex:: Complex (double re_a, double im_a)
{ re = re_a; im = im_a; }
\end{lstlisting}
\end{itemize}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The simplest member function are those we defined to extract 
the real and imaginary part of a variable. Here you have to recall that these are private data,
that is they invisible for users of the class.  We obtain a copy of these variables by defining the 
functions
\begin{lstlisting}
double Complex:: Re () const { return re; }} //  getting the real part
double Complex:: Im () const { return im; }  //   and the imaginary part
\end{lstlisting}
Note that we have introduced   the declaration  \lstinline{const}.  What does it mean? 
This declaration means that a variabale cannot be changed within  a called function.
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
If we define a variable as 
\lstinline{const double p = 3;} and then try to change its value, we will get an error when we
compile our program. This means that constant arguments in functions cannot be changed.
\begin{lstlisting}
// const arguments (in functions) cannot be changed:
void myfunc (const Complex& c)
{ c.re = 0.2; /* ILLEGAL!! compiler error... */  }
\end{lstlisting}
If we declare the function and try to change the value to $0.2$, the compiler will complain by sending
an error message. 
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
If we define a function to compute the absolute value of complex variable like
\begin{lstlisting}
double Complex:: abs ()  { return sqrt(re*re + im*im);}
\end{lstlisting}
without the constant declaration  and define thereafter a function 
\lstinline{myabs} as
\begin{lstlisting}
double myabs (const Complex& c)
{ return c.abs(); }   // Not ok because c.abs() is not a const func.
\end{lstlisting}
the compiler would not allow the c.abs() call in myabs
since \lstinline{Complex::abs} is not a constant member function. 
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Constant functions cannot change the object's state.
To avoid this we declare the function \lstinline{abs} as
\begin{lstlisting}
double Complex:: abs () const { return sqrt(re*re + im*im); } 
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
C++ (and Fortran) allow  for overloading of operators. That means we can define algebraic operations
on for example vectors or any arbitrary object.   
As an example, a vector addition of the type  ${\bf c} = {\bf a} + {\bf b}$
means that we need to write   a small part of code with a for-loop over the dimension of the array.
We would rather like to write this statement as \lstinline{c = a+b;} as this makes the code much more
readable and close to eventual equations we want to code.  To achieve this we need to extend the definition of operators.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Let us study the declarations in our complex class.
In our main function we have a statement like \lstinline{d = b;}, which means
that we call \lstinline{d.operator= (b)} and we have defined a so-called assignment operator
as a part of the class defined as
\begin{lstlisting}
Complex& Complex:: operator= (const Complex& c)
{
   re = c.re;
   im = c.im;
   return *this;
}
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
With this function, statements like
\lstinline{Complex d = b;} or \lstinline{Complex d(b);}
make a new object $d$, which becomes a copy of $b$. 
We can make simple implementations in terms of the assignment
\begin{lstlisting}
Complex:: Complex (const Complex& c)
{ *this = c; }
\end{lstlisting}
which  is a pointer to "this object", \lstinline{*this} is the present object,
so \lstinline{*this = c;} means setting the present object equal to $c$, that is
\lstinline{this->operator= (c);}.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The meaning of the addition operator $+$ for Complex objects is defined in the
function
\lstinline{Complex operator+ (const Complex& a, const Complex& b); // a+b}
The compiler translates \lstinline{c = a + b;} into \lstinline{c = operator+ (a, b);}. 
Since this implies the call to function, it brings in an additional overhead. If speed
is crucial and this function call is performed inside a loop, then it is more difficult for a 
given compiler to perform optimizations of a loop.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The solution to this is to inline functions.   We discussed inlining in chapter 
2 of the lecture notes.
Inlining means that the function body is copied directly into
the calling code, thus avoiding calling the function.
Inlining is enabled by the inline keyword
\begin{lstlisting}
inline Complex operator+ (const Complex& a, const Complex& b)
{ return Complex (a.re + b.re, a.im + b.im); }
\end{lstlisting}
Inline functions, with complete bodies must be written in the header file  complex.h.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Consider  the case \lstinline{c = a + b;}
that is,  \lstinline{c.operator= (operator+ (a,b));}
If \lstinline{operator+}, \lstinline{operator=} and the constructor \lstinline{Complex(r,i)} all
are inline functions, this transforms to
\begin{lstlisting}
c.re = a.re + b.re;
c.im = a.im + b.im;
\end{lstlisting}
by the compiler, i.e., no function calls
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The stand-alone function \lstinline{operator+} is a friend of the Complex  class
\begin{lstlisting}
class Complex
{
   ...
   friend Complex operator+ (const Complex& a, const Complex& b);
   ...
};
\end{lstlisting}
so it can read (and manipulate) the private data parts $re$ and
$im$ via
\begin{lstlisting}
inline Complex operator+ (const Complex& a, const Complex& b)
{ return Complex (a.re + b.re, a.im + b.im); }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Since we do not need to alter the re and im variables, we can
get the values by Re() and Im(), and there is no need to be a
friend function
\begin{lstlisting}
inline Complex operator+ (const Complex& a, const Complex& b)
{ return Complex (a.Re() + b.Re(), a.Im() + b.Im()); }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
The multiplication functionality can now be extended to imaginary numbers by the following code
\begin{lstlisting}
inline Complex operator* (const Complex& a, const Complex& b)
{
  return Complex(a.re*b.re - a.im*b.im, a.im*b.re + a.re*b.im);
}
\end{lstlisting}
It will be convenient to inline all functions used by this operator.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
To inline the complete expression \lstinline{a*b;}, the constructors and
\lstinline{operator=}  must also be inlined.  This can be achieved via the following piece of code
\begin{lstlisting}
inline Complex:: Complex () { re = im = 0.0; }
inline Complex:: Complex (double re_, double im_)
{ ... }
inline Complex:: Complex (const Complex& c)
{ ... }
inline Complex:: operator= (const Complex& c)
{ ... }
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
\begin{lstlisting}
// e, c, d are complex
e = c*d;
// first compiler translation:
e.operator= (operator* (c,d));
// result of nested inline functions
// operator=, operator*, Complex(double,double=0):
e.re = c.re*d.re - c.im*d.im;
e.im = c.im*d.re + c.re*d.im;
\end{lstlisting}
The definitions \lstinline{operator-} and \lstinline{operator/} follow the same set up.
}


\frame[containsverbatim]
{
  \frametitle{Programming classes}
Finally, if we wish to write to file or another device a complex number using the simple syntax
\lstinline{cout << c;}, we obtain this by defining
the effect of $<<$ for a Complex object as 
\begin{lstlisting}
ostream& operator<< (ostream& o, const Complex& c)
{ o << "(" << c.Re() << "," << c.Im() << ") "; return o;}
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Programming classes, templates}
What if we wanted to make a class which takes integers
or floating point numbers with single precision?
A simple way to achieve this is copy and paste our class and replace \lstinline{double} with for
example \lstinline{int}.

C++  allows us to do this automatically via the usage of templates, which 
are the C++ constructs for parameterizing parts of
classes. Class templates  is a template for producing classes. The declaration consists
of the keyword \lstinline{template} followed by a list of template arguments enclosed in brackets.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
We can therefore make a more general class by rewriting our original example as
\begin{lstlisting}
template<class T>
class Complex
{
private:
   T re, im; // real and imaginary part
public:
   Complex ();                              // Complex c;
   Complex (T re, T im = 0); // Definition of a complex variable;
   Complex (const Complex& c);              // Usage: Complex c(a);   // equate two complex variables
   Complex& operator= (const Complex& c); // c = a;   //  equate two complex variables, same as previous

\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
We can therefore make a more general class by rewriting our original example as
\begin{lstlisting}
  ~Complex () {}                        // destructor
   T   Re () const;        // T real_part = a.Re();
   T   Im () const;        // T imag_part = a.Im();
   T   abs () const;       // T m = a.abs(); // modulus
   friend Complex operator+ (const Complex&  a, const Complex& b);
   friend Complex operator- (const Complex&  a, const Complex& b);
   friend Complex operator* (const Complex&  a, const Complex& b);
   friend Complex operator/ (const Complex&  a, const Complex& b);
};
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
What it says is that \lstinline{Complex} is a parameterized type with $T$ as a parameter and $T$ has to be a type such as double
or float. 
The class complex is now a class template
and we would define variables in a code as 
\begin{lstlisting}
Complex<double> a(10.0,5.1);
Complex<int> b(1,0);
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Member functions of our class are defined by preceding the name of the function with the \lstinline{template} keyword. 
Consider the function we defined as \lstinline{Complex:: Complex (double re_a, double im_a)}.
We would rewrite this function as 
\begin{lstlisting}
template<class T>
Complex<T>:: Complex (T re_a, T im_a)
{ re = re_a; im = im_a; }
\end{lstlisting}
The member functions  are otherwise defined following ordinary member function definitions.
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
Here follows a very simple first class
\begin{lstlisting}
// Class to compute the square of a number
class Squared{
  public:
    // Default constructor, not used here
    Squared(){}
    
    // Overload the function operator()
    double operator()(double x){
      return x*x;
    }
};	
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Programming classes}
and we would use it as 
\begin{lstlisting}
#include <iostream>
using namespace std;

int main(){
  Squared s;
  cout << s(3) << endl;
}
\end{lstlisting}
}

\end{document}

\section[Week 37]{Week 37}
\frame
{
  \frametitle{Week 37}
  \begin{block}{Linear Algebra and differential equations}
\begin{itemize}
\item Monday: Repetition from last week
\item Discussion of some Armadillo examples and matrix operations.
\item Discussion of classes in C++
\item Iterative methods, Gauss-Seidel. Cubic spline and interpolation (chapter 6).
%\item Singular value decomposition, chapter 6
\item Tuesday: Eigenvalue problems
\item We start discussing Jacobi's algorithm, chapter 7.
\item Presentation of project 2.
\item Computer-Lab: Project 1 and project 2.
\end{itemize}
 \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Iterative methods, Chapter 6}
\begin{small}
{\scriptsize
\begin{itemize}
\item Direct solvers such as 
Gauss elimination and  LU decomposition discussed last week.
\item Iterative solvers such
as Basic iterative solvers,  Jacobi,  Gauss-Seidel,
Successive over-relaxation. These methods are easy to parallelize, as we will se later.
Much used in solutions of partial differential equations.
\item Other iterative methods such as Krylov subspace methods with
Generalized minimum residual (GMRES) and
Conjugate gradient etc will not be discussed.
\end{itemize}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Iterative methods, Jacobi's method}
\begin{small}
{\scriptsize
It is a simple method for solving
\[ \hat{A}{\bf x}={\bf b},\]
where $\hat{A}$ is a matrix and ${\bf x}$ and ${\bf b}$ are vectors. The vector ${\bf x}$ is 
the unknown.

It is an iterative scheme where we start with a guess for the unknown, and 
after $k+1$ iterations we have  
\[ {\bf x}^{(k+1)}= \hat{D}^{-1}({\bf b}-(\hat{L}+\hat{U}){\bf x}^{(k)}),\]
with $\hat{A}=\hat{D}+\hat{U}+\hat{L}$ and
$\hat{D}$ being a diagonal matrix, $\hat{U}$ an upper triangular matrix and $\hat{L}$ a  lower triangular
matrix.

If the matrix $\hat{A}$ is positive definite or diagonally dominant, one can show that this method will always converge to the exact solution. 

}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Iterative methods, Jacobi's method}
\begin{small}
{\scriptsize
We can demonstrate Jacobi's method by this $4\times 4$ matrix problem. We assume a guess
for the vector elements $x_i^{(0)}$, a guess which represents our first iteration. The new
values are obtained by substitution
\begin{eqnarray}
 x_1^{(1)} =&(b_1-a_{12}x_2^{(0)} -a_{13}x_3^{(0)} - a_{14}x_4^{(0)})/a_{11} \nonumber \\
 x_2^{(1)} =&(b_2-a_{21}x_1^{(0)} - a_{23}x_3^{(0)} - a_{24}x_4^{(0)})/a_{22} \nonumber \\
 x_3^{(1)} =&(b_3- a_{31}x_1^{(0)} -a_{32}x_2^{(0)} -a_{34}x_4^{(0)})/a_{33} \nonumber \\
 x_4^{(1)}=&(b_4-a_{41}x_1^{(0)} -a_{42}x_2^{(0)} - a_{43}x_3^{(0)})/a_{44},  \nonumber
\end{eqnarray}
which after $k+1$ iterations reads
\begin{eqnarray}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \nonumber
\end{eqnarray}
}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Iterative methods, Jacobi's method}
\begin{small}
{\scriptsize
We can generalize the above equations to
\[
 x_i^{(k+1)}=(b_i-\sum_{j=1, j\ne i}^{n}a_{ij}x_j^{(k)})/a_{ii}
\]
or in an even more compact form as
\[ {\bf x}^{(k+1)}= \hat{D}^{-1}({\bf b}-(\hat{L}+\hat{U}){\bf x}^{(k)}),\]
with $\hat{A}=\hat{D}+\hat{U}+\hat{L}$ and
$\hat{D}$ being a diagonal matrix, $\hat{U}$ an upper triangular matrix and $\hat{L}$ a  lower triangular
matrix.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Iterative methods, Gauss-Seidel's method}
\begin{small}
{\scriptsize
Our $4\times 4$ matrix problem 
\begin{eqnarray}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \nonumber
\end{eqnarray}
can be rewritten as 
\begin{eqnarray}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k+1)} -a_{32}x_2^{(k+1)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k+1)} -a_{42}x_2^{(k+1)} - a_{43}x_3^{(k+1)})/a_{44},  \nonumber
\end{eqnarray}
which allows us to utilize the preceding solution (forward substitution). This improves normally the convergence
behavior and leads to the Gauss-Seidel method!
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Iterative methods, Gauss-Seidel's method}
\begin{small}
{\scriptsize
We can generalize 
\begin{eqnarray}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k+1)} -a_{32}x_2^{(k+1)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k+1)} -a_{42}x_2^{(k+1)} - a_{43}x_3^{(k+1)})/a_{44},  \nonumber
\end{eqnarray}
to the following form
\[
 x^{(k+1)}_i = \frac{1}{a_{ii}} \left(b_i - \sum_{j>i}a_{ij}x^{(k)}_j - \sum_{j<i}a_{ij}x^{(k+1)}_j \right),\quad i=1,2,\ldots,n. 
\]

The procedure is generally continued until the changes made by an iteration are below some tolerance.

The convergence properties of the Jacobi method and the 
Gauss-Seidel method are dependent on the matrix $\hat{A}$. These methods converge when
the matrix is symmetric positive-definite, or is strictly or irreducibly diagonally dominant.
Both methods sometimes converge even if these conditions are not satisfied.
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Iterative methods, Successive over-relaxation}
\begin{small}
{\scriptsize
Given a square system of n linear equations with unknown $\mathbf x$:
\[
    \hat{A}\mathbf x = \mathbf b
\]
where:
\[
    \hat{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix}, \qquad \mathbf{x} = \begin{bmatrix} x_{1} \\ x_2 \\ \vdots \\ x_n \end{bmatrix} , \qquad \mathbf{b} = \begin{bmatrix} b_{1} \\ b_2 \\ \vdots \\ b_n \end{bmatrix}.
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Iterative methods, Successive over-relaxation}
\begin{small}
{\scriptsize
Then A can be decomposed into a diagonal component D, and strictly lower and upper triangular components L and U:
\[
    \hat{A} =\hat{D} + \hat{L} + \hat{U},
\]
where
\[
    D = \begin{bmatrix} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & a_{nn} \end{bmatrix}, \quad L = \begin{bmatrix} 0 & 0 & \cdots & 0 \\ a_{21} & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\a_{n1} & a_{n2} & \cdots & 0 \end{bmatrix}, \quad U = \begin{bmatrix} 0 & a_{12} & \cdots & a_{1n} \\ 0 & 0 & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & 0 \end{bmatrix}. 
\]
The system of linear equations may be rewritten as:
\[
    (D+\omega L) \mathbf{x} = \omega \mathbf{b} - [\omega U + (\omega-1) D ] \mathbf{x} 
\]
for a constant $\omega > 1$.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Iterative methods, Successive over-relaxation}
\begin{small}
{\scriptsize
The method of successive over-relaxation is an iterative technique that solves the left hand side of this expression for $x$, using previous value for $x$ on the right hand side. Analytically, this may be written as:
\[
    \mathbf{x}^{(k+1)} = (D+\omega L)^{-1} \big(\omega \mathbf{b} - [\omega U + (\omega-1) D ] \mathbf{x}^{(k)}\big). 
\]
However, by taking advantage of the triangular form of $(D+\omega L)$, the elements of $x^{(k+1)}$ can be computed sequentially using forward substitution:
\[
    x^{(k+1)}_i = (1-\omega)x^{(k)}_i + \frac{\omega}{a_{ii}} \left(b_i - \sum_{j>i} a_{ij}x^{(k)}_j - \sum_{j<i} a_{ij}x^{(k+1)}_j \right),\quad i=1,2,\ldots,n. 
\]
The choice of relaxation factor is not necessarily easy, and depends upon the properties of the coefficient matrix. For symmetric, positive-definite matrices it can be proven that $0 < \omega < 2$ will lead to convergence, but we are generally interested in faster convergence rather than just convergence.
}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{Cubic Splines, Chapter 6}
\begin{small}
{\scriptsize

Cubic spline interpolation is among one of the mostly used 
methods for interpolating between data points where the arguments
are organized as ascending series. In the library program we supply
such a function, based on the so-called cubic spline method to be 
described below. 

A spline function consists of polynomial pieces defined on
subintervals. The different subintervals are connected via
various continuity relations.

Assume we have at our disposal $n+1$ points $x_0, x_1, \dots x_n$ 
arranged so that $x_0<x_1<x_2< \dots x_{n-1}<x_n$ (such points are called
knots). A spline function $s$ of degree $k$ with $n+1$ knots is defined
as follows
\begin{itemize}
 \item On every subinterval $[x_{i-1},x_i)$ $s$ is a polynomial
of degree $\le k$.
\item $s$ has $k-1$ continuous derivatives in the whole interval $[x_0,x_n]$.
\end{itemize} 

}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
As an example, consider a spline function of degree $k=1$ defined as follows
\[
    s(x)=\left\{\begin{array}{cc} s_0(x)=a_0x+b_0 & x\in [x_0, x_1) \\   
                             s_1(x)=a_1x+b_1 & x\in [x_1, x_2) \\   
                             \dots & \dots \\
                             s_{n-1}(x)=a_{n-1}x+b_{n-1} & x\in 
                             [x_{n-1}, x_n] \end{array}\right.
\]

In this case the polynomial consists of series of straight lines 
connected to each other at every endpoint. The number of continuous
derivatives is then $k-1=0$, as expected when we deal with straight lines.
Such a polynomial is quite easy to construct given
$n+1$ points $x_0, x_1, \dots x_n$ and their corresponding 
function values. 
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
The most commonly used spline function is the one with $k=3$, the so-called
cubic spline function. 
Assume that we have in adddition to the $n+1$ knots a series of
functions values $y_0=f(x_0), y_1=f(x_1), \dots y_n=f(x_n)$.
By definition, the polynomials $s_{i-1}$ and $s_i$ 
are thence supposed to interpolate
the same point $i$, i.e.,
\[
    s_{i-1}(x_i)= y_i = s_i(x_i),
\]
with $1 \le i \le n-1$. In total we have $n$ polynomials of the 
type
\[
    s_i(x)=a_{i0}+a_{i1}x+a_{i2}x^2+a_{i2}x^3,
\]
yielding $4n$ coefficients to determine.
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
 Every subinterval provides
in addition the $2n$ conditions 
\[
    y_i = s(x_i),
\]
and 
\[
    s(x_{i+1})= y_{i+1},
\]
to be fulfilled. If we also assume that $s'$ and $s''$ are continuous,
then
\[
       s'_{i-1}(x_i)= s'_i(x_i),
\]
yields $n-1$ conditions. Similarly,
\[
       s''_{i-1}(x_i)= s''_i(x_i),
\]
results in additional $n-1$ conditions. In total we have $4n$ coefficients
and $4n-2$ equations to determine them, leaving us with $2$ degrees of 
freedom to be determined. 
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
Using the last equation we define two values for the second derivative,
namely
\[
       s''_{i}(x_i)= f_i,
\]
and 
\[
       s''_{i}(x_{i+1})= f_{i+1},
\]
and setting up a straight line between $f_i$ and $f_{i+1}$ we have
\[
   s_i''(x) = \frac{f_i}{x_{i+1}-x_i}(x_{i+1}-x)+
               \frac{f_{i+1}}{x_{i+1}-x_i}(x-x_i),
\]
and integrating twice one obtains
\[
   s_i(x) = \frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3
             +c(x-x_i)+d(x_{i+1}-x).
\]
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
Using the conditions $s_i(x_i)=y_i$ and $s_i(x_{i+1})=y_{i+1}$ 
we can in turn determine the constants $c$ and $d$ resulting in
\begin{eqnarray}
   s_i(x) =&\frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3 \nonumber  \\ 
            +&(\frac{y_{i+1}}{x_{i+1}-x_i}-\frac{f_{i+1}(x_{i+1}-x_i)}{6})
              (x-x_i)+
             (\frac{y_{i}}{x_{i+1}-x_i}-\frac{f_{i}(x_{i+1}-x_i)}{6})
             (x_{i+1}-x).
\end{eqnarray}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
How to determine the values of the second
derivatives $f_{i}$ and $f_{i+1}$? We use the continuity assumption 
of the first derivatives 
\[
    s'_{i-1}(x_i)= s'_i(x_i),
\]
and set $x=x_i$. Defining $h_i=x_{i+1}-x_i$ we obtain finally
the following expression
\[
   h_{i-1}f_{i-1}+2(h_{i}+h_{i-1})f_i+h_if_{i+1}=
   \frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1}),
\]
and introducing the shorthands $u_i=2(h_{i}+h_{i-1})$, 
$v_i=\frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1})$,
we can reformulate the problem as a set of linear equations to be 
solved  through e.g., Gaussian elemination
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
Gaussian elimination
\[
   \left[\begin{array}{cccccccc} u_1 & h_1 &0 &\dots & & & & \\
                                 h_1 & u_2 & h_2 &0 &\dots & & & \\
                                  0   & h_2 & u_3 & h_3 &0 &\dots & & \\
                               \dots& & \dots &\dots &\dots &\dots &\dots & \\
                                 &\dots & & &0 &h_{n-3} &u_{n-2} &h_{n-2} \\
                                 & && & &0 &h_{n-2} &u_{n-1} \end{array}\right]
   \left[\begin{array}{c} f_1 \\ 
                          f_2 \\
                          f_3\\
                          \dots \\
                          f_{n-2} \\ 
                          f_{n-1} \end{array} \right] =
   \left[\begin{array}{c} v_1 \\ 
                          v_2 \\
                          v_3\\
                          \dots \\
                          v_{n-2}\\
                          v_{n-1} \end{array} \right].
\]
Note that this is a set of tridiagonal equations and can be solved 
through only $O(n)$ operations.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
The functions supplied in the program library are $spline$ and $splint$.
In order to use cubic spline interpolation you need first to call  
\begin{verbatim}

spline(double x[], double y[], int n, double yp1, 
         double yp2, double y2[])
\end{verbatim}
This function takes as
input $x[0,..,n - 1]$ and $y[0,..,n - 1]$ containing a tabulation
$y_i = f(x_i)$ with $x_0 < x_1 < .. < x_{n - 1}$ 
together with the 
first derivatives  of $f(x)$ at $x_0$ and $x_{n-1}$, respectively. Then the
function returns $y2[0,..,n-1]$ which contains the second derivatives of
$f(x_i)$ at each point $x_i$. $n$ is the number of points.
This function provides the cubic spline interpolation for all subintervals
and is called only once.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Splines}
\begin{small}
{\scriptsize
 Thereafter, if you wish to make 
various interpolations, you need to call the function 
\begin{verbatim}

splint(double x[], double y[], double y2a[], int n, 
                    double x, double *y)
\end{verbatim}
which takes as input
the tabulated values $x[0,..,n - 1]$ and $y[0,..,n - 1]$ and the output 
y2a[0,..,n - 1] from $spline$. It returns the value $y$ corresponding
to the point $x$.
}
\end{small}
}


\section[Week 38]{Week 38}
\frame
{
  \frametitle{Week 38}
  \begin{block}{Eigenvalue problems and project 2}
\begin{itemize}
\item Monday: Repetition from last week and discussion of project 2
\item Jacobi's algorithm
\item Tuesday: 
\item Householder's algorithm and Francis' algorithm
\item Iterative methods for symmetric matrices: Lanczos' algorithm
\end{itemize}
  \end{block}
} 



\frame
{
  \frametitle{Eigenvalue Solvers}
\begin{small}
{\scriptsize
Let us consider the matrix {\bf A} of dimension n. The eigenvalues of
{\bf A} is defined through the matrix equation 
%
\[
   {\bf A}{\bf x^{(\nu)}} = \lambda^{(\nu)}{\bf x^{(\nu)}},
\]
%
where $\lambda^{(\nu)}$ are the eigenvalues and ${\bf x^{(\nu)}}$ the
corresponding eigenvectors.
Unless otherwise stated, when we use the wording eigenvector we mean the
right eigenvector. The left eigenvector is defined as 
\[
{\bf x^{(\nu)}}_L{\bf A} = \lambda^{(\nu)}{\bf x^{(\nu)}}_L
\]
The above right eigenvector problem is equivalent to a set of $n$ equations with $n$ unknowns
$x_i$.
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers}
\begin{small}
{\scriptsize
%
The eigenvalue problem can be rewritten as 

\[
   \left( {\bf A}-\lambda^{(\nu)} I \right) {\bf x^{(\nu)}} = 0,
\]
%
with $I$ being the unity matrix. This equation provides
a solution to the problem if and only if the determinant
is zero, namely
%
\[
   \left| {\bf A}-\lambda^{(\nu)}{\bf I}\right| = 0,
\]
%
which in turn means that the determinant is a polynomial
of degree $n$ in $\lambda$ and in general we will have 
$n$ distinct zeros. 
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers}
\begin{small}
{\scriptsize
The eigenvalues of a matrix 
${\bf A}\in {\mathbb{C}}^{n\times n}$
are thus the $n$ roots of its characteristic polynomial 
\[
P(\lambda) = det(\lambda{\bf I}-{\bf A}),
\]
or 
\[
  P(\lambda)= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right).
\]
The set of these roots is called the spectrum and is denoted as
$\lambda({\bf A})$.
If $\lambda({\bf A})=\left\{\lambda_1,\lambda_2,\dots ,\lambda_n\right\}$ then we have
\[
   det({\bf A})= \lambda_1\lambda_2\dots\lambda_n, 
\]
and if we define the trace of ${\bf A}$ as
\[
Tr({\bf A})=\sum_{i=1}^n a_{ii}\]
then
$Tr({\bf A})=\lambda_1+\lambda_2+\dots+\lambda_n$.
}
 \end{small}
 }











\frame
{
  \frametitle{Abel-Ruffini Impossibility Theorem}
\begin{small}
{\scriptsize
The Abel-Ruffini theorem (also known as Abel's impossibility theorem) 
states that there is no general solution in radicals to polynomial equations of degree five or higher.

The content of this theorem is frequently misunderstood. It does not assert that higher-degree polynomial equations are unsolvable. 
In fact, if the polynomial has real or complex coefficients, and we allow complex solutions, then every polynomial equation has solutions; this is the fundamental theorem of algebra. Although these solutions cannot always be computed exactly with radicals, they can be computed to any desired degree of accuracy using numerical methods such as the Newton-Raphson method or Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees.

The theorem only concerns the form that such a solution must take. The content of the theorem is 
that the solution of a higher-degree equation cannot in all cases be expressed in terms of the polynomial coefficients with a finite number of operations of addition, subtraction, multiplication, division and root extraction. Some polynomials of arbitrary degree, of which the simplest nontrivial example is the monomial equation $ax^n = b$, are always solvable with a radical.
}
 \end{small}
 }


\frame
{
  \frametitle{Abel-Ruffini Impossibility Theorem}
\begin{small}
{\scriptsize
The Abel-Ruffini theorem says that there are some fifth-degree equations whose solution cannot be so expressed. 
The equation $x^5 - x + 1 = 0$ is an example. Some other fifth degree equations can be solved by radicals, 
for example $x^5 - x^4 - x + 1 = 0$. The precise criterion that distinguishes between those equations that can be solved 
by radicals and those that cannot was given by Galois and is now part of Galois theory: 
a polynomial equation can be solved by radicals if and only if its Galois group is a solvable group.

Today, in the modern algebraic context, we say that second, third and fourth degree polynomial 
equations can always be solved by radicals because the symmetric groups $S_2, S_3$ and $S_4$ are solvable groups, 
whereas $S_n$ is not solvable for $n \ge 5$.
}
 \end{small}
 }








\frame
{
  \frametitle{Eigenvalue Solvers}
\begin{small}
{\scriptsize
In the present discussion we assume that our matrix is real and symmetric, that is 
${\bf A}\in {\mathbb{R}}^{n\times n}$.
The matrix ${\bf A}$ has $n$ eigenvalues
$\lambda_1\dots \lambda_n$ (distinct or not). Let ${\bf D}$ be the
diagonal matrix with the eigenvalues on the diagonal
%   
\[
{\bf D}=    \left( \begin{array}{ccccccc} \lambda_1 & 0 & 0   & 0    & \dots  &0     & 0 \\
                                0 & \lambda_2 & 0 & 0    & \dots  &0     &0 \\
                                0   & 0 & \lambda_3 & 0  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\lambda_{n-1} & \\
                                0   & \dots & \dots & \dots  &\dots       &0 & \lambda_n

             \end{array} \right).
\]
%
If ${\bf A}$ is real and symmetric then there exists a real orthogonal matrix ${\bf S}$ such that
\[
     {\bf S}^T {\bf A}{\bf S}= \mathrm{diag}(\lambda_1,\lambda_2,\dots ,\lambda_n),
\]
and for $j=1:n$ we have ${\bf A}{\bf S}(:,j) = \lambda_j {\bf S}(:,j)$.
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers}
\begin{small}
{\scriptsize
To obtain the eigenvalues of ${\bf A}\in {\mathbb{R}}^{n\times n}$,
the strategy is to
perform a series of similarity transformations on the original
matrix ${\bf A}$, in order to reduce it either into a  diagonal form as above
or into a  tridiagonal form. 

We say that a matrix ${\bf B}$ is a similarity
transform  of  ${\bf A}$ if 
%
\[
     {\bf B}= {\bf S}^T {\bf A}{\bf S}, \hspace*{1cm} \mbox{where} 
     \hspace*{1cm}  {\bf S}^T{\bf S}={\bf S}^{-1}{\bf S} ={\bf I}.
\]
%
The importance of a similarity transformation lies in the fact that
the resulting matrix has the same
eigenvalues, but the eigenvectors are in general different. 
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers}
\begin{small}
{\scriptsize
To prove this we
start with  the eigenvalue problem and a similarity transformed matrix ${\bf B}$.
%
\[
   {\bf Ax}=\lambda{\bf x} \hspace{1cm} \mathrm{and}\hspace{1cm} 
    {\bf B}= {\bf S}^T {\bf A}{\bf S}.
\]
%
We multiply the first equation on the left by ${\bf S}^T$ and insert
${\bf S}^{T}{\bf S} ={\bf I}$ between ${\bf A}$ and ${\bf x}$. Then we get
%
\begin{equation}
   ({\bf S^TAS})({\bf S^Tx})=\lambda{\bf S^Tx} ,
\end{equation}  
%
which is the same as 
\[
   {\bf B} \left ( {\bf S^Tx} \right ) = \lambda \left ({\bf S^Tx}
                             \right ).
\]
%
The variable  $\lambda$ is an eigenvalue of ${\bf B}$ as well, but with
eigenvector ${\bf S^Tx}$.
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers}
\begin{small}
{\scriptsize
The basic philosophy is to
%
\begin{itemize}
%
\item either apply subsequent similarity transformations (direct method) so that 
%
\begin{equation}
   {\bf S_N^T\dots S_1^TAS_1\dots S_N }={\bf D} ,
\end{equation}
%
\item  or apply subsequent similarity transformations so that 
 {\bf A} becomes tridiagonal (Householder) or upper/lower triangular (${\bf QR}$ method). 
Thereafter, techniques for obtaining
eigenvalues from tridiagonal matrices can be used.
\item or use so-called power methods
\item or use iterative methods (Krylov, Lanczos, Arnoldi). These methods are popular for huge matrix problems.
\end{itemize}
}
 \end{small}
 }




\frame
{
  \frametitle{Gaussian Elimination and Tridiagonal matrices, project 1}
\begin{small}
{\scriptsize
In project 1 we rewrote our original differential equation in terms of a discretized equation with approximations to the 
derivatives as
\[
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
\]
with $i=1,2,\dots, n$. We need to add to this system the two boundary conditions $u(a) =u_0$ and $u(b) = u_{n+1}$.
If we define a matrix 
\[
    {\bf A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 & 2 & -1 & & & \\
                           & -1 & 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 & 2 \\
                      \end{array} \right)
\]
and the corresponding vectors ${\bf u} = (u_1, u_2, \dots,u_n)^T$ and 
${\bf f}({\bf u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T$  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns 
\[
   {\bf A}{\bf u} = {\bf f}({\bf u}).
 \]
}
\end{small}
}



\frame
{
  \frametitle{Project 2, part a-b)}
\begin{small}
{\scriptsize
We are first interested in the solution of the radial part of Schr\"odinger's equation for one electron. This equation reads
\[
  -\frac{\hbar^2}{2 m} \left ( \frac{1}{r^2} \frac{d}{dr} r^2
  \frac{d}{dr} - \frac{l (l + 1)}{r^2} \right )R(r) 
     + V(r) R(r) = E R(r).
\]
In our case $V(r)$ is the harmonic oscillator potential $(1/2)kr^2$ with
$k=m\omega^2$ and $E$ is
the energy of the harmonic oscillator in three dimensions.
The oscillator frequency is $\omega$ and the energies are
\[
E_{nl}=  \hbar \omega \left(2n+l+\frac{3}{2}\right),
\]
with $n=0,1,2,\dots$ and $l=0,1,2,\dots$.
 
Since we have made a transformation to spherical coordinates it means that 
$r\in [0,\infty)$.  
The quantum number
$l$ is the orbital momentum of the electron.  
%
Then we substitute $R(r) = (1/r) u(r)$ and obtain
%
\[
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r) 
       + \left ( V(r) + \frac{l (l + 1)}{r^2}\frac{\hbar^2}{2 m}
                                    \right ) u(r)  = E u(r) .
\]
%
The boundary conditions are $u(0)=0$ and $u(\infty)=0$.
}
 \end{small}
 }





\frame
{
  \frametitle{Project 2, part a-b)}
\begin{small}
{\scriptsize
We introduce a dimensionless variable $\rho = (1/\alpha) r$
where $\alpha$ is a constant with dimension length and get
% 
\[
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       + \left ( V(\rho) + \frac{l (l + 1)}{\rho^2}
         \frac{\hbar^2}{2 m\alpha^2} \right ) u(\rho)  = E u(\rho) .
\]
%
In project 2 we set $l=0$.
Inserting $V(\rho) = (1/2) k \alpha^2\rho^2$ we end up with
\[
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       + \frac{k}{2} \alpha^2\rho^2u(\rho)  = E u(\rho) .
\]
We multiply thereafter with $2m\alpha^2/\hbar^2$ on both sides and obtain
\[
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
\]
}
 \end{small}
 }


\frame
{
  \frametitle{Project 2, part a-b)}
\begin{small}
{\scriptsize
We have from the previous slide
\[
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
\]
The constant $\alpha$ can now be fixed
so that
\[
\frac{mk}{\hbar^2} \alpha^4 = 1,
\]
or 
\[
\alpha = \left(\frac{\hbar^2}{mk}\right)^{1/4}.
\]
Defining 
\[
\lambda = \frac{2m\alpha^2}{\hbar^2}E,
\]
we can rewrite Schr\"odinger's equation as
\[
  -\frac{d^2}{d\rho^2} u(\rho) + \rho^2u(\rho)  = \lambda u(\rho) .
\]
This is the first equation to solve numerically. In three dimensions 
the eigenvalues for $l=0$ are 
$\lambda_0=3,\lambda_1=7,\lambda_2=11,\dots .$

}
 \end{small}
 }




\frame
{
  \frametitle{Project 2, part a-b)}
\begin{small}
{\scriptsize
We use the by now standard
expression for the second derivative of a function $u$
\begin{equation}
    u''=\frac{u(\rho+h) -2u(\rho) +u(\rho-h)}{h^2} +O(h^2),
    \label{eq:diffoperation}
\end{equation} 
where $h$ is our step.
Next we define minimum and maximum values for the variable $\rho$,
$\rho_{\mathrm{min}}=0$  and $\rho_{\mathrm{max}}$, respectively.
You need to check your results for the energies against different values
$\rho_{\mathrm{max}}$, since we cannot set
$\rho_{\mathrm{max}}=\infty$. 

}
 \end{small}
 }


\frame
{
  \frametitle{Project 2, part a-b)}
\begin{small}
{\scriptsize

With a given number of steps, $n_{\mathrm{step}}$, we then 
define the step $h$ as
\[
  h=\frac{\rho_{\mathrm{max}}-\rho_{\mathrm{min}} }{n_{\mathrm{step}}}.
\]
Define an arbitrary value of $\rho$ as 
\[
    \rho_i= \rho_{\mathrm{min}} + ih \hspace{1cm} i=0,1,2,\dots , n_{\mathrm{step}}
\]
we can rewrite the Schr\"odinger equation for $\rho_i$ as
\[
-\frac{u(\rho_i+h) -2u(\rho_i) +u(\rho_i-h)}{h^2}+\rho_i^2u(\rho_i)  = \lambda u(\rho_i),
\]
or in  a more compact way
\[
-\frac{u_{i+1} -2u_i +u_{i-1}}{h^2}+\rho_i^2u_i=-\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}+V_iu_i  = \lambda u_i,
\]
where $V_i=\rho_i^2$ is the harmonic oscillator potential.
}
 \end{small}
 }





\frame
{
  \frametitle{Project 2, part a-b)}
\begin{small}
{\scriptsize
Define first the diagonal matrix element
\[
   d_i=\frac{2}{h^2}+V_i,
\]
and the non-diagonal matrix element 
\[
   e_i=-\frac{1}{h^2}.
\]
In this case the non-diagonal matrix elements are given by a mere constant.
{\em All non-diagonal matrix elements are equal}.
With these definitions the Schr\"odinger equation takes the following form
\[
d_iu_i+e_{i-1}u_{i-1}+e_{i+1}u_{i+1}  = \lambda u_i,
\]
where $u_i$ is unknown. We can write the 
latter equation as a matrix eigenvalue problem 
\begin{equation}
    \left( \begin{array}{ccccccc} d_1 & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2 & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3 & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{n_{\mathrm{step}}-2} & e_{n_{\mathrm{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n_{\mathrm{step}}-1} & d_{n_{\mathrm{step}}-1}

             \end{array} \right)      \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right)=\lambda \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right) 
      \label{eq:sematrix}
\end{equation} 
}
 \end{small}
 }


\frame
{
  \frametitle{Project 2, part a-b)}
\begin{small}
{\scriptsize
or if we wish to be more detailed, we can write the tridiagonal matrix as
\begin{equation}
    \left( \begin{array}{ccccccc} \frac{2}{h^2}+V_1 & -\frac{1}{h^2} & 0   & 0    & \dots  &0     & 0 \\
                                -\frac{1}{h^2} & \frac{2}{h^2}+V_2 & -\frac{1}{h^2} & 0    & \dots  &0     &0 \\
                                0   & -\frac{1}{h^2} & \frac{2}{h^2}+V_3 & -\frac{1}{h^2}  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\frac{2}{h^2}+V_{n_{\mathrm{step}}-2} & -\frac{1}{h^2}\\
                                0   & \dots & \dots & \dots  &\dots       &-\frac{1}{h^2} & \frac{2}{h^2}+V_{n_{\mathrm{step}}-1}

             \end{array} \right)  
\label{eq:matrixse} 
\end{equation} 

Recall that the solutions are known via the boundary conditions at
$i=n_{\mathrm{step}}$ and at the other end point, that is for  $\rho_0$.
The solution is zero in both cases.
}
 \end{small}
 }


\frame
{
  \frametitle{Project 2, part c)}
\begin{small}
{\scriptsize

We are going to study two electrons in a harmonic oscillator well which
also interact via a repulsive Coulomb interaction.
Let us start with the single-electron equation written as
\[
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r) 
       + \frac{1}{2}k r^2u(r)  = E^{(1)} u(r),
\]
where $E^{(1)}$ stands for the energy with one electron only.
For two electrons with no repulsive Coulomb interaction, we have the following 
Schr\"odinger equation
\[
\left(  -\frac{\hbar^2}{2 m} \frac{d^2}{dr_1^2} -\frac{\hbar^2}{2 m} \frac{d^2}{dr_2^2}+ \frac{1}{2}k r_1^2+ \frac{1}{2}k r_2^2\right)u(r_1,r_2)  = E^{(2)} u(r_1,r_2) .
\]
}
 \end{small}
 }




\frame
{
  \frametitle{Project 2, part c)}
\begin{small}
{\scriptsize
Note that we deal with a two-electron wave function $u(r_1,r_2)$ and 
two-electron energy $E^{(2)}$.

With no interaction this can be written out as the product of two
single-electron wave functions, that is we have a solution on closed form.

We introduce the relative coordinate ${\bf r} = {\bf r}_1-{\bf r}_2$
and the center-of-mass coordinate ${\bf R} = 1/2({\bf r}_1+{\bf r}_2)$.
With these new coordinates, the radial Schr\"odinger equation reads
\[
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2} -\frac{\hbar^2}{4 m} \frac{d^2}{dR^2}+ \frac{1}{4} k r^2+  kR^2\right)u(r,R)  = E^{(2)} u(r,R).
\]

}
 \end{small}
 }






\frame
{
  \frametitle{Project 2, part c)}
\begin{small}
{\scriptsize
The equations for $r$ and $R$ can be separated via the ansatz for the 
wave function $u(r,R) = \psi(r)\phi(R)$ and the energy is given by the sum
of the relative energy $E_r$ and the center-of-mass energy $E_R$, that
is
\[
E^{(2)}=E_r+E_R.
\]

We add then the repulsive Coulomb interaction between two electrons,
namely a term 
\[
V(r_1,r_2) = \frac{\beta e^2}{|{\bf r}_1-{\bf r}_2|}=\frac{\beta e^2}{r},
\]
with $\beta e^2=1.44$ eVnm.

}
 \end{small}
 }
\frame
{
  \frametitle{Project 2, part c)}
\begin{small}
{\scriptsize

Adding this term, the $r$-dependent Schr\"odinger equation becomes
\[
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2}+ \frac{1}{4}k r^2+\frac{\beta e^2}{r}\right)\psi(r)  = E_r \psi(r).
\]
This equation is similar to the one we had previously in parts (a) and (b) 
and we introduce
again a dimensionless variable $\rho = r/\alpha$. Repeating the same
steps, we arrive at 
\[
  -\frac{d^2}{d\rho^2} \psi(\rho) 
       + \frac{mk}{4\hbar^2} \alpha^4\rho^2\psi(\rho)+\frac{m\alpha \beta e^2}{\rho\hbar^2}\psi(\rho)  = 
\frac{m\alpha^2}{\hbar^2}E_r \psi(\rho) .
\]

}
 \end{small}
 }
\frame
{
  \frametitle{Project 2, part c)}
\begin{small}
{\scriptsize
We want to manipulate this equation further to make it as similar to that in (a)
as possible. We define a 'frequency' 
\[
\omega_r^2=\frac{1}{4}\frac{mk}{\hbar^2} \alpha^4,
\]
and fix the constant $\alpha$ by requiring 
\[
\frac{m\alpha \beta e^2}{\hbar^2}=1
\]
or 
\[
\alpha = \frac{\hbar^2}{m\beta e^2}.
\]
}
 \end{small}
 }
\frame
{
  \frametitle{Project 2, part c)}
\begin{small}
{\scriptsize
Defining 
\[
\lambda = \frac{m\alpha^2}{\hbar^2}E,
\]
we can rewrite Schr\"odinger's equation as
\[
  -\frac{d^2}{d\rho^2} \psi(\rho) + \omega_r^2\rho^2\psi(\rho) +\frac{1}{\rho}\psi(\rho) = \lambda \psi(\rho).
\]
}
 \end{small}
 }

\frame
{
  \frametitle{Project 2, part c)}
\begin{small}
{\scriptsize
We treat $\omega_r$ as a parameter which reflects the strength of the oscillator potential.

Here we will study the cases $\omega_r = 0.01$, $\omega_r = 0.5$, $\omega_r =1$,
and $\omega_r = 5$   
for the ground state only, that is the lowest-lying state.


With no repulsive Coulomb interaction 
you should get a result which corresponds to 
the relative energy of a non-interacting system.   
Make sure your results are 
stable as functions of $\rho_{\mathrm{max}}$ and the number of steps.

We are only interested in the ground state with $l=0$. We omit the 
center-of-mass energy.

For specific oscillator frequencies, the above equation has closed answers,
see the article by M.~Taut, Phys. Rev. A 48, 3561 - 3566 (1993).
The article can be retrieved from the following web address
\url{http://prola.aps.org/abstract/PRA/v48/i5/p3561_1}.
}
 \end{small}
 }



\frame
{
  \frametitle{Eigenvalue Solvers}
%\begin{small}
%{\scriptsize
One speaks normally of two main approaches to solving the eigenvalue problem.

\bigskip

The first is the formal method, involving determinants and the 
\textit{characteristic polynomial}. This proves how many eigenvalues 
there are, and is the way most of you 
learned about how to solve the eigenvalue problem, but for 
matrices of dimensions greater than 2 or 3, it is rather
impractical.

\bigskip

The other general approach is to use similarity or unitary 
tranformations  to reduce a matrix to diagonal form. Almost always 
this is done in two steps: first reduce to for example a \textit{tridiagonal} 
form, and then to diagonal form. The main algorithms we will discuss 
in detail, Jacobi's and  Householder's  (so-called direct method) and Lanczos algorithms (an iterative method), follow this 
methodology. 
%}
% \end{small}
 }




\frame
{
  \frametitle{Diagonalization methods, direct methods}
  \begin{block}{}
\begin{small}
{\scriptsize
{\bf Direct or non-iterative methods}  require for matrices of dimensionality $n\times n$ typically $O(n^3)$ operations. These methods are normally called standard methods and are used for dimensionalities
$n \sim 10^5$ or smaller. A brief historical overview  
\begin{center}
\begin{tabular}{rrr}\hline
Year&$n$& \\\hline
  1950  & $n=20$    &(Wilkinson)      \\
  1965  &$n=200$    &(Forsythe et al.)      \\
  1980  &$n=2000$    &Linpack     \\
  1995  &$n=20000$    &Lapack      \\
  2012  &$n\sim 10^5$    &Lapack      \\ \hline
\end{tabular}  
\end{center}
shows that in the course of 60 years the dimension that  direct diagonalization methods can handle  has increased by almost a factor of
$10^4$. However, it pales beside the progress achieved by computer hardware, from flops to petaflops, a factor of almost $10^{15}$. We see clearly played out in history the $O(n^3)$ bottleneck  of direct matrix algorithms.\newline
Sloppily speaking, when  $n\sim 10^4$ is cubed we have $O(10^{12})$ operations, which is smaller than the $10^{15}$ increase in flops.  
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Diagonalization methods}
  \begin{block}{Why iterative methods?}
%\begin{small}
%{\scriptsize
If the matrix to diagonalize is large and sparse, direct methods simply become impractical, 
also because
many of the direct methods tend to destroy sparsity. As a result large dense matrices may arise during the diagonalization procedure.  The idea behind iterative methods is to project the 
$n-$dimensional problem in smaller spaces, so-called Krylov subspaces. 
Given a matrix $\hat{A}$ and a vector $\hat{v}$, the associated Krylov sequences of vectors
(and thereby subspaces) 
$\hat{v}$, $\hat{A}\hat{v}$, $\hat{A}^2\hat{v}$, $\hat{A}^3\hat{v},\dots$, represent
successively larger Krylov subspaces.  \newline
\begin{center}
\begin{tabular}{rrr}\hline
Matrix&$\hat{A}\hat{x}=\hat{b}$&$\hat{A}\hat{x}=\lambda\hat{x}$ \\\hline
   $\hat{A}=\hat{A}^*$ & Conjugate gradient    &Lanczos      \\
  $\hat{A}\ne \hat{A}^*$  &GMRES etc    &Arnoldi      \\ \hline
\end{tabular}  
\end{center}
%}
%\end{small}
  \end{block}
}

\frame
{
  \frametitle{Important Matrix and vector handling packages}
\begin{small}
{\scriptsize
The Numerical Recipes codes have been rewritten in Fortran 90/95 and C/C++ by us.
The original source codes are taken from the widely used software
package LAPACK, which follows two other popular packages developed in the 1970s, 
namely EISPACK
and LINPACK.
\begin{itemize}
\item LINPACK: package for linear equations 
and least square problems.
\item LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems.
From LAPACK's website \url{http://www.netlib.org}  it is 
possible to download for free all source codes from 
this library. Both C/C++ and Fortran versions are available.
\item BLAS (I, II and III): (Basic Linear Algebra Subprograms) 
are routines that provide standard building blocks for performing basic vector and matrix operations.  
Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations.
Highly parallelized and efficient codes, all available for download from 
\url{http://www.netlib.org}. 
\end{itemize}
}
\end{small}
} 





\frame
{
  \frametitle{Eigenvalue Solvers, Jacobi}
\begin{small}
{\scriptsize
Consider an  ($n\times n$) orthogonal transformation matrix 
%
\[
{\bf S}=
 \left( 
   \begin{array}{cccccccc}
   1  &    0  & \dots &   0        &    0  & \dots & 0 &   0       \\
   0  &    1  & \dots &   0        &    0  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\ 
   0  &    0  & \dots & \cos\theta  &    0  & \dots & 0 & \sin\theta \\
   0  &    0  & \dots &   0        &    1  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\
   0  &    0  & \dots &  -\sin\theta        &    0  & \dots & 1 &   \cos\theta       \\ 
   0  &    0  & \dots & 0 & \dots & \dots & 0 & 1  
   \end{array}
 \right)
\]
%
with property ${\bf S^{T}} = {\bf S^{-1}}$.
It performs a plane rotation around an angle $\theta$ in the Euclidean 
$n-$dimensional space. 
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Jacobi}
\begin{small}
{\scriptsize
It means that its matrix elements that differ
from zero are given by
%
\[
    s_{kk}= s_{ll}=\cos\theta, 
    s_{kl}=-s_{lk}= -\sin\theta, 
    s_{ii}=-s_{ii}=1\hspace{0.5cm} i\ne k \hspace{0.5cm} i \ne l,
\]
%
A similarity transformation 
%
\[
     {\bf B}= {\bf S}^T {\bf A}{\bf S},
\]
%
results in 
%
\begin{eqnarray*}
b_{ik} &=& a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} &=& a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} &=& a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} &=& a_{ll}\cos^2\theta +2a_{kl}\cos\theta sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} &=& (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{eqnarray*}
%
The angle $\theta$ is  arbitrary. The recipe is to choose  $\theta$ so that all
non-diagonal matrix elements $b_{kl}$ become zero.  

}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Jacobi}
\begin{small}
{\scriptsize
The main idea is thus to reduce systematically the 
norm of the 
off-diagonal matrix elements  of a matrix  ${\bf A}$ 
\[
\mathrm{off}({\bf A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}.
\]
 To demonstrate the algorithm, we consider the  simple $2\times 2$  similarity transformation
of the full matrix. The matrix is symmetric, we single out $ 1\le k < l \le n$  and 
use the abbreviations $c=\cos\theta$ and $s=\sin\theta$ to obtain

\[
 \left( \begin{array}{cc} b_{kk} & 0 \\
                          0 & b_{ll} \\\end{array} \right)  =  \left( \begin{array}{cc} c & -s \\
                          s &c \\\end{array} \right)  \left( \begin{array}{cc} a_{kk} & a_{kl} \\
                          a_{lk} &a_{ll} \\\end{array} \right) \left( \begin{array}{cc} c & s \\
                          -s & c \\\end{array} \right).
\]
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Jacobi}
\begin{small}
{\scriptsize
We require that the non-diagonal matrix elements $b_{kl}=b_{lk}=0$, implying that 
\[
a_{kl}(c^2-s^2)+(a_{kk}-a_{ll})cs = b_{kl} = 0.
\]
If $a_{kl}=0$ one sees immediately that $\cos\theta = 1$ and $\sin\theta=0$.

The Frobenius norm of an orthogonal transformation is always preserved. The Frobenius norm is defined
as 
\[
||{\bf A}||_F =  \sqrt{\sum_{i=1}^n\sum_{j=1}^n |a_{ij}|^2}.
\]
This means that for our $2\times 2$ case  we have
\[
2a_{kl}^2+a_{kk}^2+a_{ll}^2 = b_{kk}^2+b_{ll}^2,
\]
which leads to
\[
\mathrm{off}({\bf B})^2 = ||{\bf B}||_F^2-\sum_{i=1}^nb_{ii}^2=\mathrm{off}({\bf A})^2-2a_{kl}^2,
\]
since 
\[
||{\bf B}||_F^2-\sum_{i=1}^nb_{ii}^2=||{\bf A}||_F^2-\sum_{i=1}^na_{ii}^2+(a_{kk}^2+a_{ll}^2 -b_{kk}^2-b_{ll}^2).
\]
This results means that  the matrix ${\bf A}$ moves closer to diagonal form  for each transformation.

}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Jacobi}
\begin{small}
{\scriptsize
Defining the quantities $\tan\theta = t= s/c$ and
\[\cot 2\theta=\tau = \frac{a_{ll}-a_{kk}}{2a_{kl}},
\]
we obtain the quadratic equation (using $\cot 2\theta=1/2(\cot \theta-\tan\theta)$
\[
t^2+2\tau t-1= 0,
\]
resulting in 
\[
  t = -\tau \pm \sqrt{1+\tau^2},
\]
and $c$ and $s$ are easily obtained via
\[
   c = \frac{1}{\sqrt{1+t^2}},
\]
and $s=tc$.  Choosing $t$ to be the smaller of the roots ensures that $|\theta| \le \pi/4$ and has the 
effect of minimizing the difference between the matrices ${\bf B}$ and ${\bf A}$ since
\[
||{\bf B}-{\bf A}||_F^2=4(1-c)\sum_{i=1,i\ne k,l}^n(a_{ik}^2+a_{il}^2) +\frac{2a_{kl}^2}{c^2}.
\]
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Jacobi algo}
\begin{small}
{\scriptsize
\begin{itemize}
   \item Choose a tolerance $\epsilon$, making it a small number, typically $10^{-8}$ or smaller.
   \item Setup a \lstinline{while}-test  where one compares the norm of the newly computed off-diagonal
matrix elements  \[ \mathrm{off}({\bf A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}   >  \epsilon. \]
   \item Now choose the matrix elements $a_{kl}$ so that we have those with largest value, that is
$|a_{kl}|=\mathrm{max}_{i\ne j} |a_{ij}|$.
\item Compute thereafter $\tau = (a_{ll}-a_{kk})/2a_{kl}$, $\tan\theta$, $\cos\theta$ and
$\sin\theta$.
\item Compute thereafter the similarity transformation for this set of values $(k,l)$, obtaining the 
new matrix ${\bf B}= {\bf S}(k,l,\theta)^T {\bf A}{\bf S}(k,l,\theta)$.
   \item Compute the new norm of the off-diagonal matrix elements and continue till you have satisfied 
$\mathrm{off}({\bf B})  \le  \epsilon$
\end{itemize}


The convergence rate of the Jacobi method is however poor, one needs typically $3n^2-5n^2$ rotations and each rotation 
requires $4n$ operations, resulting in a total of $12n^3-20n^3$ operations in order to zero out non-diagonal matrix elements.
}
 \end{small}
 }



\frame
{
  \frametitle{Jacobi's method, an example to convice you about the algorithm}
\begin{small}
{\scriptsize
We specialize to a symmetric $3\times 3 $ matrix ${\bf A}$.
We start the process as follows (assuming that $a_{23}=a_{32}$ is the largest non-diagonal)
with $c=\cos{\theta}$ and $s=\sin{\theta}$
%
\[
 {\bf B} =
      \left( \begin{array}{ccc} 
                1 & 0 & 0    \\
                0 & c & -s     \\
                0 & s & c
             \end{array} \right)\left( \begin{array}{ccc} 
                a_{11} & a_{12} & a_{13}    \\
                a_{21} & a_{22} & a_{23}     \\
                a_{31} & a_{32} & a_{33}
             \end{array} \right)
              \left( \begin{array}{ccc} 
                1 & 0 & 0    \\
                0 & c & s     \\
                0 & -s & c
             \end{array} \right).
\]
We will choose the angle $\theta$ in order to have $a_{23}=a_{32}=0$.
We get (symmetric matrix)
\[
 {\bf B} =\left( \begin{array}{ccc} 
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
\]
Note that $a_{11}$ is unchanged! As it should.
}
 \end{small}
 }




\frame
{
  \frametitle{Jacobi's method, an example to convice you about the algorithm}
\begin{small}
{\scriptsize
We have
\[
 {\bf B} =\left( \begin{array}{ccc} 
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
\]
or
\begin{eqnarray*}
b_{11} &=& a_{11} \\
b_{12} &=& a_{12}\cos\theta - a_{13}\sin\theta , 1 \ne 2, 1 \ne 3 \\
b_{13} &=& a_{13}\cos\theta + a_{12}\sin\theta , 1 \ne 2, 1 \ne 3 \nonumber\\
b_{22} &=& a_{22}\cos^2\theta - 2a_{23}\cos\theta \sin\theta +a_{33}\sin^2\theta\nonumber\\
b_{33} &=& a_{33}\cos^2\theta +2a_{23}\cos\theta \sin\theta +a_{22}\sin^2\theta\nonumber\\
b_{23} &=& (a_{22}-a_{33})\cos\theta \sin\theta +a_{23}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{eqnarray*}
We will fix the angle $\theta$ so that $b_{23}=0$.
}
 \end{small}
 }


\frame
{
  \frametitle{Jacobi's method, an example to convice you about the algorithm}
\begin{small}
{\scriptsize
We get then a new matrix
\[
 {\bf B} =\left( \begin{array}{ccc} 
                b_{11} & b_{12}& b_{13}    \\
                b_{12}& b_{22}& 0    \\
                b_{13}& 0& a_{33}
             \end{array} \right).
\]
We repeat then assuming that $b_{12}$ is the largest non-diagonal matrix element and get a
new matrix 
\[
 {\bf C} =
      \left( \begin{array}{ccc} 
                c & -s & 0    \\
                s & c & 0     \\
                0 & 0 & 1
             \end{array} \right)\left( \begin{array}{ccc} 
                b_{11} & b_{12} & b_{13}    \\
                b_{12} & b_{22} & 0     \\
                b_{13} & 0 & b_{33}
             \end{array} \right)
              \left( \begin{array}{ccc} 
                c & s & 0    \\
                -s & c & 0     \\
                0 & 0 & 1
             \end{array} \right).
\]
We continue this process till all non-diagonal matrix elements are zero (ideally).
You will notice that performing the above operations that the matrix element 
$b_{23}$ which was previous zero becomes different from zero.  This is one of the problems which slows
down the jacobi procedure.


}
 \end{small}
 }





\frame
{
  \frametitle{Jacobi's method, an example to convice you about the algorithm}
\begin{small}
{\scriptsize
The more general expression for the new matrix elements are
\begin{eqnarray*}
b_{ii} &=& a_{ii}, i \ne k, i \ne l \\
b_{ik} &=& a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} &=& a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} &=& a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} &=& a_{ll}\cos^2\theta +2a_{kl}\cos\theta \sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} &=& (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{eqnarray*}
This is what we will need to code.
}
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Jacobi code example}
\begin{small}
{\scriptsize
Main part
\lstset{language=c++}  
\begin{lstlisting}
//  we have defined a matrix A and a matrix R for the eigenvector, both of dim n x n
//  The final matrix R has the eigenvectors in its row elements, it is set to one
//  for the diagonal elements in the beginning, zero else.
....
double tolerance = 1.0E-10; 
int iterations = 0;
while ( maxnondiag > tolerance && iterations <= maxiter)
{
   int p, q;
   maxnondiag  = offdiag(A, p, q, n);
   Jacobi_rotate(A, R, p, q, n);
   iterations++;
}
...
\end{lstlisting} 
}
\end{small}
}
 

\lstset{language=c++}  
\begin{lstlisting}


\end{lstlisting} 


\frame[containsverbatim]
{
  \frametitle{Jacobi code example}
\begin{small}
{\scriptsize
Finding the max nondiagonal element
\lstset{language=c++}  
\begin{lstlisting}
//  the offdiag function
double offdiag(double **A, int p, int q, int n);
{
   double max;
   for (int i = 0; i < n; ++i)
   {
       for ( int j = i+1; j < n; ++j)
       {
           double aij = fabs(A[i][j]);
           if ( aij > max)
           { 
              max = aij;  p = i; q = j;
           }
       }
   }
   return max;
}
...
\end{lstlisting}
}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{Jacobi code example}
\begin{small}
{\scriptsize
Finding the new matrix elements
\lstset{language=c++}  
\begin{lstlisting}
void Jacobi_rotate ( double ** A, double ** R, int k, int l, int n )
{
  double s, c;
  if ( A[k][l] != 0.0 ) {
    double t, tau;
    tau = (A[l][l] - A[k][k])/(2*A[k][l]);
    
    if ( tau >= 0 ) {
      t = 1.0/(tau + sqrt(1.0 + tau*tau));
    } else {
      t = -1.0/(-tau +sqrt(1.0 + tau*tau));
    }
    
    c = 1/sqrt(1+t*t);
    s = c*t;
  } else {
    c = 1.0;
    s = 0.0;
  }
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Jacobi code example}
\begin{small}
{\scriptsize
\lstset{language=c++}  
\begin{lstlisting}
  double a_kk, a_ll, a_ik, a_il, r_ik, r_il;
  a_kk = A[k][k];
  a_ll = A[l][l];
  A[k][k] = c*c*a_kk - 2.0*c*s*A[k][l] + s*s*a_ll;
  A[l][l] = s*s*a_kk + 2.0*c*s*A[k][l] + c*c*a_ll;
  A[k][l] = 0.0;  // hard-coding non-diagonal elements by hand
  A[l][k] = 0.0;  // same here
  for ( int i = 0; i < n; i++ ) {
    if ( i != k && i != l ) {
      a_ik = A[i][k];
      a_il = A[i][l];
      A[i][k] = c*a_ik - s*a_il;
      A[k][i] = A[i][k];
      A[i][l] = c*a_il + s*a_ik;
      A[l][i] = A[i][l];
    }
\end{lstlisting}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Jacobi code example}
\begin{small}
{\scriptsize
and finally the new eigenvectors
\lstset{language=c++}  
\begin{lstlisting}
    r_ik = R[i][k];
    r_il = R[i][l];

    R[i][k] = c*r_ik - s*r_il;
    R[i][l] = c*r_il + s*r_ik;
  }
  return;
} // end of function jacobi_rotate
\end{lstlisting}
}
\end{small}
}



\section[Week 39]{Week 39}
\frame
{
  \frametitle{Week 39}
  \begin{block}{Eigenvalue problems and differential equations (change of plans)}
\begin{itemize}
\item Monday: Brief repetition from last week, with discussion of project 2.
\item Discussion of Householder's and Francis' algorithm (not finished last week)
\item Discussion of  Lanczos' method (also optional part of project 2)
\item Tuesday: 
\item Introduction to differential equations
\item Differential equations, general properties.
\item Runge-Kutta methods
\end{itemize}
The material on differential equations is covered by chapters 8, 9 and 10.
  \end{block}
} 





\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
The first step  consists in finding
an orthogonal  matrix ${\bf S}$ which is the product of $(n-2)$ orthogonal matrices 
%
\[ 
   {\bf S}={\bf S}_1{\bf S}_2\dots{\bf S}_{n-2},
\]
%
each of which successively transforms one row and one column of ${\bf A}$ into the 
required tridiagonal form. Only $n-2$ transformations are required, since the last two
elements are already in tridiagonal form. In order to determine each ${\bf S_i}$ let us
see what happens after the first multiplication, namely,
%
\[
    {\bf S}_1^T{\bf A}{\bf S}_1=    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &a'_{23}  & \dots    & \dots  &\dots &a'_{2n} \\
                                0   & a'_{32} &a'_{33}  & \dots    & \dots  &\dots &a'_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & a'_{n2} &a'_{n3}  & \dots    & \dots  &\dots &a'_{nn} \\

             \end{array} \right) 
\]
%
where the primed quantities represent a matrix ${\bf A}'$ of dimension
$n-1$ which will subsequentely be transformed by ${\bf S_2}$.
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
The factor  $e_1$ is a possibly non-vanishing element. The next
transformation produced by ${\bf S_2}$ has the same effect as  ${\bf
S_1}$ but now on the submatirx ${\bf A^{'}}$ only
%
\[
   \left ({\bf S}_{1}{\bf S}_{2} \right )^{T} {\bf A}{\bf S}_{1} {\bf S}_{2}
 = \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &e_2  & 0   & \dots  &\dots &0 \\
                                0   & e_2 &a''_{33}  & \dots    & \dots  &\dots &a''_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & 0 &a''_{n3}  & \dots    & \dots  &\dots &a''_{nn} \\

             \end{array} \right) 
\]
%
{\bf Note that the effective size of the matrix on which we apply the transformation reduces
for every new step. In the previous Jacobi method each similarity
transformation is in principle performed on the full size of the original matrix.}
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
After a series of such transformations, we end with a set of diagonal
matrix elements
%
\[
  a_{11}, a'_{22}, a''_{33}\dots a^{n-1}_{nn},
\]
%
and off-diagonal matrix elements 
%
\[
   e_1, e_2,e_3,  \dots, e_{n-1}.
\]
%
The resulting matrix reads
%
\[
{\bf S}^{T} {\bf A} {\bf S} = 
    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & a''_{33} & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &a^{(n-1)}_{n-2} & e_{n-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n-1} & a^{(n-1)}_{n-1}

             \end{array} \right) .
\]
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
It remains to find a recipe for determining the transformation ${\bf S}_n$.
We illustrate the method for ${\bf S}_1$ which we assume takes the form
%
\[
    {\bf S_{1}} = \left( \begin{array}{cc} 1 & {\bf 0^{T}} \\
                              {\bf 0}& {\bf P} \end{array} \right),
\]
%
with ${\bf 0^{T}}$ being a zero row vector, ${\bf 0^{T}} = \{0,0,\cdots\}$
of dimension $(n-1)$. The matrix ${\bf P}$  is symmetric 
with dimension ($(n-1) \times (n-1)$) satisfying
${\bf P}^2={\bf I}$  and ${\bf P}^T={\bf P}$. 
A possible choice which fullfils the latter two requirements is 
%
\[
    {\bf P}={\bf I}-2{\bf u}{\bf u}^T,
\]
%
where ${\bf I}$ is the $(n-1)$ unity matrix and ${\bf u}$ is an $n-1$
column vector with norm ${\bf u}^T{\bf u}$ (inner product).
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
 Note that ${\bf u}{\bf u}^T$ is an outer product giving a
matrix of dimension ($(n-1) \times (n-1)$). 
Each matrix element of ${\bf P}$ then reads
%
\[
   P_{ij}=\delta_{ij}-2u_iu_j,
\]
%
where $i$ and $j$ range from $1$ to $n-1$. Applying the transformation  
${\bf S}_1$ results in 
\[
   {\bf S}_1^T{\bf A}{\bf S}_1 =  \left( \begin{array}{cc} a_{11} & ({\bf Pv})^T \\
                              {\bf Pv}& {\bf A}' \end{array} \right) ,
\]
%
where ${\bf v^{T}} = \{a_{21}, a_{31},\cdots, a_{n1}\}$ and {\bf P}
must satisfy (${\bf Pv})^{T} = \{k, 0, 0,\cdots \}$. Then
%
\be
    {\bf Pv} = {\bf v} -2{\bf u}( {\bf u}^T{\bf v})= k {\bf e},
    \label{eq:palpha}
\ee
with ${\bf e^{T}} = \{1,0,0,\dots 0\}$.
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
Solving the latter equation gives us ${\bf u}$ and thus the needed transformation
${\bf P}$. We do first however need to compute the scalar $k$ by taking the scalar
product of the last equation with its transpose and using the fact that ${\bf P}^2={\bf I}$.
We get then
%
\[
   ({\bf Pv})^T{\bf Pv} = k^{2} = {\bf v}^T{\bf v}=
   |v|^2 = \sum_{i=2}^{n}a_{i1}^2,
\]
%
which determines the constant $ k = \pm v$.
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
 Now we can rewrite Eq.\ (\ref{eq:palpha})
as 
\[
    {\bf v} - k{\bf e} = 2{\bf u}( {\bf u}^T{\bf v}),
\]
and taking the scalar product of this equation with itself and obtain
\be
    2( {\bf u}^T{\bf v})^2=(v^2\pm a_{21}v),
    \label{eq:pmalpha}
\ee
which finally determines 
\[
    {\bf u}=\frac{{\bf v}-k{\bf e}}{2( {\bf u}^T{\bf v})}.
\]
In solving Eq.\ (\ref{eq:pmalpha}) great care has to be exercised so as to choose
those values which make the right-hand largest in order to avoid loss of numerical
precision. 
The above steps are then repeated for every transformations till we have a 
tridiagonal matrix suitable for obtaining the eigenvalues.  
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Householder, brute force}
\begin{small}
{\scriptsize
Our Householder transformation has given us a tridiagonal matrix. We discuss here how one can use
Jacobi's iterative procedure to obtain the eigenvalues. 
Letus specialize to a $4\times 4 $ matrix.
The tridiagonal matrix takes the form
%
\[
 {\bf A} =
      \left( \begin{array}{cccc} 
                d_{1} & e_{1} & 0     &  0    \\
                e_{1} & d_{2} & e_{2} &  0    \\
                 0    & e_{2} & d_{3} & e_{3} \\
                 0    &   0   & e_{3} & d_{4} 
             \end{array} \right).
\]
%
As a first observation, if any of the elements $e_{i}$ are zero the
matrix can be separated into smaller pieces before
diagonalization. Specifically, if $e_{1} = 0$ then $d_{1}$ is an
eigenvalue.
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
 Thus, let us introduce  a transformation ${\bf S_{1}}$ which operates like
%
\[
 {\bf S_{1}} =
      \left( \begin{array}{cccc} 
                \cos \theta & 0 & 0 & \sin \theta\\
                 0       & 0 & 0 &      0      \\
                   0        & 0 & 0 &      0      \\
               \cos \theta & 0 & 0 & \cos \theta 
             \end{array} \right)
\]
%
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
Then the similarity transformation 
%
\[
{\bf S_{1}^{T} A  S_{1}} = {\bf A'} = 
      \left( \begin{array}{cccc}
              d'_{1} & e'_{1} &   0    &   0   \\
              e'_{1}  & d_{2}  & e_{2}  &   0   \\
                0    & e_{2}  & d_{3}  & e'{3} \\
                0    &   0    & e'_{3} & d'_{4}
             \end{array} \right)
\]
%
produces a matrix where the primed elements in ${\bf A'}$ have been
changed by the transformation whereas the unprimed elements are unchanged.
If we now choose $\theta$ to
give the element $a_{21}^{'} = e^{'}= 0$ then we have the first
eigenvalue  $= a_{11}^{'} = d_{1}^{'}$.
(This is actually what you are doing in project 2!!)
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder's and  Francis' algorithm}
\begin{small}
{\scriptsize
This procedure can be continued on the remaining three-dimensional
submatrix for the next eigenvalue. Thus after few transformations    
we have the wanted diagonal form.

What we see here is just a special case of the more general procedure 
developed by Francis in two articles in 1961 and 1962. Using Jacobi's method is not very efficient ether. 

The algorithm is based on the so-called {\bf QR} method (or just {\bf QR}-algorithm). It follows from a theorem by Schur which states that any square matrix can be written out in terms of an orthogonal matrix $\hat{Q}$ and an upper triangular matrix $\hat{U}$. Historically $R$ was used instead of 
$U$ since the wording right triangular matrix was first used.
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Householder's and  Francis' algorithm}
\begin{small}
{\scriptsize
The method is based on an iterative procedure similar to Jacobi's method, by a succession of
planar rotations. For a tridiagonal matrix it is simple to carry out in principle, but complicated in detail!

Schur's theorem
\[
\hat{A} = \hat{Q}\hat{U},
\]
is used to rewrite any square matrix into a unitary matrix times an upper triangular matrix.
We say that a square matrix is similar to a triangular matrix. 

Householder's algorithm which we have derived is just a special case of the general Householder algorithm. For a symmetric square matrix we obtain a tridiagonal matrix. 

There is a corollary to Schur's theorem which states that every Hermitian matrix is unitarily similar to a diagonal matrix.
}
 \end{small}
 }



\frame
{
  \frametitle{Eigenvalue Solvers, Householder's and  Francis' algorithm}
\begin{small}
{\scriptsize
It follows that we can define a new matrix
\[
\hat{A}\hat{Q} = \hat{Q}\hat{U}\hat{Q},
\]
and multiply from the left with $\hat{Q}^{-1}$ we get
\[
\hat{Q}^{-1}\hat{A}\hat{Q} = \hat{B}=\hat{U}\hat{Q},
\]
where the matrix $\hat{B}$ is a similarity transformation of $\hat{A}$ and has the same eigenvalues
as $\hat{B}$. 
}
 \end{small}
 }



\frame
{
  \frametitle{Eigenvalue Solvers, Householder's and  Francis' algorithm}
\begin{small}
{\scriptsize
Suppose $\hat{A}$ is the triangular matrix we obtained after the Householder  transformation,
\[
\hat{A} = \hat{Q}\hat{U},
\]
and multiply from the left with $\hat{Q}^{-1}$ resulting in
\[
\hat{Q}^{-1}\hat{A} = \hat{U}.
\]
Suppose that $\hat{Q}$ consists of a series of planar Jacobi like rotations acting on sub blocks
of $\hat{A}$ so that all elements below the diagonal are zeroed out
\[
\hat{Q}=\hat{R}_{12}\hat{R}_{23}\dots\hat{R}_{n-1,n}.
\]
}
 \end{small}
 }


\frame
{
  \frametitle{Eigenvalue Solvers, Householder's and  Francis' algorithm}
\begin{small}
{\scriptsize
A transformation of the type $\hat{R}_{12}$ looks like
\[
 \hat{R}_{12} =
      \left( \begin{array}{ccccccccc} 
                 c&s &0 &0 &0 &  \dots &0 & 0 & 0\\
                 -s&c &0 &0 &0 &   \dots &0 & 0 & 0\\
                 0&0 &1 &0 &0 &   \dots &0 & 0 & 0\\
                 \dots&\dots &\dots &\dots &\dots &\dots      \\
                 0&0 &0 & 0 & 0 & \dots &1 &0 &0      \\
                 0&0 &0 & 0 & 0 & \dots &0 &1 &0      \\
                 0&0 &0 & 0 & 0 & \dots &0 &0 & 1  
             \end{array} \right)
\]
}
 \end{small}
 }





\frame
{
  \frametitle{Eigenvalue Solvers, Householder's and Francis' algorithm}
\begin{small}
{\scriptsize
The matrix $\hat{U}$ takes then the form
\[
 \hat{U} =
      \left( \begin{array}{ccccccccc} 
                 x&x &x &0 &0 &  \dots &0 & 0 & 0\\
                 0&x &x &x &0 &   \dots &0 & 0 & 0\\
                 0&0 &x &x &x &   \dots &0 & 0 & 0\\
                 \dots&\dots &\dots &\dots &\dots &\dots      \\
                 0&0 &0 & 0 & 0 & \dots &x &x &x      \\
                 0&0 &0 & 0 & 0 & \dots &0 &x &x      \\
                 0&0 &0 & 0 & 0 & \dots &0 &0 & x  
             \end{array} \right)
\]
which has a second superdiagonal.
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder's and Francis' algorithm}
\begin{small}
{\scriptsize
We have now found $\hat{Q}$ and $\hat{U}$ and this allows us to find the matrix $\hat{B}$
which is, due to Schur's theorem,  unitarily similar to a triangular matrix (upper in our case) 
since we have that 
\[
\hat{Q}^{-1}\hat{A}\hat{Q} = \hat{B}, 
\]
from Schur's theorem the  matrix $\hat{B}$ is triangular and the eigenvalues the same as those of 
$\hat{A}$ and are given by the diagonal matrix elements of 
$\hat{B}$. Why?  

Our matrix $\hat{B}=\hat{U}\hat{Q}$. 

}
 \end{small}
 }







\frame
{
  \frametitle{Another iterative procedure}
\begin{small}
{\scriptsize
The matrix $\hat{A}$ is transformed into a tridiagonal form and the last
step is to transform it into a diagonal matrix giving the eigenvalues
on the diagonal. 

The eigenvalues of a  matrix can be obtained using the characteristic polynomial 
\[
P(\lambda) = det(\lambda{\bf I}-{\bf A})= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right),
\]
which rewritten in matrix form reads 
\[
P(\lambda)= \left( \begin{array}{ccccccc} d_1-\lambda & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2-\lambda & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3-\lambda & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{N_{\mathrm{step}}-2}-\lambda & e_{N_{\mathrm{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{N_{\mathrm{step}}-1} & d_{N_{\mathrm{step}}-1}-\lambda

             \end{array} \right)
\] 
}
 \end{small}
 }

\frame
{
  \frametitle{Eigenvalue Solvers, Householder}
\begin{small}
{\scriptsize
We can solve this equation in an iterative manner. 
We let $P_k(\lambda)$ be the value of $k$ subdeterminant of the above matrix of dimension
$n\times n$. The polynomial $P_k(\lambda)$ is clearly a polynomial of degree $k$.
Starting with $P_1(\lambda)$ we have $P_1(\lambda)=d_1-\lambda$. The next polynomial reads
$P_2(\lambda)=(d_2-\lambda)P_1(\lambda)-e_1^2$. By expanding the determinant for $P_k(\lambda)$ 
in terms of the minors of the $n$th column we arrive at the recursion relation
\[ 
   P_k(\lambda)=(d_k-\lambda)P_{k-1}(\lambda)-e_{k-1}^2P_{k-2}(\lambda).
\]
Together with the starting values $P_1(\lambda)$ and $P_2(\lambda)$ and good root searching methods
we arrive at an efficient computational scheme for finding the roots of $P_n(\lambda)$. 
However, for large matrices this algorithm is rather inefficient and time-consuming.
}
 \end{small}
 }








\frame
{
  \frametitle{Eigenvalue Solvers, Householder functions}
\begin{small}
{\scriptsize
The programs  which performs these transformations are
$\mbox{matrix} \quad {\bf A} \longrightarrow \mbox{tridiagonal matrix}
 \longrightarrow \mbox{diagonal matrix}$

\begin{center} 
%
\begin{tabular}{ll}
%
C++: &void trd2(double $**$a, int n, double d[], double e[])\\
   &void tqli(double d[], double[], int n, double $**$z)\\
Fortran:  &CALL tred2(a, n, d, e)\\
          &CALL tqli(d, e, n, z)
\end{tabular}
%
\end{center}
%
}
 \end{small}
 }



\frame[containsverbatim]
{
  \frametitle{Using Lapack to solve linear algebra and eigenvalue problems}
\begin{small}
{\scriptsize
Suppose you wanted to solve a general system of linear equations $\hat{A}{\bf x} =  {\bf b}$, where $\hat{A}$is an $n \times n$ square matrix and $x$ and $b$ are $n$-element column vectors. You opt to use the routine 
{\bf dgesv}. The man page abstract obtained with
\begin{verbatim} man dgesv\end{verbatim} if Lapack is properly installed. The C++ declaration is \begin{verbatim} void dgesv(int n, int  nrhs,  double  *da,  int  lda,  int *ipivot, double *db, int ldb, int *info) ;
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Lapack example}
\begin{lstlisting}
#include <iostream>
#define MAX 10
using namespace std;
int main(){
   // Values needed for dgesv
   int n;
   int nrhs = 1;
   double a[MAX][MAX];
   double b[1][MAX];
   int lda = MAX;
   int ldb = MAX;
   int ipiv[MAX];
   int info;
   // Other values
   int i,j;
}
\end{lstlisting}
}

\frame[containsverbatim]
{
  \frametitle{Lapack example}
\begin{lstlisting}
   // Read the values of the matrix
   cout << "Enter n \n";
   cin >> n;
   cout << "On each line type a row of the matrix A followed by one element of b:\n";
   for(i = 0; i < n; i++){
     cout << "row " << i << " ";
     for(j = 0; j < n; j++)std::cin >> a[j][i];
     cin >> b[0][i];
   }
}
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Lapack example}
\begin{lstlisting}
   // Solve the linear system
   dgesv(n, nrhs, &a[0][0], lda, ipiv, &b[0][0], ldb, &info);
   // Check for success
   if(info == 0)
   {
      // Write the answer
      cout << "The answer is\n";
      for(i = 0; i < n; i++)
        cout << "b[" << i << "]\t" << b[0][i] << "\n";
   }
   else
   {
      // Write an error message
      cerr << "dgesv returned error " << info << "\n";
   }
   return info;
}
\end{lstlisting}
}


\frame
{
  \frametitle{Lanczos' iteration}
%\begin{small}
%{\scriptsize
Basic features with a real symmetric matrix (and normally huge $n> 10^6$ and sparse) 
$\hat{A}$ of dimension $n\times n$:
\begin{itemize}
\item Lanczos' algorithm generates a sequence of real tridiagonal matrices $T_k$ of dimension $k\times k$ with $k\le n$, with the property that the extremal eigenvalues of $T_k$ are progressively better estimates of $\hat{A}$' extremal eigenvalues.
\item The method converges to the extremal eigenvalues.
\item The similarity transformation is 
\[
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
\]
with the first vector $\hat{Q}\hat{e}_1=\hat{q}_1$.
\end{itemize}
%}
%\end{small}
}


\frame
{
  \frametitle{Lanczos' iteration}
%\begin{small}
%{\scriptsize
We are going to solve iteratively
\[
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
\]
with the first vector $\hat{Q}\hat{e}_1=\hat{q}_1$.
We can write out the matrix $\hat{Q}$ in terms of its column vectors 
\[
\hat{Q}=\left[\hat{q}_1\hat{q}_2\dots\hat{q}_n\right].
\]
%}
%\end{small}
}


\frame
{
  \frametitle{Lanczos' iteration}
%\begin{small}
%{\scriptsize
The matrix
\[
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
\]
can be written as 
\[
    \hat{T} = \left(\begin{array}{cccccc}
                           \alpha_1& \beta_1 & 0 &\dots   & \dots &0 \\
                           \beta_1 & \alpha_2 & \beta_2 &0 &\dots &0 \\
                           0& \beta_2 & \alpha_3 & \beta_3 & \dots &0 \\
                           \dots& \dots   & \dots &\dots   &\dots & 0 \\
                           \dots&   &  &\beta_{n-2}  &\alpha_{n-1}& \beta_{n-1} \\
                           0&  \dots  &\dots  &0   &\beta_{n-1} & \alpha_{n} \\
                      \end{array} \right)
\]
%}
%\end{small}
}


\frame
{
  \frametitle{Lanczos' iteration}
%\begin{small}
%{\scriptsize
Using the fact that $\hat{Q}\hat{Q}^T=\hat{I}$, 
we can rewrite 
\[
\hat{T}= \hat{Q}^{T}\hat{A}\hat{Q},
\]
as 
\[
\hat{Q}\hat{T}= \hat{A}\hat{Q},
\]
and if we equate columns (recall from the previous slide)
\[
    \hat{T} = \left(\begin{array}{cccccc}
                           \alpha_1& \beta_1 & 0 &\dots   & \dots &0 \\
                           \beta_1 & \alpha_2 & \beta_2 &0 &\dots &0 \\
                           0& \beta_2 & \alpha_3 & \beta_3 & \dots &0 \\
                           \dots& \dots   & \dots &\dots   &\dots & 0 \\
                           \dots&   &  &\beta_{n-2}  &\alpha_{n-1}& \beta_{n-1} \\
                           0&  \dots  &\dots  &0   &\beta_{n-1} & \alpha_{n} \\
                      \end{array} \right)
\]
we obtain
\[
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1}.
\]
%}
%\end{small}
}



\frame
{
  \frametitle{Lanczos' iteration}
%\begin{small}
%{\scriptsize
We have thus
\[
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1},
\]
with $\beta_0\hat{q}_0=0$ for $k=1:n-1$. \newline
Remember that the vectors $\hat{q}_k$  are orthornormal and this implies
\[
\alpha_k=\hat{q}_k^T\hat{A}\hat{q}_k,
\]
and these vectors are called Lanczos vectors.
%}
%\end{small}
}



\frame
{
  \frametitle{Lanczos' iteration, the algorithm}
%\begin{small}
%{\scriptsize
We have thus
\[
\hat{A}\hat{q}_k=\beta_{k-1}\hat{q}_{k-1}+\alpha_k\hat{q}_k+\beta_k\hat{q}_{k+1},
\]
with $\beta_0\hat{q}_0=0$ for $k=1:n-1$ and 
\[
\alpha_k=\hat{q}_k^T\hat{A}\hat{q}_k.
\]
If 
\[
\hat{r}_k=(\hat{A}-\alpha_k\hat{I})\hat{q}_k-\beta_{k-1}\hat{q}_{k-1},
\]
is non-zero, then 
\[
\hat{q}_{k+1}=\hat{r}_{k}/\beta_k,
\]
with $\beta_k=\pm ||\hat{r}_{k}||_2$.
%}
%\end{small}
}

\frame[containsverbatim]
{
  \frametitle{A simple implementation of the Lanczos algorithm}
\begin{verbatim}
  r_0 = q_1; beta_0=1; q_0=0; int k = 0;
  while (beta_k != 0)
      q_{k+1} = r_k/beta_k
      k = k+1
      alpha_k = q_k^T A q_k
      r_k = (A-alpha_k I) q_k  -beta_{k-1}q_{k-1}
      beta_k = || r_k||_2
  end while
\end{verbatim}
}



\frame
{
  \frametitle{Differential equations program}
\begin{itemize}
\item Ordinary differential equations, Runge-Kutta method,chapter 8
\item Ordinary differential equations with boundary conditions:
one-variable equations to be solved by shooting and Green's function methods, chapter 9
\item We can solve such equations by a finite difference scheme as
well, turning the equation into an eigenvalue problem.
Still one variable. Done in projects 1 and 2.
\item If we have more than one variable, we need to solve 
partial differential equations, see Chapter 10
\item Fourier transforms and Fast Fourier transforms if we get time.
\end{itemize}
Project 3 deals with ordinary differential equations (most likely the solar system).
} 







\frame
{
  \frametitle{Differential Equations, chapter 8}
\begin{small}
{\scriptsize
The order of the ODE refers to the order of the derivative 
on the left-hand side in the equation
\begin{equation} 
   \frac{dy}{dt}=f(t,y).
\end{equation}
This equation is of first order and $f$ is an arbitrary function.
A second-order equation goes typically like
\begin{equation} 
   \frac{d^2y}{dt^2}=f(t,\frac{dy}{dt},y).
\end{equation}
A well-known second-order equation is Newton's second law
\begin{equation} 
   m\frac{d^2x}{dt^2}=-kx,
   \label{eq:newton}
\end{equation}
where $k$ is the force constant. ODE depend only on one
variable
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
partial differential equations like the time-dependent Schr\"odinger
equation 
\begin{equation}
   i\hbar\frac{\partial \psi({\bf x},t)}{\partial t}=
   -\frac{\hbar^2}{2m}\left( \frac{\partial^2 \psi({\bf r},t)}{\partial x^2} +
                            \frac{\partial^2 \psi({\bf r},t)}{\partial y^2}+
                            \frac{\partial^2 \psi({\bf r},t)}{\partial z^2}\right) + V({\bf x})\psi({\bf x},t),
\end{equation}
may depend on several variables. In certain cases, like the above
equation, the wave function can be factorized in functions of the separate
variables, so that the Schr\"odinger equation 
can be rewritten in terms of sets of ordinary differential equations.
These equations are discussed in chapter 10. Involve boundary conditions in addition to initial conditions.
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
We distinguish also between linear and non-linear differential
equation where e.g.,
\begin{equation} 
   \frac{dy}{dt}=g^3(t)y(t),
\end{equation}
is an example of a linear equation, while 
\begin{equation} 
   \frac{dy}{dt}=g^3(t)y(t)-g(t)y^2(t),
\end{equation}
     is a non-linear ODE.
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
Another concept which dictates the numerical method chosen
for solving an ODE, is that of initial and boundary conditions.
To give an example, if we study white dwarf stars or neutron stars
we will need to solve two coupled first-order differential
equations, one for the total mass $m$ and one for the 
pressure $P$ as functions of 
$\rho$ 
\[
\frac{dm}{dr}=4\pi r^{2}\rho (r)/c^2,
\]
and
\[
\frac{dP}{dr}=-\frac{Gm(r)}{r^{2}}\rho (r)/c^2.
\]
where $\rho$ is the mass-energy density.
The initial conditions are dictated by the mass being
zero at the center of the star, i.e., when $r=0$,
yielding $m(r=0)=0$. The other condition is that
the pressure vanishes at the surface of the star.

In the solution of the Schr\"odinger equation for a particle
in a potential, we may need to apply boundary conditions as well,
such as demanding continuity of the wave function and its derivative. 

}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
In many cases it is possible to rewrite a second-order
differential equation in terms of two first-order differential
equations. Consider again the case of Newton's second law in Eq.\
(\ref{eq:newton}). If we define the position $x(t)=y^{(1)}(t)$ 
and the velocity $v(t)=y^{(2)}(t)$ as its derivative 
\begin{equation} 
   \frac{dy^{(1)}(t)}{dt}=\frac{dx(t)}{dt}=y^{(2)}(t),
\end{equation}
we can rewrite Newton's second law as two coupled first-order
differential equations
\begin{equation} 
   m\frac{dy^{(2)}(t)}{dt}=-kx(t)=-ky^{(1)}(t),
    \label{eq:n1}
\end{equation}
and 
\begin{equation}
\frac{dy^{(1)}(t)}{dt}=y^{(2)}(t).
    \label{eq:n2}
\end{equation}

}
\end{small}
}


\frame
{
  \frametitle{Differential Equations, Finite Difference}
\begin{small}
{\scriptsize
These methods fall under the general class of one-step methods.
The algoritm is rather simple. 
Suppose we have an initial value for the function $y(t)$ given by
\begin{equation}
  y_0=y(t=t_0).
\end{equation}
We are interested in solving a differential equation in a region
in space [a,b]. We define a step $h$ by splitting the interval
in $N$ sub intervals, so that we have
\begin{equation}
  h=\frac{b-a}{N}.
\end{equation}
With this step and the derivative of $y$ we can construct the
next value of the function $y$ at
\begin{equation}
   y_1=y(t_1=t_0+h),
\end{equation}
and so forth. 
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
If the function is rather well-behaved in the domain
[a,b], we can use a fixed step size. If not, adaptive steps 
may be needed. Here we concentrate on fixed-step
methods only. 
Let us try to generalize the above procedure by writing the 
step $y_{i+1}$ in terms of the previous step $y_i$
\begin{equation}
  y_{i+1}=y(t=t_i+h)=y(t_i) + h\Delta(t_i,y_i(t_i)) + O(h^{p+1}),
\end{equation}
where $O(h^{p+1})$ represents the truncation error. To determine 
$\Delta$, we Taylor expand our function $y$
\begin{equation}
     y_{i+1}=y(t=t_i+h)=y(t_i) + h(y'(t_i)+\dots +y^{(p)}(t_i)\frac{h^{p-1}}{p!}) + O(h^{p+1}),
\label{eq:taylor}
\end{equation}
where we will associate the derivatives in the parenthesis with
\begin{equation}
\Delta(t_i,y_i(t_i))=(y'(t_i)+\dots +y^{(p)}(t_i)\frac{h^{p-1}}{p!}).
\label{eq:delta}
\end{equation}
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
We define 
\begin{equation}
  y'(t_i)=f(t_i,y_i)   
\end{equation}
and if we truncate $\Delta$ at the first derivative, we have
\begin{equation}
   y_{i+1}=y(t_i) + hf(t_i,y_i) + O(h^2),
   \label{eq:euler}
\end{equation}
which when complemented with $t_{i+1}=t_i+h$ forms
the algorithm for the well-known Euler method. 
Note that at every step we make an approximation error
of the order of $O(h^2)$, however the total error is the sum over all
steps $N=(b-a)/h$, yielding thus a global error which goes like
$NO(h^2)\approx O(h)$. 
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
To make Euler's method more precise we can obviously
decrease $h$ (increase $N$). However, if we are computing the 
derivative $f$ numerically  
by e.g., the two-steps formula
\[
    f'_{2c}(x)= \frac{f(x+h)-f(x)}{h}+O(h),
\]
we can enter into roundoff error problems when we subtract 
two almost equal numbers $f(x+h)-f(x)\approx 0$. 
Euler's method is not recommended for precision calculation,
although it is handy to use in order to get a first
view on how a solution may look like. As an example,
consider Newton's equation rewritten in Eqs.\ 
(\ref{eq:n1}) and (\ref{eq:n2}). We define $y_0=y^{(1)}(t=0)$
an $v_0=y^{(2)}(t=0)$. The first steps in Newton's equations
are then
\begin{equation} 
   y^{(1)}_1=y_0+hv_0+O(h^2)
\end{equation}
and 
\begin{equation}
      y^{(2)}_1=v_0-hy_0k/m+O(h^2).
\end{equation}
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
The Euler method is asymmetric in time, since it uses information about the derivative at the beginning
of the time interval. This means that we evaluate the position at $y^{(1)}_1$ using the velocity
at $y^{(2)}_0=v_0$. A simple variation is to determine $y^{(1)}_{n+1}$ using the velocity at
$y^{(2)}_{n+1}$, that is (in a slightly more generalized form)
\begin{equation} 
   y^{(1)}_{n+1}=y^{(1)}_{n}+h y^{(2)}_{n+1}+O(h^2)
\end{equation}
and 
\begin{equation}
   y^{(2)}_{n+1}=y^{(2)}_{n}+h a_{n}+O(h^2).
\end{equation}
The acceleration $a_n$ is a function of $a_n(y^{(1)}_{n}, y^{(2)}_{n},t)$ and needs to be evaluated
as well. This is the Euler-Cromer method.
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
Let us then include the second derivative in our Taylor expansion. 
We have then
\begin{equation}
 \Delta(t_i,y_i(t_i))=f(t_i)+\frac{h}{2}\frac{df(t_i,y_i)}{dt}+O(h^3).
\end{equation}
The second derivative can be rewritten as
\begin{equation}
  y''=f'=\frac{df}{dt}=\frac{\partial f}{\partial t}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial t}=\frac{\partial f}{\partial t}+\frac{\partial f}{\partial y}f
\label{eq:derivatives}
\end{equation}
and we can rewrite Eq.\ (\ref{eq:taylor}) as 
\begin{equation}
     y_{i+1}=y(t=t_i+h)=y(t_i) +hf(t_i)+
     \frac{h^2}{2}\left(\frac{\partial f}{\partial t}+\frac{\partial f}{\partial y}f\right) + O(h^{3  }),
\end{equation}
which has a local approximation error $O(h^{3  })$ and a global
error $O(h^{2})$.
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
These approximations can be generalized by using the derivative $f$ to
arbitrary order so that we have
\begin{equation}
     y_{i+1}=y(t=t_i+h)=y(t_i) + h(f(t_i,y_i)+\dots f^{(p-1)}(t_i,y_i)
     \frac{h^{p-1}}{p!}) + O(h^{p+1}).
\end{equation}
These methods, based on higher-order derivatives, are in general not used
in numerical computation, since they rely on evaluating 
derivatives several times. Unless one has analytical expressions
for these, the risk of roundoff errors is large.
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
The most obvious improvements to Euler's and Euler-Cromer's algorithms, 
avoiding in addition the need for computing a
second derivative, is the so-called midpoint method. We have then
\begin{equation} 
   y^{(1)}_{n+1}=y^{(1)}_{n}+\frac{h}{2}\left(y^{(2)}_{n+1}+y^{(2)}_{n}\right)+O(h^2)
\end{equation}
and 
\begin{equation}
   y^{(2)}_{n+1}=y^{(2)}_{n}+h a_{n}+O(h^2),
\end{equation}
yielding
\begin{equation} 
   y^{(1)}_{n+1}=y^{(1)}_{n}+hy^{(2)}_{n}+\frac{h^2}{2}a_n+O(h^3)
\end{equation}
implying that the local truncation error in the position is now $O(h^3)$, whereas Euler's or Euler-Cromer's
methods have a local error of  $O(h^2)$.
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
 Thus, the midpoint method yields a global error with 
second-order accuracy for
the position and first-order accuracy for the velocity. However, although these methods yield exact results for
constant accelerations, the error increases in general with each time step.

One method that avoids this is the so-called half-step method. Here we define
\begin{equation}
   y^{(2)}_{n+1/2}=y^{(2)}_{n-1/2}+h a_{n}+O(h^2),
\end{equation}
and 
\begin{equation}
   y^{(1)}_{n+1}=y^{(1)}_{n}+hy^{(2)}_{n+1/2} +O(h^2).
\end{equation}
Note that this method needs the calculation of $y^{(2)}_{1/2}$. This is done using 
e.g., Euler's method
\begin{equation}
   y^{(2)}_{1/2}=y^{(2)}_{0}+h a_{0}+O(h^2).
\end{equation}
As this method is numerically stable, it is often used instead of Euler's method.
}
\end{small}
}


\frame
{
  \frametitle{Differential Equations}
\begin{small}
{\scriptsize
Another method which one may encounter is the Euler-Richardson method
with
\begin{equation}
   y^{(2)}_{n+1}=y^{(2)}_{n}+h a_{n+1/2}+O(h^2),
   \label{eq:er1}
\end{equation}
and 
\begin{equation}
   \label{eq:er2}
   y^{(1)}_{n+1}=y^{(1)}_{n}+hy^{(2)}_{n+1/2} +O(h^2).
\end{equation}
The program program2.cpp includes all of the above methods.

}
\end{small}
}


\section{Week 40}
\frame
{
  \frametitle{Week 40}
  \begin{block}{Ordinary differential equations (ODEs) and Partial differential equations (PDEs)}
\begin{itemize}
\item Monday: Repetition from last week
\item Runge-Kutta methods, with adaptive methods as well
\item Examples with codes
\item Tuesday: 
\item Discussion of project 3
\item Diffusion equation, implicit, explicit (if we get there!!)
\end{itemize}
Chapter 9, ODEs with boundary conditions will not be discussed.
  \end{block}
} 



\frame
{
  \frametitle{Differential Equations, Runge-Kutta methods}
\begin{small}
{\scriptsize
Runge-Kutta (RK) methods are based on Taylor expansion formulae, but yield
in general better algorithms for solutions of an ODE.
The basic philosophy is that it provides an intermediate step
in the computation of $y_{i+1}$.

To see this, consider first the following definitions
\begin{equation}
   \frac{dy}{dt}=f(t,y),  
\end{equation} 
and 
\begin{equation}
   y(t)=\int f(t,y) dt,  
\end{equation} 
and 
\begin{equation}
  y_{i+1}=y_i+ \int_{t_i}^{t_{i+1}} f(t,y) dt.
\end{equation}
}
\end{small}
}

\frame
{
  \frametitle{Differential Equations, Runge-Kutta methods}
\begin{small}
{\scriptsize
To demonstrate the philosophy behind RK methods, let us consider
the second-order RK method, RK2.
The first approximation consists in Taylor expanding $f(t,y)$
around the center of the integration interval $t_i$ to $t_{i+1}$,
i.e., at $t_i+h/2$, $h$ being the step.
Using the midpoint formula for an integral, 
defining $y(t_i+h/2) = y_{i+1/2}$ and   
$t_i+h/2 = t_{i+1/2}$, we obtain
\begin{equation}
    \int_{t_i}^{t_{i+1}} f(t,y) dt \approx hf(t_{i+1/2},y_{i+1/2}) +O(h^3).
\end{equation} 
This means in turn that we have
\begin{equation}
     y_{i+1}=y_i + hf(t_{i+1/2},y_{i+1/2}) +O(h^3).
\end{equation}
}
\end{small}
}



\frame
{
  \frametitle{Differential Equations, Runge-Kutta methods}
\begin{small}
{\scriptsize
However, we do not know the value of   $y_{i+1/2}$.
Here comes thus the next approximation, namely, we use Euler's
method to approximate $y_{i+1/2}$. We have then
\begin{equation}
   y_{(i+1/2)}=y_i + \frac{h}{2}\frac{dy}{dt} =
   y(t_i) + \frac{h}{2}f(t_i,y_i).
\end{equation}
This means that we can define the following algorithm for 
the second-order Runge-Kutta method, RK2.
\begin{equation} 
  k_1=hf(t_i,y_i),
\end{equation} 
\begin{equation}
  k_2=hf(t_{i+1/2},y_i+k_1/2),
\end{equation}
with the final value
\begin{equation} 
  y_{i+i}\approx y_i + k_2 +O(h^3). 
\end{equation}
}
\end{small}
}

\frame
{
  \frametitle{Differential Equations, Runge-Kutta methods}
\begin{small}
{\scriptsize
The difference between the previous one-step methods 
is that we now need an intermediate step in our evaluation,
namely $t_i+h/2 = t_{(i+1/2)}$ where we evaluate the derivative $f$. 
This involves more operations, but the gain is a better stability
in the solution.
}
\end{small}
}

\frame
{
  \frametitle{Differential Equations, Runge-Kutta methods}
\begin{small}
{\scriptsize
The fourth-order Runge-Kutta, RK4, which we will employ in the solution
of various differential equations below, has the following
algorithm
\begin{equation} 
  k_1=hf(t_i,y_i),
\end{equation}
\begin{equation}
  k_2=hf(t_i+h/2,y_i+k_1/2),
\end{equation}
\begin{equation}
  k_3=hf(t_i+h/2,y_i+k_2/2)
\end{equation}
\begin{equation}
  k_4=hf(t_i+h,y_i+k_3)
\end{equation}
with the final value
\begin{equation} 
  y_{i+1}=y_i +\frac{1}{6}\left( k_1 +2k_2+2k_3+k_4\right).
\end{equation}
Thus, the algorithm consists in first calculating $k_1$ 
with $t_i$, $y_1$ and $f$ as inputs. Thereafter, we increase the step
size by $h/2$ and calculate $k_2$, then $k_3$ and finally $k_4$. Global error
as $O(h^4)$.
}
\end{small}
}



\frame
{
  \frametitle{Simple Example, Block tied to a Wall}
Our first example is the classical case of simple harmonic
  oscillations, namely a block sliding on a horizontal frictionless
  surface. The block is tied to a wall with a spring.
  If the spring is not compressed or stretched too far, the force
  on the block at a given position $x$ is 
  \[
      F=-kx.
  \]
}


\frame
{
  \frametitle{Simple Example, Block tied to a Wall}
\begin{small}
{\scriptsize
The negative sign means that the force acts to restore the object to an
  equilibrium position. Newton's equation of motion for this idealized system
  is then 
  \[
    m\frac{d^2x}{dt^2}=-kx,
  \]
  or we could rephrase it as
  \[
   \frac{d^2x}{dt^2}=-\frac{k}{m}x=-\omega_0^2x,
    \label{eq:newton1}
  \]
  with the angular frequency $\omega_0^2=k/m$. 

  The above differential equation has the advantage that it can be solved 
  analytically with solutions on the form
  \[
     x(t)=Acos(\omega_0t+\nu),
  \]
  where $A$ is the amplitude and $\nu$ the phase constant.   
  This provides in turn an important test for the numerical
  solution and the development of a program for more complicated cases
  which cannot be solved analytically. 
 }
 \end{small}
}


\frame
{
  \frametitle{Simple Example, Block tied to a Wall}
\begin{small}
{\scriptsize
 With the position $x(t)$ and the velocity 
  $v(t)=dx/dt$ we can reformulate Newton's equation in the following way
  \[
      \frac{dx(t)}{dt}=v(t),
  \]
  and
  \[
      \frac{dv(t)}{dt}=-\omega_0^2x(t).
  \]

  We are now going to solve these equations using the Runge-Kutta method
  to fourth order discussed previously. 
  }
 \end{small}
}


\frame
{
  \frametitle{Simple Example, Block tied to a Wall}
\begin{small}
{\scriptsize
 Before proceeding however, it is important to note that in addition
  to the exact solution, we have at least two further tests which can be
  used to check our solution. 

  Since functions like $cos$ are periodic with a period $2\pi$, 
  then the solution
  $x(t)$ has also to be periodic. This means that
  \[
     x(t+T)=x(t),
  \]
  with $T$ the period defined as 
  \[
     T=\frac{2\pi}{\omega_0}=\frac{2\pi}{\sqrt{k/m}}.
  \]

  Observe that $T$ depends only on $k/m$ and not on the amplitude
  of the solution. 

 }
 \end{small}
}


\frame
{
  \frametitle{Simple Example, Block tied to a Wall}
\begin{small}
{\scriptsize
  In addition to the periodicity test, the total energy has also to be
  conserved. 

  Suppose we choose the initial conditions 
  \[
     x(t=0)=1\hspace{0.1cm} \mathrm{m}\hspace{1cm} v(t=0)=0\hspace{0.1cm}\mathrm{m/s},
  \]
  meaning that block is at rest at $t=0$ but with a potential energy
  \[
    E_0=\frac{1}{2}kx(t=0)^2=\frac{1}{2}k.
  \]
  The total energy at any time $t$ has however to be conserved, meaning
  that our solution has to fulfil the condition
  \[
    E_0=\frac{1}{2}kx(t)^2+\frac{1}{2}mv(t)^2.
  \]
 }
 \end{small}
}


\frame
{
  \frametitle{Simple Example, Block tied to a Wall}
\begin{small}
{\scriptsize
 An algorithm which implements these equations is included below.
  \begin{enumerate}
  \item Choose the initial position and speed, with the most common choice
	$v(t=0)=0$ and some fixed value for the position. 
  \item Choose the method you wish to employ in solving the problem.
\item	Subdivide
	the time interval $[t_i,t_f] $ into a grid with step size
       \[
	  h=\frac{t_f-t_i}{N},
      \]
	where $N$ is the number of mesh points. 
  \item Calculate now the total energy given by 
  \[
    E_0=\frac{1}{2}kx(t=0)^2=\frac{1}{2}k.
  \]
  \item The Runge-Kutta method is used to obtain
	$x_{i+1}$ and $v_{i+1}$ starting from 
	the previous values $x_i$ and $v_i$..
  \item When we have computed $x(v)_{i+1}$ we upgrade  $t_{i+1}=t_i+h$.
  \item This iterative  
	 process continues till we reach the maximum time
	 $t_f$.
  \item  The results are checked against the exact solution. Furthermore,
	 one has to check the stability of the numerical solution against 
	 the chosen number of mesh points $N$.      
  \end{enumerate}

 }
 \end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple Example, Block tied to a Wall}
\begin{small}
{\scriptsize
\begin{verbatim}
    y[0] = initial_x;                 // initial position  
    y[1] = initial_v;                // initial velocity  
    t=0.;                             // initial time      
    E0 = 0.5*y[0]*y[0]+0.5*y[1]*y[1];  // the initial total energy
    // now we start solving the differential 
    // equations using the RK4 method 
    while (t <= tmax){
      derivatives(t, y, dydt);   // initial derivatives              
      runge_kutta_4(y, dydt, n, t, h, yout, derivatives); 
      for (i = 0; i < n; i++) {
	   y[i] = yout[i];  
      }
      t += h;
      output(t, y, E0);   // write to file 
    }
\end{verbatim}
 }
 \end{small}
}


\frame[containsverbatim]
{
  \frametitle{Simple Example, Block tied to a Wall}
\begin{small}
{\scriptsize
\begin{verbatim}
  //   this function sets up the derivatives for this special case  
  void derivatives(double t, double *y, double *dydt)
  {
    dydt[0]=y[1];    // derivative of x 
    dydt[1]=-y[0]; // derivative of v 
  } // end of function derivatives
\end{verbatim}  
 }
 \end{small}
}

\frame[containsverbatim]
{
  \frametitle{Runge-Kutta methods, code}
\begin{small}
{\scriptsize
\begin{verbatim}
void runge_kutta_4(double *y, double *dydx, int n, 
                 double x, double h, 
          double *yout, void (*derivs)(double, double *, double *))
{
  int i;
  double      xh,hh,h6; 
  double *dym, *dyt, *yt;
  //   allocate space for local vectors   
  dym = new double [n];
  dyt =  new double [n];
  yt =  new double [n];
  hh = h*0.5;
  h6 = h/6.;
  xh = x+hh;
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Runge-Kutta methods, code}
\begin{small}
{\scriptsize
\begin{verbatim}
  for (i = 0; i < n; i++) {
    yt[i] = y[i]+hh*dydx[i];
  }
  (*derivs)(xh,yt,dyt);     // computation of k2 
  for (i = 0; i < n; i++) {
    yt[i] = y[i]+hh*dyt[i];
  }
  (*derivs)(xh,yt,dym); //  computation of k3
  for (i=0; i < n; i++) {
    yt[i] = y[i]+h*dym[i];
    dym[i] += dyt[i];
  }
  (*derivs)(x+h,yt,dyt);    // computation of k4
  //      now we upgrade y in the array yout  
  for (i = 0; i < n; i++){
    yout[i] = y[i]+h6*(dydx[i]+dyt[i]+2.0*dym[i]);
  }
  delete []dym;
  delete [] dyt;
  delete [] yt;
}       //  end of function Runge-kutta 4  
\end{verbatim}
}
\end{small}
}


\frame
{
  \frametitle{The classical pendulum}
\begin{small}
{\scriptsize
The angular equation of motion of the pendulum is given by
Newton's equation and with no external force it reads 
\begin{equation}
  ml\frac{d^2\theta}{dt^2}+mgsin(\theta)=0,
\end{equation}
with an angular velocity and acceleration given by
\begin{equation}
     v=l\frac{d\theta}{dt},
\end{equation}
and 
\begin{equation}
     a=l\frac{d^2\theta}{dt^2}.
\end{equation}
}
\end{small}
}


\frame
{
  \frametitle{More on the Pendulum}
\begin{small}
{\scriptsize
We do however expect that the motion will gradually come to an end
due a viscous drag torque acting on the pendulum. 
In the presence of the drag, the above equation becomes
\begin{equation}
   ml\frac{d^2\theta}{dt^2}+\nu\frac{d\theta}{dt}  +mgsin(\theta)=0,
\label{eq:pend1}
\end{equation}
where $\nu$ is now a positive constant parameterizing the viscosity
of the medium in question. In order to maintain the motion against
viscosity, it is necessary to add some external driving force. 
We choose here a periodic driving force. The last equation becomes then
\begin{equation}
   ml\frac{d^2\theta}{dt^2}+\nu\frac{d\theta}{dt}  +mgsin(\theta)=Asin(\omega t),
\label{eq:pend2}
\end{equation}
with $A$ and $\omega$ two constants representing the amplitude and 
the angular frequency respectively. The latter is called the driving frequency.
}
\end{small}
}





\frame
{
  \frametitle{More on the Pendulum}
\begin{small}
{\scriptsize
  We define 
  \[
      \omega_0=\sqrt{g/l},
  \]
  the so-called natural frequency
  and the new dimensionless quantities
  \[
      \hat{t}=\omega_0t,
  \]
with the dimensionless driving frequency
  \[
     \hat{\omega}=\frac{\omega}{\omega_0},
  \]
  and introducing the quantity $Q$, called the {\em quality factor},
  \[
     Q=\frac{mg}{\omega_0\nu},
  \]
  and the dimensionless amplitude 
  \[
     \hat{A}=\frac{A}{mg}
  \]
}
\end{small}
}

\frame
{
  \frametitle{More on the Pendulum}
\begin{small}
{\scriptsize
  we have 
  \[
    \frac{d^2\theta}{d\hat{t}^2}+\frac{1}{Q}\frac{d\theta}{d\hat{t}}  
     +sin(\theta)=\hat{A}cos(\hat{\omega}\hat{t}).
  \]

  This equation can in turn be recast in terms of two coupled first-order
  differential equations as follows
  \[
     \frac{d\theta}{d\hat{t}}=\hat{v},
  \]
  and
  \[
     \frac{d\hat{v}}{d\hat{t}}=-\frac{\hat{v}}{Q}-sin(\theta)+\hat{A}cos(\hat{\omega}\hat{t}).
  \]

  These are the equations to be solved. 
  The factor $Q$ represents the number of oscillations of the undriven system that must occur
  before its energy is significantly reduced due to the viscous  drag. The amplitude $\hat{A}$
  is measured in units of the maximum possible gravitational torque while 
  $\hat{\omega}$ is the angular frequency of the external torque measured in units of the pendulum's
  natural frequency.
}
\end{small}
}





\frame
{
  \frametitle{Classes for ODE methods}
It can be very useful to make a Class which contains all possible methods discussed. In Fortran we can use the MODULE
keyword in order to can methods and keep the variables private and hidden from other parts of our code.
This allows for a generalization which can be used to tackle other ODEs as well.
}


\frame
{
  \frametitle{Classes for ODE methods}
\begin{small}
{\scriptsize
In program2.cpp of chapter 8 we have canned the following methods
\begin{itemize}
\item   void euler();
\item   void euler\_cromer();
\item   void midpoint();
 \item  void euler\_richardson();
\item   void half\_step();
\item   void rk2(); //runge-kutta-second-order
\item   void rk4\_step(double,double*,double*,double); // we need it in function rk4() and asc()
\item   void rk4(); //runge-kutta-fourth-order
\item   void asc(); //runge-kutta-fourth-order with adaptive stepsize control
\end{itemize}
 }
 \end{small}
 }


 \frame[containsverbatim]
 {
   \frametitle{Classes for ODE methods}
 \begin{small}
 {\scriptsize
\begin{verbatim}
class pendulum
 {
 private:
   double Q, A_roof, omega_0, omega_roof,g; //
   double y[2];          //for the initial-values of phi and v
   int n;                // how many steps
   double delta_t,delta_t_roof;

 public:
   void derivatives(double,double*,double*);
   void initialise();
   void euler();
   void euler_cromer();
   void midpoint();
   void euler_richardson();
   void half_step();
   void rk2(); //runge-kutta-second-order
   void rk4_step(double,double*,double*,double); // we need it in function rk4() and asc()
   void rk4(); //runge-kutta-fourth-order
   void asc(); //runge-kutta-fourth-order with adaptive stepsize control
 };
\end{verbatim}
 }
 \end{small}
 }



 \frame[containsverbatim]
 {
   \frametitle{Classes for ODE methods}
 \begin{small}
 {\scriptsize
\begin{verbatim}
void pendulum::derivatives(double t, double* in, double* out)
{ /* Here we are calculating the derivatives at (dimensionless) time t
     'in' are the values of phi and v, which are used for the calculation
     The results are given to 'out' */
  
  out[0]=in[1];             //out[0] = (phi)'  = v
  if(Q)
    out[1]=-in[1]/((double)Q)-sin(in[0])+A_roof*cos(omega_roof*t);  //out[1] = (phi)''
  else
    out[1]=-sin(in[0])+A_roof*cos(omega_roof*t);  //out[1] = (phi)''
}
\end{verbatim}
 }
 \end{small}
 }



 \frame[containsverbatim]
 {
   \frametitle{Classes for ODE methods}
 \begin{small}
 {\scriptsize
\begin{verbatim}
int main()
{
  pendulum testcase;
  testcase.initialise();
  testcase.euler();
  testcase.euler_cromer();
  testcase.midpoint();
  testcase.euler_richardson();
  testcase.half_step();
  testcase.rk2();
  testcase.rk4();
  return 0;
}  // end of main function
\end{verbatim}
 }
 \end{small}
 }



 \frame[containsverbatim]
 {
   \frametitle{Classes for ODE methods}
 \begin{small}
 {\scriptsize
In Fortran we would use
\begin{verbatim}
MODULE pendulum
   USE CONSTANTS
   IMPLICIT NONE
   REAL(DP), PRIVATE :: Q, A_roof, omega_0, omega_roof,g
   REAL(DP), PRIVATE :: y(2)         ! for the initial-values of phi and v
   INTEGER, PRIVATE ::  n               ! how many steps
   REAL(DP), PRIVATE :: delta_t,delta_t_roof

   CONTAINS
    SUBROUTINE derivatives(..)
    SUBROUTINE initialise(..)
    SUBROUTINE euler(..)
    SUBROUTINE euler_cromer(..)
    SUBROUTINE midpoint(..)
    etc

END MODULE pendulum
\end{verbatim}
 }
 \end{small}
 }

\frame
{
  \frametitle{The report: how to write a good scienfitic/technical report}
  \begin{block}{What should it contain? A typical structure}
\begin{itemize}
\item An introduction where you explain the aims and rationale for the physics case and 
what you have done. At the end of the introduction you should give a brief
summary of the structure of the report
\item Theoretical models and technicalities. This is the methods section.
\item Results and discussion
\item Conclusions and perspectives
\item Appendix with extra material
\item Bibliography
\end{itemize}
Keep always a good log of what you do.
  \end{block}
} 
  


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Introduction}
You don't need to answer all questions in a chronological order.  When you write the introduction you could focus on the following aspects
\begin{itemize}
\item Motivate the reader, the first part of the introduction gives always a motivation and tries to give the overarching ideas
\item What I have done
\item The structure of the report, how it is organized etc
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Methods sections}
\begin{itemize}
\item Describe the methods and algorithms
\item You need to explain how you implemented the methods and also say something about the structure of your algorithm and present some parts of your code
\item You should plug in some calculations to demonstrate your code, such as selected runs used to validate and verify your results. The latter is extremely important!!  A reader needs to understand that your code reproduces selected benchmarks and reproduces previous results, either numerical and/or well-known  closed form expressions.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Results}
\begin{itemize}
\item Present your results
\item Give a critical discussion of your work and place it in the correct context.
\item Relate your work to other calculations/studies
\item An eventual reader should be able to reproduce your calculations if she/he wants to do so. All input variables should be properly explained.
\item Make sure that figures and tables should contain enough information in their captions, axis labels etc so that an eventual reader can gain a first impression of your work by studying figures and tables only.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Conclusions}
\begin{itemize}
\item State your main findings and interpretations
\item Try as far as possible to present perspectives for future work
\item Try to discuss the pros and cons of the methods and possible improvements
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? additional material}
\begin{itemize}
\item Additional calculations used to validate the codes
\item Selected calculations, these can be listed with 
few comments
\item Listing of the code if you feel this is necessary
\end{itemize}
You can consider moving parts of the material from the methods section to the appendix. You can also place additional material on your webpage. 
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? References}
\begin{itemize}
\item Give always references to material you base your work on, either 
scientific articles/reports or books.
\item {\em Wikipedia is not accepted as a scientific reference}. Under no circumstances.
\item Refer to articles as: name(s) of author(s), journal, volume (boldfaced), page and year
in parenthesis.
\item Refer to books as: name(s) of author(s), title of book, publisher, place and year, eventual page numbers
\end{itemize}
  \end{block}
} 
  





\section{Week 41}
\frame
{
  \frametitle{Week 41}
  \begin{block}{Ordinary differential equations (ODEs) and Partial differential equations (PDEs)}
\begin{itemize}
\item Monday: Repetition from last week
\item Adaptive Runge-Kutta methods and stiff equations
\item Examples with codes
\item Discussion of project 3
\item Begin partial differential equations, discussion of the diffusion equation
\item Tuesday: 
\item Diffusion equation in one spatial dimension, implicit and explicit scheme and the Crank-Nicolson scheme
\end{itemize}
Chapter 9, ODEs with boundary conditions will not be discussed.
  \end{block}
} 

\frame
{
  \frametitle{Project 3: An object oriented example program for project 3}
  \begin{block}{See \url{http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem}}
\begin{itemize}
\item \url{http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/solarsystem.cpp}
\item \url{http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/planet.cpp}
\item \url{http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/planet.h}
\item \url{http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/constants.cpp}
\item \url{http://folk.uio.no/mhjensen/compphys/programs/chapter08/cpp/solarsystem/constants.h}
\end{itemize}
  \end{block}
} 

\frame
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
In case the function to integrate varies slowly or fast in different integration domains, adaptive methods are normally used. One strategy is always to decrease the step size. As we have seen earlier, this leads to more CPU cycles and may lead to loss or numerical precision. An alternative is to use higher-order RK methods for example. However, this leads again to more cycles, furthermore, there is no guarantee that higher-order leads to an improved error.
 }
 \end{small}
 }


\frame
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
Assume the exact result is $\tilde{x}$ and that we are using an RKM method. Suppose we run two calculations, one with $h$ (called $x_1$) and one with $h/2$ (called $x_2$). Then
\[
\tilde{x}=x_1+Ch^{M+1}+O(h^{M+2}),
\] 
and 
\[
\tilde{x}=x_2+2C(h/2)^{M+1}+O(h^{M+2}),
\] 
with $C$ a constant. Note that we calculate two halves in the last equation. We get then
\[
|x_1-x_2| = Ch^{M+1}(1-\frac{1}{2^M}).
\]
yielding 
\[
C=\frac{|x_1-x_2|}{(1-2^{-M})h^{M+1}}.
\]
We rewrite 
\[
\tilde{x}=x_2+\epsilon+O((h)^{M+2}),
\] 
with 
\[
\epsilon = \frac{|x_1-x_2|}{2^M-1}.
\]
 }
 \end{small}
 }


\frame
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
With RK4 the expressions become
\[
\tilde{x}=x_2+\epsilon+O((h)^{6}),
\] 
with 
\[
\epsilon = \frac{|x_1-x_2|}{15}.
\]
The estimate is one order higher than the original RK4. But this method is normally rather inefficient since it requires a lot of computations. We solve typically the equation three times at each time step. 
However, we can compare the estimate $\epsilon$ with some by us given accuracy $\xi$. 
We can then ask the question: what is, with a given $x_j$ and $t_j$, the largest possible step size $\tilde{h}$ that leads to a truncation error below $\xi$?
We want
\[
C\tilde{h} \le \xi,
\]
which leads to 
\[
\left(\frac{\tilde{h}}{h}\right)^{M+1}\frac{|x_1-x_2|}{(1-2^{-M})}\le \xi, 
\]
meaning that
\[
\tilde{h}=h\left(\frac{\xi}{\epsilon}\right)^{1+1/M}. 
\]

 }
 \end{small}
 }



\frame
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
With 
\[
\tilde{h}=h\left(\frac{\xi}{\epsilon}\right)^{1+1/M}. 
\]
we can design the following algorithm:
\begin{itemize}
\item If the two answers are close, keep the approximation to $h$.
\item If $\epsilon > \xi$ we need to decrease the step size in the next time step.
\item If $\epsilon < \xi$ we need to increase the step size in the next time step.
\end{itemize}
A much used algorithm is the so-called RKF45 which uses a combination of a fourth and fifth order RK methods. 
 }
 \end{small}
 }


\frame
{
  \frametitle{Adaptive methods, RKF45}
\begin{small}
{\scriptsize
At
each step, two different approximations for the solution are made and compared. If the
two answers are in close agreement, the approximation is accepted. If the two answers
do not agree to a specified accuracy, the step size is reduced. If the answers agree to
more significant digits than required, the step size is increased.
Each step requires the use of the following six values:
\[
k_1 = h f (t_k , y_k ),
\]
\[
k_2 = h f (t_k + \frac{1}{4}h, y_k + \frac{1}{4}k_1) ,
\]
\[
k_3 = h f (t_k + \frac{3}{8}h, y_k + \frac{3}{32}k_1 + \frac{9}{32}k_2) ,
\]
\[
k_4 = h f (t_k + \frac{12}{13}h, y_k + \frac{1932}{2197}k_1 + \frac{7200}{2197}k_2+\frac{7296}{2197}k_3),
\]


\[
k_5 = h f (t_k + h, y_k + \frac{439}{216}k_1 -8k_2+ \frac{3680}{513}k_3+\frac{845}{4104}k_4),
\]


\[
k_6 = h f (t_k + \frac{1}{2}h, y_k - \frac{8}{27}k_1 + 2k_2-\frac{3544}{2565}k_2+\frac{1859}{4104}k_4-+\frac{11}{40}k_5).
\]
 }
 \end{small}
 }


\frame
{
  \frametitle{Adaptive methods, RKF45}
\begin{small}
{\scriptsize
Then an approximation to the solution of the ODE is made using a Runge-Kutta
method of order 4:
\[
y_{k+1} = y_k  + \frac{25}{216}k_1+\frac{1408}{2565}k_3 +\frac{2197}{4101}k_4-\frac{1}{5}k_5,
\]
where the four function values $k_1$ , $k_3$ , $k_4$ , and $k_5$ are used. Notice that $k_2$ is not used  here.
A better value for the solution is determined using a Runge-Kutta
method of order 5:
\[
z_{k+1} = y_k + \frac{16}{135}k_1+\frac{6656}{12825}k_3 +\frac{28561}{56430}k_4-\frac{9}{50}k_5+\frac{2}{55}k_6.
\]

The optimal time step $\alpha h$ is then determined by
\[
\alpha = \left( \frac{\xi h}{2|z_{k+1}-y_{k+1}|}\right)^{1/4},
\]
with $\xi$ our defined tolerance.
 }
 \end{small}
 }




\frame
{
  \frametitle{Partial Differential Equations, chapter 10}
\begin{small}
{\scriptsize
General 2+1-dim PDE
\[
A(x,y)\frac{\partial^2 U}{\partial x^2}+B(x,y)\frac{\partial^2 U}{\partial x\partial y}
+C(x,y)\frac{\partial^2 U}{\partial y^2}=F(x,y,U,\frac{\partial U}{\partial x}, \frac{\partial U}{\partial y})
\]
Examples
\[
  B=C=0,
\]
give e.g., 1+1-dim diffusion equation
\[
 A\frac{\partial^2 U}{\partial x^2}=\frac{\partial U}{\partial t}
\]
and is an example of a parabolic PDE
 }
 \end{small}
 }


\frame
{
  \frametitle{Partial Differential Equations}
\begin{small}
{\scriptsize
More examples
2+1-dim wave equation
\[
 A\frac{\partial^2 U}{\partial x^2}+C\frac{\partial^2 U}{\partial y^2}=\frac{\partial^2 U}{\partial t^2}
\]
Poisson's (Laplace's $\rho =0$)  equation
\[
 \nabla^2 U({\bf x})=-4\pi \rho({\bf x}).
\]
 }
 \end{small}
 }

\frame
{
  \frametitle{Heat/Diffusion Equation}
\begin{small}
{\scriptsize
Diffusion equation
\[
 \frac{\kappa}{C\rho}\nabla^2 T({\bf x},t) =\frac{\partial T({\bf x},t)}{\partial t}
\]
\[
 \frac{\kappa}{C\rho({\bf x},t)}\nabla^2 T({\bf x},t) =\frac{\partial T({\bf x},t)}{\partial t}
\]
 }
 \end{small}
 }

\frame
{
  \frametitle{Explicit Scheme for the Diffusion Equation}
\begin{small}
{\scriptsize
In one dimension we have thus the following equation
\be
 \nabla^2 u(x,t) =\frac{\partial u(x,t)}{\partial t},
\ee
or 
\be
u_{xx} = u_t,
\ee
with initial conditions, i.e., the conditions at $t=0$, 
\be
u(x,0)= g(x) \hspace{0.5cm} 0 \le x \le L
\ee
with $L=1$ the length of the $x$-region of interest. The 
boundary conditions are 
\be
u(0,t)= a(t) \hspace{0.5cm} t \ge 0,
\ee
and 
\be
u(L,t)= b(t) \hspace{0.5cm} t \ge 0,
\ee
where $a(t)$ and $b(t)$ are two functions which depend on time only, while
$g(x)$ depends only on the position $x$.
 }
 \end{small}
 }



\frame
{
  \frametitle{Explicit Scheme, Forward Euler}
\begin{small}
{\scriptsize
\be
u_t\approx \frac{u_{i,j+1}-u_{i,j}}{\Delta t}, 
\ee
and
\be
u_{xx}\approx \frac{u_{i+i,j}-2u_{i,j}+u_{i-1,j}}{\Delta x^2}.
\ee
The one-dimensional diffusion equation can then be rewritten in its
discretized version as 
\be
\frac{u_{i,j+1}-u_{i,j}}{\Delta t}=\frac{u_{i+i,j}-2u_{i,j}+u_{i-1,j}}{\Delta x^2}.
\ee
Defining $\alpha = \Delta t/\Delta x^2$ results in the explicit scheme
\be
\label{eq:explicitpde}
 u_{i,j+1}= \alpha u_{i-1,j}+(1-2\alpha)u_{i,j}+\alpha u_{i+1,j}.
\ee
 }
 \end{small}
 }



\frame
{
  \frametitle{Explicit Scheme}
\begin{small}
{\scriptsize
\[
   V_{j+1} = AV_{j}
\]
with
\[
 A=\left(\begin{array}{cccc}1-2\alpha&\alpha&0& 0\dots\\ 
                            \alpha&1-2\alpha&\alpha & 0\dots \\ 
                            \dots & \dots & \dots & \dots \\
                      0\dots & 0\dots &\alpha& 1-2\alpha\end{array}
\right)
\]
yielding
\[
   V_{j+1} = AV_{j}=\dots = A^jV_0
\]
The explicit scheme, although being rather simple to implement has a very weak 
stability condition given by 
\be
  \Delta t/\Delta x^2 \le 1/2
\ee
 }
 \end{small}
 }

\frame
{
  \frametitle{Implicit Scheme}
\begin{small}
{\scriptsize
Choose now
\[
u_t\approx \frac{u(x_i,t_j)-u(x_i,t_j-k)}{k}
\]
and
\[
u_{xx}\approx \frac{u(x_i+h,t_j)-2u(x_i,t_j)+u(x_i-h,t_j)}{h^2}
\]
Define $\alpha = k/h^2$. Gives
\[
 u_{i,j-1}= -\alpha u_{i-1,j}+(1-2\alpha)u_{i,j}-\alpha u_{i+1,j}
\]
Here $u_{i,j-1}$ is the only unknown quantity.
 }
 \end{small}
 }

\frame
{
  \frametitle{}
\begin{small}
{\scriptsize
Have
\[
   AV_{j} = V_{j-1}
\]
with
\[
 A=\left(\begin{array}{cccc}1+2\alpha&-\alpha&0& 0\dots\\ 
                            -\alpha&1+2\alpha&-\alpha & 0\dots \\ 
                            \dots & \dots & \dots & \dots \\
                      0\dots & 0\dots &-\alpha& 1+2\alpha\end{array}
\right)
\]
which gives
\[
   V_{j} = A^{-1}V_{j-1}=\dots = A^{-j}V_0
\]
Need only to invert a matrix
 }
 \end{small}
 }


\frame[containsverbatim]
{
  \frametitle{Brute Force Implicit Scheme, inefficient algo}
\begin{small}
{\scriptsize
\begin{verbatim}
!  now invert the matrix
       CALL matinv( a, ndim, det)
       DO i = 1, m
          DO l=1, ndim
             u(l) = DOT_PRODUCT(a(l,:),v(:)) 
          ENDDO
          v = u
          t = i*k
          DO  j=1, ndim
              WRITE(6,*) t, j*h, v(j) 
          ENDDO 
       ENDDO
\end{verbatim}
}
 \end{small}
 }






\frame
{
  \frametitle{Brief Summary of the  Explicit and the Implicit Methods}
\begin{small}
{\scriptsize
\begin{itemize}
\item Explicit is straightforward to code, but avoid doing the matrix vector
multiplication since the matrix is tridiagonal.
 \[
u_t\approx \frac{u(x,t+\Delta t)-u(x,t)}{\Delta t}=\frac{u(x_i,t_j+\Delta t)-u(x_i,t_j)}{\Delta t}
\]
\item The implicit method can be applied in a brute force way as well as long as the
element of the matrix are constants. 
\[
u_t\approx \frac{u(x,t)-u(x,t-\Delta t)}{\Delta t}=\frac{u(x_i,t_j)-u(x_i,t_j-\Delta t)}{\Delta t}
\]
\item However, it is more efficient to use a linear algebra solver for tridiagonal matrices.
\end{itemize}
 }
 \end{small}
 }







\frame
{
  \frametitle{Crank-Nicolson}
\begin{small}
{\scriptsize
\[
  \frac{\theta}{\Delta x^2}\left(u_{i-1,j}-2u_{i,j}+u_{i+1,j}\right)+
  \frac{1-\theta}{\Delta x^2}\left(u_{i+1,j-1}-2u_{i,j-1}+u_{i-1,j-1}\right)=
  \frac{1}{\Delta t}\left(u_{i,j}-u_{i,j-1}\right),
\]
which for $\theta=0$ yields the forward formula for the first derivative and
the explicit scheme, while $\theta=1$ yields the backward formula and the implicit
scheme. These two schemes are called the backward and forward Euler schemes, respectively.
For $\theta = 1/2$ we obtain a new scheme after its inventors, Crank and Nicolson.


 }
 \end{small}
 }

\frame
{
  \frametitle{Crank Nicolson}
\begin{small}
{\scriptsize
Using our previous definition of $\alpha=\Delta t/\Delta x^2$ we can rewrite the latter 
equation as
\[
  -\alpha u_{i-1,j}+\left(2+2\alpha\right)u_{i,j}-\alpha u_{i+1,j}=
  \alpha u_{i-1,j-1}+\left(2-2\alpha\right)u_{i,j-1}+\alpha u_{i+1,j-1},
\]
or in matrix-vector form as
\[
  \left(2\hat{I}+\alpha\hat{B}\right)V_{j}=
  \left(2\hat{I}-\alpha\hat{B}\right)V_{j-1},
\]
 where the vector $V_{j}$ is the same as defined in the implicit case while the matrix
$\hat{B}$ is 
\[
 \hat{B}=\left(\begin{array}{cccc}2&-1&0& 0\dots\\ 
                            -1&2&-1 & 0\dots \\ 
                            \dots & \dots & \dots & \dots \\
                      0\dots & 0\dots && 2\end{array}
\right)
\]
 }
 \end{small}
 }


\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
We start with the forward Euler scheme and Taylor expand $u(x,t+\Delta t)$,
$u(x+\Delta x, t)$ and $u(x-\Delta x,t)$
\begin{eqnarray}
u(x+\Delta x,t)&=u(x,t)+\frac{\partial u(x,t)}{\partial x} \Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2}\Delta x^2+\mathcal{O}(\Delta x^3),\\ \nonumber
u(x-\Delta x,t)&=u(x,t)-\frac{\partial u(x,t)}{\partial x}\Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2} \Delta x^2+\mathcal{O}(\Delta x^3),\\ \nonumber
u(x,t+\Delta t)&=u(x,t)+\frac{\partial u(x,t)}{\partial t}\Delta t+  \mathcal{O}(\Delta t^2).
\label{eq:deltat0}
\end{eqnarray}
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
With these Taylor expansions the approximations for the derivatives takes the form 
\begin{eqnarray}
&\left[\frac{\partial u(x,t)}{\partial t}\right]_{\text{approx}} =\frac{\partial u(x,t)}{\partial t}+\mathcal{O}(\Delta t) , \\ \nonumber
&\left[\frac{\partial^2 u(x,t)}{\partial x^2}\right]_{\text{approx}}=\frac{\partial^2 u(x,t)}{\partial x^2}+\mathcal{O}(\Delta x^2).
\label{eq:diesonne}
\end{eqnarray}
It is easy to convince oneself that the backward Euler method must have the same truncation errors as the forward Euler scheme.
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
For the Crank-Nicolson scheme we also need to Taylor expand $u(x+\Delta x, t+\Delta t)$ and $u(x-\Delta x, t+\Delta t)$ around $t'=t+\Delta t/2$.
\begin{eqnarray}
u(x+\Delta x, t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\notag \\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\ \nonumber
u(x-\Delta x, t+\Delta t)&=u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} - \notag\\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\
u(x+\Delta x,t)&=u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} -\notag \\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\  \nonumber
u(x-\Delta x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\notag \\  \nonumber
&\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3)\\  \nonumber
u(x,t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial t}\frac{\Delta_t}{2} +\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 + \mathcal{O}(\Delta t^3)\\  \nonumber
u(x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2}+\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 + \mathcal{O}(\Delta t^3)
\label{eq:deltat}
\end{eqnarray}
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
We now insert these expansions in the approximations for the derivatives to find
\begin{eqnarray}
&\left[\frac{\partial u(x,t')}{\partial t}\right]_{\text{approx}} =\frac{\partial u(x,t')}{\partial t}+\mathcal{O}(\Delta t^2) , \\ \nonumber
&\left[\frac{\partial^2 u(x,t')}{\partial x^2}\right]_{\text{approx}}=\frac{\partial^2 u(x,t')}{\partial x^2}+\mathcal{O}(\Delta x^2).
\end{eqnarray}
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
The following table summarizes the three methods.
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|} \hline
\emph{Scheme:} & \emph{Truncation Error:} & \emph{Stability requirements:} \\ \hline \hline 
Crank-Nicolson & $\mathcal{O}(\Delta x^2)$ and $\mathcal{O}(\Delta t^2)$ & Stable for all $\Delta t$ and $\Delta x$. \\ \hline 
Backward Euler & $\mathcal{O}(\Delta x^2)$ and $\mathcal{O}(\Delta t)$ & Stable for all $\Delta t$ and $\Delta x$. \\\hline
Forward Euler & $\mathcal{O}(\Delta x^2)$ and $\mathcal{O}(\Delta t)$ & $\Delta t\leq \frac{1}{2}\Delta x^2$ \\ \hline
\end{tabular}
\caption{Comparison of the different schemes.}
\label{tbl:comparison}
\end{center}
\end{table}
}
\end{small}
}

\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
It cannot be repeated enough, it is always useful to find cases where one can compare the numerics
and the developed algorithms and codes with analytic solution.  The above case is also particularly simple. 
We have the following partial differential equation 
\[
 \nabla^2 u(x,t) =\frac{\partial u(x,t)}{\partial t},
\] 
with initial conditions 
\[
u(x,0)= g(x) \hspace{0.5cm} 0 < x < L.
\]
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
The 
boundary conditions are 
\[
u(0,t)= 0 \hspace{0.5cm} t \ge 0,  \hspace{1cm}  u(L,t)= 0 \hspace{0.5cm} t \ge 0,
\]

 We assume that we have solutions of the form (separation of variable)
\begin{equation}
u(x,t)=F(x)G(t),
\end{equation}
which inserted in the partial differential equation results in
\begin{equation}
\frac{F''}{F}=\frac{G'}{G},
\end{equation}
where the derivative is with respect to $x$ on the left hand side and with respect to $t$ on right hand side.
This equation  should hold for all $x$ and $t$. We must require the rhs and lhs to be equal to a constant. 
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
We call this constant $-\lambda^2$. This gives us the two differential equations, 
\begin{equation}
F''+\lambda^2F=0;  \hspace{1cm} G'=-\lambda^2G,
\end{equation}
with general solutions
\begin{equation}
F(x)=A\sin(\lambda x)+B\cos(\lambda x); \hspace{1cm} G(t)=Ce^{-\lambda^2t}.
\end{equation}
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
To satisfy the boundary conditions we require $B=0$ and $\lambda=n\pi/L$. One solution is therefore found to be
\begin{equation}
u(x,t)=A_n\sin(n\pi x/L)e^{-n^2\pi^2 t/L^2}.
\end{equation}
But there are infinitely many  possible $n$ values (infinite number of solutions). Moreover, 
the diffusion equation is linear and because of this we know that a superposition of solutions 
will also be a solution of the equation. We may therefore write
\begin{equation}
u(x,t)=\sum_{n=1}^{\infty} A_n \sin(n\pi x/L) e^{-n^2\pi^2 t/L^2}.
\end{equation}
}
\end{small}
}
\frame
{
  \frametitle{Analysis of diffusion equation}
\begin{small}
{\scriptsize
The coefficient $A_n$ is in turn determined from the initial condition. We require
\begin{equation}
u(x,0)=g(x)=\sum_{n=1}^{\infty} A_n \sin(n\pi x/L).
\end{equation}
The coefficient $A_n$ is the Fourier coefficients for the function $g(x)$. Because of this, $A_n$ is given by (from the theory on Fourier series)
\begin{equation}
A_n=\frac{2}{L}\int_0^L g(x)\sin(n\pi x/L) \mathrm{d}x.
\end{equation}
Different $g(x)$ functions will obviously result in different results for $A_n$.
}
\end{small}
}


\section{Week 42}
\frame
{
  \frametitle{Week 42}
  \begin{block}{Partial differential equations (chapter 10) and begin numerical integration (chapter 5)}
\begin{itemize}
\item Monday: Repetition from last week
\item Discussion of project 3, with an emphasis on object orientation
\item Diffusion equation in two spatial dimensions
\item Poisson's and Laplace's equations
\item Tuesday: 
\item Wave equation in one and two dimensions.
\item If we get time, we will start with numerical integration
\end{itemize}
  \end{block}
} 

\frame
{
  \frametitle{Laplace's and Poisson's equations}
\begin{small}
{\scriptsize
Laplace's equation reads
\be
 \nabla^2 u({\bf x})=u_{xx}+u_{yy}=0.
\ee
with possible boundary conditions
$u(x,y) = g(x,y) $ on the border. There is no time-dependence.
Choosing equally many steps in both directions we have a quadratic or rectangular
grid, depending on whether we choose equal steps lengths or not in the $x$ and
the $y$ directions. Here we set $\Delta x = \Delta y = h$ and obtain 
a discretized version
\be
u_{xx}\approx \frac{u(x+h,y)-2u(x,y)+u(x-h,y)}{h^2},
\ee
and
\be
u_{yy}\approx \frac{u(x,y+h)-2u(x,y)+u(x,y-h)}{h^2},
\ee
 }
 \end{small}
 }

\frame
{
  \frametitle{Laplace's and Poisson's equations}
\begin{small}
{\scriptsize
\be
u_{xx}\approx \frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2},
\ee
and
\be
u_{yy}\approx \frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2},
\ee
which gives when inserted in Laplace's equation
\be
\label{eq:laplacescheme}
  u_{i,j}= \frac{1}{4}\left[u_{i,j+1}+u_{i,j-1}+u_{i+1,j}+u_{i-1,j}\right].
\ee
This is our final numerical scheme for solving Laplace's equation.
Poisson's equation adds only a minor complication 
to the above equation since in this case we have 
\[
    u_{xx}+u_{yy}=-\rho({\bf x}),
\]
and we need only to add a discretized version of $\rho({\bf x})$
resulting in 
\be
\label{eq:poissonscheme}
  u_{i,j}= \frac{1}{4}\left[u_{i,j+1}+u_{i,j-1}+u_{i+1,j}+u_{i-1,j}\right]
           +\rho_{i,j}.
\ee
 }
 \end{small}
 }

\frame
{
  \frametitle{Solution Approach}
\begin{small}
{\scriptsize
The way we solve these equations is based on an iterative scheme we discussed in connection
with linear algebra, namely the so-called Jacobi, Gauss-Seidel and 
relaxation methods. The steps are rather simple. We start with an initial guess
for $u_{i,j}^{(0)}$ where all values are known. To obtain a new solution we
solve Eq.~(\ref{eq:laplacescheme}) or Eq.~(\ref{eq:poissonscheme})
in order to obtain a new solution $u_{i,j}^{(1)}$. 
Most likely this solution will not be a solution to 
Eq.~(\ref{eq:laplacescheme}). This solution is in turn
used to obtain a new and improved $u_{i,j}^{(2)}$. We continue this process
till we obtain a result which satisfies some specific convergence criterion.
 }
 \end{small}
 }

\frame[containsverbatim]
{
  \frametitle{Code example for the two-dimensional diff equation/Laplace}
\begin{small}
{\scriptsize
\begin{lstlisting}
int DiffusionJacobi(int N, double dx, double dt, 
		      double **A, double **q, double abstol){
  int i,j,k;
  int maxit = 100000;
  double sum;
  double ** Aold = CreateMatrix(N,N);
  
  double D = dt/(dx*dx);
  

\end{lstlisting}

}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Code example for the two-dimensional diff equation/Laplace}
\begin{small}
{\scriptsize
\begin{lstlisting}
  for(i=1; i<N-1; i++)
    for(j=1;j<N-1;j++)
      Aold[i][j] = 1.0;
  /* Boundary Conditions -- all zeros */
  for(i=0;i<N;i++){
    A[0][i] = 0.0;
    A[N-1][i] = 0.0;
    A[i][0] = 0.0;
    A[i][N-1] = 0.0;
  }
\end{lstlisting}

}
\end{small}
}





\frame[containsverbatim]
{
  \frametitle{Code example for the two-dimensional diff equation/Laplace}
\begin{small}
{\scriptsize
\begin{lstlisting}
  for(k=0; k<maxit; k++){
    for(i = 1; i<N-1; i++){
      for(j=1; j<N-1; j++){
	A[i][j] = dt*q[i][j] + Aold[i][j] +
	  D*(Aold[i+1][j] + Aold[i][j+1] - 4.0*Aold[i][j] + 
	     Aold[i-1][j] + Aold[i][j-1]);
      }
    }
    sum = 0.0;
    for(i=0;i<N;i++){
      for(j=0;j<N;j++){
	sum += (Aold[i][j]-A[i][j])*(Aold[i][j]-A[i][j]);
	Aold[i][j] = A[i][j];
      }
    }
    if(sqrt(sum)<abstol){DestroyMatrix(Aold,N,N);
      return k;
    }
  }
}
\end{lstlisting}

}
\end{small}
}


\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
Consider first the two-dimensional wave equation for a vibrating square membrane given by the 
following initial and boundary conditions 
\[
\left\{\begin{array}{cc} \lambda\left(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}\right) = \frac{\partial^2u}{\partial t^2}& x,y\in[0,1], t \ge 0 \\
                         u(x,y,0) = sin(\pi x)sin(2\pi y)& x,y\in (0,1) \\
                         u = 0 \hspace{0.2cm} \mathrm{boundary} & t \ge 0\\
                         \partial u/\partial t|_{t=0}=0 & x,y\in (0,1)\\
                       \end{array}\right. . 
\]
The boundary is defined by $x=0$, $x=1$, $y=0$ and $y=1$.
Here we set $\lambda = 1$.
}
\end{small}
}



\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
Our equations depend on three variables whose discretized versions
are now
\be
 \left\{\begin{array}{cc} t_l=l\Delta t& l \ge 0 \\
                          x_i=i\Delta x& 0 \le i \le n_x\\
                          y_j=j\Delta y& 0 \le j \le n_y\end{array}\right. , 
\ee
and we will let $\Delta x=\Delta y = h$ and $n_x=n_y$ for the sake of 
simplicity.
We have now the following discretized partial derivatives
\be
u_{xx}\approx \frac{u_{i+1,j}^l-2u_{i,j}^l+u_{i-1,j}^l}{h^2},
\ee
and
\be
u_{yy}\approx \frac{u_{i,j+1}^l-2u_{i,j}^l+u_{i,j-1}^l}{h^2},
\ee
and
\be
u_{tt}\approx \frac{u_{i,j}^{l+1}-2u_{i,j}^{l}+u_{i,j}^{l-1}}{\Delta t^2}.
\ee
}
\end{small}
}



\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
We merge this into the discretized $2+1$-dimensional wave equation
as 
\be
\label{eq:21wavescheme}
u_{i,j}^{l+1}
=2u_{i,j}^{l}-u_{i,j}^{l-1}+\frac{\Delta t^2}{h^2}\left(u_{i+1,j}^l-4u_{i,j}^l+u_{i-1,j}^l+u_{i,j+1}^l+u_{i,j-1}^l\right),
\ee
where again we have an explicit scheme with $u_{i,j}^{l+1}$ as the only
unknown quantity. 
It is easy to account for different step lengths for $x$ and $y$.
The partial derivative is treated in much the same way
as for the one-dimensional case, except that we now have an additional
index due to the extra spatial dimension, viz., we need to compute 
$u_{i,j}^{-1}$ through 
\be
u_{i,j}^{-1}=u_{i,j}^0+\frac{\Delta t}{2h^2}\left(u_{i+1,j}^0-4u_{i,j}^0+u_{i-1,j}^0+u_{i,j+1}^0+u_{i,j-1}^0\right),
\ee
in our setup of the initial conditions.
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Code example for the two-dimensional wave equation}
\begin{small}
{\scriptsize
We show here how to implement the two-dimensional wave equation
\begin{lstlisting}
  //  After initializations and declaration of variables
  for ( int i = 0; i < n; i++ ) {
    u[i] = new double [n];
    uLast[i] = new double [n];
    uNext[i] = new double [n];
    x[i] = i*h;
    y[i] = x[i];
  }
  // initializing
  for ( int i = 0; i < n; i++ ) {  // setting initial step
    for ( int j = 0; j < n; j++ ) {
      uLast[i][j] = sin(PI*x[i])*sin(2*PI*y[j]);
    }
  }
\end{lstlisting}

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Code example for the two-dimensional wave equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
  for ( int i = 1; i < (n-1); i++ ) {  // setting first step using the initial derivative
    for ( int j = 1; j < (n-1); j++ ) {
      u[i][j] = uLast[i][j] - ((tStep*tStep)/(2.0*h*h))*
	(4*uLast[i][j] - uLast[i+1][j] - uLast[i-1][j] - uLast[i][j+1] - uLast[i][j-1]);
    }
    u[i][0] = 0;  // setting boundaries once and for all
    u[i][n-1] = 0;
    u[0][i] = 0;
    u[n-1][i] = 0;

    uNext[i][0] = 0;
    uNext[i][n-1] = 0;
    uNext[0][i] = 0;
    uNext[n-1][i] = 0;
  }
\end{lstlisting}

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Code example for the two-dimensional wave equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // iterating in time
  double t = 0.0 + tStep;
  int iter = 0;

  while ( t < tFinal ) {
    iter ++;
    t = t + tStep;

    for ( int i = 1; i < (n-1); i++ ) {  // computing next step
      for ( int j = 1; j < (n-1); j++ ) {
	uNext[i][j] = 2*u[i][j] - uLast[i][j] - ((tStep*tStep)/(h*h))*
	  (4*u[i][j] - u[i+1][j] - u[i-1][j] - u[i][j+1] - u[i][j-1]);
      }
    }
\end{lstlisting}

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Code example for the two-dimensional wave equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
    for ( int i = 1; i < (n-1); i++ ) {  // shifting results down
      for ( int j = 1; j < (n-1); j++ ) {
	uLast[i][j] = u[i][j];
	u[i][j] = uNext[i][j];
      }
    }
  }
\end{lstlisting}

}
\end{small}
}




\frame
{
  \frametitle{Closed form solution of the wave equation}
\begin{small}
{\scriptsize
We develop here the analytic solution for the $2+1$ dimensional wave equation with the following boundary and initial conditions
\[
 \left\{\begin{array}{cc} c^2(u_{xx}+u_{yy}) = u_{tt}& x,y\in(0,L), t>0 \\
                         u(x,y,0) = f(x,y) & x,y\in (0,L) \\
                         u(0,0,t)=u(L,L,t)=0 & t > 0\\
                         \partial u/\partial t|_{t=0}= g(x,y) & x,y\in (0,L)\\
                       \end{array}\right. . 
\]
}
\end{small}
}

\frame
{
  \frametitle{Closed form solution of the wave equation}
\begin{small}
{\scriptsize
Our first step is to make the ansatz 
\[
   u(x,y,t) = F(x,y) G(t),
\]
resulting  in the equation
\[
   FG_{tt}= c^2(F_{xx}G+F_{yy}G),
 \]
or
\[
   \frac{G_{tt}}{c^2G} =  \frac{1}{F}(F_{xx}+F_{yy}) = -\nu^2.
 \]
The lhs and rhs are independent of each other and we obtain two differential equations
\[
   F_{xx}+F_{yy}+F\nu^2=0,
\]
and
\[ 
   G_{tt} + Gc^2\nu^2 =    G_{tt} + G\lambda^2 =  0,
\]
with $\lambda = c\nu$. 
}
\end{small}
}


\frame
{
  \frametitle{Closed form solution of the wave equation}
\begin{small}
{\scriptsize
We can in turn make the following ansatz for the $x$  and $y$ dependent part 
\[
    F(x,y) = H(x)Q(y),
\]
which results in 
\[
   \frac{1}{H}H_{xx} =  -\frac{1}{Q}(Q_{yy}+Q\nu^2)= -\kappa^2.
 \]
Since the lhs and rhs are again independent of each other, we can separate the latter equation into two independent 
equations, one for $x$ and one for $y$, namely
\[ 
   H_{xx} + \kappa^2H =  0,
\]
and 
\[ 
   Q_{yy} + \rho^2Q = 0,
\]
with $\rho^2= \nu^2-\kappa^2$. 
}
\end{small}
}



\frame
{
  \frametitle{Closed form solution of the wave equation}
\begin{small}
{\scriptsize
The second step is to solve these differential equations, which all have trigonometric functions as solutions, viz.
\[
H(x) = A\cos(\kappa x)+B\sin(\kappa x),
\]
and 
\[
Q(y) = C\cos(\rho y)+D\sin(\rho y).
\]
The boundary conditions require that $F(x,y) = H(x)Q(y)$ are zero at the boundaries, meaning that
$H(0)=H(L)=Q(0)=Q(L)=0$.  This yields the solutions
\[
  H_m(x) = \sin(\frac{m\pi x}{L}) \hspace{1cm} Q_n(y) = \sin(\frac{n\pi y}{L}),
\]
or  
\[
  F_{mn}(x,y) = \sin(\frac{m\pi x}{L})\sin(\frac{n\pi y}{L}).
\]
}
\end{small}
}
\frame
{
  \frametitle{Closed form solution of the wave equation}
\begin{small}
{\scriptsize
With $\rho^2= \nu^2-\kappa^2$ and $\lambda = c\nu$ we have an eigenspectrum $\lambda=c\sqrt{\kappa^2+\rho^2}$ 
or $\lambda_{mn}= c\pi/L\sqrt{m^2+n^2}$. 
The solution for $G$ is 
\[
G_{mn}(t) = B_{mn}\cos(\lambda_{mn} t)+D_{mn}\sin(\lambda_{mn} t),
\]
with the general solution of the form
\[
u(x,y,t) = \sum_{mn=1}^{\infty} u_{mn}(x,y,t) = \sum_{mn=1}^{\infty}F_{mn}(x,y)G_{mn}(t).
\]
}
\end{small}
}
\frame
{
  \frametitle{Closed form solution of the wave equation}
\begin{small}
{\scriptsize
The final step is to determine the coefficients $B_{mn}$ and $D_{mn}$ from the Fourier coefficients.
The equations for these  are determined by the initial conditions $u(x,y,0) = f(x,y)$ and 
$\partial u/\partial t|_{t=0}= g(x,y)$. 
The final expressions are
\[
B_{mn} = \frac{2}{L}\int_0^L\int_0^L dxdy f(x,y) \sin(\frac{m\pi x}{L})\sin(\frac{n\pi y}{L}),
\]
and  
\[
D_{mn} = \frac{2}{L}\int_0^L\int_0^L dxdy g(x,y) \sin(\frac{m\pi x}{L})\sin(\frac{n\pi y}{L}).
\]
Inserting the particular functional forms of $f(x,y)$ and $g(x,y)$ one obtains the final analytic expressions.
}
\end{small}
}

\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
We can check our results as function of the number of mesh points and in particular against
the stability condition 
\[
\Delta t \le \frac{1}{\sqrt{\lambda}}\left(\frac{1}{\Delta x^2}+\frac{1}{\Delta y^2}\right)^{-1/2}
\]
where $\Delta t$, $\Delta x$ and $\Delta y$ are the chosen step lengths. In our case
$\Delta x=\Delta y=h$.   How do we find this condition?  In one dimension we can proceed
as we did for the diffusion equation.  
}
\end{small}
}


\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
The analytic solution of the wave equation in $2+1$ dimensions has a characteristic 
wave component which reads
\[
u(x,y,t) = A \exp{ (i(k_xx+k_yy-\omega t))}
\]
Then from
\[
u_{xx}\approx \frac{u_{i+1,j}^l-2u_{i,j}^l+u_{i-1,j}^l}{\Delta x^2},
\]
we get, with $u_i=\exp{ikx_i}$
\[
u_{xx}\approx \frac{u_i}{\Delta x^2}\left(\exp{ik\Delta x}-2+\exp{(-ik\Delta x)}\right),
\]
or
\[
u_{xx}\approx 2\frac{u_i}{\Delta x^2}\left(cos(k\Delta x)-1\right)=
-4\frac{u_i}{\Delta x^2}sin^2(k\Delta x/2)
\]
We get similar results for $t$ and $y$.

}
\end{small}
}


\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
We have 
\[
\lambda\left(\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}\right) = \frac{\partial^2u}{\partial t^2},
\]
resulting in 
\[
\lambda\left(-4\frac{u_{ij}^l}{\Delta x^2}\sin^2{(k_x\Delta x/2)}-4\frac{u_{ij}^l}{\Delta y^2}\sin^2{(k_y\Delta y/2)}\right)=-4\frac{u_{ij}^l}{\Delta t^2}\sin^2{(\omega\Delta t/2)}, 
\]
resulting in
\[
\sin{(\omega\Delta t/2)}=\pm \sqrt{\lambda}\Delta t\left(\frac{1}{\Delta x^2}\sin^2{(k_x\Delta x/2)}+\frac{1}{\Delta y^2}\sin^2{(k_y\Delta y/2)}\right)^{1/2}.
\]

The squared sine functions can at most be unity. The frequency 
$\omega$ is real and our wave is neither damped
nor amplified.


}
\end{small}
}



\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
We have
\[
\sin{(\omega\Delta t/2)}=\pm \sqrt{\lambda}\Delta t\left(\frac{1}{\Delta x^2}\sin^2{(k_x\Delta x/2)}+\frac{1}{\Delta y^2}\sin^2{(k_y\Delta y/2)}\right)^{1/2}.
\]
The squared sine functions can at most be unity. $\omega$ is real and our wave is neither damped
nor amplified. The numerical $\omega$ must also be real  which is the case when 
$\sin{(\omega\Delta t/2)}$ is less than or equal to unity, meaning that
\[
\Delta t \le \frac{1}{\sqrt{\lambda}}\left(\frac{1}{\Delta x^2}+\frac{1}{\Delta y^2}\right)^{-1/2}.
\]

}
\end{small}
}





\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
 We modify now the wave equation in order to consider 
a $2+1$ dimensional wave equation with a position dependent velocity, given by 
\[
\frac{\partial^2 u}{\partial t^2} = \nabla\cdot (\lambda(x,y) \nabla u).
\]
If $\lambda$ is  constant, we obtain the standard wave equation discussed in the two previous points.
The solution $u(x,y,t)$ could represent a model for  water waves. It represents then the surface elevation from still water.
We can model $\lambda$ as
\[
\lambda = gH(x,y),
\]
with $g$ being the acceleration of gravity and $H(x,y)$ is the still water depth.

The function $H(x,y)$ simulates the water depth using for example measurements of still water depths 
in say a fjord or the north sea. The boundary conditions are then determined by the coast lines as discussed in point d) below.  We have assumed that the vertical motion is negligible and that 
we deal with long wavelenghts $\tilde{\lambda}$ compared with the depth of the sea $H$, that
is $\tilde{\lambda}/H \gg 1$.  We neglect normally Coriolis effects in such calculations.
}
\end{small}
}


\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
You can discretize 
\[
\nabla \cdot (\lambda(x,y) \nabla u)=  \frac{\partial }{\partial x}\left(\lambda(x,y)\frac{\partial u}{\partial x}\right)+
\frac{\partial }{\partial y}\left(\lambda(x,y)\frac{\partial u}{\partial y}\right), 
\]
as follows using  again a quadratic domain for $x$ and $y$:
\[
\frac{\partial }{\partial x}\left(\lambda(x,y)\frac{\partial u}{\partial x}\right)\approx
\frac{1}{\Delta x} \left(\lambda_{i+1/2,j}\left[\frac{u_{i+1,j}^l-u_{i,j}^l}{\Delta x}\right]
-\lambda_{i-1/2,j}\left[\frac{u_{i,j}^l-u_{i-1,j}^l}{\Delta x}\right]\right),
\]
and 
\[
\frac{\partial }{\partial y}\left(\lambda(x,y)\frac{\partial u}{\partial y}\right)\approx
\frac{1}{\Delta y} \left(\lambda_{i,j+1/2}\left[\frac{u_{i,j+1}^l-u_{i,j}^l}{\Delta y}\right]
-\lambda_{i,j-1/2}\left[\frac{u_{i,j}^l-u_{i,j-1}^l}{\Delta y}\right]\right).
\]
}
\end{small}
}


\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
How did we do that? Look at the derivative wrt $x$ only:\newline
First we compute the derivative
\[
   \frac{d}{dx}\left(\lambda(x)\frac{du}{dx}\right)|_{x=x_i} \approx 
   \frac{1}{\Delta x}\left(\lambda\frac{du}{dx}|_{x=x_{i+1/2}}-\lambda\frac{du}{dx}|_{x=x_{i-1/2}}\right),
\]
where we approximated it at the midpoint by going half a step to the right and half a step to 
the left.  Then we approximate 
\[
\lambda\frac{du}{dx}|_{x=x_{i+1/2}}\approx \lambda_{x_{i+1/2}}\frac{u_{i+1}-u_i}{\Delta x},
\]
and similarly for $x = x_i-1/2$.
}
\end{small}
}


\section{Week 43}
\frame
{
  \frametitle{Week 43}
  \begin{block}{Numerical integration (chapter 5)}
\begin{itemize}
\item Monday: Repetition from last week
\item Numerical integration, Trapezoidal and Simpson's rules (equal step methods)
\item Gaussian quadrature (better methods)
\item Tuesday: 
\item Gaussian quadrature, continued
\item Our first encounter with parallelization, this week MPI, next week also OpenMP with examples
\end{itemize}
Next week we start also with Monte Carlo methods, which will keep us busy till the end of November.
  \end{block}
} 




\frame
{
  \frametitle{Numerical integration and Equal Step Methods}
  \begin{block}{Generalities}
\begin{small}
{\scriptsize
\begin{itemize}
   \item Choose a step size 
    \[ 
        h=\frac{b-a}{N}
    \]
   where $N$ is the number of steps and $a$ and $b$ the lower and upper limits
   of integration.
   \item Choose then to stop the Taylor expansion of the function $f(x)$ at a 
         certain derivative.  
   \item With these approximations to $f(x)$ perform the integration.
\end{itemize}
\[
    \int_a^bf(x) dx= \int_a^{a+2h}f(x)dx + \int_{a+2h}^{a+4h}f(x)dx+\dots \int_{b-2h}^{b}f(x)dx.
\]
The strategy then is to find a reliable Taylor expansion for $f(x)$ in the smaller
sub intervals. Consider e.g., evaluating $\int_{-h}^{+h}f(x)dx$
}
\end{small}
  \end{block}
}


\frame
{
  \frametitle{Equal Step Methods}
  \begin{block}{Trapezoidal Rule}
\begin{small}
{\scriptsize
Taylor expansion 
\[
   f(x)=f_0 + \frac{f_h-f_0}{h}x+O(x^2),
\]
for $x=x_0$ to $x=x_0+h$ and  
\[
   f(x)=f_0 + \frac{f_0-f_{-h}}{h}x+O(x^2),
\]
for $x=x_0-h$ to $x=x_0$. The error goes like $O(x^2)$.
If we then evaluate the integral we obtain
\[
   \int_{-h}^{+h}f(x)dx=\frac{h}{2}\left(f_h + 2f_0 + f_{-h}\right)+O(h^3),
\]
which is the well-known trapezoidal rule.  Local error
$O(h^3)=O((b-a)^3/N^3)$, and the {\em global error} goes like $\approx O(h^2)$. 
}
\end{small}
  \end{block}
}



\frame
{
  \frametitle{Equal Step Methods}
  \begin{block}{Trapezoidal Rule}
\begin{small}
{\scriptsize
Easy to implement  numerically through the following simple algorithm
\begin{itemize}
   \item Choose the number of mesh points and fix the step.
   \item calculate $f(a)$ and $f(b)$ and multiply with $h/2$
   \item Perform a loop over $n=1$ to $n-1$ ($f(a)$ and $f(b)$ are known) and sum up
         the terms $f(a+h) +f(a+2h)+f(a+3h)+\dots +f(b-h)$. Each step in the loop
         corresponds to a given value $a+nh$. 
   \item Multiply the final result by $h$ and add $hf(a)/2$ and $hf(b)/2$.
\end{itemize}
}
\end{small}
  \end{block}
}



\frame[containsverbatim]
{
  \frametitle{Trapezoidal Rule}
\begin{small}
{\scriptsize
\begin{lstlisting} 
double trapezoidal_rule(double a, double b, int n, 
                        double (*func)(double))
{
      double trapez_sum;
      double fa, fb, x, step;
      int    j;
      step=(b-a)/((double) n);
      fa=(*func)(a)/2. ;
      fb=(*func)(b)/2. ;
      trapez_sum=0.;
      for (j=1; j <= n-1; j++){
         x=j*step+a;
         trapez_sum+=(*func)(x);
      }
      trapez_sum=(trapez_sum+fb+fa)*step;
      return trapez_sum;
}  // end function for trapezoidal rule 
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Trapezoidal Rule}
\begin{small}
{\scriptsize

Pay attention to the way we transfer the name of a function. This gives us the possibility to define a general trapezoidal method, where we give as input the name of the function.
\begin{lstlisting} 
double trapezoidal_rule(double a, double b, int n, 
                        double (*func)(double))
\end{lstlisting}
We call this function simply as something like this 
\begin{lstlisting} 
integral =  trapezoidal_rule(a, b, n,  mysuperduperfunction); 
\end{lstlisting}
}
\end{small}
}



\frame
{
  \frametitle{Equal Step Methods}
  \begin{block}{Simpson}
\begin{small}
{\scriptsize
The first and second 
derivatives are given by 
\[
   \frac{f_h-f_{-h}}{2h}=f'_0+\sum_{j=1}^{\infty}\frac{f_0^{(2j+1)}}{(2j+1)!}h^{2j},
\]
and
\[
 \frac{ f_h -2f_0 +f_{-h}}{h^2}=f_0''+2\sum_{j=1}^{\infty}\frac{f_0^{(2j+2)}}{(2j+2)!}h^{2j},
\]
results in
$f(x)=f_0 + \frac{f_h-f_{-h}}{2h}x + \frac{ f_h -2f_0 +f_{-h}}{h^2}x^2 +O(x^3)$.
Inserting this formula in the integral 
\[
   \int_{-h}^{+h}f(x)dx=\frac{h}{3}\left(f_h + 4f_0 + f_{-h}\right)+O(h^5),
\]
which is Simpson's rule. 
}
\end{small}
  \end{block}
}



\frame
{
  \frametitle{Equal Step Methods}
  \begin{block}{Simpson's rule}
\begin{small}
{\scriptsize
Note that the improved accuracy in the evaluation of
the derivatives gives a better error approximation, $O(h^5)$ vs.\ $O(h^3)$ .
But this is just the {\em local error approximation}. 
Using Simpson's rule we arrive at the composite rule
\[
   I=\int_a^bf(x) dx=\frac{h}{3}\left(f(a) + 4f(a+h) +2f(a+2h)+
                          \dots +4f(b-h)+ f_{b}\right),
   \label{eq:simpson}
\]
with a global error which goes like $O(h^4)$. 
Algo
\begin{itemize}
   \item Choose the number of mesh points and fix the step.
   \item calculate $f(a)$ and $f(b)$
   \item Perform a loop over $n=1$ to $n-1$ ($f(a)$ and $f(b)$ are known) and sum up
         the terms $4f(a+h) +2f(a+2h)+4f(a+3h)+\dots +4f(b-h)$. Odd values of $n$ give $4$ as factor
         while even values yield $2$ as factor. 
   \item Multiply the final result by $\frac{h}{3}$.
\end{itemize}
}
\end{small}
\end{block}
}



\frame
{
  \frametitle{Equal Step Methods}
\begin{small}
{\scriptsize
The basic idea behind all integration methods is to approximate the integral
\[ 
   I=\int_a^bf(x)dx \approx \sum_{i=1}^N\omega_if(x_i),  
\]
where $\omega$ and $x$ are the weights and the chosen mesh points, respectively.
Simpson's rule gives
\[
   \omega : \left\{h/3,4h/3,2h/3,4h/3,\dots,4h/3,h/3\right\},
\]
for the weights, while the trapezoidal rule resulted in 
\[
   \omega : \left\{h/2,h,h,\dots,h,h/2\right\}.
\]
In general, an integration formula which is based on a Taylor series using $N$ points,
will integrate exactly a polynomial $P$ of degree $N-1$. That is, the $N$ weights
$\omega_n$ can be chosen to satisfy $N$ linear equations
}
\end{small}
}


\frame
{
  \frametitle{Equal Step Methods, Polynomials and Newton-Cotes}
\begin{small}
{\scriptsize
Given $n+1$ distinct points $x_0,\dots, x_n\in[a,b]$ and $n+1$ values $y_0,\dots,y_n$ there exists a 
unique polynomial $p_n$ with the property 
\[ 
   p_n(x_j) = y_j\hspace{0.2cm} j=0,\dots,n
\]
In the Lagrange representation this interpolation polynomial is given by
\[
p_n = \sum_{k=0}^nl_ky_k,
\]
with the Lagrange factors
\[
   l_k(x) = \prod_{\begin{array}{c}i=0 \\ i\ne k\end{array}}^n\frac{x-x_i}{x_k-x_i}\hspace{0.2cm} k=0,\dots,n
\]
Example: $n=1$ 
\[
p_1(x) = y_0\frac{x-x_1}{x_0-x_1}+y_1\frac{x-x_0}{x_1-x_0}=\frac{y_1-y_0}{x_1-x_0}x-\frac{y_1x_0+y_0x_1}{x_1-x_0},
\]
which we recognize as the equation for a straight line.
}
\end{small}
}

\frame
{
  \frametitle{Equal Step Methods, Polynomials and Newton-Cotes}
\begin{small}
{\scriptsize
The polynomial interpolatory quadrature of order $n$ with equidistant quadrature points $x_k=a+kh$
and step $h=(b-a)/n$ is called the Newton-Cotes quadrature formula of order $n$.
The integral is
\[
  \int_a^bf(x)dx \approx \int_a^bp_n(x)dx = \sum_{k=0}^nw_kf(x_k)
\]
with 
\[
   w_k = h\frac{(-1)^{n-k}}{k!(n-k)!}\int_0^n\prod_{\begin{array}{c}j=0 \\ j\ne k\end{array}}^n(z-j)dz,
\]
for $k=0,\dots,n$.
}
\end{small}
}

\frame
{
  \frametitle{Equal Step Methods, Polynomials and Newton-Cotes}
\begin{small}
{\scriptsize
The local error for the trapezoidal rule is
\[
\int_a^bf(x)dx -\frac{b-a}{2}\left[f(a)+f(b)\right]=-\frac{h^3}{12}f^{(2)}(\xi),
\]
and the global error (composite formula) 
\[
\int_a^bf(x)dx -T_h(f)=-\frac{b-a}{12}h^2f^{(2)}(\xi).
\]
For Simpson's rule we have
\[
\int_a^bf(x)dx -\frac{b-a}{6}\left[f(a)+4f((a+b)/2)+f(b)\right]=-\frac{h^5}{90}f^{(4)}(\xi),
\]
and the global error
\[
\int_a^bf(x)dx -S_h(f)=-\frac{b-a}{180}h^4f^{(4)}(\xi).
\]
with $\xi\in[a,b]$.
}
\end{small}
}

\frame
{
  \frametitle{Gaussian Quadrature}
\begin{small}
{\scriptsize
\begin{itemize} 
  \item Methods based on Taylor series using $n+1$ points will
        integrate exactly a polynomial $P$ of degree $n$. If a function $f(x)$
        can be approximated with a polynomial of degree $n$
        \[ 
          f(x)\approx P_{n}(x), 
        \]
         with $n+1$ mesh points we should be able to integrate exactly the 
         polynomial $P_{n}$. 
   \item Gaussian quadrature methods promise more than this. We can get a better
         polynomial approximation with order greater than $n+1$  to $f(x)$ and still
         get away with only $n+1$ mesh points. More precisely, we approximate
         \[
            f(x) \approx P_{2n+1}(x),
         \]
         and with only $n+1$ mesh points these methods promise that 
         \[
            \int f(x)dx \approx \int P_{2n+1}(x)dx=\sum_{i=0}^{n} P_{2n+1}(x_i)\omega_i,
         \]
\end{itemize}
}
\end{small}
}





\frame
{
  \frametitle{Gaussian Quadrature}
What we have done till now is called Newton-Cotes quadrature. The numerical
approximation  goes like $O(h^n)$, where $n$ is method-dependent.

A greater precision for a given amount of numerical work can  be achieved
if we are willing to give up the requirement of equally spaced integration points.  
In Gaussian quadrature (hereafter GQ), both the mesh points and the weights are to
be determined. The points will not be equally spaced
The theory behind GQ is to obtain an arbitrary weight $\omega$ through the use of
so-called orthogonal polynomials. These polynomials are orthogonal in some
interval say e.g., [-1,1]. Our points $x_i$ are chosen in some optimal sense subject
only to the constraint that they should lie in this interval. Together with the weights
we have then $2(n+1)$ ($n+1$ the number of points) parameters at our disposal.  
}



\frame
{
  \frametitle{Gaussian Quadrature}

Even though the integrand is not smooth, we could render it smooth by extracting
from it the weight function of an orthogonal polynomial, i.e.,
we are rewriting
\[
   I=\int_a^bf(x)dx =\int_a^bW(x)g(x)dx\approx \sum_{i=0}^n\omega_ig(x_i),  
\]
where $g$ is smooth and $W$ is the weight function, which is to  be associated with a given 
orthogonal polynomial.
}




\frame
{
  \frametitle{Gaussian Quadrature}
  \begin{block}{Weight Functions}
\begin{small}
{\scriptsize
The weight function $W$ is non-negative in the integration interval 
$x\in [a,b]$ such that
for any $n \ge 0$ $\int_a^b |x|^n W(x) dx$ is integrable. The naming
weight function arises from the fact that it may be used to give more emphasis
to one part of the interval than another. 
\begin{center}
\begin{tabular}{rrr}\hline
Weight function&Interval&Polynomial \\\hline
  $W(x)=1$  &$x\in [a,b]$    &Legendre      \\
  $W(x)=e^{-x^2}$  &$-\infty \le x \le \infty$    &Hermite      \\
  $W(x)=e^{-x}$  &$0 \le x \le \infty$    &Laguerre      \\
  $W(x)=1/(\sqrt{1-x^2})$  &$-1 \le x \le 1$    &Chebyshev      \\ \hline
\end{tabular}  
\end{center}  
}
\end{small}
  \end{block}
}






\frame
{
  \frametitle{Legendre}
\begin{small}
{\scriptsize
\[
   I=\int_{-1}^{1}f(x)dx 
\]
\[
C(1-x^2)P-m_l^2P+(1-x^2)\frac{d}{dx}\left((1-x^2)\frac{dP}{dx}\right)=0.
\]
$C$ is a constant. For $m_l=0$ we obtain the Legendre polynomials
as solutions, whereas $m_l \ne 0$ yields the so-called associated Legendre
polynomials. 
The corresponding polynomials $P$ are
\[
   L_k(x)=\frac{1}{2^kk!}\frac{d^k}{dx^k}(x^2-1)^k \hspace{1cm} k=0,1,2,\dots,
\]
which, up to a factor, are the Legendre polynomials $L_k$. 
The latter fulfil the orthorgonality relation
\[
  \int_{-1}^1L_i(x)L_j(x)dx=\frac{2}{2i+1}\delta_{ij},
\]
and the recursion relation
\[
  (j+1)L_{j+1}(x)+jL_{j-1}(x)-(2j+1)xL_j(x)=0.
\]
}
\end{small}
}



\frame
{
  \frametitle{Laguerre}
\begin{small}
{\scriptsize
\[
   I=\int_0^{\infty}f(x)dx =\int_0^{\infty}x^{\alpha}e^{-x}g(x)dx.
\]
These polynomials arise from the solution of the differential
equation
\[
\left(\frac{d^2 }{dx^2}-\frac{d }{dx}+\frac{\lambda}{x}-\frac{l(l+1)}{x^2}\right){\cal L}(x)=0,
\]
where $l$ is an integer $l\ge 0$ and $\lambda$ a constant. 
They fulfil the orthorgonality relation
\[
  \int_{-\infty}^{\infty}e^{-x}{\cal L}_n(x)^2dx=1,
\]
and the recursion relation
\[
  (n+1){\cal L}_{n+1}(x)=(2n+1-x){\cal L}_{n}(x)-n{\cal L}_{n-1}(x).
\]
}
\end{small}
}



\frame
{
  \frametitle{Hermite}
\begin{small}
{\scriptsize
In a similar way, for an integral which goes like
\[ 
   I=\int_{-\infty}^{\infty}f(x)dx =\int_{-\infty}^{\infty}e^{-x^2}g(x)dx.
\]
we could use the Hermite polynomials in order to extract weights and mesh points.
The Hermite polynomials are the solutions of the following differential
equation
\[
   \frac{d^2H(x)}{dx^2}-2x\frac{dH(x)}{dx}+
       (\lambda-1)H(x)=0.
   \label{eq:hermite}
\]
They fulfil the orthorgonality relation
\[
  \int_{-\infty}^{\infty}e^{-x^2}H_n(x)^2dx=2^nn!\sqrt{\pi},
\]
and the recursion relation
\[
  H_{n+1}(x)=2xH_{n}(x)-2nH_{n-1}(x).
\]
}
\end{small}
}

\frame
{
  \frametitle{Gaussian Quadrature, general Properties}
\begin{small}
{\scriptsize
A quadrature formula 
\[ \int_a^bW(x)f(x)dx \approx \sum_{i=0}^n\omega_if(x_i), \]
with $n+1$ distinct quadrature points (mesh points) is a called a Gaussian quadrature 
formula if it integrates all polynomials $p\in P_{2n+1}$ exactly, that is
\[ \int_a^bW(x)p(x)dx =\sum_{i=0}^n\omega_ip(x_i), \] 
It is assumed that $W(x)$ is continuous and positive and that the integral
\[ \int_a^bW(x)dx , \]
exists. Note that the replacement of $f\rightarrow Wg$ is normally a better approximation
due to the fact that we may isolate possible singularities of $W$ and its 
derivatives at the endpoints of the interval. 
}
\end{small}
}




\frame[containsverbatim]
 {
   \frametitle{Numerical integration: A simple example}
 \begin{small}
 {\scriptsize
We want to compute 
\[
I= \int_0^{\infty} x\exp{(-x)}\sin{x}=\frac{1}{2},
\]
using brute force Trapezoidal rule, Simpson's rule, Gauss-Legendre, Gauss-Laguerre and Gauss-Legendre again but with a smarter mapping.
\begin{itemize}
\item Before we start it can be useful to study the integrand.
\item How should we pick the integration limits?
\end{itemize}
 }
 \end{small}
 }

\frame[containsverbatim]
 {
   \frametitle{Simple integral}
 \begin{small}
 {\scriptsize
Approximate
\[ \int_0^{\infty}f(x)dx \approx \int_0^{\Lambda}f(x)dx\]
\begin{lstlisting}
     int n;
     double a, b, alf, xx;
     cout << "Read in the number of integration points" << endl;
     cin >> n;
     cout << "Read in integration limits" << endl;
     cin >> a >> b;
\end{lstlisting}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Simple integral}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
//   reserve space in memory for vectors containing the mesh points
//   weights and function values for the use of the gauss-legendre
//   method
     double *x = new double [n];
     double *w = new double [n];
     // Gauss-Laguerre is old-fashioned translation of F77 --> C++
     // arrays start at 1 and end at n
     double *xgl = new double [n+1];
     double *wgl = new double [n+1];
\end{lstlisting}
 }
 \end{small}
 }

\frame[containsverbatim]
 {
   \frametitle{Simple integral}
 \begin{small}
 {\scriptsize
Note the parameter $alf$  in $x^{alpha}\exp{-x}$ 
\begin{lstlisting}
//   set up the mesh points and weights
     gauleg(a, b,x,w, n);
//   set up the mesh points and weights
     alf = 1.0;  //  <---  Note alf
     gauss_laguerre(xgl,wgl, n, alf);
//   evaluate the integral with the Gauss-Legendre method
//   Note that we initialize the sum. Here brute force gauleg
     double int_gauss = 0.;
     for ( int i = 0;  i < n; i++){
        int_gauss+=w[i]*int_function(x[i]);
     }
\end{lstlisting}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Simple integral, importance integration/sampling}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
//   evaluate the integral with the Gauss-Laguerre method
//   Note that we initialize the sum
     double int_gausslag = 0.;
     for ( int i = 1;  i <= n; i++){
       int_gausslag+=wgl[i]*sin(xgl[i]);     }
\end{lstlisting}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Simple integral}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
//   evaluate the integral with the Gauss-Laguerre method
//   Here we change the mesh points with a mapping
//   Need to call gauleg from -1 to + 1
     gauleg(-1.0, 1.0,x,w, n);
     double pi_4 = acos(-1.0)*0.25;
     for ( int i = 0;  i < n; i++){
       xx=pi_4*(x[i]+1.0); 
       r[i]= tan(xx);
       s[i]=pi_4/(cos(xx)*cos(xx))*w[i];
     }
     double int_gausslegimproved = 0.;
     for ( int i = 0;  i < n; i++){
       int_gausslegimproved += s[i]*int_function(r[i]);     
     }
\end{lstlisting}
 }
 \end{small}
 }






\frame[containsverbatim]
 {
   \frametitle{A six-dimensional integral, project 3 2010}
 \begin{small}
 {\scriptsize
The ansatz for the wave function for two electrons is given by the product of two
$1s$ wave functions as 
\[
   \Psi({\bf r}_1,{\bf r}_2)  =   e^{-\alpha (r_1+r_2)}.
\]
Note that it is not possible to find a closed form  solution to Schr\"odinger's equation for 
two interacting electrons in the helium atom. 

The integral we need to solve is the quantum mechanical expectation value of the correlation
energy between two electrons, namely
\begin{equation}\label{eq:correlationenergy}
   \langle \frac{1}{|{\bf r}_1-{\bf r}_2|} \rangle =
   \int_{-\infty}^{\infty} d{\bf r}_1d{\bf r}_2  e^{-2\alpha (r_1+r_2)}\frac{1}{|{\bf r}_1-{\bf r}_2|}=\frac{5\pi^2}{16^2}=0.192765711.
\end{equation}
Note that our wave function is not normalized. There is a normalization factor missing. 
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Brute force, six-dimensional integral, Gauss-Legendre}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
     double *x = new double [N];
     double *w = new double [N];
//   set up the mesh points and weights
     gauleg(a,b,x,w, N);

//   evaluate the integral with the Gauss-Legendre method
//   Note that we initialize the sum
     double int_gauss = 0.;
     for (int i=0;i<N;i++){
	     for (int j = 0;j<N;j++){
	     for (int k = 0;k<N;k++){
	     for (int l = 0;l<N;l++){
	     for (int m = 0;m<N;m++){
	     for (int n = 0;n<N;n++){
        int_gauss+=w[i]*w[j]*w[k]*w[l]*w[m]*w[n]
       *int_function(x[i],x[j],x[k],x[l],x[m],x[n]);
     		}}}}}
	}
\end{lstlisting}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{The six-dimensional integral with Gauss-Legendre}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
//  this function defines the function to integrate
double int_function(double x1, double y1, double z1, double x2, double y2, double z2)
{
   double alpha = 2.;
// evaluate the different terms of the exponential
   double exp1=-2*alpha*sqrt(x1*x1+y1*y1+z1*z1);
   double exp2=-2*alpha*sqrt(x2*x2+y2*y2+z2*z2);
   double deno=sqrt(pow((x1-x2),2)+pow((y1-y2),2)+pow((z1-z2),2));
  if(deno <pow(10.,-6.)) { return 0;}
  else return exp(exp1+exp2)/deno;
} // end of function to evaluate

\end{lstlisting}
 }
 \end{small}
 }



\frame[containsverbatim]
 {
   \frametitle{Switch to spherical coordinates}
 \begin{small}
 {\scriptsize
Useful to change to spherical coordinates
\[
   d{\bf r}_1d{\bf r}_2  = r_1^2dr_1 r_2^2dr_2 dcos(\theta_1)dcos(\theta_2)d\phi_1d\phi_2,
\]
and 
\[
   \frac{1}{r_{12}}= \frac{1}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
\]
with 
\[
\cos(\beta) = \cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))
\]
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Switch to spherical coordinates}
 \begin{small}
 {\scriptsize
This means that our integral becomes
\[
   \int_0^{\infty} r_1^2dr_1 \int_0^{\infty}r_2^2dr_2 \int_0^{\pi}dcos(\theta_1)\int_0^{\pi}dcos(\theta_2)\int_0^{2\pi}d\phi_1\int_0^{2\pi}d\phi_2   \times
\]
\[
\frac{e^{-2\alpha (r_1+r_2)}}{\sqrt{r_1^2+r_2^2-2r_1r_2\cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))   }}
\]
since
\[
   \frac{1}{r_{12}}= \frac{1}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
\]
with 
\[
\cos(\beta) = \cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))
\]
 }
 \end{small}
 }


\frame
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
Before we abondon totally methods like the trapezoidal rule, 
we mention breefly how
an adaptive integration method can be implemented.

The above methods are all based on a defined step length, normally provided by the user,
dividing the integration domain with a fixed number of subintervals.
This is rather simple to implement may be inefficient, in particular if the integrand
varies considerably in certain areas of the integration domain. In these areas the number of fixed integration points may not be adequate. In other regions, the integrand may vary slowly
and fewer integration points may be needed.
}
\end{small}
}


\frame
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
In order to account for such features, it may be convenient to first study the properties of
integrand, via for example a plot of the function to integrate. If this function
oscillates largely in some specific domain we may then opt for adding more integration points
to that particular domain. However, this procedure needs to be repeated for every new integrand and lacks obviously the advantages of a more generic code.  
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
Assume that we want to compute an integral using say the trapezoidal rule. We limit ourselves
to a one-dimensional integral.
Our integration domain is defined by $x\in [a,b]$. The algorithm goes as follows
\begin{itemize}
\item We compute our first approximation by computing the integral for the full domain. We label this as $I^{(0)}$. It is obtained by calling our previously discussed function
{\bf trapezoidal\_rule} as
\begin{lstlisting} 
I0 = trapezoidal_rule(a, b, n, function);    
\end{lstlisting}
\item In the next step  we split the integration in two, with $c= (a+b)/2$. We compute then the two integrals $I^{(1L)}$ and $I^{(1R)}$
\begin{lstlisting}
I1L = trapezoidal_rule(a, c, n, function);
\end{lstlisting}
and 
\lstset{language=c++}
\begin{lstlisting}
I1R = trapezoidal_rule(c, b, n, function);
\end{lstlisting}
With a given defined tolerance, being a small number provided by us, we estimate the difference
$|I^{(1L)}+I^{(1R)}-I^{(0)}| < \mathrm{tolerance}$. If this test is satisfied, our first approximation is satisfactory.
\item If not, we can set up a recursive procedure where the integral is split into subsequent
subintervals until our tolerance is satisfied. 
\end{itemize}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
\lstset{language=c++}
\begin{lstlisting}
//     Simple recursive function that implements the 
//     adaptive integration using the trapezoidal rule
const int maxrecursions = 50;
const double tolerance = 1.0E-10;
//  Takes as input the integration  limits, number of points, function to integrate
//  and the number of steps 
void adaptive_integration(double a, double b, double *Integral, int n, int steps, double (*func)(double))
     if ( steps > maxrecursions){ 
        cout << 'Too many recursive steps, the function varies too much' << endl;
        break;
     }
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Adaptive methods}
\begin{small}
{\scriptsize
\lstset{language=c++}
\begin{lstlisting}
     double c = (a+b)*0.5;  
     // the whole integral
     double I0 = trapezoidal_rule(a, b,n, func);
     //  the left half
     double I1L = trapezoidal_rule(a, c,n, func);
     //  the right half
     double I1R = trapezoidal_rule(c, b,n, func);
     if (fabs(I1L+I1R-I0) < tolerance )  integral = I0;
     else
     { 
        adaptive_integration(a, c, integral, int n, ++steps, func)
        adaptive_integration(c, b, integral, int n, ++steps, func)
     }
}
// end function Adaptive integration
\end{lstlisting}
The variables {\bf Integral} and {\bf steps} should be initialized to zero by the function
that calls the adaptive procedure.
}
\end{small}
}


\section{Week 44}
\frame
{
  \frametitle{Week 44}
  \begin{block}{Monte Carlo methods}
\begin{itemize}
\item Monday: Repetition from last week
\item Parallelization using MPI
\item Introduction to Monte Carlo methods and Monte Carlo integration
\item Tuesday: 
\item Monte Carlo integration and importance sampling
\item Discussion of project 4.
%\item Diffusion and Master equation
%\item Detailed balance and the Metropolis(-Hastings) algorithm
%\item Examples of the use of the Metropolis(-Hastings) algorithm: Maxwell-Boltzmann distribution. 
\end{itemize}
  \end{block}
} 


\frame[containsverbatim]
{
  \frametitle{Going Parallel: Divide et impera - Divide and Conquer (accordingly after Julius Caesar)}
We will first meet the concept of

{\bf Task parallelism}: the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize. 
Monte Carlo simulation or integrations are examples of this. 
It is almost embarrassingly trivial to parallelize
Monte Carlo and numerical integration codes.

We will use MPI=Message Passing Interface. 
MPI is a message-passing library where all the routines
have corresponding C/C++-binding
\begin{lstlisting}
   MPI_Command_name
\end{lstlisting}
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
\begin{lstlisting}
   MPI_COMMAND_NAME
\end{lstlisting}
} 


\frame[containsverbatim]
{
  \frametitle{What is Message Passing Interface (MPI)? Yet another library!}

MPI is a library, not a language. It specifies the names, calling sequences and results of functions
or subroutines to be called from C or Fortran programs, and the classes and methods that make up the MPI C++
library. The programs that users write in Fortran, C or C++ are compiled with ordinary compilers and linked
with the MPI library.

MPI is a specification, not a particular implementation. MPI programs should be able to run
on all possible machines and run all MPI implementetations without change.

An MPI computation is a collection of processes communicating with messages.

See chapter 5.5 of lecture notes for more details.
} 

\frame[containsverbatim]
{
  \frametitle{MPI}

MPI is a library specification for the message passing interface,
proposed as a standard.
\begin{itemize}
\item independent of hardware;
\item not a language or compiler specification;
\item not a specific implementation or product.
\end{itemize}


A message passing standard for portability and ease-of-use. 
Designed for high performance.

Insert communication and synchronization functions where necessary.
} 



\frame[containsverbatim]
{
  \frametitle{Demands from the HPC community}
In the field of scientific computing, there is an ever-lasting wish
to do larger simulations using shorter computer time.

Development of the capacity for single-processor computers can
hardly keep up with the pace of scientific computing:
\begin{itemize}
\item processor speed
\item memory size/speed
\end{itemize}

Solution: parallel computing!
} 


\frame[containsverbatim]
{
  \frametitle{The basic ideas of parallel computing}
\begin{itemize}
\item Pursuit of shorter computation time and larger simulation size
gives rise to parallel computing.
\item Multiple processors are involved to solve a global problem.
\item The essence is to divide the entire computation evenly among
collaborative processors.  Divide and conquer.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{A rough classification of hardware models}
\begin{itemize}
\item
Conventional single-processor computers can be called SISD
(single-instruction-single-data) machines.
\item SIMD (single-instruction-multiple-data) machines incorporate the
idea of parallel processing, which use a large number of process-
ing units to execute the same instruction on different data.
\item Modern parallel computers are so-called MIMD (multiple-instruction-
multiple-data) machines and can execute different instruction
streams in parallel on different data.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{Shared memory and distributed memory}
\begin{itemize}
\item One way of categorizing modern parallel computers is to look at
the memory configuration.
\item In shared memory systems the CPUs share the same address
space. Any CPU can access any data in the global memory.
\item In distributed memory systems each CPU has its own memory.
The CPUs are connected by some network and may exchange
messages.
\end{itemize}
} 



\frame[containsverbatim]
{
  \frametitle{Different parallel programming paradigms}
\begin{itemize}
\item {\bf Task parallelism:} the work of a global problem can be divided
into a number of independent tasks, which rarely need to synchronize. 
Monte Carlo simulation is one example. Integration is another. However this
paradigm is of limited use.
\item {\bf Data parallelism:} use of multiple threads (e.g. one thread per
processor) to dissect loops over arrays etc. 
This paradigm requires a single memory address space. 
Communication and synchronization between processors are often hidden, thus easy to
program. However, the user surrenders much control to a specialized compiler.
Examples of data parallelism are compiler-based parallelization
and OpenMP directives.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{Today's situation of parallel computing}
\begin{itemize}
\item Distributed memory is the dominant hardware configuration. There
is a large diversity in these machines, from 
MPP (massively parallel processing) systems to clusters of off-the-shelf PCs, which
are very cost-effective.
\item Message-passing is a mature programming paradigm and widely
accepted. It often provides an efficient match to the hardware.
It is primarily used for the distributed memory systems, but can also
be used on shared memory systems.
\end{itemize}
In these lectures we consider only message-passing for 
writing parallel programs.
} 



\frame[containsverbatim]
{
  \frametitle{Overhead present in parallel computing}
\begin{itemize}
\item {\bf Uneven load balance}:  not all the processors can perform useful
work at all time.
\item {\bf Overhead of synchronization.}
\item {\bf Overhead of communication}.
\item {Extra computation due to parallelization}.
\end{itemize}
Due to the above overhead and that certain part of a sequential
algorithm cannot be parallelized we may not achieve an optimal parallelization.
} 


\frame[containsverbatim]
{
  \frametitle{Parallelizing a sequential algorithm}
\begin{itemize}
\item Identify the part(s) of a sequential algorithm that can be 
executed in parallel. This is the difficult part,
\item Distribute the global work and data among $P$ processors.
\end{itemize}
} 




\frame[containsverbatim]
{
  \frametitle{Process and processor}
\begin{itemize}
\item We refer to process as a logical unit which executes its own code,
in an MIMD style.
\item The processor is a physical device on which one or several processes
are executed.
\item The MPI standard uses the concept process consistently throughout 
its documentation.
\end{itemize}
} 



\frame[containsverbatim]
{
  \frametitle{Bindings to MPI routines}

MPI is a message-passing library where all the routines
have corresponding C/C++-binding
\begin{lstlisting}
   MPI_Command_name
\end{lstlisting}
and Fortran-binding (routine names are in uppercase, but can also be in lower case)
\begin{lstlisting}
   MPI_COMMAND_NAME
\end{lstlisting}
The discussion in these slides focuses on the C++ binding.
} 




\frame[containsverbatim]
{
  \frametitle{The most important/used MPI routines}
\begin{itemize}
\item MPI\_ Init - initiate an MPI computation
\item MPI\_Finalize - terminate the MPI computation and clean up
\item MPI\_Comm\_size - how many processes participate in a given MPI
communicator?
\item MPI\_Comm\_rank - which one am I? (A number between 0 and size-1.)
\item MPI\_Reduce(Allreduce) - Collect data from all nodes and either sum them up in one or all (Allreduce). Useful for numerical integration
\item MPI\_Send - send a message to a particular process within an MPI
communicator
\item MPI\_Recv - receive a message from a particular process within an
MPI communicator
\end{itemize}
} 

\frame[containsverbatim]
{
  \frametitle{Note the MPI\_COMM\_WORLD declaration}
\begin{itemize}
\item A group of MPI processes with a name (context).
\item Any process is identified by its rank. The rank is only meaningful
within a particular communicator.
\item By default communicator MPI\_COMM\_WORLD contains all the MPI
processes.
\item Mechanism to identify subset of processes.
\item Promotes modular design of parallel libraries.
\end{itemize}
} 



\frame[containsverbatim]
{
  \frametitle{The first MPI C/C++ program}
Let every process write "Hello world" on the standard output.  This is program2.cpp of chapter 4.
\begin{lstlisting}
using namespace std;
#include <mpi.h>
#include <iostream>
int main (int nargs, char* args[])
{
int numprocs, my_rank;
//   MPI initializations
MPI_Init (&nargs, &args);
MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
cout << "Hello world, I have  rank " << my_rank << " out of " 
     << numprocs << endl;
//  End MPI
MPI_Finalize ();
\end{lstlisting}
} 





\frame[containsverbatim]
{
  \frametitle{The Fortran program}
\begin{lstlisting}
PROGRAM hello
INCLUDE "mpif.h"
INTEGER:: size, my_rank, ierr

CALL  MPI_INIT(ierr)
CALL MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierr)
CALL MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)
WRITE(*,*)"Hello world, I've rank ",my_rank," out of ",size
CALL MPI_FINALIZE(ierr)

END PROGRAM hello
\end{lstlisting}
} 

\frame[containsverbatim]
{
  \frametitle{Note 1}
The output to screen is not ordered since all processes are trying to write  to screen simultaneously.
It is then the operating system which opts for an ordering.  
If we wish to have an organized output, starting from the first process, we may rewrite our program as in the next example
(program3.cpp), see again chapter 5.7 of lecture notes.
} 



\frame[containsverbatim]
{
  \frametitle{Ordered output with MPI\_Barrier}
\begin{lstlisting}
int main (int nargs, char* args[])
{
 int numprocs, my_rank, i;
 MPI_Init (&nargs, &args);
 MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
 MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
 for (i = 0; i < numprocs; i++) {}
 MPI_Barrier (MPI_COMM_WORLD);
 if (i == my_rank) {
 cout << "Hello world, I have  rank " << my_rank << 
        " out of " << numprocs << endl;}
      MPI_Finalize ();
\end{lstlisting}
} 


\frame[containsverbatim]
{
  \frametitle{Note 2}
Here we have used the $MPI\_Barrier$ function to ensure that
that every process has completed  its set of instructions in  a particular order.
A barrier is a special collective operation that does not allow the processes to continue
until all processes in the communicator (here $MPI\_COMM\_WORLD$ have called 
$MPI\_Barrier$. 
The barriers make sure that all processes have reached the same point in the code. Many of the collective operations
like $MPI\_ALLREDUCE$ to be discussed later, have the same property; viz.~no process can exit the operation
until all processes have started. 
However, this is slightly more time-consuming since the processes synchronize between themselves as many times as there
are processes.  In the next Hello world example we use the send and receive functions in order to a have a synchronized
action.
} 


\frame
{
  \frametitle{Strategies}
\begin{itemize}
\item Develop codes locally, run with some few processes and test your codes.  Do benchmarking, timing
and so forth on local nodes, for example your laptop.  You can install MPICH2 on your laptop
(most new laptos come with dual cores).  You can test with one node at the lab. 
\item When you are convinced that your codes run correctly, you start your production runs on 
available supercomputers. At UiO we have a new machine among the top 100, Abel.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{How do I run MPI on the machines at the lab (MPICH2)}
The machines at the lab are all quad-cores
\begin{itemize}
\item Compile with mpicxx or mpic++
\item Set up collaboration between processes and run 
\begin{lstlisting}
mpd --ncpus=4 &
#  run code with
mpiexec -n 4 ./nameofprog
\end{lstlisting}
Here we declare that we will use 4 processes via the $-ncpus$ option
and via $-n 4 $ when running.
\item End with
\begin{lstlisting}
mpdallexit
\end{lstlisting} 
\end{itemize}
} 



\frame[containsverbatim]
{
  \frametitle{Can I do it on my own PC/laptop?}
Of course:
\begin{itemize}
\item go to \url{http://www.mcs.anl.gov/research/projects/mpich2/}
\item follow the instructions and install it on your own PC/laptop
\end{itemize}
It works on Windows, Mac and Linux. For Linux (Ubuntu, Linux Mint), go to synaptic package manager 
and search for MPI. You will get several options.
} 





\frame[containsverbatim]
{
  \frametitle{Ordered output with MPI\_Recv and MPI\_Send}
\begin{lstlisting}
.....
int numprocs, my_rank, flag;
MPI_Status status;
MPI_Init (&nargs, &args);
MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
if (my_rank > 0)
MPI_Recv (&flag, 1, MPI_INT, my_rank-1, 100, 
           MPI_COMM_WORLD, &status);
cout << "Hello world, I have  rank " << my_rank << " out of " 
<< numprocs << endl;
if (my_rank < numprocs-1)
MPI_Send (&my_rank, 1, MPI_INT, my_rank+1, 
          100, MPI_COMM_WORLD);
MPI_Finalize ();
\end{lstlisting}
} 

\frame[containsverbatim]
{
  \frametitle{Note 3}
The basic sending of messages is given by the function $MPI\_SEND$, which in C/C++
is defined as 
\begin{lstlisting}
int MPI_Send(void *buf, int count, 
             MPI_Datatype datatype, 
             int dest, int tag, MPI_Comm comm)}
\end{lstlisting}
This single command allows the passing of any kind of variable, even a large array, to any group of tasks. 
The variable {\bf buf} is the variable we wish to send while {\bf count} 
is the  number of variables we are passing. If we are passing only a single value, this should be 1. 
If we transfer an array, it is  the overall size of the array. 
For example, if we want to send a 10 by 10 array, count would be $10\times 10=100$ 
since we are  actually passing 100 values.  
}

\frame[containsverbatim]
{
  \frametitle{Note 4}
Once you have  sent a message, you must receive it on another task. The function {\bf MPI\_RECV}
is similar to the send call.
\begin{lstlisting}
int MPI_Recv( void *buf, int count, MPI_Datatype datatype, 
            int source, 
            int tag, MPI_Comm comm, MPI_Status *status )
\end{lstlisting}

The arguments that are different from those in $MPI\_SEND$ are
{\bf buf} which  is the name of the variable where you will  be storing the received data, 
{\bf source} which  replaces the destination in the send command. This is the return ID of the sender.

Finally,  we have used  {\bf MPI\_Status\ status;} 
where one can check if the receive was completed.

The output of this code is the same as the previous example, but now
process 0 sends a message to process 1, which forwards it further
to process 2, and so forth.

Armed with this wisdom, performed all hello world greetings, we are now ready for serious work. 
}


\frame
{
  \frametitle{Integrating $\pi$}
\begin{columns}
\column{5.5cm}
%\begin{center}
\includegraphics[scale=0.8]{pi.jpg}
\column{4.5cm}
  \begin{block}{Examples}
\begin{itemize}
\item Go to the program package
\item Go to the MPI directory and then chapter 5
\item Look at program6.cpp. 
\item This code computes $\pi$ using the trapezoidal rule.
\end{itemize}
  \end{block}
\end{columns}
} 



\frame
{
  \frametitle{Integration algos}
 \begin{small}
 {\scriptsize
The trapezoidal rule (program6.cpp)
\[
   I=\int_a^bf(x) dx=h\left(f(a)/2 + f(a+h) +f(a+2h)+
                          \dots +f(b-h)+ f_{b}/2\right).
\]

Another very simple approach is the so-called midpoint or rectangle method.
In this case the integration area is split in a given number of rectangles with length $h$ and
heigth given by the mid-point value of the function.  This gives the following simple rule for
approximating an integral
\[
   I=\int_a^bf(x) dx \approx  h\sum_{i=1}^N f(x_{i-1/2}), 
\]
where $f(x_{i-1/2})$ is the midpoint value of $f$ for a given rectangle.  This is used in program5.cpp.
}
\end{small}
} 



\frame[containsverbatim]
{
  \frametitle{MPI\_reduce and MPI\_Allreduce}
 \begin{small}
 {\scriptsize
The parallel integration MPI instructions are simple if we use the functions
MPI\_Reduce or MPI\_Allreduce.\newline\newline
The first function takes information from all processes and sends the result of the MPI operation to one process only,
typically the master node.  If we use MPI\_Allreduce, the result is sent back to all processes, a feature which is
useful when all nodes need the value of a joint operation.  We limit ourselves to MPI\_Reduce since it is only one 
process which will print out the final number of our calculation, The arguments to MPI\_Allreduce are the same.  
 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{MPI\_reduce}
 \begin{small}
 {\scriptsize
Call as
\begin{lstlisting}
MPI_reduce( void *senddata, void* resultdata, int count, 
     MPI_Datatype datatype, MPI_Op, int root, MPI_Comm comm)
\end{lstlisting}

The two variables $senddata$ and $resultdata$ are obvious, besides the fact that one sends the address
of the variable or the first element of an array.  If they are arrays they need to have the same size. 
The variable $count$ represents the total dimensionality, 1 in case of just one variable, 
while MPI\_Datatype 
defines the type of variable which is sent and received.  

The new feature is MPI\_Op. It defines the type
of operation we want to do. 
In our case, since we are summing
the rectangle  contributions from every process we define  MPI\_Op = MPI\_SUM.
If we have an array or matrix we can search for the largest og smallest element by sending either MPI\_MAX or 
MPI\_MIN.  If we want the location as well (which array element) we simply transfer 
MPI\_MAXLOC or MPI\_MINOC. If we want the product we write MPI\_PROD. 

MPI\_Allreduce is defined as
\begin{lstlisting}     
MPI_Alreduce( void *senddata, void* resultdata, int count, 
          MPI_Datatype datatype, MPI_Op, MPI_Comm comm)}.        
\end{lstlisting} 
}
 \end{small}
} 





\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
//    Trapezoidal rule and numerical integration usign MPI, example program6.cpp
using namespace std;
#include <mpi.h>
#include <iostream>

//     Here we define various functions called by the main program

double int_function(double );
double trapezoidal_rule(double , double , int , double (*)(double));

//   Main function begins here
int main (int nargs, char* args[])
{
  int n, local_n, numprocs, my_rank; 
  double a, b, h, local_a, local_b, total_sum, local_sum;   
  double  time_start, time_end, total_time;
\end{lstlisting}
 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
  //  MPI initializations
  MPI_Init (&nargs, &args);
  MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
  time_start = MPI_Wtime();
  //  Fixed values for a, b and n 
  a = 0.0 ; b = 1.0;  n = 1000;
  h = (b-a)/n;    // h is the same for all processes 
  local_n = n/numprocs;  
  // make sure n > numprocs, else integer division gives zero
  // Length of each process' interval of
  // integration = local_n*h.  
  local_a = a + my_rank*local_n*h;
  local_b = local_a + local_n*h;
\end{lstlisting}
 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
  total_sum = 0.0;
  local_sum = trapezoidal_rule(local_a, local_b, local_n, 
                               &int_function); 
  MPI_Reduce(&local_sum, &total_sum, 1, MPI_DOUBLE, 
              MPI_SUM, 0, MPI_COMM_WORLD);
  time_end = MPI_Wtime();
  total_time = time_end-time_start;
  if ( my_rank == 0) {
    cout << "Trapezoidal rule = " <<  total_sum << endl;
    cout << "Time = " <<  total_time  
         << " on number of processors: "  << numprocs  << endl;
  }
  // End MPI
  MPI_Finalize ();  
  return 0;
}  // end of main program
\end{lstlisting}

 }
 \end{small}
} 



\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
We use MPI\_reduce to collect data from each process. Note also the use of the function 
MPI\_Wtime. The final functions are
\begin{lstlisting}
//  this function defines the function to integrate
double int_function(double x)
{
  double value = 4./(1.+x*x);
  return value;
} // end of function to evaluate

\end{lstlisting}
 }
 \end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Dissection of example program6.cpp}
 \begin{small}
 {\scriptsize
Implementation of the trapezoidal rule.
\begin{lstlisting}
//  this function defines the trapezoidal rule
double trapezoidal_rule(double a, double b, int n, 
                         double (*func)(double))
{
  double trapez_sum;
  double fa, fb, x, step;
  int    j;
  step=(b-a)/((double) n);
  fa=(*func)(a)/2. ;
  fb=(*func)(b)/2. ;
  trapez_sum=0.;
  for (j=1; j <= n-1; j++){
    x=j*step+a;
    trapez_sum+=(*func)(x);
  }
  trapez_sum=(trapez_sum+fb+fa)*step;
  return trapez_sum;
}  // end trapezoidal_rule 
\end{lstlisting}
 }
 \end{small}
} 



\frame
{
  \frametitle{Plan for Monte Carlo Lectures, chapters 11-14 in Lecture notes}
\begin{itemize}
\item This week: intro, MC integration and probability distribution functions (PDFs)
\item Next week: More on integration, PDFs, MC integration and random walks.
\item Third week: random walks and statistical physics.
\item Fourth week: Statistical physics. 
\item Fifth week: Most likely quantum Monte Carlo
\end{itemize}
Approximately from this week till the end of November. 
}


\frame
{
  \frametitle{Monte Carlo Keywords}
Consider it is a numerical experiment
\begin{itemize}
 \item Be able to generate random variables following a given probability distribution function PDF
          \item Find a probability distribution function (PDF).
          \item Sampling rule for accepting a move
          \item Compute standard deviation and other expectation values
          \item Techniques for improving errors
\end{itemize}
Enhances algorithmic thinking!
}

\frame
{
  \frametitle{Probability Distribution Functions PDF}
\begin{small}
{\scriptsize
\begin{center}
\begin{tabular}{lcc}\hline
   & Discrete PDF& continuous PDF\\\hline
Domain & $\left\{x_1, x_2, x_3, \dots, x_N\right\}$ & $[a,b]$ \\
probability & $p(x_i)$ &  $p(x)dx$ \\
Cumulative  & $P_i=\sum_{l=1}^ip(x_l)$ & $P(x)=\int_a^xp(t)dt$ \\
Positivity  & $ 0 \le p(x_i) \le 1$ & $ p(x) \ge 0$ \\
Positivity  & $ 0 \le P_i \le 1$ & $ 0 \le P(x) \le 1$ \\
Monotonuous    & $P_i \ge P_j$ if $x_i \ge x_j$ & $P(x_i) \ge P(x_j)$ if $x_i \ge x_j$ \\
Normalization & $P_N=1$ & $P(b)=1$\\
\hline
\end{tabular}
\end{center}
}
\end{small}
}


\frame
{
  \frametitle{Expectation Values}
\begin{small}
{\scriptsize
\begin{itemize}
\item \small  Discrete PDF
\[
    E[x^k]= \langle x^k\rangle=\frac{1}{N}\sum_{i=1}^{N}x_i^kp(x_i),
\]
provided that the sums (or integrals) $ \sum_{i=1}^{N}p(x_i)$ converge absolutely (viz ,
$ \sum_{i=1}^{N}|p(x_i)|$ converges)
\item \small  Continuous PDF
\[
    E[x^k]=\langle x^k\rangle=\int_a^b x^kp(x)dx,
\]
\item \small Function $f(x)$
 \[
    E[f^k]=\langle f^k\rangle=\int_a^b f^kp(x)dx,
\]
\item \small Variance
 \[
    \sigma^2_f=E[f^2]-(E[f])^2=\langle f^2\rangle-\langle f\rangle^2
\]
\end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{Important PDFs}
\begin{small}
{\scriptsize
\begin{itemize}
\item   uniform distribution
\[
  p(x)=\frac{1}{b-a}\Theta(x-a)\Theta(b-x),
\]
which gives for $a=0,b=1$ $p(x)=1$ for $x\in [0,1]$ and zero else.  
\item    exponential distribution
\[
  p(x)=\alpha e^{-\alpha x},
\]
with probability different from zero in  $[0,\infty]$

\item  normal distribution (Gaussian)
\[
   p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)}
\]
 with probability different from zero in $[-\infty,\infty]$
\end{itemize}
All random number generators use the uniform distribution for $x\in[0,1]$.
}
\end{small}
}



\frame
 {
   \frametitle{Why Monte Carlo?}
 \begin{small}
 {\scriptsize
An example from quantum mechanics: most
 problems of interest in e.g., atomic, molecular, nuclear and solid state
 physics consist of a large number of
 interacting electrons and ions or nucleons.
 The total number of particles $N$ is usually sufficiently large
 that an exact solution cannot be found.
 Typically,
 the expectation value for a chosen hamiltonian for a system of
 $N$ particles is
 {\scriptsize
 \[
    \langle H \rangle =
 \]
 \[
    \frac{\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
          \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           H({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)}
         {\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
         \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
         \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)},
 \]
 }
 an in general intractable problem.

 This integral is actually the starting point in a Variational Monte Carlo calculation.\newline
 {\bf Gaussian quadrature: Forget it!} given 10 particles and 10 mesh points for each degree of freedom 
and an
 ideal 1 petaflops machine (all operations take the same time), how long will it ta
ke to compute the above integral? Lifetime of the universe $T\approx 4.7 \times
10^{17}$s.
 }
 \end{small}
 }

 \frame
 {
 \frametitle{More on dimensionality}
 \begin{small}
 {\scriptsize
 As an example from the nuclear many-body problem, we have Schr\"odinger's
 equation as
 a differential equation
 \[
   \hat{H}\Psi({\bf r}_1,..,{\bf r}_A,\alpha_1,..,\alpha_A)=E\Psi({\bf r}_1,..,{
\bf r}_A,\alpha_1,..,\alpha_A)
 \]
 where
 \[
   {\bf r}_1,..,{\bf r}_A,
 \]
 are the coordinates and
 \[
   \alpha_1,..,\alpha_A,
 \]
 are sets of relevant quantum numbers such as spin and isospin for a system of
 $A$ nucleons ($A=N+Z$, $N$ being the number of neutrons and $Z$ the number of protons).
 }
 \end{small}
 }


 \frame
 {
   \frametitle{Even more on dimensionality}
 \begin{small}
 {\scriptsize

 There are
 \[
  2^A\times \left(\begin{array}{c} A\\ Z\end{array}\right)
 \]
 coupled second-order differential equations in $3A$ dimensions.

 For a nucleus like $^{10}$Be this number is
 {\bf 215040}.
 This is a truely challenging many-body problem.
 }
 \end{small}
 }

\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
A crude approach consists in setting all weights equal 1, $\omega_i=1$.
With $dx=h=(b-a)/N$ where $b=1$, $a=0$ in our case and $h$ is the 
step size. The integral 
\[
   I=\int_0^1 f(x)dx\approx \frac{1}{N}\sum_{i=1}^Nf(x_i),
\]
can be rewritten using the concept of the average of the function $f$ for a given PDF $p(x)$ as 
\[
   E[f]=\langle f \rangle = \frac{1}{N}\sum_{i=1}^Nf(x_i)p(x_i),
\]
and identify $p(x)$ with the uniform distribution, viz
$ p(x)=1$ when $x\in [0,1]$ and zero for all other values of $x$.
The integral
is then  the average of $f$ over the interval $x \in [0,1]$ 
\[
      I=\int_0^1 f(x)dx\approx E[f]=\langle f \rangle. 
\]
 }
 \end{small}
 }



\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
In addition to the average value $\langle f \rangle$ the other 
important quantity in a  
Monte-Carlo calculation is the variance $\sigma^2$ and 
the standard deviation $\sigma$. We define first the variance
of the integral with $f$ for a uniform distribution in the interval 
$x \in [0,1]$ to be
\[
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^N(f(x_i)-\langle f\rangle)^2p(x_i), 
\]
and inserting the uniform distribution this yields 
\[
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^Nf(x_i)^2- 
  \left(\frac{1}{N}\sum_{i=1}^Nf(x_i)\right)^2,
\]
or 
\[
  \sigma^2_f=E[f^2]-(E[f])^2=\left(\langle f^2\rangle - 
                                 \langle f \rangle^2\right).
\]
which is nothing but a measure of the extent to
which $f$ deviates from its average over the region of integration. 
The standard deviation is defined as the square root of the variance.
 }
 \end{small}
 }


\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
If we consider the above results for 
a fixed value of $N$ as a measurement, 
we could however recalculate the 
above average and variance for a series of different measurements.
If each such measumerent produces a set of averages for the 
integral $I$ denoted $\langle f\rangle_l$, we have for $M$ measurements
that the integral is given by 
\[
   \langle I \rangle_M=\frac{1}{M}\sum_{l=1}^{M}\langle f\rangle_l.
\]
If we can consider the probability of 
correlated events to be zero, we can rewrite
the variance of these series of measurements as (equating $M=N$) 
\[
  \sigma^2_N\approx \frac{1}{N}\left(\langle f^2\rangle - 
                                 \langle f \rangle^2\right)=\frac{\sigma^2_f}{N}.
\]

We note that the standard deviation is proportional with the inverse square root of 
the number of measurements 
\[
   \sigma_N \sim \frac{1}{\sqrt{N}}.
\]
{\em The aim in Monte Carlo calculations is to have $\sigma_N$ as small as possible after $N$ samples. }
The results from one  sample represents, 
since we are using concepts from statistics,
a 'measurement'. 
 }
 \end{small}
 }



\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
\begin{itemize}
\item We saw that the trapezoidal
rule carries a truncation error $O(h^2)$, with $h$ the step length.
\item 
Quadrature rules such as Newton-Cotes have a truncation
error which goes like $\sim O(h^k)$, with $k \ge 1$. 
Recalling that the step size is defined as $h=(b-a)/N$, we have an
error which goes like $\sim N^{-k}$.  
\item Monte Carlo integration is more efficient in higher dimensions.
Assume that our integration volume is a hypercube 
with side $L$ and dimension $d$. This cube contains hence 
$N=(L/h)^d$ points and therefore the error in the result scales as
$N^{-k/d}$ for the traditional methods. 
\item The error in the Monte carlo integration is 
however independent of $d$ and scales as $\sigma\sim 1/\sqrt{N}$, always! 
\item
Comparing
this with traditional methods, shows that
Monte Carlo integration is more efficient than an order-k algorithm
when $d>2k$
\end{itemize}
 }
 \end{small}
 }


\frame
{
  \frametitle{Some simple examples: Example 1: Particles in a Box}
\begin{small}
{\scriptsize
Consider a box divided into two equal halves separated by a wall.
At the beginning, time $t=0$, there are $N$ particles on the left
side. A small hole in the wall is then opened and one particle
can pass through the hole per unit time. 

After some time the system reaches its equilibrium state with
equally many particles in both halves, $N/2$. 
Instead of determining complicated initial conditions for a system 
of $N$ particles, we model the system by a simple statistical model. 
In order to simulate this system, which may consist of $N \gg 1$ particles,
we assume that all particles in the left half have equal probabilities
of going to the right half. 
}
\end{small}
}


\frame
{
  \frametitle{Particles in a Box}
\begin{small}
{\scriptsize
We introduce the label $n_l$ to denote the 
 number of particles at every time on the left side, and $n_r=N-n_l$ for those
on the right side. 
The probability for a move to the right during a time step  $\Delta t$
is $n_l/N$. The algorithm for simulating this problem may then look
like as follows
\begin{itemize}
   \item Choose the number of particles $N$.
   \item Make a loop over time, where the maximum time should be larger
         than the number of particles $N$.
   \item For every time step $\Delta t$ there is a probability $n_l/N$ 
         for a move
         to the right.  Compare this probability with a random number $x$.
   \item If $ x \le n_l/N$, decrease the number of particles in the left
         half by one, i.e., $n_l=n_l-1$. Else, move a particle from the 
         right half to the left, i.e., $n_l=n_l+1$.
         {\bf This is our sampling rule}
   \item Increase the time by one unit (the external loop).
\end{itemize}
In this case, a Monte Carlo sample corresponds to one time unit
$\Delta t$. 
}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{Particles in a Box}
\begin{small}
{\scriptsize
\begin{verbatim}
  // setup of initial conditions
  nleft = initial_n_particles;
  max_time = 10*initial_n_particles;
  idum = -1;
  // sampling over number of particles
  for( time=0; time <= max_time; time++){
    random_n = ((int) initial_n_particles*ran0(&idum));
    if ( random_n <= nleft){
      nleft -= 1;
    }
    else{
      nleft += 1;
    }
    ofile << setiosflags(ios::showpoint | ios::uppercase);
    ofile << setw(15) << time;
    ofile << setw(15) << nleft << endl;
  }
\end{verbatim}
}
\end{small}
}


\frame
{
  \frametitle{Example 2: Radioactive Decay}
\begin{small}
{\scriptsize
Assume that a the time $t=0$ we have $N(0)$ nuclei of type $X$ 
which can decay radioactively. At a time $t>0$ we are left with 
$N(t)$ nuclei. With a transition probability $\omega$, 
which expresses the probability that the system will make a transition to 
another state during a time step of one second, we have the following first-order
differential equation
\[
   dN(t)=-\omega N(t)dt,
\]
whose  solution is
\[
   N(t)=N(0)e^{-\omega t},
\]
where we have defined the mean lifetime $\tau$ of $X$ as
\[
   \tau =\frac{1}{\omega}.
\]

If a nucleus $X$ decays to a daugther nucleus $Y$ which also can decay, we get
the following coupled equations
\[
   \frac{dN_X(t)}{dt}=-\omega_XN_X(t),
\]
and
\[
   \frac{dN_Y(t)}{dt}=-\omega_YN_Y(t)+\omega_XN_X(t).
\]
}
\end{small}
}


\frame
{
  \frametitle{Radioactive Decay}
\begin{small}
{\scriptsize
Probability for a decay of a particle during a time step
$\Delta t$
is
\[
   \frac{\Delta N(t)}{N(t)\Delta t}= -\lambda
\]
$\lambda$ is inversely proportional to the lifetime
\begin{itemize}
   \item Choose the number of particles $N(t=0)=N_0$.
   \item Make a loop over the number of time steps, 
         with maximum time bigger than the number of particles $N_0$
\item At every time step there is a probability $\lambda$ for decay.
      Compare this probability with a random number $x$.
   \item If $ x \le \lambda$, reduce the number of particles with one 
         i.e., $N=N-1$. If not, keep the same number of particles till the next
         time step.
         {\bf This is our sampling rule}
   \item Increase by one the time step (the external loop)
\end{itemize}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Radioactive Decay}
\begin{small}
{\scriptsize
\begin{verbatim}
  idum=-1;  // initialise random number generator
  // loop over monte carlo cycles 
  // One monte carlo loop is one sample  
  for (cycles = 1; cycles <= number_cycles; cycles++){   
    n_unstable = initial_n_particles;
    //  accumulate the number of particles per time step per trial 
    ncumulative[0] += initial_n_particles;
    // loop over each time step 
    for (time=1; time <= max_time; time++){
      // for each time step, we check each particle
      particle_limit = n_unstable;
      for ( np = 1; np <=  particle_limit; np++) {
        if( ran0(&idum) <= decay_probability) {
          n_unstable=n_unstable-1;
        }
      }  // end of loop over particles 
      ncumulative[time] += n_unstable;
    }  // end of loop over time steps 
  }    // end of loop over MC trials 
}   // end mc_sampling function  
\end{verbatim}
}
\end{small}
}


\frame
{
  \frametitle{Monte Carlo Keywords}
Consider it is a numerical experiment
\begin{itemize}
 \item Be able to generate random variables following a given probability distribution function PDF
          \item Find a probability distribution function (PDF).
          \item Sampling rule for accepting a move
          \item Compute standard deviation and other expectation values
          \item Techniques for improving errors
\end{itemize}
Enhances algorithmic thinking!
}


\frame
 {
   \frametitle{Why Monte Carlo integration?}
 \begin{small}
 {\scriptsize
An example from quantum mechanics: most
 problems of interest in e.g., atomic, molecular, nuclear and solid state
 physics consist of a large number of
 interacting electrons and ions or nucleons.
 The total number of particles $N$ is usually sufficiently large
 that an exact solution cannot be found.
 Typically,
 the expectation value for a chosen hamiltonian for a system of
 $N$ particles is
 {\scriptsize
 \[
    \langle H \rangle =
 \]
 \[
    \frac{\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
          \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           H({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
           \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)}
         {\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
         \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
         \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)},
 \]
 }
 an in general intractable problem.

 This integral is actually the starting point in a Variational Monte Carlo calculation.\newline
 {\bf Gaussian quadrature: Forget it!} given 10 particles and 10 mesh points for each degree of freedom 
and an
 ideal 1 petaflops machine (all operations take the same time), how long will it ta
ke to compute the above integral? Lifetime of the universe $T\approx 4.7 \times
10^{17}$s.
 }
 \end{small}
 }

\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
A crude approach consists in setting all weights equal 1, $\omega_i=1$.
With $dx=h=(b-a)/N$ where $b=1$, $a=0$ in our case and $h$ is the 
step size. The integral 
\[
   I=\int_0^1 f(x)dx\approx \frac{1}{N}\sum_{i=1}^Nf(x_i),
\]
can be rewritten using the concept of the average of the function $f$ for a given PDF $p(x)$ as 
\[
   E[f]=\langle f \rangle = \frac{1}{N}\sum_{i=1}^Nf(x_i)p(x_i),
\]
and identify $p(x)$ with the uniform distribution, viz
$ p(x)=1$ when $x\in [0,1]$ and zero for all other values of $x$.
The integral
is then  the average of $f$ over the interval $x \in [0,1]$ 
\[
      I=\int_0^1 f(x)dx\approx E[f]=\langle f \rangle. 
\]
 }
 \end{small}
 }



\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
In addition to the average value $\langle f \rangle$ the other 
important quantity in a  
Monte-Carlo calculation is the variance $\sigma^2$ and 
the standard deviation $\sigma$. We define first the variance
of the integral with $f$ for a uniform distribution in the interval 
$x \in [0,1]$ to be
\[
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^N(f(x_i)-\langle f\rangle)^2p(x_i), 
\]
and inserting the uniform distribution this yields 
\[
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^Nf(x_i)^2- 
  \left(\frac{1}{N}\sum_{i=1}^Nf(x_i)\right)^2,
\]
or 
\[
  \sigma^2_f=E[f^2]-(E[f])^2=\left(\langle f^2\rangle - 
                                 \langle f \rangle^2\right).
\]
which is nothing but a measure of the extent to
which $f$ deviates from its average over the region of integration. 
The standard deviation is defined as the square root of the variance.
 }
 \end{small}
 }


\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
If we consider the above results for 
a fixed value of $N$ as a measurement, 
we could however recalculate the 
above average and variance for a series of different measurements.
If each such measumerent produces a set of averages for the 
integral $I$ denoted $\langle f\rangle_l$, we have for $M$ measurements
that the integral is given by 
\[
   \langle I \rangle_M=\frac{1}{M}\sum_{l=1}^{M}\langle f\rangle_l.
\]
If we can consider the probability of 
correlated events to be zero, we can rewrite
the variance of these series of measurements as (equating $M=N$) 
\[
  \sigma^2_N\approx \frac{1}{N}\left(\langle f^2\rangle - 
                                 \langle f \rangle^2\right)=\frac{\sigma^2_f}{N}.
\]

We note that the standard deviation is proportional with the inverse square root of 
the number of measurements 
\[
   \sigma_N \sim \frac{1}{\sqrt{N}}.
\]
{\em The aim in Monte Carlo calculations is to have $\sigma_N$ as small as possible after $N$ samples. }
The results from one  sample represents, 
since we are using concepts from statistics,
a 'measurement'. 
 }
 \end{small}
 }



\frame
 {
   \frametitle{But what do we gain by  Monte Carlo Integration?}
 \begin{small}
 {\scriptsize
\begin{itemize}
\item We saw that the trapezoidal
rule carries a truncation error $O(h^2)$, with $h$ the step length.
\item 
Quadrature rules such as Newton-Cotes have a truncation
error which goes like $\sim O(h^k)$, with $k \ge 1$. 
Recalling that the step size is defined as $h=(b-a)/N$, we have an
error which goes like $\sim N^{-k}$.  
\item Monte Carlo integration is more efficient in higher dimensions.
Assume that our integration volume is a hypercube 
with side $L$ and dimension $d$. This cube contains hence 
$N=(L/h)^d$ points and therefore the error in the result scales as
$N^{-k/d}$ for the traditional methods. 
\item The error in the Monte carlo integration is 
however independent of $d$ and scales as $\sigma\sim 1/\sqrt{N}$, always! 
\item
Comparing
this with traditional methods, shows that
Monte Carlo integration is more efficient than an order-k algorithm
when $d>2k$
\end{itemize}
 }
 \end{small}
 }




\frame
 {
   \frametitle{Example: Acceptance-Rejection Method}
 \begin{small}
 {\scriptsize
This is a rather simple and appealing
method after von Neumann. Assume that we are looking at an interval
$x\in [a,b]$, this being the domain of the PDF $p(x)$. Suppose also that
the largest value our distribution function takes in this interval
is $M$, that is
\[
    p(x) \le M \hspace{1cm}  x\in [a,b].
\]
Then we generate a random number $x$ from the uniform distribution
for $x\in [a,b]$ and a corresponding number $s$ for the uniform
distribution between $[0,M]$.
If 
\[
p(x) \ge s,
\] 
we accept the new value of $x$, else we generate
again two new random numbers $x$ and $s$ and perform the test
in the latter equation again.   

 }
 \end{small}
 }


\frame
 {
   \frametitle{Acceptance-Rejection Method}
 \begin{small}
 {\scriptsize
As an example, consider the evaluation of the integral
\[
   I=\int_0^3\exp{(x)}dx.
\]
Obviously to derive it analytically is much easier, however the integrand could pose some more
difficult challenges. The aim here is simply to show how to implent the acceptance-rejection algorithm.
The integral is the area below the curve $f(x)=\exp{(x)}$. If we uniformly fill the rectangle
spanned by $x\in [0,3]$ and $y\in [0,\exp{(3)}]$, the fraction below the curve obatained from a uniform distribution, and
multiplied by the area of the rectangle, should approximate the chosen integral. It is rather
easy to implement this numerically, as shown in the following code.
 }
 \end{small}
 }

\frame
{
  \frametitle{Simple Plot of the Accept-Reject Method}
\begin{center}
\includegraphics[scale=0.35]{acceptreject}
\end{center}
}


\frame[containsverbatim]
 {
   \frametitle{Acceptance-Rejection Method}
 \begin{small}
 {\scriptsize
\begin{verbatim}
//   Loop over Monte Carlo trials n
     integral =0.;
     for ( int i = 1;  i <= n; i++){
//   Finds a random value for x in the interval [0,3]
          x = 3*ran0(&idum);
//   Finds y-value between [0,exp(3)]
          y = exp(3.0)*ran0(&idum);
//   if the value of y at exp(x) is below the curve, we accept
//   THIS IS OUR SAMPLING RULE
          if ( y  < exp(x)) s = s+ 1.0;
//   The integral is area enclosed below the line f(x)=exp(x)
    }
//  Then we multiply with the area of the rectangle and 
//  divide by the number of cycles 
    Integral = 3.*exp(3.)*s/n
\end{verbatim}
 }
 \end{small}
 }


\frame
{
  \frametitle{Monte Carlo Integration}
\begin{small}
{\scriptsize
With uniform distribution $p(x)=1$ for $x\in [0,1]$ and zero else
\[
   I=\int_0^1 f(x)dx\approx \frac{1}{N}\sum_{i=1}^Nf(x_i),
\]

\[
      I=\int_0^1 f(x)dx\approx E[f]=\langle f \rangle.
\]

\[
  \sigma^2_f=\frac{1}{N}\sum_{i=1}^Nf(x_i)^2-
  \left(\frac{1}{N}\sum_{i=1}^Nf(x_i)\right)^2,
\]
or
\[
  \sigma^2_f=E[f^2]-(E[f])^2=\left(\langle f^2\rangle -
                                 \langle f \rangle^2\right).
\]
}
\end{small}
}


\frame
{
  \frametitle{Brute Force Algorithm for Monte Carlo Integration}
\begin{small}
{\scriptsize
\begin{itemize}
  \item Choose the number of  Monte Carlo samples $N$.
   \item Make a loop over  $N$ and for every step generate a random number
         $x_i$ in the  interval $x_i\in [0,1]$ by calling a random number generator.
   \item Use this number to compute $f(x_i)$.
   \item Find the contribution to the variance and the mean value for every loop contribution.
   \item After $N$ samplings, compute the final mean value and the standard deviation
\end{itemize}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Brute Force Integration}
\begin{small}
{\scriptsize
\begin{lstlisting}
// crude mc function to calculate pi
     int i, n;
     long idum;
     double crude_mc, x, sum_sigma, fx, variance; 
     cout << "Read in the number of Monte-Carlo samples" << endl;
     cin >> n;
     crude_mc = sum_sigma=0. ; idum=-1 ;  
//    evaluate the integral with the a crude Monte-Carlo method    
      for ( i = 1;  i <= n; i++){
           x=ran0(&idum);
           fx=func(x);
           crude_mc += fx;
           sum_sigma += fx*fx;
      }
      crude_mc = crude_mc/((double) n );
      sum_sigma = sum_sigma/((double) n );
      variance=sum_sigma-crude_mc*crude_mc;
\end{lstlisting}
}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{Or: another Brute Force Integration}
\begin{small}
{\scriptsize
\begin{lstlisting}
// crude mc function to calculate pi
int main()
{
  const int n = 1000000;
  double x, fx, pi, invers_period, pi2;
  int i;
  invers_period = 1./RAND_MAX;
  srand(time(NULL));
  pi = pi2 = 0.;
  for (i=0; i<n;i++)
    {
      x = double(rand())*invers_period;
      //   This is our sampling rule, all points accepted
      fx = 4./(1+x*x);
      pi += fx;
      pi2 += fx*fx;
    }
  pi /= n;  pi2 = pi2/n - pi*pi;
  cout << "pi=" << pi << " sigma^2=" << pi2 << endl;
  return 0;
}
\end{lstlisting}
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Brute Force Integration}
\begin{small}
{\scriptsize
Note the call to a function which generates random numbers according to the uniform distribution
\begin{lstlisting}
     long idum;
     idum=-1 ;  
     .....
     x=ran0(&idum);
     ....
\end{lstlisting}
or
\begin{lstlisting}
  ...
  invers_period = 1./RAND_MAX;
  srand(time(NULL));
  ...
  x = double(rand())*invers_period;
\end{lstlisting}
}
\end{small}
}
 

\frame
{
  \frametitle{Algorithm for Monte Carlo Integration, Results}
\begin{small}
{\scriptsize
\begin{tabular}{rll}\hline
$N$&$I$&$\sigma_N$\\\hline
10 & 3.10263E+00 &  3.98802E-01\\  
100  & 3.02933E+00 &  4.04822E-01       \\
1000  &   3.13395E+00 &  4.22881E-01      \\
10000  & 3.14195E+00 &  4.11195E-01      \\
100000 &  3.14003E+00 &  4.14114E-01     \\
1000000&  3.14213E+00 &  4.13838E-01   \\
10000000& 3.14177E+00 & 4.13523E-01  \\
$10^{9}$& 3.14162E+00 &  4.13581E-01  \\
\hline
\end{tabular} \newline
We note that as $N$ increases, the integral itself never reaches more than an agreement 
to the fourth or fifth digit. The variance also oscillates around its exact value
$4.13581E-01$. Note well that the variance need not be zero but can, 
with appropriate redefinitions
of the integral be made smaller. A smaller variance yields also a smaller standard deviation. 
This is the topic of importance sampling.
}
\end{small}
}


\frame
 {
   \frametitle{Transformation of Variables}
 \begin{small}
 {\scriptsize
The starting point is always the uniform distribution
\[
p(x)dx=\left\{\begin{array}{cc} dx & 0 \le x \le 1\\
                                0  & else\end{array}\right.
\]
with $p(x)=1$ and 
satisfying
\[
  \int_{-\infty}^{\infty}p(x)dx=1.
\]
All random number generators provided in the program library 
generate numbers in this domain.

When we attempt a 
transformation to a new variable 
$x\rightarrow y$ 
we have to conserve the probability
\[
   p(y)dy=p(x)dx,
\]
which for the uniform distribution implies
\[
   p(y)dy=dx.  
\]
 }
 \end{small}
 }




\frame
 {
   \frametitle{Transformation of Variables}
 \begin{small}
 {\scriptsize
Let us assume that $p(y)$ is a  PDF different from the uniform
PDF $p(x)=1$ with $x \in [0,1]$.
If we integrate the last expression we arrive at
\[
   x(y)=\int_0^y p(y')dy',
\]
which is nothing but the cumulative distribution of $p(y)$, i.e.,
\[
   x(y)=P(y)=\int_0^y p(y')dy'.
\]

This is an important result which has consequences for eventual
improvements over the brute force Monte Carlo. 
 }
 \end{small}
 }



\frame
 {
   \frametitle{Example 1, a general Uniform Distribution}
 \begin{small}
 {\scriptsize
Suppose we have the general uniform distribution
\[
p(y)dy=\left\{\begin{array}{cc} \frac{dy}{b-a} & a \le y \le b\\
                                0  & else\end{array}\right.
\]
If we wish to relate this distribution to the one in the interval
$x \in [0,1]$
we have 
\[
   p(y)dy=\frac{dy}{b-a}=dx,  
\]
and integrating we obtain the cumulative function
\[
   x(y)=\int_a^y \frac{dy'}{b-a}, 
\]
yielding
\[
    y=a+(b-a)x,
\]
a well-known result!

 }
 \end{small}
 }



\frame
 {
   \frametitle{Example 2, from Uniform to Exponential}
 \begin{small}
 {\scriptsize
Assume that
\[
  p(y)=e^{-y},
\]
which is the exponential distribution, important for the analysis
of e.g., radioactive decay. Again, 
$p(x)$ is given by the uniform distribution with 
$x \in [0,1]$, and 
with the assumption that the probability is conserved we have
\[
   p(y)dy=e^{-y}dy=dx,  
\]
which yields after integration
\[
   x(y)=P(y)=\int_0^y \exp{(-y')}dy'=1-\exp{(-y)},
\]
or
\[
   y(x)=-ln(1-x).
\]
This gives us the new random variable $y$ in the domain
$y \in [0,\infty)$
determined through the random variable $x \in [0,1]$ generated by
our favorite random generator. 
 }
 \end{small}
 }



\frame[containsverbatim]
 {
   \frametitle{Example 2, from Uniform to Exponential}
 \begin{small}
 {\scriptsize
This means that if we can factor out 
$\exp{(-y)}$ from an integrand we may have 
\[
   I=\int_0^{\infty}F(y)dy=\int_0^{\infty}\exp{(-y)}G(y)dy   
\]
which we rewrite as
\[
  \int_0^{\infty}\exp{(-y)}G(y)dy=
   \int_0^{\infty}\frac{dx}{dy}G(y)dy\approx 
   \frac{1}{N}\sum_{i=1}^NG(y(x_i)),
\]
where $x_i$ is a random number in the interval
[0,1]. 

Note that in practical implementations, our random number generators for the 
uniform distribution never return exactly 0 or 1, but we we may come very close.
We  should thus in principle set $x\in (0,1)$.  
 }
 \end{small}
 }

\frame[containsverbatim]
 {
   \frametitle{Example 2, from Uniform to Exponential}
 \begin{small}
 {\scriptsize
The algorithm is rather simple. 
In the function which sets up the integral, we simply need
the random number generator for the uniform distribution
in order to obtain numbers 
in the interval [0,1]. We obtain $y$ by the taking the logarithm of
$(1-x)$. Our calling function which sets up the new random
variable $y$ may then include statements like
\begin{verbatim}
.....
idum=-1;
x=ran0(&idum);
y=-log(1.-x);
.....
\end{verbatim}
 }
 \end{small}
 }




\frame
 {
   \frametitle{Example 3}
 \begin{small}
 {\scriptsize
Another function which provides an example for a PDF is
\[
   p(y)dy=\frac{dy}{(a+by)^n},
\]
with $n > 1$. It is normalizable, positive definite, analytically
integrable and the integral is invertible, allowing thereby
the expression of a new variable in terms of the old one. 
The integral
\[
   \int_0^{\infty} \frac{dy}{(a+by)^n}=\frac{1}{(n-1)ba^{n-1}},
\]
gives
\[
   p(y)dy=\frac{(n-1)ba^{n-1}}{(a+by)^n}dy,
\]
which in turn gives the cumulative function
\[
   x(y)=P(y)=\int_0^y \frac{(n-1)ba^{n-1}}{(a+bx)^n}dy'=,
\]
resulting in
\[
   y=\frac{a}{b}\left((1-x)^{-1/(n-1)}-1\right).
\]
 }
 \end{small}
 }


\frame
 {
   \frametitle{Example 4, from Uniform to Normal}
 \begin{small}
 {\scriptsize
For the normal distribution, expressed here as
\[
  g(x,y)=\exp{(-(x^2+y^2)/2)}dxdy.
\]
it is rather difficult to find an inverse since the cumulative
distribution is given by the error function $erf(x)$.

If we however switch to polar coordinates, we have
for $x$ and $y$
\[
   r=\left(x^2+y^2\right)^{1/2} \hspace{1cm}
   \theta =tan^{-1}\frac{x}{y},
\]
resulting in 
\[
  g(r,\theta)=r\exp{(-r^2/2)}drd\theta,
\]
where the angle $\theta$ could be given by a uniform 
distribution in the region $[0,2\pi]$.
Following example 1 above, this implies simply 
multiplying random numbers 
$x\in [0,1]$ by $2\pi$. 
 }
 \end{small}
 }


\frame
 {
   \frametitle{Example 4, from Uniform to Normal}
 \begin{small}
 {\scriptsize
The variable 
$r$, defined for $r \in [0,\infty)$ needs to be related to
to random numbers $x'\in [0,1]$. To achieve that, we introduce a new variable
\[
   u=\frac{1}{2}r^2,
\]
and define a PDF
\[
  \exp{(-u)}du,
\]
with $u\in [0,\infty)$. 
Using the results from example 2, we have that 
\[
   u=-ln(1-x'),
\]
where $x'$ is a random number generated for $x'\in [0,1]$. 
With 
\[
  x=r\cos(\theta)=\sqrt{2u}\cos(\theta),
\]
and
\[
  y=r\sin(\theta)=\sqrt{2u}\sin(\theta),
\]
we can obtain new random numbers $x,y$ through
\[
  x=\sqrt{-2ln(1-x')}\cos(\theta),
\]
and
\[
  y=\sqrt{-2ln(1-x')}\sin(\theta),
\]
with $x'\in [0,1]$ and $\theta \in 2\pi [0,1]$. 
 }
 \end{small}
 }



\frame[containsverbatim]
 {
   \frametitle{Example 4, from Uniform to Normal}
 \begin{small}
 {\scriptsize
A function which yields such random numbers for the normal
distribution would include statements like 
\begin{verbatim}
.....
idum=-1;
radius=sqrt(-2*ln(1.-ran0(idum)));
theta=2*pi*ran0(idum);
x=radius*cos(theta);
y=radius*sin(theta);
.....
\end{verbatim}

 }
 \end{small}
 }

\frame[containsverbatim]
 {
   \frametitle{Box-Mueller Method for Normal Deviates}
 \begin{small}
 {\scriptsize
\begin{lstlisting}
// random numbers with gaussian distribution
double gaussian_deviate(long * idum)
{
  static int iset = 0;
  static double gset;
  double fac, rsq, v1, v2;
  if ( idum < 0) iset =0;
  if (iset == 0) {
    do {
      v1 = 2.*ran0(idum) -1.0;
      v2 = 2.*ran0(idum) -1.0;
      rsq = v1*v1+v2*v2;
    } while (rsq >= 1.0 || rsq == 0.);
    fac = sqrt(-2.*log(rsq)/rsq);
    gset = v1*fac;
    iset = 1;
    return v2*fac;
  } else {
    iset =0;
    return gset;
  }
\end{lstlisting}
 }
 \end{small}
 }



%\section{Numerical integration and MPI}





\frame
 {
   \frametitle{Importance Sampling, chapter 11.4}
 \begin{small}
 {\scriptsize
With the aid of the above variable transformations we address now
one of the most widely used approaches to Monte Carlo integration,
namely importance sampling. 

Let us assume that  $p(y)$ is a PDF whose behavior resembles that of a function
$F$ defined in a certain interval $[a,b]$. The normalization condition is
\[
   \int_a^bp(y)dy=1.
\]
We can rewrite our integral as 
\[
   I=\int_a^b F(y) dy =\int_a^b p(y)\frac{F(y)}{p(y)} dy.
   \label{eq:impsampl1}
\]
 }
 \end{small}
 }


\frame
 {
   \frametitle{Importance Sampling}
 \begin{small}
 {\scriptsize
Since random numbers are generated for the uniform distribution $p(x)$
with $x\in [0,1]$, we need to perform a change of variables $x\rightarrow y$
through
\[
     x(y)=\int_a^y p(y')dy',
\]
where we used 
\[
   p(x)dx=dx=p(y)dy. 
\] 
If we can invert $x(y)$, we find $y(x)$ as well.
 }
 \end{small}
 }




\frame
 {
   \frametitle{Importance Sampling}
 \begin{small}
 {\scriptsize
With this change of variables we can express the integral of 
Eq.~(\ref{eq:impsampl1}) as 
\[
   I=\int_a^b p(y)\frac{F(y)}{p(y)} dy=\int_a^b\frac{F(y(x))}{p(y(x))} dx,
\]
meaning that a Monte Carlo evalutaion of the above integral gives
\[
\int_a^b\frac{F(y(x))}{p(y(x))} dx=
\frac{1}{N}\sum_{i=1}^N\frac{F(y(x_i))}{p(y(x_i))}.
\]
The advantage of such a change of variables in case $p(y)$ follows
closely $F$ is that the integrand becomes smooth and we can sample
over relevant values for the integrand. It is however not trivial
to find such a function $p$.
The conditions on $p$ which allow us to perform these transformations
are
\begin{enumerate}
\item $p$ is normalizable and positive definite, 
\item it is analytically integrable and 
\item the integral is invertible, allowing us thereby 
      to express a new variable in terms of the old one. 
\end{enumerate}

 }
 \end{small}
 }


\frame
 {
   \frametitle{Importance Sampling}
 \begin{small}
 {\scriptsize
The algorithm for this procedure is 
\begin{itemize}
  \item  Use the uniform distribution to find the random variable
  $y$ in the interval [0,1]. $p(x)$ is a user provided PDF.
  \item Evaluate thereafter 
\[
   I=\int_a^b F(x) dx =\int_a^b p(x)\frac{F(x)}{p(x)} dx,
\]
  by rewriting
\[
   \int_a^b p(x)\frac{F(x)}{p(x)} dx =   
   \int_a^b\frac{F(x(y))}{p(x(y))} dy,
\]
since
\[
   \frac{dy}{dx}=p(x).
\]
\item Perform then a Monte Carlo sampling for
\[
\int_a^b\frac{F(x(y))}{p(x(y))} dy,\approx
 \frac{1}{N}\sum_{i=1}^N\frac{F(x(y_i))}{p(x(y_i))},
\]
with $y_i\in [0,1]$,
\item Evaluate the variance
\end{itemize}
 }
 \end{small}
 }
\frame
 {
   \frametitle{Demonstration of Importance Sampling}
 \begin{small}
 {\scriptsize
\[
   I=\int_0^1 F(x) dx = \int_0^1 \frac{1}{1+x^2} dx = \frac{\pi}{4}.
\]

We choose the following PDF (which follows closely the function to
integrate)
\[
   p(x)=\frac{1}{3}\left(4-2x\right) \hspace{1cm} \int_0^1p(x)dx=1,
\]
resulting
\[
   \frac{F(0)}{p(0)}=\frac{F(1)}{p(1)}=\frac{3}{4}.
\]
Check that it fullfils the requirements of a PDF.
We perform then the change of variables (via the Cumulative function) 
\[
     y(x)=\int_0^x p(x')dx'=\frac{1}{3}x\left(4-x\right),
\]
or
\[
   x=2-\left(4-3y\right)^{1/2}
\]
We have that when $y=0$ then  $x=0$ and when  $y=1$ we have $x=1$.
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Simple Code}
 \begin{small}
 {\scriptsize
\begin{verbatim}
//   evaluate the integral with importance sampling
     for ( int i = 1;  i <= n; i++){
       x = ran0(&idum);  // random numbers in [0,1]
       y = 2 - sqrt(4-3*x);  // new random numbers
       fy=3*func(y)/(4-2*y); // weighted function
       int_mc += fy;
       sum_sigma += fy*fy;
     }
     int_mc = int_mc/((double) n );
     sum_sigma = sum_sigma/((double) n );
     variance=(sum_sigma-int_mc*int_mc);
\end{verbatim}
}
\end{small}
 }

\frame
 {
   \frametitle{Test Runs and Comparison with Brute Force for $\pi=3.14159$}
 \begin{small}
 {\scriptsize

The suffix $cr$ stands for the brute force approach 
while $is$ stands for the use of importance sampling.
All calculations use ran0 as function to generate the uniform distribution.
\begin{center}
\begin{tabular}{rllll}\hline
$N$&$I_{cr}$ &$\sigma_{cr}$   &$I_{is}$  &$\sigma_{is}$\\\hline
10000  & 3.13395E+00 & 4.22881E-01 & 3.14163E+00    & 6.49921E-03 \\
100000 & 3.14195E+00&  4.11195E-01 &   3.14163E+00    &  6.36837E-03 \\
1000000& 3.14003E+00 & 4.14114E-01 & 3.14128E+00&  6.39217E-03\\
10000000 &3.14213E+00 & 4.13838E-01 & 3.14160E+00 & 6.40784E-03 \\ 
\hline
\end{tabular} 
\end{center}   
However, it is unfair to study one-dimensional integrals with MC methods! 
 }
 \end{small}
 }

\frame
 {
   \frametitle{Multidimensional Integrals}
 \begin{small}
 {\scriptsize
When we deal with multidimensional integrals of the form
\[
   I=\int_0^1dx_1\int_0^1dx_2\dots \int_0^1dx_d g(x_1,\dots,x_d),
\]
with 
$x_i$ defined in the interval  $[a_i,b_i]$ we would typically
need a transformation
of variables of the form 
\[
   x_i=a_i+(b_i-a_i)t_i,
\]
if we were to use the uniform distribution on the interval $[0,1]$.
In this case, we need a 
Jacobi determinant
\[
  \prod_{i=1}^d (b_i-a_i),
\]
and to convert the function $g(x_1,\dots,x_d)$ to 
\[
   g(x_1,\dots,x_d)\rightarrow 
   g(a_1+(b_1-a_1)t_1,\dots,a_d+(b_d-a_d)t_d).
\]
 }
 \end{small}
 }






\frame
 {
   \frametitle{Example: 6-dimensional Integral}
 \begin{small}
 {\scriptsize
Consider the following six-dimensional
integral
\[
   \int_{-\infty}^{\infty}{\bf dxdy}g({\bf x, y}),
\] 
where
\[
  g({\bf x, y})=\exp{(-{\bf x}^2-{\bf y}^2-({\bf x-y})^2/2)},
\]
with  $d=6$.
 }
 \end{small}
 }


\frame
 {
   \frametitle{Example: 6-dimensional Integral}
 \begin{small}
 {\scriptsize

We can solve this integral by employing our brute force scheme,
or using importance sampling and random variables distributed 
according to a gaussian PDF. For the latter, if we set
the mean value 
$\mu=0$ and the standard deviation  $\sigma=1/\sqrt{2}$, we have
\[
   \frac{1}{\sqrt{\pi}}\exp{(-x^2)},
\]
and through
\[
   \pi^3\int\prod_{i=1}^6\left(
    \frac{1}{\sqrt{\pi}}\exp{(-x_i^2)}\right)
    \exp{(-({\bf x-y})^2/2)}dx_1.\dots dx_6,
\]
we can rewrite our integral as 
\[
   \int f(x_1,\dots,x_d)F(x_1,\dots,x_d)\prod_{i=1}^6dx_i,
\]
where $f$ is the gaussian distribution.
 }
 \end{small}
 }

\frame[containsverbatim]
 {
   \frametitle{Brute Force I}
 \begin{small}
 {\scriptsize
\begin{verbatim}
.....
//   evaluate the integral without importance sampling    
//   Loop over Monte Carlo Cycles
     for ( int i = 1;  i <= n; i++){
//   x[] contains the random numbers for all dimensions
       for (int j = 0; j< 6; j++) {
           x[j]=-length+2*length*ran0(&idum);
       }
       fx=brute_force_MC(x); 
       int_mc += fx;
       sum_sigma += fx*fx;
     }
     int_mc = int_mc/((double) n );
     sum_sigma = sum_sigma/((double) n );
     variance=sum_sigma-int_mc*int_mc;
......
\end{verbatim}

 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Brute Force II}
 \begin{small}
 {\scriptsize
\begin{verbatim}
double  brute_force_MC(double *x) 
{
   double a = 1.; double b = 0.5; 
// evaluate the different terms of the exponential
   double xx=x[0]*x[0]+x[1]*x[1]+x[2]*x[2];
   double yy=x[3]*x[3]+x[4]*x[4]+x[5]*x[5];
   double xy=pow((x[0]-x[3]),2)+pow((x[1]-x[4]),2)+pow((x[2]-x[5]),2);
   return exp(-a*xx-a*yy-b*xy);
\end{verbatim}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Importance Sampling I}
 \begin{small}
 {\scriptsize
\begin{verbatim}
..........
//   evaluate the integral with importance sampling    
     for ( int i = 1;  i <= n; i++){
//   x[] contains the random numbers for all dimensions
       for (int j = 0; j < 6; j++) {
	 x[j] = gaussian_deviate(&idum)*sqrt2;
       }
       fx=gaussian_MC(x); 
       int_mc += fx;
       sum_sigma += fx*fx;
     }
     int_mc = int_mc/((double) n );
     sum_sigma = sum_sigma/((double) n );
     variance=sum_sigma-int_mc*int_mc;
.............
\end{verbatim}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Importance Sampling II}
 \begin{small}
 {\scriptsize
\begin{verbatim}
// this function defines the integrand to integrate 
 
double  gaussian_MC(double *x) 
{
   double a = 0.5; 
// evaluate the different terms of the exponential
   double xy=pow((x[0]-x[3]),2)+pow((x[1]-x[4]),2)+pow((x[2]-x[5]),2);
   return exp(-a*xy);
} // end function for the integrand
\end{verbatim}
 }
 \end{small}
 }



\frame
 {
   \frametitle{Test Runs for six-dimensional Integral}
 \begin{small}
 {\scriptsize

Results for as function of number of Monte Carlo samples $N$. 
The exact answer is $I\approx 10.9626$ for the integral.
The suffix $cr$ stands for the brute force approach 
while $gd$ stands for the use of a Gaussian distribution function.
All calculations use ran0 as function to generate the uniform distribution.
\begin{center}
\begin{tabular}{rll}\hline
$N$&$I_{cr}$&$I_{gd}$\\\hline
10000  & 1.15247E+01& 1.09128E+01   \\
100000 & 1.29650E+01 & 1.09522E+01   \\
1000000& 1.18226E+01 & 1.09673E+01    \\
10000000&1.04925E+01  &   1.09612E+01  \\
\hline
\end{tabular} 
\end{center}   
 }
 \end{small}
 }



\section{Week 45}
\frame
{
  \frametitle{Week 45}
  \begin{block}{Monte Carlo methods}
\begin{itemize}
\item Monday: Repetition from last week
\item More on importance sampling and multi-dimensional integrals
\item Central limit theorem and discussion on variance, covariance and standard deviation
\item Tuesday: 
\item Central limit theorem and discussion on variance, covariance and standard deviation
\item Random number generators (RNG).
\item Random walks, Markov chains, Diffusion and Master equation
%\item Detailed balance and the Metropolis(-Hastings) algorithm
%\item Examples of the use of the Metropolis(-Hastings) algorithm: Maxwell-Boltzmann distribution. 
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{Monte Carlo Keywords}
Consider it is a numerical experiment
\begin{itemize}
 \item Be able to generate random variables following a given probability distribution function PDF
          \item Find a probability distribution function (PDF).
          \item Sampling rule for accepting a move
          \item Compute standard deviation and other expectation values
          \item Techniques for improving errors
\end{itemize}
Enhances algorithmic thinking!
}


\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral (project 3 2012)}
 \begin{small}
 {\scriptsize
The task of this project is to integrate in a brute force manner a six-dimensional integral which is used
to determine the ground state correlation energy between two electrons 
in a helium atom.  We will employ both Gaussian quadrature and Monte-Carlo integration.
We assume that the wave function of each electron can be modelled like the single-particle
wave function of an electron in the hydrogen atom. The single-particle wave function  for an electron $i$ in the 
$1s$ state 
is given in terms of a dimensionless variable    (the wave function is not properly normalized)
\[
   {\bf r}_i =  x_i {\bf e}_x + y_i {\bf e}_y +z_i {\bf e}_z ,
\]
as
\[
   \psi_{1s}({\bf r}_i)  =   e^{-\alpha r_i},
\]
where $\alpha$ is a parameter and 
\[
r_i = \sqrt{x_i^2+y_i^2+z_i^2}.
\]
We will fix $\alpha=2$, which should correspond to the charge of the helium atom $Z=2$. 
 }
 \end{small}
 }

\frame[containsverbatim]
 {
   \frametitle{Switch to spherical coordinates}
 \begin{small}
 {\scriptsize
Useful to change to spherical coordinates
\[
   d{\bf r}_1d{\bf r}_2  = r_1^2dr_1 r_2^2dr_2 dcos(\theta_1)dcos(\theta_2)d\phi_1d\phi_2,
\]
and 
\[
   \frac{1}{r_{12}}= \frac{1}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
\]
with 
\[
\cos(\beta) = \cos(\theta_1)\cos(\theta_2)+\sin(\theta_1)\sin(\theta_2)\cos(\phi_1-\phi_2))
\]
 }
 \end{small}
 }

\frame[containsverbatim]
 {
   \frametitle{How do I do importance sampling in spherical coordinates}
 \begin{small}
 {\scriptsize
\begin{itemize}
\item $r_{1,2} \in [0,\infty)$, here we use the mapping $r_{1,2}=-ln(1-ran)$
with $ran \in [0,1]$, a uniform distribution point.
\item $\theta_{1,2} \in [0,\pi]$, use mapping $\theta_{1,2}=\pi*ran$
with $ran \in [0,1]$ a uniform distribution point.
\item $\phi_{1,2} \in [0,2\pi]$, use mapping $\phi_{1,2}=2\pi*ran$
with $ran \in [0,1]$ a uniform distribution point.
\item Be careful with the integrand
\[
\frac{\exp{(-4(r_1+r_2))}r_1^2dr_1 r_2^2dr_2 dcos(\theta_1)dcos(\theta_2)d\phi_1d\phi_2}{\sqrt{r_1^2+r_2^2-2r_1r_2cos(\beta)}}
\]
\end{itemize}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral (project 3 2012)}
 \begin{small}
 {\scriptsize
The ansatz for the wave function for two electrons is then given by the product of two
$1s$ wave functions as 
\[
   \Psi({\bf r}_1,{\bf r}_2)  =   e^{-\alpha (r_1+r_2)}.
\]
Note that it is not possible to find an analytic solution to Schr\"odinger's equation for 
two interacting electrons in the helium atom. 

The integral we need to solve is the quantum mechanical expectation value of the correlation
energy between two electrons, namely
\begin{equation}\label{eq:correlationenergy}
   \langle \frac{1}{|{\bf r}_1-{\bf r}_2|} \rangle =
   \int_{-\infty}^{\infty} d{\bf r}_1d{\bf r}_2  e^{-2\alpha (r_1+r_2)}\frac{1}{|{\bf r}_1-{\bf r}_2|}=\frac{5\pi^2}{16^2}=0.192765711.
\end{equation}
Note that our wave function is not normalized. There is a normalization factor missing, but for this project
we don't need to worry about that.
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral (project 3 2012)}
 \begin{small}
 {\scriptsize
\begin{enumerate}
\item[a-b)] Use Gaussian  quadrature and compute the integral by integrating 
for each variable $x_1,y_1,z_1,x_2,y_2,z_2$ from $-\infty$ to $\infty$.
How many mesh points do you need before the results converges at the level of the fourth 
leading digit?  Hint:  the single-particle wave function $e^{-\alpha r_i}$  is more or less zero at
$r_i \approx ?$.  You can therefore replace the integration limits $-\infty$ and $\infty$ with 
$-\Lambda$ and $\Lambda$, respectively.  You need to check that this approximation is satisfactory.
\item[c)] Compute the same integral but now with brute force Monte Carlo
and compare your results with those from the previous point. Discuss the differences.
With bruce force we mean that you should use the uniform distribution.
\item[d)] Improve your brute force Monte Carlo calculation by using importance sampling.
Hint: use the exponential distribution.
Does the variance decrease? Does the CPU time used compared with the brute force 
Monte Carlo decrease in order to achieve the same accuracy? Comment your results.
\end{enumerate}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral, Gauss-Legendre}
 \begin{small}
 {\scriptsize
\begin{verbatim}
     double *x = new double [N];
     double *w = new double [N];
//   set up the mesh points and weights
     gauleg(a,b,x,w, N);

//   evaluate the integral with the Gauss-Legendre method
//   Note that we initialize the sum
     double int_gauss = 0.;
     for (int i=0;i<N;i++){
	     for (int j = 0;j<N;j++){
	     for (int k = 0;k<N;k++){
	     for (int l = 0;l<N;l++){
	     for (int m = 0;m<N;m++){
	     for (int n = 0;n<N;n++){
        int_gauss+=w[i]*w[j]*w[k]*w[l]*w[m]*w[n]
       *int_function(x[i],x[j],x[k],x[l],x[m],x[n]);
     		}}}}}
	}
\end{verbatim}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral, Gauss-Legendre}
 \begin{small}
 {\scriptsize
\begin{verbatim}
//  this function defines the function to integrate
double int_function(double x1, double y1, double z1, 
                    double x2, double y2, double z2)
{
   double alpha = 2.;
// evaluate the different terms of the exponential
   double exp1=-2*alpha*sqrt(x1*x1+y1*y1+z1*z1);
   double exp2=-2*alpha*sqrt(x2*x2+y2*y2+z2*z2);
   double deno=sqrt(pow((x1-x2),2)+pow((y1-y2),2)+pow((z1-z2),2));
  if(deno <pow(10.,-6.)) { return 0;}
  else return exp(exp1+exp2)/deno;
} // end of function to evaluate

\end{verbatim}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral, brute force MC}
 \begin{small}
 {\scriptsize
\begin{verbatim}
     double int_mc = 0.;  double variance = 0.;
     double sum_sigma= 0. ; long idum=-1 ;  
     double length=1.5; // we fix the max size of the box to L=3
     double volume=pow((2*length),6.);

//   evaluate the integral with importance sampling    
     for ( int i = 1;  i <= n; i++){
//   x[] contains the random numbers for all dimensions
       for (int j = 0; j< 6; j++) {
           // Maps U[0,1] to U[-L,L]
           x[j]=-length+2*length*ran0(&idum); 
       }
       fx=brute_force_MC(x); 
       int_mc += fx;
       sum_sigma += fx*fx;
     }
     int_mc = int_mc/((double) n );
     sum_sigma = sum_sigma/((double) n );
     variance=sum_sigma-int_mc*int_mc;
     ....
\end{verbatim}
 }
 \end{small}
 }


\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral, brute force MC}
 \begin{small}
 {\scriptsize
\begin{verbatim}
double brute_force_MC(double *x) 
{
   double alpha = 2.;
// evaluate the different terms of the exponential
   double exp1=-2*alpha*sqrt(x[0]*x[0]+x[1]*x[1]+x[2]*x[2]);
   double exp2=-2*alpha*sqrt(x[3]*x[3]+x[4]*x[4]+x[5]*x[5]);
   double deno=sqrt(pow((x[0]-x[3]),2)
          +pow((x[1]-x[4]),2)+pow((x[2]-x[5]),2));
   double value=exp(exp1+exp2)/deno;
	return value;
} // end function for the integrand
\end{verbatim}
 }
 \end{small}
 }




\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral, importance sampling}
 \begin{small}
 {\scriptsize
\begin{verbatim}
	double int_mc = 0.;  double variance = 0.;
	double sum_sigma= 0. ; long idum=-1 ;  
// The 'volume' contains 4 jacobideterminants(pi,pi,2pi,2pi) 
// and a scaling factor 1/16
	double volume=4*pow(acos(-1.),4.)*1./16;
//   evaluate the integral with importance sampling    
	for ( int i = 1;  i <= n; i++){
	   for (int j = 0; j < 2; j++) {
		y=ran0(&idum);
		x[j]=-0.25*log(1.-y);
 	   }
	   for (int j = 2; j < 4; j++) {
		x[j] = 2*acos(-1.)*ran0(&idum); 
	   }
	   for (int j = 4; j < 6; j++) {
			 x[j] = acos(-1.)*ran0(&idum);
  	   }
	fx=integrand_MC(x); 
        ....
\end{verbatim}
 }
 \end{small}
 }





\frame[containsverbatim]
 {
   \frametitle{Example, six-dimensional integral, importance sampling}
 \begin{small}
 {\scriptsize
\begin{verbatim}

// this function defines the integrand to integrate 
 
double  integrand_MC(double *x) 
{
double num=x[0]*x[0]*x[1]*x[1]*sin(x[4])*sin(x[5]);
double deno=sqrt(x[0]*x[0]+x[1]*x[1]-2*x[0]*x[1]*
(sin(x[4])*sin(x[5])*cos(x[2]-x[3])+cos(x[4])*cos(x[5]))); 
return num/deno;
} // end function for the integrand


  






\end{verbatim}
 }
 \end{small}
 }


\frame
 {
   \frametitle{Test Runs and Comparison with Brute Force and Gauss-Legendre}
 \begin{small}
 {\scriptsize

The suffix $br$ stands for the brute force approach 
while $is$ stands for the use of importance sampling.
\begin{center}
\begin{tabular}{rllllll}\hline
$N$&$I_{br}$ &$\sigma_{br}$ &time(s)  &$I_{is}$  &$\sigma_{is}$ &time(s)\\\hline
1E6  & 0.19238 &3.85124E-4  & 0.6  &0.19176   & 1.01515E-4 & 1.4 \\
10E6   &0.18607  &1.18053E-4  & 6 &0.192254    &1.22430E-4  &14 \\
100E6   &0.18846  &4.37163E-4  & 57 &0.192720    &1.03346E-4  &138 \\
1000E6   &0.18843  &1.35879E-4  &581 &0.192789   &3.28795E-5  &1372 \\
\hline
\end{tabular} 
\end{center}   
Gauss-Legendre results:
\begin{center}
\begin{tabular}{rllll}\hline
$N$ &time(s)  &$I_n$     &$|I_n-I$ \\\hline
20  & 31          &0.18047         &   1.123E-2                 \\
30  & 354         & 0.18501        &   7.76E-3              \\
40  & 1999         & 0.18653         & 6.24E-3                \\
50  & 7578         & 0.18722        &  5.54E-3               \\
\hline
\end{tabular} 
\end{center}   

 }
 \end{small}
 }



\frame{
  \frametitle{Variance, covariance, errors etc, chapter 11.2}
  \begin{block}{Statistical analysis}
    \begin{itemize}
    \item Monte Carlo simulations can be treated as {\em computer
      experiments}
    \item The results can be analysed with the same statistics tools
      we would use in analysing laboraty experiments
    \item As in all other experiments, we are looking for expectation
      values and an estimate of how accurate they are, i.e., the error
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Variance, covariance, errors etc}
  \begin{block}{Statistical analysis}
    \begin{itemize}
    \item As in other experiments, Monte Carlo experiments have two
      classes of errors:
      \begin{itemize}
      \item Statistical errors
      \item Systematic errors
      \end{itemize}
    \item Statistical errors can be estimated using standard tools
      from statistics
    \item Systematic errors are method specific and must be treated
      differently from case to case.
    \end{itemize}
  \end{block}
}



\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
A \emph{stochastic process} is a process that produces sequentially a
chain of values:
\bdm
\{x_1, x_2,\dots\,x_k,\dots\}.
\edm
We will call these
values our \emph{measurements} and the entire set as our measured
\emph{sample}.  The action of measuring all the elements of a sample
we will call a stochastic \emph{experiment} (since, operationally,
they are often associated with results of empirical observation of
some physical or mathematical phenomena; precisely an experiment). We
assume that these values are distributed according to some 
PDF $p_X^{\phantom X}(x)$, where $X$ is just the formal symbol for the
stochastic variable whose PDF is $p_X^{\phantom X}(x)$. Instead of
trying to determine the full distribution $p$ we are often only
interested in finding the few lowest moments, like the mean
$\mu_X^{\phantom X}$ and the variance $\sigma_X^{\phantom X}$.
}
\end{small}
}


\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
The \emph{probability distribution function (PDF)} is a function
$p(x)$ on the domain which, in the discrete case, gives us the
probability or relative frequency with which these values of $X$
occur:
\bdm
p(x) = \prob(X=x)
\edm
In the continuous case, the PDF does not directly depict the
actual probability. Instead we define the probability for the
stochastic variable to assume any value on an infinitesimal interval
around $x$ to be $p(x)dx$. The continuous function $p(x)$ then gives us
the \emph{density} of the probability rather than the probability
itself. The probability for a stochastic variable to assume any value
on a non-infinitesimal interval $[a,\,b]$ is then just the integral:
\bdm
\prob(a\leq X\leq b) = \int_a^b p(x)dx
\edm
Qualitatively speaking, a stochastic variable represents the values of
numbers chosen as if by chance from some specified PDF so that the
selection of a large set of these numbers reproduces this PDF.
}
\end{small}
}


\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
Also of interest to us is the \emph{cumulative probability
distribution function (CDF)}, $P(x)$, which is just the probability
for a stochastic variable $X$ to assume any value less than $x$:
\bdm
P(x)=\mathrm{Prob(}X\leq x\mathrm{)} =
\int_{-\infty}^x p(x^{\prime})dx^{\prime}
\edm
The relation between a CDF and its corresponding PDF is then:
\bdm
p(x) = \frac{d}{dx}P(x)
\edm
}
\end{small}
}


\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
A particularly useful class of special expectation values are the
\emph{moments}. The $n$-th moment of the PDF $p$ is defined as
follows:
\bdm
\mean{x^n} \equiv \int\! x^n p(x)\,dx
\edm
The zero-th moment $\mean{1}$ is just the normalization condition of
$p$. The first moment, $\mean{x}$, is called the \emph{mean} of $p$
and often denoted by the letter $\mu$:
\bdm
\mean{x} = \mu \equiv \int\! x p(x)\,dx
\edm
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
A special version of the moments is the set of \emph{central moments},
the n-th central moment defined as:
\bdm
\mean{(x-\mean{x})^n} \equiv \int\! (x-\mean{x})^n p(x)\,dx
\edm
The zero-th and first central moments are both trivial, equal $1$ and
$0$, respectively. But the second central moment, known as the
\emph{variance} of $p$, is of particular interest. For the stochastic
variable $X$, the variance is denoted as $\sigma^2_X$ or $\var(X)$:
\beaN
\sigma^2_X\ \ =\ \ \var(X) & = & \mean{(x-\mean{x})^2} =
\int\! (x-\mean{x})^2 p(x)\,dx\\
& = & \int\! \left(x^2 - 2 x \mean{x}^{\phantom{2}} +
  \mean{x}^2\right)p(x)\,dx\\
& = & \mean{x^2} - 2 \mean{x}\mean{x} + \mean{x}^2\\
& = & \mean{x^2} - \mean{x}^2
\eeaN
The square root of the variance, $\sigma =
\sqrt{\mean{(x-\mean{x})^2}}$ is called the \emph{standard
  deviation} of $p$. It is clearly just the RMS (root-mean-square)
value of the deviation of the PDF from its mean value, interpreted
qualitatively as the ``spread'' of $p$ around its mean.
}
\end{small}
}


\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
Another important quantity is the so called covariance, a variant of
the above defined variance. Consider again the set $\{X_i\}$ of $n$
stochastic variables (not necessarily uncorrelated) with the
multivariate PDF $P(x_1,\dots,x_n)$. The \emph{covariance} of two
of the stochastic variables, $X_i$ and $X_j$, is defined as follows:
\bea
\cov(X_i,\,X_j) &\equiv& \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\nonumber\\
&=&
\int\!\cdots\!\int\!(x_i-\mean{x_i})(x_j-\mean{x_j})\,
P(x_1,\dots,x_n)\,dx_1\dots dx_n
\label{eq:def_covariance}
\eea
with
\bdm
\mean{x_i} =
\int\!\cdots\!\int\!x_i\,P(x_1,\dots,x_n)\,dx_1\dots dx_n
\edm
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
If we consider the above covariance as a matrix $C_{ij} =
\cov(X_i,\,X_j)$, then the diagonal elements are just the familiar
variances, $C_{ii} = \cov(X_i,\,X_i) = \var(X_i)$. It turns out that
all the off-diagonal elements are zero if the stochastic variables are
uncorrelated. This is easy to show, keeping in mind the linearity of
the expectation value. Consider the stochastic variables $X_i$ and
$X_j$, ($i\neq j$):
\beaN
\cov(X_i,\,X_j) &=& \meanb{(x_i-\mean{x_i})(x_j-\mean{x_j})}\\
&=&\mean{x_i x_j - x_i\mean{x_j} - \mean{x_i}x_j + \mean{x_i}\mean{x_j}}\\
&=&\mean{x_i x_j} - \mean{x_i\mean{x_j}} - \mean{\mean{x_i}x_j} +
\mean{\mean{x_i}\mean{x_j}}\\
&=&\mean{x_i x_j} - \mean{x_i}\mean{x_j} - \mean{x_i}\mean{x_j} +
\mean{x_i}\mean{x_j}\\
&=&\mean{x_i x_j} - \mean{x_i}\mean{x_j}
\eeaN
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
If $X_i$ and $X_j$ are independent, we get $\mean{x_i x_j} =
\mean{x_i}\mean{x_j}$, resulting in $\cov(X_i, X_j) = 0\ \ (i\neq j)$.

Also useful for us is the covariance of linear combinations of
stochastic variables. Let $\{X_i\}$ and $\{Y_i\}$ be two sets of
stochastic variables. Let also $\{a_i\}$ and $\{b_i\}$ be two sets of
scalars. Consider the linear combination:
\bdm
U = \sum_i a_i X_i \qquad V = \sum_j b_j Y_j
\edm
By the linearity of the expectation value
\bdm
\cov(U, V) = \sum_{i,j}a_i b_j \cov(X_i, Y_j)
\edm
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
Now, since the variance is just $\var(X_i) = \cov(X_i, X_i)$, we get
the variance of the linear combination $U = \sum_i a_i X_i$:
\be
\var(U) = \sum_{i,j}a_i a_j \cov(X_i, X_j)
\label{eq:variance_linear_combination}
\ee
And in the special case when the stochastic variables are
uncorrelated, the off-diagonal elements of the covariance are as we
know zero, resulting in:
\bdm
\var(U) = \sum_i a_i^2 \cov(X_i, X_i) = \sum_i a_i^2 \var(X_i)
\edm
\bdm
\var(\sum_i a_i X_i) = \sum_i a_i^2 \var(X_i)
\edm
which will become very useful in our study of the error in the mean
value of a set of measurements.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
In practical situations a sample is always of finite size. Let that
size be $n$. The expectation value of a sample, the \emph{sample
mean}, is then defined as follows:
\bdm
\bar x_n \equiv \frac{1}{n}\sum_{k=1}^n x_k
\edm
The \emph{sample variance} is:
\bdm
\var(x) \equiv \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2
\edm
its square root being the \emph{standard deviation of the sample}. The
\emph{sample covariance} is:
\bdm
\cov(x)\equiv\frac{1}{n}\sum_{kl}(x_k - \bar x_n)(x_l - \bar x_n)
\edm
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
Note that the sample variance is the sample covariance without the
cross terms. In a similar manner as the covariance in
eq.~(\ref{eq:def_covariance}) is a measure of the correlation between
two stochastic variables, the above defined sample covariance is a
measure of the sequential correlation between succeeding measurements
of a sample.

These quantities, being known experimental values, differ
significantly from and must not be confused with the similarly named
quantities for stochastic variables, mean $\mu_X$, variance $\var(X)$
and covariance $\cov(X,Y)$.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
The law of large numbers
states that as the size of our sample grows to infinity, the sample
mean approaches the true mean $\mu_X^{\phantom X}$ of the chosen PDF:
\bdm
\lim_{n\to\infty}\bar x_n = \mu_X^{\phantom X}
\edm
The sample mean $\bar x_n$ works therefore as an estimate of the true
mean $\mu_X^{\phantom X}$.

What we need to find out is how good an approximation $\bar x_n$ is to
$\mu_X^{\phantom X}$. In any stochastic measurement, an estimated
mean is of no use to us without a measure of its error. A quantity
that tells us how well we can reproduce it in another experiment. We
are therefore interested in the PDF of the sample mean itself. Its
standard deviation will be a measure of the spread of sample means,
and we will simply call it the \emph{error} of the sample mean, or
just sample error, and denote it by $\mathrm{err}_X^{\phantom X}$. In
practice, we will only be able to produce an \emph{estimate} of the
sample error since the exact value would require the knowledge of the
true PDFs behind, which we usually do not have.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
The straight forward brute force way of estimating the sample error is
simply by producing a number of samples, and treating the mean of each
as a measurement. The standard deviation of these means will then be
an estimate of the original sample error. If we are unable to produce
more than one sample, we can split it up sequentially into smaller
ones, treating each in the same way as above. This procedure is known
as \emph{blocking} and will be given more attention shortly. At this
point it is worth while exploring more indirect methods of estimation
that will help us understand some important underlying principles of
correlational effects.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
Let us first take a look at what happens to the sample error as the
size of the sample grows. In a sample, each of the measurements $x_i$
can be associated with its own stochastic variable $X_i$. The
stochastic variable $\overline X_n$ for the sample mean $\bar x_n$ is
then just a linear combination, already familiar to us:
\bdm
\overline X_n = \frac{1}{n}\sum_{i=1}^n X_i
\edm
All the coefficients are just equal $1/n$. The PDF of $\overline X_n$,
denoted by $p_{\overline X_n}(x)$ is the desired PDF of the sample
means. 
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
The probability density of obtaining a sample mean $\bar x_n$
is the product of probabilities of obtaining arbitrary values $x_1,
x_2,\dots,x_n$ with the constraint that the mean of the set $\{x_i\}$
is $\bar x_n$:
\bdm
p_{\overline X_n}(x) = \int p_X^{\phantom X}(x_1)\cdots
\int p_X^{\phantom X}(x_n)\ 
\delta\!\left(x - \frac{x_1+x_2+\dots+x_n}{n}\right)dx_n \cdots dx_1
\edm
And in particular we are interested in its variance $\var(\overline
X_n)$.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
It is generally not possible to express $p_{\overline X_n}(x)$ in a
closed form given an arbitrary PDF $p_X^{\phantom X}$ and a number
$n$. But for the limit $n\to\infty$ it is possible to make an
approximation. The very important result is called \emph{the central
  limit theorem}. It tells us that as $n$ goes to infinity,
$p_{\overline X_n}(x)$ approaches a Gaussian distribution whose mean
and variance equal the true mean and variance, $\mu_{X}^{\phantom X}$
and $\sigma_{X}^{2}$, respectively:
\be
\lim_{n\to\infty} p_{\overline X_n}(x) =
\left(\frac{n}{2\pi\var(X)}\right)^{1/2}
e^{-\frac{n(x-\bar x_n)^2}{2\var(X)}}
\label{eq:central_limit_gaussian}
\ee
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
The desired variance
$\var(\overline X_n)$, i.e. the sample error squared
$\mathrm{err}_X^2$, is given by:
\be
\mathrm{err}_X^2 = \var(\overline X_n) = \frac{1}{n^2}
\sum_{ij} \cov(X_i, X_j)
\label{eq:error_exact}
\ee
We see now that in order to calculate the exact error of the sample
with the above expression, we would need the true means
$\mu_{X_i}^{\phantom X}$ of the stochastic variables $X_i$. To
calculate these requires that we know the true multivariate PDF of all
the $X_i$. But this PDF is unknown to us, we have only got the measurements of
one sample. The best we can do is to let the sample itself be an
estimate of the PDF of each of the $X_i$, estimating all properties of
$X_i$ through the measurements of the sample.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
Our estimate of $\mu_{X_i}^{\phantom X}$ is then the sample mean $\bar x$
itself, in accordance with the the central limit theorem:
\bdm
\mu_{X_i}^{\phantom X} = \mean{x_i} \approx \frac{1}{n}\sum_{k=1}^n
x_k = \bar x
\edm
Using $\bar x$ in place of $\mu_{X_i}^{\phantom X}$ we can give an
\emph{estimate} of the covariance in eq.~(\ref{eq:error_exact}):
\beaN
\cov(X_i, X_j) &=& \mean{(x_i-\mean{x_i})(x_j-\mean{x_j})}
\approx\mean{(x_i - \bar x)(x_j - \bar{x})}\\
&\approx&\frac{1}{n} \sum_{l}^n \left(\frac{1}{n}\sum_{k}^n (x_k -
\bar x_n)(x_l - \bar x_n)\right)
=\frac{1}{n}\frac{1}{n} \sum_{kl} (x_k -
\bar x_n)(x_l - \bar x_n)\\
&=&\frac{1}{n}\cov(x)
\eeaN
}
\end{small}
}



\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
By the same procedure we can use the sample variance as an
estimate of the variance of any of the stochastic variables $X_i$:
\bea
\var(X_i)
&=&\mean{x_i - \mean{x_i}} \approx \mean{x_i - \bar x_n}\nonumber\\
&\approx&\frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)\nonumber\\
&=&\var(x)
\label{eq:var_estimate_i_think}
\eea
Now we can calculate an estimate of the error
$\mathrm{err}_X^{\phantom X}$ of the sample mean $\bar x_n$:
\bea
\mathrm{err}_X^2
&=&\frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) \nonumber \\
&\approx&\frac{1}{n^2}\sum_{ij}\frac{1}{n}\cov(x) =
\frac{1}{n^2}n^2\frac{1}{n}\cov(x)\nonumber\\
&=&\frac{1}{n}\cov(x)
\label{eq:error_estimate}
\eea
which is nothing but the sample covariance divided by the number of
measurements in the sample.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
In the special case that the measurements of the sample are
uncorrelated (equivalently the stochastic variables $X_i$ are
uncorrelated) we have that the off-diagonal elements of the covariance
are zero. This gives the following estimate of the sample error:
\bea
\mathrm{err}_X^2 &=& \frac{1}{n^2}\sum_{ij} \cov(X_i, X_j) =
\frac{1}{n^2} \sum_i \var(X_i)\nonumber\\
&\approx&
\frac{1}{n^2} \sum_i \var(x)\nonumber\\ &=& \frac{1}{n}\var(x)
\label{eq:error_estimate_uncorrel}
\eea
where in the second step we have used eq.~(\ref{eq:var_estimate_i_think}).
The error of the sample is then just its standard deviation divided by
the square root of the number of measurements the sample contains.
This is a very useful formula which is easy to compute. It acts as a
first approximation to the error, but in numerical experiments, we
cannot overlook the always present correlations.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
For computational purposes one usually splits up the estimate of
$\mathrm{err}_X^2$, given by eq.~(\ref{eq:error_estimate}), into two
parts:
\bea
\mathrm{err}_X^2 &=&
\frac{1}{n}\var(x) + \frac{1}{n}(\cov(x)-\var(x))\nonumber\\&=&
\frac{1}{n^2}\sum_{k=1}^n (x_k - \bar x_n)^2 +
\frac{2}{n^2}\sum_{k<l} (x_k - \bar x_n)(x_l - \bar x_n)
\label{eq:error_estimate_split_up}
\eea
The first term is the same as the error in the uncorrelated case,
eq.~(\ref{eq:error_estimate_uncorrel}). This means that the second
term accounts for the error correction due to correlation between the
measurements. For uncorrelated measurements this second term is zero.
}
\end{small}
}

\frame
{
  \frametitle{Variance, covariance, errors etc}
\begin{small}
{\scriptsize
Computationally the uncorrelated first term is much easier to treat
efficiently than the second.
\bdm
\var(x) = \frac{1}{n}\sum_{k=1}^n (x_k - \bar x_n)^2 =
\left(\frac{1}{n}\sum_{k=1}^n x_k^2\right) - \bar x_n^2
\edm
We just accumulate separately the values $x^2$ and $x$ for every
measurement $x$ we receive. The correlation term, though, has to be
calculated at the end of the experiment since we need all the
measurements to calculate the cross terms. Therefore, all measurements
have to be stored throughout the experiment.
}
\end{small}
}


\frame
 {
  \frametitle{Random Numbers, chapter 11.3}
 \begin{center}
\includegraphics[scale=0.5]{random.jpg}
 \end{center}
 }


\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
Most used are so-called 'Linear congruential'
\[
  N_i=(aN_{i-1}+c) MOD (M),
\]
and to find a number in $x\in [0,1]$
\[
  x_i=N_i/M
\]
$M$ is called the period and should be as big as possible. 
The start value is $N_0$ and is called the seed. 

\begin{itemize}
\item The random variables should result in the uniform distribution
\item No correlations between numbers (zero covariance)
\item As big as possible period $M$
\item Fast algo
\end{itemize}

}
\end{small}
}



\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize

The problem with such generators is that their outputs are periodic;
they 
will start to repeat themselves with a period that is at most $M$. If however
the parameters $a$ and $c$ are badly chosen, the period may be even shorter.

Consider the following example
\[
  N_i=(6N_{i-1}+7) \mathrm{MOD} (5),
\]
with a seed $N_0=2$. This generator produces the sequence
$4,1,3,0,2,4,1,3,0,2,...\dots$, i.e., a sequence with period $5$.
However, increasing $M$ may not guarantee a larger period as the following
example shows
\[
  N_i=(27N_{i-1}+11) \mathrm{MOD} (54),
\]
which still, with $N_0=2$, results in $11,38,11,38,11,38,\dots$, a period of
just $2$.

}
\end{small}
}


\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
Typical periods for the random generators provided in the program library 
are of the order of $\sim 10^9$ or larger. Other random number generators which have
become increasingly popular are so-called shift-register generators.
In these generators each successive number depends on many preceding
values (rather than the last values as in the linear congruential
generator).
For example, you could make a shift register generator whose $l$th 
number is the sum of the $l-i$th and $l-j$th values with modulo $M$,
\be
   N_l=(aN_{l-i}+cN_{l-j})\mathrm{MOD}(M).
\ee
Such a generator again produces a sequence of pseudorandom numbers
but this time with a period much larger than $M$.
It is also possible to construct more elaborate algorithms by including
more than two past terms in the sum of each iteration.
One example is the generator of Marsaglia and Zaman (Computers in Physics {\bf 8} (1994) 117)
which consists of two congruential relations
\[
   N_l=(N_{l-3}-N_{l-1})\mathrm{MOD}(2^{31}-69),
   \label{eq:mz1}
\]
followed by
\[
   N_l=(69069N_{l-1}+1013904243)\mathrm{MOD}(2^{32}),
   \label{eq:mz2}
\]
which according to the authors has a period larger than $2^{94}$.

}
\end{small}
}


\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
Using modular addition, we could use the bitwise
exclusive-OR ($\oplus$) operation so that
\be
   N_l=(N_{l-i})\oplus (N_{l-j})
\ee
where the bitwise action of $\oplus$ means that if $N_{l-i}=N_{l-j}$ the result is
$0$ whereas if $N_{l-i}\ne N_{l-j}$ the result is
$1$. As an example, consider the case where  $N_{l-i}=6$ and $N_{l-j}=11$. The first
one has a bit representation (using 4 bits only) which reads $0110$ whereas the 
second number is $1011$. Employing the $\oplus$ operator yields 
$1101$, or $2^3+2^2+2^0=13$.

In Fortran90, the bitwise $\oplus$ operation is coded through the intrinsic
function $\mathrm{IEOR}(m,n)$ where $m$ and $n$ are the input numbers, while in $C$
it is given by $m\wedge n$.
}
\end{small}
}


\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
The function $ran0$ implements 
\[
  N_i=(aN_{i-1}) \mathrm{MOD} (M).
\]
Note that $c=0$ and that it cannot be initialized with $N_0=0$.
However, since $a$ and $N_{i-1}$ are integers and their multiplication 
could become greater than the standard 32 bit integer, there is a trick via 
Schrage's algorithm which approximates the multiplication
of large integers through the factorization
\[
  M=aq+r,
\]
where we have defined
\[
   q=[M/a],
\]
and 
\[
  r = M\hspace{0.1cm}\mathrm{MOD} \hspace{0.1cm}a.
\]
where the brackets denote integer division. In the code below the numbers 
$q$ and $r$ are chosen so that $r < q$.
}
\end{small}
}


\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
To see how this works we note first that
\[
(aN_{i-1}) \mathrm{MOD} (M)= (aN_{i-1}-[N_{i-1}/q]M)\mathrm{MOD} (M),
\label{eq:rntrick1}
\]
since we can add or subtract any integer multiple of $M$ from $aN_{i-1}$.
The last term $ [N_{i-1}/q]M \mathrm{MOD} (M)$ is zero since the integer division 
$[N_{i-1}/q]$ just yields a constant which is multiplied with $M$. 
Rewrite as
\[
(aN_{i-1}) \mathrm{MOD} (M)= (aN_{i-1}-[N_{i-1}/q](aq+r))\mathrm{MOD} (M),
\label{eq:rntrick2}
\]
}
\end{small}
}


\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
It gives
\[
(aN_{i-1}) \mathrm{MOD} (M)= \left(a(N_{i-1}-[N_{i-1}/q]q)-[N_{i-1}/q]r)\right)\mathrm{MOD} (M),
\label{eq:rntrick3}
\]
yielding
\[
(aN_{i-1}) \mathrm{MOD} (M)= \left(a(N_{i-1}\mathrm{MOD} (q)) -[N_{i-1}/q]r)\right)\mathrm{MOD} (M).
\label{eq:rntrick4}
\]
\begin{itemize}
\item $[N_{i-1}/q]r$ is always smaller or equal $N_{i-1}(r/q)$ and with $r < q$ we obtain always a 
number smaller than $N_{i-1}$, which is smaller than $M$.
\item $N_{i-1}\mathrm{MOD} (q)$ is between zero and $q-1$ then
$a(N_{i-1}\mathrm{MOD} (q))< aq$. 
\item Our definition of $q=[M/a]$ ensures that 
this term is also smaller than $M$ meaning that both terms fit into a
32-bit signed integer. None of these two terms can be negative, but their difference could.
\end{itemize}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
\begin{verbatim}
/*  ran0() is an "Minimal" random number generator of Park and Miller
** Set or reset the input value
** idum to any integer value (except the unlikely value MASK)
** to initialize the sequence; idum must not be altered between
** calls for sucessive deviates in a sequence.
** The function returns a uniform deviate between 0.0 and 1.0.
*/
double ran0(long &idum)
{
   const int a = 16807, m = 2147483647, q = 127773;
   const int r = 2836, MASK = 123459876;
   const double am = 1./m;
   long     k;
   double   ans;
   idum ^= MASK;
   k = (*idum)/q;
   idum = a*(idum - k*q) - r*k;
   // add m if negative difference
   if(idum < 0) idum += m;
   ans=am*(idum);
   idum ^= MASK;
   return ans;
} // End: function ran0() 
\end{verbatim}
}
\end{small}
}


\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
Important tests  of random numbers are the standard deviation $\sigma$ and the mean
$\mu=\langle x\rangle$.

For the uniform distribution we have 
\[
   \langle x^k\rangle=\int_0^1dxp(x)x^k=\int_0^1dxx^k=\frac{1}{k+1},
\]
since $p(x)=1$. 
The mean value $\mu$ is then
\[
  \mu=\langle x\rangle=\frac{1}{2}
\]
while the standard deviation is
\[
   \sigma=\sqrt{\langle x^2\rangle-\mu^2}=\frac{1}{\sqrt{12}}=0.2886.
\]
}
\end{small}
}
\frame
{
  \frametitle{Random Numbers}
\begin{small}
{\scriptsize
Number of $x$-values for various intervals 
generated by 4 random number generators,
their corresponding mean values and standard deviations. All calculations
have been initialized with the variable $idum=-1$.
\begin{tabular}{crrrr}\hline
$x$-bin &ran0&ran1&ran2&ran3\\\hline
0.0-0.1 &1013 &991 &938 &1047 \\
0.1-0.2 &1002 &1009 &1040 &1030 \\
0.2-0.3 &989 &999 &1030 &993 \\
0.3-0.4 &939 &960 &1023 &937 \\
0.4-0.5 &1038 &1001 &1002 &992 \\
0.5-0.6 &1037 &1047 &1009 & 1009\\
0.6-0.7 &1005 &989 &1003 &989 \\
0.7-0.8 &986 &962 &985 &954 \\
0.8-0.9 &1000 &1027 &1009 &1023 \\
0.9-1.0 &991 &1015 &961 &1026 \\ \hline
$\mu$ &0.4997 &0.5018 &0.4992 & 0.4990\\
$\sigma$ &0.2882 &0.2892 &0.2861 &0.2915 \\
\hline
\end{tabular} 
}
\end{small}
}


\frame
 {
  \frametitle{Random Number}
 \begin{center}
\includegraphics[scale=0.25]{../Cartoons/skann0001.jpg}
 \end{center}
 }

\frame
 {
  \frametitle{Random Numbers}
 \begin{center}
\includegraphics[scale=0.4]{../Cartoons/skann0003.jpg}
 \end{center}
 }






\section{Week 46}
\frame
{
  \frametitle{Week 46}
  \begin{block}{Monte Carlo methods, chapter 12}
\begin{itemize}
\item Monday: Repetition from last week
\item Brownian motion and Markov chains, chapters 12.2 and 12.3 of lecture notes
\item Tuesday: 
\item More on Markov chains (chapter 12.4 and 12.5)
\item The Metropolis algorithm (chapter 12.5) and simple implementations of it
\item Discussion of project 5 , there are three versions, available from Tuesday the 12th.
%\item How to use titan.uio.no
\end{itemize}
Next week we will continue to discuss the various projects. For the quantum mechanical version we will also discuss how to perform quantum mechanical calculations. We will also discuss how to parallelize the diffusion equation.
  \end{block}
} 


\frame
{
  \frametitle{Why Markov Chains?}
\begin{small}
{\scriptsize
\begin{itemize}  
\item We want to study a physical system which evolves towards equilibrium, from given  initial
conditions.
\item We start with a PDF $w(x_0,t_0)$  and we want to understand how it evolves with time.
\item We want to reach a situation where after a given number of time steps we obtain a steady state.
This means that the system reaches its most likely state (equilibrium situation)
\item Our PDF is normally a multidimensional object whose normalization constant is impossible to find.
\item Analytical calculations from $w(x,t)$ are not possible.
\item To sample directly from from $w(x,t)$ is not possible/difficult.
\item The transition probability $W$ is also not  known.
\item How can we establish that we have reached a steady state?   Sounds impossible! 
Use Markov chain Monte Carlo
\end{itemize}
}
\end{small}
}




\frame
{
  \frametitle{Brownian Motion and Markov Processes}
A Markov process is a random walk with a selected probability for making a
move. The new move is independent of the previous history of the system. 
The Markov process is used repeatedly in Monte Carlo simulations in order to generate
new random states. 
The reason for choosing a Markov process is that when it is run for a 
long enough time starting with a random state, 
we will eventually reach the most likely state of the system.
In thermodynamics, this means that after a certain number of Markov processes
we reach an equilibrium distribution. 


This mimicks the way a real system reaches 
its most likely state at a given temperature of the surroundings. 
}


\frame
{
  \frametitle{Brownian Motion and Markov Processes}
To reach this distribution, the Markov process needs to obey two important conditions, that of
{\bf ergodicity} and {\bf detailed balance}. These conditions impose then constraints on our algorithms
for accepting or rejecting new random states.\newline\newline\newline
The Metropolis algorithm discussed here 
abides to both these constraints.
The Metropolis algorithm is widely used in Monte Carlo 
simulations and the understanding of it rests within 
the interpretation of random walks and Markov processes.
}



\frame
{
  \frametitle{Brownian Motion and Markov Processes}
In a random walk one defines a mathematical entity called a {\bf walker}, whose attributes 
completely define the state of the system in question. The state of the system can refer to any physical quantities,
from the vibrational state of a molecule specified by a set of quantum numbers, to the brands of coffee
in your favourite supermarket.\newline\newline
The walker moves in an appropriate state space by a combination of deterministic and random displacements from its previous 
position. 

This sequence of steps forms a {\bf chain}.
}


\frame
{
  \frametitle{Sequence of ingredients}
\begin{small}
{\scriptsize
\begin{itemize}  
\item We want to study a physical system which evolves towards equilibrium, from given  initial
conditions.
\item Markov chains are intimately linked with the physical process of diffusion. Proof in lecture notes.
\item From a Markov chain we can then derive the conditions for detailed balance and ergodicity.
These are the conditions needed for obtaining a steady state.
\item The widely used algorithm for doing this is the so-called Metropolis algorithm,
in its refined form the Metropolis-Hastings algorithm. 
\end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{Applications: almost every field}
\begin{small}
{\scriptsize
\begin{itemize}  
\item Financial engineering, see for example Patriarca {\em et al}, Physica {\bf 340}, page 334 (2004).
\item Neuroscience, see for example Lipinski, Physics Medical Biology {\bf 35}, page 441 (1990) or
Farnell and Gibson, Journal of Computational Physics {\bf 208}, page 253 (2005)
\item Tons of applications in physics
\item and chemistry
\item and biology, medicine
\item Nobel prize in economy to Black and Scholes
\[
\frac{\partial V}{\partial t}+\frac{1}{2}\sigma^{2}S^{2}\frac{\partial^{2} V}{\partial S^{2}}+rS\frac{\partial V}{\partial S}-rV=0.
\]
The 
Black and Scholes equation is a partial differential equation, which describes the price 
of the option over time. The derivation is based on Brownian motion (Langevin and Fokker-Planck, 12.6)
\item ...the list is almost infinite
\end{itemize}
}
\end{small}
}

\frame
{
  \frametitle{A simple Example}
\begin{small}
{\scriptsize
The obvious case is that of a random walker on a one-, or two- or three-dimensional lattice
(dubbed coordinate space hereafter)

Consider a system whose energy is defined by the orientation of single spins.
Consider the state $i$, with given energy $E_i$ represented by the following $N$ spins
\[
\begin{array}{cccccccccc}
\uparrow&\uparrow&\uparrow&\dots&\uparrow&\downarrow&\uparrow&\dots&\uparrow&\downarrow\\
1&2&3&\dots& k-1&k&k+1&\dots&N-1&N\end{array}
\]
We may be  interested in the transition with one single  spinflip to a new state $j$ with energy $E_j$
\[
\begin{array}{cccccccccc}
\uparrow&\uparrow&\uparrow&\dots&\uparrow&\uparrow&\uparrow&\dots&\uparrow&\downarrow\\
1&2&3&\dots& k-1&k&k+1&\dots&N-1&N\end{array}
\]
This change from one microstate $i$ (or spin configuration)  to another microstate $j$ is the
{\bf configuration space}  analogue to a random walk on a lattice. Instead of jumping from 
one place to another in space, we 'jump' from one microstate to another.
}
\end{small}
}





\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
From experiment there are strong indications that the flux of particles $j(x,t)$, viz., the number of particles passing $x$ at a time $t$ is proportional to the 
gradient of $w(x,t)$. This proportionality is expressed mathematically through 
\begin{equation}
    j(x,t) = -D\frac{\partial w(x,t)}{\partial x},
\end{equation}
where $D$ is the so-called diffusion constant, with dimensionality length$^2$ per time.
If the number of particles is conserved, we have the continuity equation
\begin{equation}
    \frac{\partial j(x,t)}{\partial x} = -\frac{\partial w(x,t)}{\partial t},
\end{equation}
which leads to
\begin{equation}\label{eq:diffequation1}
    \frac{\partial w(x,t)}{\partial t} = 
    D\frac{\partial^2w(x,t)}{\partial x^2},
\end{equation}
which is the diffusion equation in one dimension. Solved as a partial differential equation
in chapter 10. 
}
\end{small}
}

\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
With the probability distribution function $w(x,t)dx$ we can 
compute expectation values such as  the mean distance
\begin{equation}
   \langle x(t)\rangle = \int_{-\infty}^{\infty}xw(x,t)dx,
\end{equation}
or
\begin{equation}
   \langle x^2(t)\rangle = \int_{-\infty}^{\infty}x^2w(x,t)dx, 
\end{equation}
which allows for the computation of the variance
$\sigma^2=\langle x^2(t)\rangle-\langle x(t)\rangle^2$. Note well that 
these expectation values are time-dependent. In a similar way we can also
define expectation values of functions $f(x,t)$ as 
\begin{equation}
   \langle f(x,t)\rangle = \int_{-\infty}^{\infty}f(x,t)w(x,t)dx.
\end{equation}

}
\end{small}
}


\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
Since $w(x,t)$ is now treated as a PDF, it needs to obey the same criteria
as discussed in the previous chapter. However, the normalization condition
\begin{equation}
   \int_{-\infty}^{\infty}w(x,t)dx=1
\end{equation}
imposes significant constraints on $w(x,t)$. These are
\begin{equation}
   w(x=\pm \infty,t)=0 \hspace{1cm} 
   \frac{\partial^{n}w(x,t)}{\partial x^n}|_{x=\pm\infty} = 0,
\end{equation}
implying that when we study the time-derivative
${\partial\langle x(t)\rangle}/{\partial t}$, we obtain after integration by parts and using 
Eq.~(\ref{eq:diffequation1})  
\begin{equation}
   \frac{\partial \langle x\rangle}{\partial t} = 
   \int_{-\infty}^{\infty}x\frac{\partial w(x,t)}{\partial t}dx=
   D\int_{-\infty}^{\infty}x\frac{\partial^2w(x,t)}{\partial x^2}dx,
 \end{equation}
leading to
\begin{equation}
   \frac{\partial \langle x\rangle}{\partial t} = 
   Dx\frac{\partial w(x,t)}{\partial x}|_{x=\pm\infty}-
   D\int_{-\infty}^{\infty}\frac{\partial w(x,t)}{\partial x}dx,
 \end{equation}
implying that
\begin{equation}
   \frac{\partial \langle x\rangle}{\partial t} = 0.
 \end{equation}
}
\end{small}
}


\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
This means in turn that $\langle x\rangle$ is independent of time.
If we choose the initial position $x(t=0)=0$,
the average displacement $\langle x\rangle= 0$.
If we link this discussion to a random walk in one dimension with equal probability
of jumping to the left or right and with an initial position $x=0$, then our probability
distribution remains centered around $\langle x\rangle= 0$ as function of time.
However, the variance is not necessarily 0. Consider first
\begin{equation}
   \frac{\partial \langle x^2\rangle}{\partial t} = 
   Dx^2\frac{\partial w(x,t)}{\partial x}|_{x=\pm\infty}-
   2D\int_{-\infty}^{\infty}x\frac{\partial w(x,t)}{\partial x}dx,
 \end{equation}
where we have performed an integration by parts as we did 
for $\frac{\partial \langle x\rangle}{\partial t}$. A further integration by parts 
results in  
\begin{equation}
   \frac{\partial \langle x^2\rangle}{\partial t} = 
   -Dxw(x,t)|_{x=\pm\infty}+
   2D\int_{-\infty}^{\infty}w(x,t)dx=2D,
 \end{equation}
leading to
\begin{equation}
   \langle x^2\rangle = 2Dt,
 \end{equation}
and the variance as 
\begin{equation}\label{eq:variancediffeq}
   \langle x^2\rangle-\langle x\rangle^2 = 2Dt.
 \end{equation}
The root mean square displacement after a time $t$ is then 
\begin{equation}
   \sqrt{\langle x^2\rangle-\langle x\rangle^2} = \sqrt{2Dt}.
 \end{equation}
}
\end{small}
}

\frame
{
  \frametitle{Random walks, chapter 12.2}
\begin{small}
{\scriptsize
Consider now a random walker in one dimension, with probability $R$ of moving to the right
and $L$ for moving to the left. 
At $t=0$ we place the walker at $x=0$.
The walker can then jump, with the above probabilities, either to the left or to the
right for each time step. Note that in principle we could also have the possibility that the
walker remains in the same position. This is not implemented in this example.
Every step has length $\Delta x = l$. Time is discretized and we have a jump either to the left or
to the right at every time step.

}
\end{small}
}


\frame
{
  \frametitle{Random walks}
\begin{small}
{\scriptsize
Let us now assume that we have 
equal probabilities for jumping to the left or to the right, i.e., 
$L=R=1/2$.
The average displacement
after $n$ time steps is
\[
   \langle x(n)\rangle = \sum_{i}^{n} \Delta x_i = 0 \hspace{1cm} \Delta x_i=\pm l,
\]
since we have an equal probability of jumping either to the left or to right.
The value of $\langle x(n)^2\rangle$ is
\[
   \langle x(n)^2\rangle = \left(\sum_{i}^{n} \Delta x_i\right)^2=\sum_{i}^{n} \Delta x_i^2+
\sum_{i\ne j}^{n} \Delta x_i\Delta x_j=l^2n.
\]
For many enough steps the non-diagonal contribution is
\[
   \sum_{i\ne j}^{N} \Delta x_i\Delta x_j=0,
\]
since $\Delta x_{i,j} = \pm l$.
}
\end{small}
}



\frame
{
  \frametitle{Random walks}
\begin{small}
{\scriptsize
The variance is then
\[
   \langle x(n)^2\rangle - \langle x(n)\rangle^2 = l^2n.
   \label{eq:rwvariance}
\]
It is also rather straightforward to compute the variance for $L\ne R$. The result is
\[
   \langle x(n)^2\rangle - \langle x(n)\rangle^2 = 4LRl^2n.
\]
The variable $n$ represents the number of time
steps. If we define $n=t/\Delta t$, we can then couple the variance result 
from a random walk
in one dimension with the variance  from diffusion
by defining the diffusion constant as 
\[
   D = \frac{l^2}{\Delta t}.
\]
}
\end{small}
}

\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize

When solving partial differential equations such as the diffusion equation numerically,
the derivatives are always discretized. 
We can rewrite the time derivative as
\begin{equation}
    \frac{\partial w(x,t)}{\partial t} \approx 
    \frac{w(i,n+1)-w(i,n)}{\Delta t},
\end{equation}
whereas the gradient is approximated as
\begin{equation}
    D\frac{\partial^2w(x,t)}{\partial x^2}\approx 
    D\frac{w(i+1,n)+w(i-1,n)-2w(i,n)}{(\Delta x)^2},
\end{equation}
resulting in the discretized diffusion equation
\be
   \frac{w(i,n+1)-w(i,n)}{\Delta t}=D\frac{w(i+1,n)+w(i-1,n)-2w(i,n)}{(\Delta x)^2},
\ee
where $n$ represents a given time step and $i$ a step in the $x$-direction.
}
\end{small}
}



\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
A Markov process allows in principle for a microscopic description of Brownian motion.
As with the random walk, we consider a particle 
which moves along the  $x$-axis in the form of a series of jumps with step length 
$\Delta x = l$. Time and space are discretized and the subsequent moves are
statistically independent, i.e., the new move depends only on the previous step
and not on the results from earlier trials. 
We start at a position $x=jl=j\Delta x$ and move to 
a new position $x =i\Delta x$ during a step $\Delta t=\epsilon$, where 
$i\ge  0$ and $j\ge 0$ are integers. 
The original probability distribution function (PDF) of the particles is given by  
$w_i(t=0)$ where $i$ refers to a specific position on a grid, with $i=0$ representing $x=0$. 
The function $w_i(t=0)$ is now the discretized version of $w(x,t)$.
We can regard the discretized PDF as a vector.
}
\end{small}
}

\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
For the Markov process we have a transition probability from a position
$x=jl$ to a position $x=il$ given by 
\be
   W_{ij}(\epsilon)=W(il-jl,\epsilon)=\left\{\begin{array}{cc}\frac{1}{2} & |i-j| = 1\\
                                             0 & \mathrm{else} \end{array} \right.
\ee
We call $W_{ij}$ for the transition probability and we can represent it, see below,
as a matrix. Our new PDF $w_i(t=\epsilon)$ is now related to the PDF at
$t=0$ through the relation 
\be 
   w_i(t=\epsilon) = W(j\rightarrow i)w_j(t=0).
\ee   
}
\end{small}
}

\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
This equation represents the discretized time-development of an original 
PDF. 
Since both $W$ and $w$ represent probabilities, they have to be normalized, i.e., we require
that at each time step we have 
\be 
   \sum_i w_i(t) = 1, 
\ee
and 
\be 
   \sum_j W(j\rightarrow i) = 1.
\ee
The further constraints are
$0 \le W_{ij} \le 1$  and  $0 \le w_{j} \le 1$.
Note that the probability for remaining at the same place is in general 
not necessarily equal zero. In our Markov process we allow only for jumps to the left or to 
the right.

}
\end{small}
}

\frame
{
  \frametitle{Diffusion from Markov Chain}
\begin{small}
{\scriptsize
The time development of our initial PDF can now be represented through the action of
the transition probability matrix applied $n$ times. At a 
time  $t_n=n\epsilon$ our initial distribution has developed into 
\be
   w_i(t_n) = \sum_jW_{ij}(t_n)w_j(0),
\ee
and defining 
\be
   W(il-jl,n\epsilon)=(W^n(\epsilon))_{ij}
\ee
we obtain 
\be
   w_i(n\epsilon) = \sum_j(W^n(\epsilon))_{ij}w_j(0),
\ee
or in matrix form
\be\label{eq:wfinal}
   \hat{w(n\epsilon)} = \hat{W}^n(\epsilon)\hat{w}(0).
\ee
}
\end{small}
}


\frame
{
  \frametitle{Brownian Motion and Markov Processes}
\begin{small}
{\scriptsize
We wish to study the time-development of a PDF after a given number of time steps.
We define our PDF by the function $w(t)$. In addition we define a transition probability 
$W$. 
The time development of our PDF $w(t)$, after one time-step from $t=0$ is given by
\[
   w_i(t=\epsilon) = W(j\rightarrow i)w_j(t=0).
\]   
Normally we don't know the form of $W$!!
This equation represents the discretized time-development of an original 
PDF.  We can rewrite this as a 
\[
   w_i(t=\epsilon) = W_{ij}w_j(t=0).
\]
with the transition matrix $W$ for a random walk left or right (cannot stay in the same position) given by
\[
   W_{ij}(\epsilon)=W(il-jl,\epsilon)=\left\{\begin{array}{cc}\frac{1}{2} & |i-j| = 1\\
                                             0 & \mathrm{else} \end{array} \right.
\]
We call $W_{ij}$ for the transition probability and we represent it
as a matrix.  
}
\end{small}
}




\frame
{
  \frametitle{Brownian Motion and Markov Processes}
\begin{small}
{\scriptsize
Both  $W$ and $w$ represent probabilities and they have to be normalized, meaning that
that at each time step we have 
\[
   \sum_i w_i(t) = 1, 
\]
and 
\[ 
   \sum_j W(j\rightarrow i) = 1.
\]
Further constraints are
$0 \le W_{ij} \le 1$  and  $0 \le w_{j} \le 1$.
We can thus write the action of $W$ as 
\[
   w_i(t+1) = \sum_jW_{ij}w_j(t),
\]
or as vector-matrix relation
\[
   {\bf \hat{w}}(t+1) = {\bf \hat{W}\hat{w}}(t),
\]
and if we have that $||{\bf \hat{w}}(t+1)-{\bf \hat{w}}(t)||\rightarrow 0$, we say that 
we have reached the most likely state of the system, the so-called steady state or equilibrium state.
Another way of phrasing this is
       \[ {\bf w}(t=\infty) = {\bf Ww}(t=\infty). \]

}
\end{small}
}




\frame
{
  \frametitle{Brownian Motion and Markov Processes, a simple Example}
\begin{small}
{\scriptsize
Consider the simple $3\times 3$ matrix $\hat{W}$
\[
   \hat{W} = \left(\begin{array}{ccc} 1/4 & 1/8 & 2/3\\                   
                                 3/4 & 5/8 & 0\\                   
                                 0 & 1/4 & 1/3\\   \end{array} \right),
\]
and we choose our initial state as 
\[
\hat{w}(t=0)=  \left(\begin{array}{c} 1\\                   
                                 0\\                   
                                 0 \end{array} \right).
\]
The first iteration is
\[
   w_i(t=\epsilon) = W(j\rightarrow i)w_j(t=0),
\]   
resulting in
\[
\hat{w}(t=\epsilon)=  \left(\begin{array}{c} 1/4\\                   
                                3/4 \\                   
                                 0 \end{array} \right).
\]
}
\end{small}
}


\frame
{
  \frametitle{Brownian Motion and Markov Processes, a simple Example}
\begin{small}
{\scriptsize
The next iteration results in 
\[
   w_i(t=2\epsilon) = W(j\rightarrow i)w_j(t=\epsilon),
\]   
resulting in
\[
\hat{w}(t=2\epsilon)=  \left(\begin{array}{c} 5/32\\                   
                                21/32 \\                   
                                6/32 \end{array} \right).
\]
Note that the vector $\hat{w}$ is always normalized to $1$. We find the steady state of the system by solving the linear set of equations
\[ {\bf w}(t=\infty) = {\bf Ww}(t=\infty). \]
}
\end{small}
}

\frame
{
  \frametitle{Brownian Motion and Markov Processes, a simple Example}
\begin{small}
{\scriptsize
This linear set of equations reads
\begin{eqnarray}
 W_{11}w_1(t=\infty) +W_{12}w_2(t=\infty) +W_{13}w_3(t=\infty)=&w_1(t=\infty) \nonumber \\
W_{21}w_1(t=\infty) + W_{22}w_2(t=\infty) + W_{23}w_3(t=\infty)=&w_2(t=\infty) \nonumber \\
W_{31}w_1(t=\infty) + W_{32}w_2(t=\infty) + W_{33}w_3(t=\infty)=&w_3(t=\infty) \nonumber \\
\end{eqnarray}
with the constraint that 
\[
   \sum_i w_i(t=\infty) = 1, 
\]
yielding as solution
\[
\hat{w}(t=\infty)=  \left(\begin{array}{c} 4/15\\                   
                                8/15 \\                   
                                3/15 \end{array} \right).
\]
}
\end{small}
}


\frame
 {
   \frametitle{Brownian Motion and Markov Processes, a simple Example}
 \begin{small}
 {\scriptsize
Convergence of the simple example
\begin{center}
\begin{tabular}{rllll}\hline
Iteration &$w_1$   &$w_2$  &$w_3$\\\hline
0  & 1.00000 &0.00000  &0.00000 \\
1  & 0.25000 &0.75000  &0.00000 \\
2  & 0.15625 &0.62625  &0.18750 \\
3  & 0.24609 &0.52734  &0.22656 \\
4   &0.27848 &0.51416 &0.20736 \\
5   &0.27213 &0.53021 &0.19766 \\
6   &0.26608 &0.53548 &0.19844 \\
7  &0.26575 &0.53424 &0.20002 \\
8  &0.26656 &0.53321 &0.20023 \\
9  &0.26678 &0.53318 &0.20005 \\
10  &0.26671 &0.53332 &0.19998 \\
11  &0.26666 &0.53335 &0.20000 \\
12  &0.26666 &0.53334 &0.20000 \\
13  &0.26667 &0.53333 &0.20000 \\
$\hat{w}(t=\infty)$ &0.26667 &0.53333 &0.20000 \\
\hline
\end{tabular} 
\end{center}   
{\bf In a Markov chain Monte Carlo  $w$ is normally given, we need to find $W$!}
 }
 \end{small}
 }


\frame
 {
   \frametitle{Brownian Motion and Markov Processes, what is happening?}
 \begin{small}
 {\scriptsize
We have after $t$-steps
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}^t\hat{w}}(0),
\]
with ${\bf \hat{w}}(0)$ the distribution at $t=0$ and ${\bf \hat{W}}$ representing the 
transition probability matrix. 
We can always expand ${\bf \hat{w}}(0)$ in terms of the right eigenvectors 
${\bf \hat{v}}$ of ${\bf \hat{W}}$ as 
\[
    {\bf \hat{w}}(0)  = \sum_i\alpha_i{\bf \hat{v}}_i,
\]
resulting in 
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}}^t{\bf \hat{w}}(0)={\bf \hat{W}}^t\sum_i\alpha_i{\bf \hat{v}}_i=
\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i,
\]
with $\lambda_i$ the $i^{\mathrm{th}}$ eigenvalue corresponding to  
the eigenvector ${\bf \hat{v}}_i$. 
 }
 \end{small}
 }


\frame
 {
   \frametitle{Brownian Motion and Markov Processes, what is happening?}
 \begin{small}
 {\scriptsize
If we assume that $\lambda_0$ is the largest eigenvector we see that in the limit $t\rightarrow \infty$,
${\bf \hat{w}}(t)$ becomes proportional to the corresponding eigenvector 
${\bf \hat{v}}_0$. This is our steady state or final distribution. 

In our discussion below in connection with the entropy of a system and later statistical physics and
quantum physics
applications, we will relate these properties to correlation functions such as 
the time-correlation function. 

That will allow us to define the so-called {\em equilibration time},viz the time needed for the system
to reach its most likely state. From that state and on we can can compute contributions to various statistical
variables.  
 }
 \end{small}
 }


\frame
 {
   \frametitle{Brownian Motion and Markov Processes, what is happening?}
 \begin{small}
 {\scriptsize
We anticipate parts of the discussion on statistical physics.

We can relate this property to an observable like the mean magnetization of say a magnetic material.
With the probabilty ${\bf \hat{w}}(t)$ we
can write the mean magnetization as 
\[
 \langle {\cal M}(t) \rangle  = \sum_{\mu} {\bf \hat{w}}(t)_{\mu}{\cal M}_{\mu},
\]  
or as the scalar of a  vector product
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m},
\]  
with ${\bf m}$ being the vector whose elements are the values of ${\cal M}_{\mu}$ in its 
various microstates $\mu$.

Recall our definition of an expectation value with a discrete PDF $p(x_i)$:
\[
    E[x^k]= \langle x^k\rangle=\frac{1}{N}\sum_{i=1}^{N}x_i^kp(x_i),
\]
provided that the sums (or integrals) $ \sum_{i=1}^{N}p(x_i)$ converge absolutely (viz ,
$ \sum_{i=1}^{N}|p(x_i)|$ converges)
 }
 \end{small}
 }



\frame
 {
   \frametitle{Brownian Motion and Markov Processes, what is happening?}
 \begin{small}
 {\scriptsize

We rewrite the last relation as
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m}=\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i{\bf m}_i.
\]  
If we define $m_i={\bf \hat{v}}_i{\bf m}_i$ as the expectation value of
${\cal M}$ in the $i^{\mathrm{th}}$ eigenstate we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
\] 
Since we have that in the limit $t\rightarrow \infty$ the mean magnetization is dominated by the 
largest eigenvalue $\lambda_0$, we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
\] 
 }
 \end{small}
 }





\frame
 {
   \frametitle{Brownian Motion and Markov Processes, what is happening?}
 \begin{small}
 {\scriptsize
We define the quantity
\[
   \tau_i=-\frac{1}{log\lambda_i},
\]
and rewrite the last expectation value as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\] 
The quantities $\tau_i$ are the correlation times for the system. They control also the time-correlation functions.

The longest correlation time is obviously given by the second largest
eigenvalue $\tau_1$, which normally defines the correlation time discussed above. For large times, this is the 
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from 
$\lambda_1$ and we simulate long enough,  $\tau_1$ may well define the correlation time. 
In other cases we may not be able to extract a reliable result for $\tau_1$. 
 }
 \end{small}
 }




\frame
{
  \frametitle{Entropy and Equilibrium, section 12.4}
The definition of the entropy $S$ (as a dimensionless quantity here) is
\be
   S = -\sum_i w_i ln(w_i),
\ee
where $w_i$ is the probability of finding our system in a state $i$. For our one-dimensional randow walk 
it represents the probability for being at position $i=i\Delta x$ after a given number of time steps.\newline\newline
Assume now that we have  $N$ random walkers at
$i=0$ and $t=0$ and let these random walkers diffuse as function of time. 
}



\frame
{
  \frametitle{Entropy and Equilibrium, section 12.4}
We compute then the probability distribution for $N$ walkers after a given number of steps $i$ along $x$ and 
time steps $j$.
We can then compute an entropy $S_j$ for a given number of time steps by summing over all probabilities $i$.
The code used to compute these results is in programs/chapter12/program4.cpp.
Here we have used 
100 walkers on a lattice of length from $L=-50$ to $L=50$ employing periodic boundary conditions meaning
that if a walker reaches the point $x=L$ it is shifted to $x=-L$ and if $x=-L$ it is shifted to $x=L$.
}





\frame[containsverbatim]
{
  \frametitle{Entropy}
\begin{lstlisting}
// loop over all time steps
  for (int step=1; step <= time_steps; step++){
    // move all walkers with periodic boundary conditions
    for (int walks = 1; walks <= walkers; walks++){   
      if (ran0(&idum) <= move_probability) {
	if ( x[walks] +1 > length) {
	  x[walks] = -length;
	}
	else{
	  x[walks] += 1; 
	}
      } 
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Entropy}
\begin{lstlisting}
      else {
	if ( x[walks] -1 < -length) {
	  x[walks] = length;
        }
	else{
	  x[walks] -= 1; 
	}
      }
    }  // end of loop over walks
  } // end of loop over trials
\end{lstlisting}
}




\frame[containsverbatim]
{
  \frametitle{Entropy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // at the final time step we compute the probability
  // by counting the number of walkers at every position
  for ( int i = -length; i <= length; i++){
    int count = 0; 
    for( int j = 1; j <= walkers; j++){
      if ( x[j] == i ) {
        count += 1;
      }
    }
    probability[i+length] = count;
  }
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Entropy}
\begin{small}
{\scriptsize
\begin{lstlisting}
// Writes the results to screen
void output(int length, int time_steps, int walkers, int *probability)
{
  double entropy, histogram;
  // find norm of probability
  double norm = 1.0/walkers;
  // compute the entropy
  entropy = 0.; histogram = 0.;
  for( int  i = -length; i <=  length; i++){
    histogram = (double) probability[i+length]*norm;
    if ( histogram > 0.0) {
    entropy -= histogram*log(histogram);
    }
  }
// then write entropy to file
\end{lstlisting}
}
\end{small}
}

\frame
{
  \frametitle{Entropy}

At small time steps
the entropy is very small, reflecting the fact that we have an ordered state. As time elapses, the random walkers spread
out in space (here in one dimension) and the entropy increases as there are more states, that is positions accesible 
to the system. We say that the system shows an increased degree of disorder. 
After several time steps, we see that the entropy  reaches a constant value, a situation called a steady state.
This signals that the system has reached its equilibrium situation and that the random walkers spread out to
occupy all possible available states. At equilibrium it means thus that all states
are equally probable and this is not baked into any dynamical equations such as Newton's law of motion. 
}


\frame
{
  \frametitle{Entropy}
It occurs because the system is allowed to explore all possibilities. An important hypothesis is the ergodic 
hypothesis which states that in equilibrium all available states of a closed
system have equal probability. 
This hypothesis states also that if we are able to simulate long enough, then one should be able to trace through all
possible paths in the space of available states to reach the equilibrium situation. 
Our Markov process should be able to reach any state of the system from any other state if we run for long enough.
}




\frame
{
  \frametitle{Detailed Balance}
{\bf In a Markov Monte Carlo  $w$ is normally given, we need to find $W$!}
But we need to find which distribution we obtain when the steady state has been achieved.
\begin{itemize}
 \item \small Markov process with transition probability from a state $j$ to another
       state $i$
       \[ \sum_j W(j\rightarrow i) = 1 \]
Note that the probability for remaining at the same place is not necessarily equal zero.
 \item \small PDF $w_i$ at time  $t=n\epsilon$
       \[ w_i(t) = \sum_j W(j\rightarrow i)^nw_j(t=0) \]   
\item \small \[ \sum_i w_i(t) = 1 \]
\end{itemize}
}
\frame
{
  \frametitle{Detailed Balance}
\begin{itemize}
 \item \small Detailed balance condition 
       \[ \sum_i W(j\rightarrow i)w_j= \sum_i W(i\rightarrow j)w_i  \]
Ensures that it is the correct  distribution which is achieved when equilibrium
is reached.
 \item \small When a Markow process reaches equilibrium we have
       \[ {\bf w}(t=\infty) = {\bf Ww}(t=\infty) \]   
\item \small General condition at equilibrium
       \[ W(j\rightarrow i)w_j= W(i\rightarrow j)w_i  \]
which is the detailed balance condition.  Proof is simple.
\end{itemize}
}


\frame
{
  \frametitle{Detailed Balance}
To derive the conditions for equilibrium, we start from the so-called Master equation, which relates the temporal development of a PDF $w_i(t)$. The equation is given
\[
\frac{d w_i(t)}{dt} = \sum_j\left[ W(j\rightarrow i)w_j-W(i\rightarrow j)w_i\right],
\] 
which simply states that the rate at which the systems moves from a state $j$
to a final state $i$ (the first term on the right-hand side of the last equation) is balanced by the rate at which the systems undergoes transitions from the state $i$ to a state $j$ (the second term). If we have reached the so-called steady state, then the temporal dependence is zero. This  means that in equilibrium we have
\be
\frac{d w_i(t)}{dt} = 0.
\ee 
}





\frame
{
  \frametitle{Ergodicity}
It should be possible for any Markov process to reach every possible state of the system
from any starting point if the simulations is carried out for a long enough time.

Any state in a Boltzmann distribution has a probability different from zero and if such 
a state cannot be reached from a given starting point, then the system is not ergodic.

}






\frame
{
  \frametitle{Example:  Boltzmann Distribution}
\begin{itemize}
 \item \small At equilibrium detailed balance gives
       \[ \frac{W(j\rightarrow i)}{W(i\rightarrow j)}=\frac{w_i}{w_j}  \]
 \item \small Boltzmann distribution
       \[ \frac{w_i}{w_j}= \exp{(-\beta(E_i-E_j))}  \]
\end{itemize}
}




\frame
{
  \frametitle{Selection Rule}
\begin{itemize}
 \item In general
       \[ W(i\rightarrow j)=g(i\rightarrow j)A(i\rightarrow j)  \]
where $g$ is a selection probability while  $A$ is the probability for accepting a
move. It is also called the acceptance ratio.
 \item With detailed balance this gives
       \[ \frac{g(j\rightarrow i)A(j\rightarrow i)}{g(i\rightarrow j)A(i\rightarrow j)}= \exp{(-\beta(E_i-E_j))}  \]
\end{itemize}
}




\frame
{
  \frametitle{Metropolis Algorithm}
For a system which follows the Boltzmann distribution the Metropolis algorithm reads
       \[ A(j\rightarrow i)=\left\{\begin{array}{cc} \exp{(-\beta(E_i-E_j))} & E_i-E_j > 0 \\ 1 & else \end{array} \right.  \]
This algorithm satisfies the condition for detailed balance and ergodicity.
}



\frame
{
  \frametitle{Implementation}
\begin{small}
{\scriptsize
\begin{itemize}
\item \small Establish an initial energy $E_b$
\item \small Do a random change of this initial state by e.g., flipping 
an individual spin. This new state has energy 
$E_t$. Compute then $\Delta E=E_t-E_b$ 
\item \small If $\Delta E \le 0$ accept the new configuration.
\item \small If $\Delta E >  0$, compute $w=e^{-(\beta \Delta E)}$.
\item \small Compare $w$ with a random number $r$. If
$r \le w$
accept, else keep the old configuration.
\item \small Compute the terms in the sums $\sum A_s P_s$.
\item \small Repeat the above steps in order to have a large enough number of
microstates
\item \small For a given number of MC cycles, compute then expectation values.
\end{itemize}
}
\end{small}
}

\frame
{
  \frametitle{Test of the Metropolis Algorithm}
\begin{small}
{\scriptsize
Want  to show that the Metropolis algorithm
generates the Boltzmann distribution
\be
   P(\beta)=\frac{e^{-\beta E}}{Z},
\ee
with $\beta=1/kT$ being the inverse temperature, $E$ is the energy of
the system and
$Z$ is the partition function. The only functions you will need are those
to generate random numbers.

We are going to study one single particle in equilibrium with 
its surroundings, the latter modeled via a large heat bath
with temperature $T$.

The model used to describe this particle is that of an ideal gas
in {\bf one} dimension and with velocity $-v$ or $v$. 
We are interested in finding  $P(v)dv$, which expresses the probability
for finding the system with a given velocity $v\in [v,v+dv]$.
The energy for this one-dimensional system is
\be
  E=\frac{1}{2}kT=\frac{1}{2}v^2,
\ee
with mass $m=1$.
}
\end{small}
}



\frame
{
  \frametitle{Test of the Metropolis Algorithm}
\begin{small}
{\scriptsize
Want  to show that the Metropolis algorithm
generates the Boltzmann distribution
\be
   P(\beta)=\frac{e^{-\beta E}}{Z},
\ee
with $\beta=1/kT$ being the inverse temperature, $E$ is the energy of
the system and
$Z$ is the partition function. The only functions you will need are those
to generate random numbers.

We are going to study one single particle in equilibrium with 
its surroundings, the latter modeled via a large heat bath
with temperature $T$.

The model used to describe this particle is that of an ideal gas
in {\bf one} dimension and with velocity $-v$ or $v$. 
We are interested in finding  $P(v)dv$, which expresses the probability
for finding the system with a given velocity $v\in [v,v+dv]$.
The energy for this one-dimensional system is
\be
  E=\frac{1}{2}kT=\frac{1}{2}v^2,
\ee
with mass $m=1$.
}
\end{small}
}


\frame
{
  \frametitle{Test of the Metropolis Algorithm, closed form results}
\begin{small}
{\scriptsize
  The partition function of the system of interest is:
  \begin{equation}
    Z = \int_{-\infty}^{+\infty} e^{-\beta v^2 /2} dv = \sqrt{2\pi} \beta^{-1/2} \nonumber
  \end{equation}
  The mean velocity
  \begin{equation}
    \langle v \rangle =  \int_{-\infty}^{+\infty} v e^{-\beta v^2 /2} dv = 0 \nonumber
  \end{equation}
The  expressions for $\langle E \rangle$ and $\sigma_E$ assume the following form:
  \begin{equation}
    \langle E \rangle =  \int_{-\infty}^{+\infty} \frac{v^2}{2}\, e^{-\beta v^2 /2} dv = -\frac{1}{Z} \frac{\partial Z}{\partial \beta} = 
    \frac{1}{2} \beta^{-1} =  \frac{1}{2} T \nonumber
  \end{equation}
  \begin{equation}
    \langle E^2 \rangle =  \int_{-\infty}^{+\infty} \frac{v^4}{4}\, e^{-\beta v^2 /2} dv = \frac{1}{Z} \frac{\partial^2 Z}{\partial \beta^2} = 
    \frac{3}{4} \beta^{-2} =  \frac{3}{4} T^2 \nonumber
  \end{equation}
  and
  \begin{equation}
    \sigma_E = \langle E^2 \rangle - \langle E \rangle^2 = \frac{1}{2} T^2 \nonumber
  \end{equation}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Test of the Metropolis Algorithm}
\begin{small}
{\scriptsize
\begin{verbatim}
   for( montecarlo_cycles=1; Max_cycles; montecarlo_cycles++) {
      ...
      // change speed as function of delta v
      v_change = (2*ran1(&idum) -1 )* delta_v;
      v_new = v_old+v_change;
      // energy change
      delta_E = 0.5*(v_new*v_new - v_old*v_old) ;
      ......
      // Metropolis algorithm begins here
        if ( ran1(&idum) <= exp(-beta*delta_E)  ) {
            accept_step = accept_step + 1 ;      
            v_old = v_new ;
        }
      // thereafter we must fill in  P[N] as a function of
      // the new speed 
      // upgrade mean velocity, energy and variance
      }
\end{verbatim}
}
\end{small}
}



\frame
{
  \frametitle{Test of the Metropolis Algorithm}
\begin{small}
{\scriptsize
Analytical vs numerical results. $T=4$, $10^8$ MC tries, $\Delta v = 0.2$
    \begin{tabular}{ccc}
      \\
      \hline
      Observable & Analytical value & Numerical value \\
      \hline
      \\
      $\langle v \rangle $ &   0.00000  & -0.00679  \\
      $\langle E \rangle $ &   2.00000  &  1.99855  \\
      $\sigma_E          $ &   8.00000  &  8.06669  \\
      \\
      \hline
    \end{tabular}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Code for Metropolis test}
\begin{small}
{\scriptsize
\begin{verbatim}
  v_current = v0;
  
  // start simulation
  ofile.open("evsmc.dat");
  for (tries = 1; tries <= MC; tries++){   

    v_change = (2.*ran0(&idum) - 1.) * dv; 
    v_trial  = v_current + v_change;

    // evaluate dE
    delta_E = 0.5 * ( v_trial * v_trial - v_current * v_current );

\end{verbatim}
}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{Code for Metropolis test}
\begin{small}
{\scriptsize
\begin{verbatim}
    // Metropolis test
    if (delta_E <= 0) {
      acceptance++; v_current = v_trial;
    }
    else if (ran0(&idum) <= exp( -beta * delta_E )){
      acceptance++; v_current = v_trial;
    }
    
    // check if velocity value lies within given limits
    if (abs(v_current) > v_max) {
      cout<<"Velocity out of range."; exit(1);
    }
\end{verbatim}
}
\end{small}
}

\frame
 {
  \frametitle{Keep or scratch?}
 \begin{center}
\includegraphics[scale=0.2]{skann0002.jpg}
 \end{center}
 }


\frame[containsverbatim]
{
  \frametitle{Code for Metropolis test}
\begin{small}
{\scriptsize
\begin{verbatim}
    // save event in P array
    address = (int) floor( v_current / dv ) + N/2 + 1;
    P[address]++;
 
    // update mean velocity, mean energy and energy variance values
    mean_v += v_current;
    mean_E += 0.5 * v_current * v_current;
    E_variance += 0.25 * v_current * v_current * v_current * v_current;
\end{verbatim}
}
\end{small}
}

\frame[containsverbatim]
{
\frametitle{Code for Metropolis test}
\begin{small}
{\scriptsize
\begin{verbatim}
  // initialize model parameters
  beta = 1./T; v_max = 10. * sqrt (T);
  // calculate amount of P-array elements
  N = 2 * (int)(v_max/dv) + 1;
  // initialize P-array
  P = new int [N]; 
      
  for (int i=0; i < N; i++) P[i] = 0;

  mean_v = 0.; mean_E = 0.; E_variance = 0.;
  acceptance = 0;

}// initialize
\end{verbatim}
}
\end{small}
}


\section{Week 47}
\frame
{
  \frametitle{Week 47}
  \begin{block}{Summary and exam discussion}
\begin{itemize}
\item Monday: 
\item Brief repetition from last week and discussion of projects 
\item Metropolis algorithm and Variational Monte Carlo
\item Emphasis on the Monte Carlo project (integration and Variational Monte Carlo)
\item Tuesday:
\item Discussion of projects and parallelization (all projects)
\item Discussion of Monte Carlo simulation of Markov chains (linked with the diffusion project)
\end{itemize}
  \end{block}
} 





\frame
{
  \frametitle{Pros and Cons of Quantum Monte Carlo}
\begin{small}
{\scriptsize
\begin{itemize} 
\item Is physically intuitive.
\item Allows one to study systems with many degrees of freedom. Diffusion Monte Carlo (DMC) and Green's function Monte Carlo (GFMC) yield
in principle the exact solution to Schr\"odinger's equation.
\item Variational Monte Carlo (VMC) is easy  to implement but needs 
a reliable trial wave function, can be difficult to obtain. 
\item DMC/GFMC for fermions (spin with half-integer values, electrons, baryons, neutrinos, quarks) 
has a sign problem. Nature prefers an anti-symmetric wave function. PDF in this case given
distribution of random walkers ($p\ge 0$).
\item The solution has a statistical error, which can be large. 
\item There is a limit for how large systems one can study, DMC needs a huge number of random walkers
in order to achieve stable results. 
\item Obtain only the lowest-lying states with a given symmetry. Can get excited states.
\end{itemize}
}
\end{small}
}




\frame
{
  \frametitle{Where and why do we use Monte Carlo Methods in Quantum Physics}
\begin{small}
{\scriptsize
\begin{itemize} 
\item Quantum systems with many particles at finite temperature: Path Integral 
Monte Carlo with applications to dense matter and quantum liquids (phase transitions from
normal fluid to superfluid). Strong correlations.
\item Bose-Einstein condensation of dilute gases, method transition from 
non-linear PDE to Diffusion Monte Carlo as density increases.
\item Light atoms, molecules, solids and nuclei. 
\item Lattice Quantum-Chromo Dynamics. Impossible to solve without MC calculations. 
\item Simulations of systems in solid state physics, from semiconductors to 
spin systems. Many electrons active and possibly strong correlations.
\end{itemize}
Chapter 13 on statistical physics simulations will not be discussed this year.
}
\end{small}
}


\frame
 {
  \frametitle{Bose-Einstein Condensation of atoms, thousands of Atoms in one State, Possible project 5}
 \begin{center}
\includegraphics[scale=0.3]{BEC_three_peaks}
 \end{center}
 }

\frame
{
  \frametitle{Quantum Monte Carlo and Schr\"odinger's equation}
\begin{small}
{\scriptsize
For one-body problems (one dimension)
\[
    -\frac{\hbar^2}{2m}\nabla^2\Psi(x,t)+
V(x,t)\Psi(x,t)=
    \imath\hbar\frac{\partial \Psi(x,t)}{\partial t},
\]

\[
   P(x,t)=\Psi(x,t)^*\Psi(x,t)
\]

\[
   P(x,t)dx=\Psi(x,t)^*\Psi(x,t)dx
\]
Interpretation: probability of finding the system in a region between
$x$ and $x+dx$.
Always real
\[ 
   \Psi(x,t)=R(x,t)+\imath I(x,t)
\]
yielding
\[ 
   \Psi(x,t)^*\Psi(x,t)=(R-\imath I)(R+\imath I)=R^2+I^2
\] 
Variational Monte Carlo uses only $P(x,t)$!!
}
\end{small}
}















\frame
{
  \frametitle{Quantum Monte Carlo and Schr\"odinger's equation}
Petit digression\newline
\begin{small}
{\scriptsize
Choose $\tau = it/\hbar$. 

The time-dependent  (1-dim) Schr\"odinger equation becomes then
\[
  \frac{\partial \Psi(x,\tau)}{\partial \tau} = 
  \frac{\hbar^2}{2m} \frac{\partial^2\Psi(x,\tau)}{\partial x^2}
   -V(x,\tau)\Psi(x,\tau).
\]
With $V=0$ we have a diffusion equation in complex time with 
diffusion constant  
\[
   D= \frac{\hbar^2}{2m}.
\]
Used in diffusion Monte Carlo calculations. Topic for FYS4411, Computational Physics II
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo and Schr\"odinger's equation}
\begin{small}
{\scriptsize
Conditions which $\Psi$ has to satisfy:
\begin{enumerate}
\item Normalization
\[
   \int_{-\infty}^{\infty}P(x,t)dx=\int_{-\infty}^{\infty}\Psi(x,t)^*\Psi(x,t)dx=1
\]
meaning that
\[
  \int_{-\infty}^{\infty}\Psi(x,t)^*\Psi(x,t)dx < \infty
\]
\item $\Psi(x,t)$ and $\partial \Psi(x,t)/\partial x$ must be finite
\item $\Psi(x,t)$ and $\partial \Psi(x,t)/\partial x$ must be continuous.
\item $\Psi(x,t)$ and $\partial \Psi(x,t)/\partial x$ must be single valued
\end{enumerate}
Square integrable functions.
}
\end{small}
}


\frame
{
  \frametitle{First Postulate}
\begin{small}
{\scriptsize
Any physical quantity $A(\vec{r},\vec{p})$ which depends on position
$\vec{r}$ and momentum
$\vec{p}$ has a corresponding quantum mechanical operator by replacing
$\vec{p}$  
$-i\hbar \vec{\bigtriangledown}$, yielding the quantum mechanical operator
%
\[
\OP{A} = A(\vec{r},-i\hbar \vec{\bigtriangledown)}.
\]
\begin{tabular}{|l|l|l|}  \hline
Quantity & Classical definition & QM operator\\
\hline
Position            & $\vec{r}$           & $\OP{\vec{r}} = \vec{r}$\\
Momentum    & $\vec{p}$
						  & $\OP{\vec{p}} = -i \hbar \vec{\bigtriangledown}$\\
Orbital momentum           & $\vec{L} = \vec{r} \times \vec{p}$
		  & $\OP{\vec{L}} = \vec{r} \times (-i\hbar \vec{\bigtriangledown})$\\
Kinetic energy     & $T = (\vec{p})^2 / 2 m$
						  & $\OP{T} = - (\hbar^2 / 2 m) (\vec{\bigtriangledown})^2$\\
Total energy 		  & $H = (p^2 / 2 m) + V(\vec{r})$
						  & $\OP{H} = - ( \hbar^2 / 2 m )(\vec{\bigtriangledown})^2
										  + V(\vec{r})$\\
\hline
\end{tabular}
}
\end{small}
}


\frame
{
  \frametitle{Second Postulate}
\begin{small}
{\scriptsize
{\sl The only possible outcome of  an ideal measurement of the physical
quantity $A$ are the eigenvalues of the corresponding quantum mechanical
operator $\OP{A}$.}
%
\[
\OP{A} \psi_{\nu}
	 = a_{\nu} \psi_{\nu},
\]
%
resulting in the eigenvalues  $ a_1, a_2, a_3,\cdots$
as the only outcomes of a measurement. The corresponding
eigenstates
$ \psi_1, \psi_2, \psi_3 \cdots$
contain all relevant information about the system.
}
\end{small}
}


\frame
{
  \frametitle{Third Postulate}
\begin{small}
{\scriptsize
Assume $\Phi$ is
a linear combination of the eigenfunctions
$\psi_{\nu}$
for $\OP{A}$,
%
\[
\Phi = c_1 \psi_1 + c_2 \psi_2 + \cdots
  = \sum_{\nu} c_{\nu} \psi_{\nu}.
\]
%
The eigenfunctions are orthogonal 
and we get
%
\[
c_{\nu} = \int (\Phi)^{\ast} \psi_{\nu} d\tau.
\]
%
From this we can formulate the third postulate:\newline
\sl
When the eigenfunction is  $\Phi$, the probability of 
obtaining the value $a_{\nu}$ as the outcome of a measurement of the 
physical quantity
$A$ is given by $|c_{\nu}|^2$ and $\psi_{\nu}$ is an eigenfunction of
$\OP{A}$ with eigenvalue  $a_{\nu}$.
}
\end{small}
}


\frame
{
  \frametitle{Third Postulate}
\begin{small}
{\scriptsize
As a consequence one can show that:\newline
{\sl when a quantal system is in the state $\Phi$,
the mean value or expectation value of a physical quantity
$A(\vec{r}, \vec{p})$
is given by}
%
\[
\langle A \rangle 
	= \int (\Phi)^{\ast} \OP{A}(\vec{r}, -i \hbar\vec{\bigtriangledown})
		 \Phi d\tau.
\]
We  have assumed that
$\Phi$ has been normalized, viz., $\int (\Phi)^{\ast} \Phi d\tau = 1$.
Else
%
\[
 \langle A \rangle = \frac{\int (\Phi)^{\ast} \OP{A} \Phi d\tau}
			  {\int (\Phi)^{\ast} \Phi d\tau}.
\]
}
\end{small}
}


\frame
{
  \frametitle{Fourth Postulate}
\begin{small}
{\scriptsize
The time development of of a quantal system is given by 
%
\[
i \hbar \frac{\partial \Psi}{\partial t} = \OP{H} \Psi,
\]
%
with $\OP{H}$ the quantal Hamiltonian operator for the system.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Most quantum mechanical  
problems of interest in e.g., atomic, molecular, nuclear and solid state 
physics consist of a large number of 
interacting electrons and ions or nucleons. 
The total number of particles $N$ is usually sufficiently large
that an exact solution cannot be found. 
Typically, 
the expectation value for a chosen hamiltonian for a system of 
$N$ particles is
\[
   \langle H \rangle =
\]
\[
   \frac{\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
         \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
          H({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
          \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)}
        {\int d{\bf R}_1d{\bf R}_2\dots d{\bf R}_N
        \Psi^{\ast}({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)
        \Psi({\bf R_1},{\bf R}_2,\dots,{\bf R}_N)},
\]
}
an in general intractable problem.
\end{small}
}



\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Given a hamiltonian $H$ and a trial
wave function $\Psi_T$, the variational principle states that
the expectation value of $\langle H \rangle$, defined through 
\[
   E[H]= \langle H \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})},
\]
is an upper bound to the ground state energy $E_0$ of the hamiltonian $H$, that
is 
\[
    E_0 \le \langle H \rangle .
\]
In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones. Traditional integration methods
such as the Gauss-Legendre will not be adequate for say the 
computation of the energy of a many-body system.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The trial wave function can be expanded
in the eigenstates of the hamiltonian since they form a complete set, viz.,
\[
   \Psi_T({\bf R})=\sum_i a_i\Psi_i({\bf R}),
\]
and assuming the set of eigenfunctions to be normalized one obtains 
\[
     \frac{\sum_{nm}a^*_ma_n \int d{\bf R}\Psi^{\ast}_m({\bf R})H({\bf R})\Psi_n({\bf R})}
        {\sum_{nm}a^*_ma_n \int d{\bf R}\Psi^{\ast}_m({\bf R})\Psi_n({\bf R})} =\frac{\sum_{n}a^2_n E_n}
        {\sum_{n}a^2_n} \ge E_0,
\]
where we used that $H({\bf R})\Psi_n({\bf R})=E_n\Psi_n({\bf R})$.
In general, the integrals involved in the calculation of various  expectation
values  are multi-dimensional ones. 
The variational principle yields the lowest state of a given symmetry.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
In most cases, a wave function has only small values in large parts of 
configuration space, and a straightforward procedure which uses
homogenously distributed random points in configuration space 
will most likely lead to poor results. This may suggest that some kind
of importance sampling combined with e.g., the Metropolis algorithm 
may be  a more efficient way of obtaining the ground state energy.
The hope is then that those regions of configurations space where
the wave function assumes appreciable values are sampled more 
efficiently. 

}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The tedious part in a VMC calculation is the search for the variational
minimum. A good knowledge of the system is required in order to carry out
reasonable VMC calculations. This is not always the case, 
and often VMC calculations 
serve rather as the starting
point for so-called diffusion Monte Carlo calculations (DMC). DMC is a way of
solving exactly the many-body Schr\"odinger equation by means of 
a stochastic procedure. A good guess on the binding energy
and its wave function is however necessary. 
A carefully performed VMC calculation can aid in this context. 
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
\begin{itemize}
\item Construct first a trial wave function $\psi_T^{\alpha}({\bf R})$, 
for a many-body
system consisting of $N$ particles located at positions
${\bf R=(R_1,\dots ,R_N)}$. The trial wave function depends
on $\alpha$ variational parameters 
${\bf \alpha}=(\alpha_1,\dots ,\alpha_N)$.
\item Then we evaluate the expectation value of the hamiltonian $H$ 
\[
   E[H]=\langle H \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_{T_{\alpha}}({\bf R})H({\bf R})
         \Psi_{T_{\alpha}}({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_{T_{\alpha}}({\bf R})\Psi_{T_{\alpha}}({\bf R})}.
\]
\item Thereafter we vary $\alpha$ according to some minimization
algorithm and return to the first step.
\end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Choose a trial wave function
$\psi_T({\bf R})$.
\[
   P({\bf R})= \frac{\left|\psi_T({\bf R})\right|^2}{\int \left|\psi_T({\bf R})\right|^2d{\bf R}}.
\]
This is our new probability distribution function  (PDF).
The approximation to the expectation value of the Hamiltonian is now 
\[
   E[H]\approx 
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}.
\]
Define a new quantity
\[
   E_L({\bf R})=\frac{1}{\psi_T({\bf R})}H\psi_T({\bf R}),
   \label{eq:locale1}
\]
called the local energy, which, together with our trial PDF yields
\[
  E[H]=\langle H \rangle \approx \int P({\bf R})E_L({\bf R}) d{\bf R}\approx \frac{1}{N}\sum_{i=1}^NE_L({\bf R_i}
  \label{eq:vmc1}
\]
with $N$ being the number of Monte Carlo samples.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Algo:
       \begin{itemize}
          \item Initialisation: Fix the number of Monte Carlo steps. Choose an initial ${\bf R}$ and
                variational parameters $\alpha$ and 
                calculate
                $\left|\psi_T^{\alpha}({\bf R})\right|^2$. 
          \item Initialise the energy and the variance and start the Monte Carlo calculation (thermalize)
                \begin{enumerate}
                  \item Calculate  a trial position  ${\bf R}_p={\bf R}+r*step$
                        where $r$ is a random variable $r \in [0,1]$.
                  \item Metropolis algorithm to accept
                        or reject this move                         \[
                           w = P({\bf R}_p)/P({\bf R}).
                        \]
                  \item If the step is accepted, then we set 
                        ${\bf R}={\bf R}_p$. Update averages
                 \end{enumerate}
          \item Finish and
compute final averages.
      \end{itemize}
Observe that the jumping in space is governed by the variable $step$. Called brute-force sampling.
Need importance sampling to get more relevant sampling.
}
\end{small}
}

\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The radial Schr\"odinger equation for the hydrogen atom can be
written as
\[
-\frac{\hbar^2}{2m}\frac{\partial^2 u(r)}{\partial r^2}-
\left(\frac{ke^2}{r}-\frac{\hbar^2l(l+1)}{2mr^2}\right)u(r)=Eu(r),
\]
or with dimensionless variables
\[
-\frac{1}{2}\frac{\partial^2 u(\rho)}{\partial \rho^2}-
\frac{u(\rho)}{\rho}+\frac{l(l+1)}{2\rho^2}u(\rho)-\lambda u(\rho)=0,
\label{eq:hydrodimless1}
\]
with the hamiltonian
\[
H=-\frac{1}{2}\frac{\partial^2 }{\partial \rho^2}-
\frac{1}{\rho}+\frac{l(l+1)}{2\rho^2}.
\]
Use variational parameter $\alpha$ in the trial
wave function 
\[
   u_T^{\alpha}(\rho)=\alpha\rho e^{-\alpha\rho}. 
   \label{eq:trialhydrogen}
\]

}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Inserting this wave function into the expression for the
local energy $E_L$ gives
\[
   E_L(\rho)=-\frac{1}{\rho}-
              \frac{\alpha}{2}\left(\alpha-\frac{2}{\rho}\right).
\]

\begin{tabular}{rrrr}\hline
$\alpha$&$\langle H \rangle $&$\sigma^2$&$\sigma/\sqrt{N}$ \\\hline
 7.00000E-01 & -4.57759E-01 &  4.51201E-02 &  6.71715E-04 \\ 
 8.00000E-01 & -4.81461E-01 &  3.05736E-02 &  5.52934E-04 \\ 
 9.00000E-01 & -4.95899E-01 &  8.20497E-03 &  2.86443E-04 \\ 
 1.00000E-00 & -5.00000E-01 &  0.00000E+00 &  0.00000E+00 \\ 
 1.10000E+00 & -4.93738E-01 &  1.16989E-02 &  3.42036E-04 \\ 
 1.20000E+00 & -4.75563E-01 &  8.85899E-02 &  9.41222E-04 \\ 
 1.30000E+00 & -4.54341E-01 &  1.45171E-01 &  1.20487E-03 \\ 
\end{tabular}
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
We note that at $\alpha=1$ we obtain the exact
result, and the variance is zero, as it should. The reason is that 
we then have the exact wave function, and the action of the hamiltionan
on the wave function
\[
   H\psi = \mathrm{constant}\times \psi,
\]
yields just a constant. The integral which defines various 
expectation values involving moments of the hamiltonian becomes then
\[
   \langle H^n \rangle =
   \frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})H^n({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}=
\mathrm{constant}\times\frac{\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}
        {\int d{\bf R}\Psi^{\ast}_T({\bf R})\Psi_T({\bf R})}=
\mathrm{constant}.
\]
{\bf This gives an important information: the exact wave function leads to zero variance!}
Variation is then performed by minimizing both the energy and the variance.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The helium atom consists of two electrons and a nucleus with
charge $Z=2$. 
The contribution  
to the potential energy due to the attraction from the nucleus is
\[
   -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2},
\] 
and if we add the repulsion arising from the two 
interacting electrons, we obtain the potential energy
\[
 V(r_1, r_2)=-\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
\]
with the electrons separated at a distance 
$r_{12}=|{\bf r}_1-{\bf r}_2|$.
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
The hamiltonian becomes then
\[
   \OP{H}=-\frac{\hbar^2\nabla_1^2}{2m}-\frac{\hbar^2\nabla_2^2}{2m}
          -\frac{2ke^2}{r_1}-\frac{2ke^2}{r_2}+
               \frac{ke^2}{r_{12}},
\]
and  Schr\"odingers equation reads
\[
   \OP{H}\psi=E\psi.
\]
All observables are evaluated with respect to the probability distribution
\[
   P({\bf R})= \frac{\left|\psi_T({\bf R})\right|^2}{\int \left|\psi_T({\bf R})\right|^2d{\bf R}}.
\]
generated by the trial wave function.   
The trial wave function must approximate an exact 
eigenstate in order that accurate results are to be obtained. 
Improved trial
wave functions also improve the importance sampling, 
reducing the cost of obtaining a certain statistical accuracy. 
}
\end{small}
}


\frame
{
  \frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
Choice of trial wave function for Helium:
Assume $r_1 \rightarrow 0$.
\[
   E_L({\bf R})=\frac{1}{\psi_T({\bf R})}H\psi_T({\bf R})=
     \frac{1}{\psi_T({\bf R})}\left(-\frac{1}{2}\nabla^2_1
     -\frac{Z}{r_1}\right)\psi_T({\bf R}) + \mathrm{finite \hspace{0.1cm}terms}.
\]
\[ 
    E_L(R)=
    \frac{1}{{\cal R}_T(r_1)}\left(-\frac{1}{2}\frac{d^2}{dr_1^2}-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right){\cal R}_T(r_1) + \mathrm{finite\hspace{0.1cm} terms}
\]
For small values of $r_1$, the terms which dominate are
\[ 
    \lim_{r_1 \rightarrow 0}E_L(R)=
    \frac{1}{{\cal R}_T(r_1)}\left(-
     \frac{1}{r_1}\frac{d}{dr_1}
     -\frac{Z}{r_1}\right){\cal R}_T(r_1),
\]
since the second derivative does not diverge due to the finiteness of 
$\Psi$ at the origin.
}
\end{small}
}


\frame
{
\frametitle{Quantum Monte Carlo}
\begin{small}
{\scriptsize
This results in
\[
     \frac{1}{{\cal R}_T(r_1)}\frac{d {\cal R}_T(r_1)}{dr_1}=-Z,
\]
and
\[
   {\cal R}_T(r_1)\propto e^{-Zr_1}.
\]
A similar condition applies to electron 2 as well. 
For orbital momenta $l > 0$ we have 
\[
     \frac{1}{{\cal R}_T(r)}\frac{d {\cal R}_T(r)}{dr}=-\frac{Z}{l+1}.
\]
Similalry, studying the case $r_{12}\rightarrow 0$ we can write 
a possible trial wave function as
\[
   \psi_T({\bf R})=e^{-\alpha(r_1+r_2)}e^{\beta r_{12}}.
    \label{eq:wavehelium2}
\]
The last equation can be generalized to
\[
   \psi_T({\bf R})=\phi({\bf r}_1)\phi({\bf r}_2)\dots\phi({\bf r}_N)
                   \prod_{i< j}f(r_{ij}),
\]
for a system with $N$ electrons or particles. 
}
\end{small}
}







\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp}
\begin{small}
{\scriptsize
\begin{lstlisting}
//  Here we define global variables  used in various functions
//  These can be changed by reading from file the different parameters
int dimension = 3; // three-dimensional system
int charge = 2;  //  we fix the charge to be that of the helium atom
int my_rank, numprocs;  //  these are the parameters used by MPI  to 
                        //    define which node and how many
double step_length = 1.0;  //  we fix the brute force jump to 1 Bohr radius
int number_particles  = 2;  //  we fix also the number of electrons to be 2
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
  //  MPI initializations
  MPI_Init (&argc, &argv);
  MPI_Comm_size (MPI_COMM_WORLD, &numprocs);
  MPI_Comm_rank (MPI_COMM_WORLD, &my_rank);
  time_start = MPI_Wtime();

  if (my_rank == 0 && argc <= 2) {
    cout << "Bad Usage: " << argv[0] << 
      " read also output file on same line" << endl;
    exit(1);
  }
  if (my_rank == 0 && argc > 2) {
    outfilename=argv[1];
    ofile.open(outfilename); 
  }
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
// Setting output file name for this rank:
ostringstream ost;
ost << "blocks_rank" << my_rank << ".dat";
// Open file for writing:
blockofile.open(ost.str().c_str(), ios::out | ios::binary);

total_cumulative_e = new double[max_variations+1];
total_cumulative_e2 = new double[max_variations+1];
cumulative_e = new double[max_variations+1];
cumulative_e2 = new double[max_variations+1];

//  initialize the arrays  by zeroing them
for( i=1; i <= max_variations; i++){
   cumulative_e[i] = cumulative_e2[i]  = 0.0; 
   total_cumulative_e[i] = total_cumulative_e2[i]  = 0.0;
}
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
// broadcast the total number of  variations
MPI_Bcast (&max_variations, 1, MPI_INT, 0, MPI_COMM_WORLD);
MPI_Bcast (&number_cycles, 1, MPI_INT, 0, MPI_COMM_WORLD);
total_number_cycles = number_cycles*numprocs; 
// array to store all energies for last variation of alpha
all_energies = new double[number_cycles+1];
//  Do the mc sampling  and accumulate data with MPI_Reduce
mc_sampling(max_variations, number_cycles, cumulative_e, 
              cumulative_e2, all_energies);
//  Collect data in total averages
for( i=1; i <= max_variations; i++){
  MPI_Reduce(&cumulative_e[i], &total_cumulative_e[i], 1, MPI_DOUBLE, 
                                        MPI_SUM, 0, MPI_COMM_WORLD);
  MPI_Reduce(&cumulative_e2[i], &total_cumulative_e2[i], 1, MPI_DOUBLE, 
                               MPI_SUM, 0, MPI_COMM_WORLD);
}
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, main part}
\begin{small}
{\scriptsize
\begin{lstlisting}
blockofile.write((char*)(all_energies+1), 
		   number_cycles*sizeof(double));
blockofile.close();
delete [] total_cumulative_e; delete [] total_cumulative_e2; 
delete [] cumulative_e; delete [] cumulative_e2; delete [] all_energies; 
// End MPI
MPI_Finalize ();  
return 0;
}  //  end of main function
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
 alpha = 0.5*charge;
 // every node has its own seed for the random numbers
 idum = -1-my_rank;
 // allocate matrices which contain the position of the particles  
 r_old =(double **)matrix(number_particles,dimension,sizeof(double));
 r_new =(double **)matrix(number_particles,dimension,sizeof(double));
 for (i = 0; i < number_particles; i++) { 
    for ( j=0; j < dimension; j++) {
      r_old[i][j] = r_new[i][j] = 0;
    }
  }
 // loop over variational parameters  

\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
  for (variate=1; variate <= max_variations; variate++){
    // initialisations of variational parameters and energies 
    alpha += 0.1;  
    energy = energy2 = 0; accept =0; delta_e=0;
    //  initial trial position, note calling with alpha 
    for (i = 0; i < number_particles; i++) { 
      for ( j=0; j < dimension; j++) {
	  r_old[i][j] = step_length*(ran2(&idum)-0.5);
      }
    }
    wfold = wave_function(r_old, alpha);
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
// loop over monte carlo cycles 
for (cycles = 1; cycles <= number_cycles; cycles++){ 
   // new position 
   for (i = 0; i < number_particles; i++) { 
       for ( j=0; j < dimension; j++) {
  	   r_new[i][j] = r_old[i][j]+step_length*(ran2(&idum)-0.5);
     }  
//  for the other particles we need to set the position to the old position since
//  we move only one particle at the time
     for (k = 0; k < number_particles; k++) {
	 if ( k != i) {
	    for ( j=0; j < dimension; j++) {
	      r_new[k][j] = r_old[k][j];
	    }
	  } 
        }
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
      wfnew = wave_function(r_new, alpha); 
// The Metropolis test is performed by moving one particle at the time
      if(ran2(&idum) <= wfnew*wfnew/wfold/wfold ) { 
	  for ( j=0; j < dimension; j++) {
	    r_old[i][j]=r_new[i][j];
	  }
	  wfold = wfnew;
	}
      }  //  end of loop over particles
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, sampling}
\begin{small}
{\scriptsize
\begin{lstlisting}
      // compute local energy  
      delta_e = local_energy(r_old, alpha, wfold);
      // save all energies on last variate
      if(variate==max_variations){
	   all_energies[cycles] = delta_e;
      }
      // update energies
      energy += delta_e;
      energy2 += delta_e*delta_e;
    }   // end of loop over MC trials   
    // update the energy average and its squared 
    cumulative_e[variate] = energy;
    cumulative_e2[variate] = energy2;
  }    // end of loop over variational  steps 
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, wave function}
\begin{small}
{\scriptsize
\begin{lstlisting}
// Function to compute the squared wave function, simplest form 

double  wave_function(double **r, double alpha)
{
  int i, j, k;
  double wf, argument, r_single_particle, r_12;
  
  argument = wf = 0;
  for (i = 0; i < number_particles; i++) { 
    r_single_particle = 0;
    for (j = 0; j < dimension; j++) { 
      r_single_particle  += r[i][j]*r[i][j];
    }
    argument += sqrt(r_single_particle);
  }
  wf = exp(-argument*alpha) ;
  return wf;
}
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
// Function to calculate the local energy with num derivative

double  local_energy(double **r, double alpha, double wfold)
{
  int i, j , k;
  double e_local, wfminus, wfplus, e_kinetic, e_potential, r_12, 
    r_single_particle;
  double **r_plus, **r_minus;
  
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // allocate matrices which contain the position of the particles  
  // the function matrix is defined in the progam library 
  r_plus =(double **)matrix(number_particles,dimension,sizeof(double));
  r_minus =(double **)matrix(number_particles,dimension,sizeof(double));
  for (i = 0; i < number_particles; i++) { 
    for ( j=0; j < dimension; j++) {
      r_plus[i][j] = r_minus[i][j] = r[i][j];
    }
  }
\end{lstlisting}
}
\end{small}
}





\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // compute the kinetic energy  
  e_kinetic = 0;
  for (i = 0; i < number_particles; i++) {
    for (j = 0; j < dimension; j++) { 
      r_plus[i][j] = r[i][j]+h;
      r_minus[i][j] = r[i][j]-h;
      wfminus = wave_function(r_minus, alpha); 
      wfplus  = wave_function(r_plus, alpha); 
      e_kinetic -= (wfminus+wfplus-2*wfold);
      r_plus[i][j] = r[i][j];
      r_minus[i][j] = r[i][j];
    }
  }
// include electron mass and hbar squared and divide by wave function 
  e_kinetic = 0.5*h2*e_kinetic/wfold;
\end{lstlisting}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // compute the potential energy 
  e_potential = 0;
  // contribution from electron-proton potential  
  for (i = 0; i < number_particles; i++) { 
    r_single_particle = 0;
    for (j = 0; j < dimension; j++) { 
      r_single_particle += r[i][j]*r[i][j];
    }
    e_potential -= charge/sqrt(r_single_particle);
  }
\end{lstlisting}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{VMC code for helium, vmc\_para.cpp, local energy}
\begin{small}
{\scriptsize
\begin{lstlisting}
  // contribution from electron-electron potential  
  for (i = 0; i < number_particles-1; i++) { 
    for (j = i+1; j < number_particles; j++) {
      r_12 = 0;  
      for (k = 0; k < dimension; k++) { 
	r_12 += (r[i][k]-r[j][k])*(r[i][k]-r[j][k]);
      }
      e_potential += 1/sqrt(r_12);          
    }
  }
\end{lstlisting}
}
\end{small}
}




\frame
{
  \frametitle{Structuring the code}
\begin{small}
{\scriptsize
During the development of our code we need to make several checks. It is also very instructive to compute a closed form expression for the local energy. Since our wave function is rather simple  it is straightforward
to find an analytic expressions.  Consider first the case of the simple helium function
\[
   \Psi_T(\mathbf{r}_1,\mathbf{r}_2) = e^{-\alpha(r_1+r_2)}
\]
The local energy is for this case 
\[ 
E_{L1} = \left(\alpha-Z\right)\left(\frac{1}{r_1}+\frac{1}{r_2}\right)+\frac{1}{r_{12}}-\alpha^2
\]
which gives an expectation value for the local energy given by
\[
\langle E_{L1} \rangle = \alpha^2-2\alpha\left(Z-\frac{5}{16}\right)
\]
}
\end{small}
}

\frame
{
  \frametitle{Structuring the code}
\begin{small}
{\scriptsize
With analytic formulae we  can speed up the computation of the correlation. In our case
we write it as 
\[
\Psi_C= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
which means that the gradient needed for the local energy 
can be calculated analytically.
This will speed up your code since the computation of the correlation part and the Slater determinant are the most 
time consuming parts in your code. 

We will refer to this correlation function as $\Psi_C$ or the \emph{linear Pad\'e-Jastrow}.

}
\end{small}
}



\frame
{
  \frametitle{Structuring the code}
\begin{small}
{\scriptsize
We can test this by computing the local energy for our helium wave function

\[
   \psi_{T}({\bf r}_1,{\bf r}_2) = 
   \exp{\left(-\alpha(r_1+r_2)\right)}
   \exp{\left(\frac{r_{12}}{2(1+\beta r_{12})}\right)}, 
\]

with $\alpha$ and $\beta$ as variational parameters.

The local energy is for this case 
\[ 
E_{L2} = E_{L1}+\frac{1}{2(1+\beta r_{12})^2}\left\{\frac{\alpha(r_1+r_2)}{r_{12}}(1-\frac{\mathbf{r}_1\mathbf{r}_2}{r_1r_2})-\frac{1}{2(1+\beta r_{12})^2}-\frac{2}{r_{12}}+\frac{2\beta}{1+\beta r_{12}}\right\}
\]
Getting a closed form expression for the local energy means that you don't need to
compute derivatives numerically.  
}
\end{small}
}

\frame
{
  \frametitle{Structuring the code, simple task}
\begin{small}
{\scriptsize
\begin{itemize}
\item Make another copy of your code.
\item Implement the closed form expression for the local energy
\item Compile the new and old codes with the -pg option for profiling.
\item Run both codes and profile them afterwards using $gprof <executable> > outprofile$
\item Study the time usage in the file $outprofile$
\end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{Structuring the code, simple task}
\begin{small}
{\scriptsize
\begin{itemize}
\item Use also better compiler options like $c++ -O3$
\item Can speed up considerably your code
\end{itemize}
}
\end{small}
}



\section{Week 48}
\frame
{
  \frametitle{Week 48}
  \begin{block}{Summary and exam discussion}
\begin{itemize}
\item Monday: 
\item Brief repetition from last week and discussion of projects
\item Project discussions and structure of report
\item Summary and exam discussion
\end{itemize}
  \end{block}
No lecture on Tuesday. Monday is the last lecture this semester.
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should it contain? A possible structure}
\begin{itemize}
\item An introduction where you explain the rational for the physics case and 
what you have done. At the end of the introduction you should give a brief
summary of the structure of the report
\item Theoretical models and technicalities. This is the methods section.
\item Results and discussion
\item Conclusions and perspectives
\item Appendix with extra material, include your program!!
\item Bibliography
\end{itemize}
  \end{block}
} 

\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Introduction}
You don't need to answer all questions in a chronological order.  When you write the introduction you could focus on the following aspects
\begin{itemize}
\item Motivate the reader, the first part of the introduction gives always a motivation and tries to give the overarching ideas
\item What I have done
\item The structure of the report, how it is organized etc
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Methods sections}
\begin{itemize}
\item Describe the methods and algorithms
\item You need to explain how you implemented the methods and also say something about the structure of your algorithm and present some parts of your code
\item You should plug in some calculations to demonstrate your code, such as selected runs used to validate and verify your results. The latter is extremely important!!  A reader needs to understand that your code reproduces selected benchmarks and reproduces previous results, either numerical and/or well-known  closed form expressions.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Results}
\begin{itemize}
\item Present your results
\item Give a critical discussion of your work and place it in the correct context.
\item Relate your work to other calculations/studies
\item An eventual reader should be able to reproduce your calculations if she/he wants to do so. All input variables should be properly explained.
\item Make sure that figures and tables should contain enough information in their captions, axis labels etc so that an eventual reader can gain a first impression of your work by studying figures and tables only.
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? Conclusions}
\begin{itemize}
\item State your main findings and interpretations
\item Try as far as possible to present perspectives for future work
\item Try to discuss the pros and cons of the methods and possible improvements
\end{itemize}
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? additional material}
\begin{itemize}
\item Additional calculations used to validate the codes
\item Selected calculations, these can be listed with 
few comments
\item Listing of the code if you feel this is necessary
\end{itemize}
You can consider moving parts of the material from the methods section to the appendix. You can also place additional material on your webpage. 
  \end{block}
} 


\frame
{
  \frametitle{The report}
  \begin{block}{What should I focus on? References}
\begin{itemize}
\item Give always references to material you base your work on, either 
scientific articles/reports or books.
\item {\em Wikipedia is not accepted as a scientific reference}. Under no circumstances.
\item Refer to articles as: name(s) of author(s), journal, volume (boldfaced), page and year
in parenthesis.
\item Refer to books as: name(s) of author(s), title of book, publisher, place and year, eventual page numbers
\end{itemize}
  \end{block}
} 
  


\frame
{ 
  \frametitle{Exam FYS3150/FYS4150}
  \begin{block}{Pensum/syllabus}
Go to \url{http://www.uio.no/studier/emner/matnat/fys/FYS3150/h13/} 
and click on syllabus. 

For the written exam, you can bring with you 2 pages (written on both sides) with own notes. See also the exams 
from last years. 
  \end{block}
} 


\frame
{ 
  \frametitle{Exam FYS3150/FYS4150}
  \begin{block}{Topics}
\begin{itemize}
\item Linear algebra and eigenvalue problems. (Lecture notes chapters 6.1-6.5  and 7.1-7.5 and projects 1 and 2).
\item Numerical integration, standard methods and Monte Carlo methods (Lecture notes chapters 5.1-5.5 and 11 and project 5).
\item Ordinary differential equations (Lecture notes chapter 8  and projects 3 and 5 )
\item Partial differential equations (Lecture notes chapter 10 and projects 4 and 5)
\item Monte Carlo methods in physics (Lecture notes chapters 11, 12 and 14, project 5)
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Exam FYS3150/FYS4150}
  \begin{block}{Linear algebra and eigenvalue problems, chapters 6.1-6.5 and 7.1-7.5}
\begin{itemize}
\item Know Gaussian elimination and LU decomposition   (project 1)
\item How to solve linear equations   (project 1)
\item How to obtain the inverse and the determinant of a real symmetric matrix
\item Cubic spline
\item Tridiagonal matrix decomposition   (project 1)
\item Householder's tridiagonalization technique and finding eigenvalues based on this
\item Jacobi's method for finding eigenvalues (project 2)
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Exam FYS3150/FYS4150}
  \begin{block}{Numerical integration, standard methods and Monte Carlo methods (5.1-5.5 and 11)}
\begin{itemize}
\item Trapezoidal, rectangle  and Simpson's rules
\item Gaussian quadrature, emphasis on Legendre polynomials, but you need
to know about other polynomials as well (project 5).  
\item Brute force Monte Carlo integration (project 5)
\item Random numbers (simplest algo, ran0) and probability distribution functions, expectation values
\item Improved Monte Carlo integration and importance sampling (project 5).
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Exam FYS3150/FYS4150}
  \begin{block}{Monte Carlo methods in physics (12 and 14)}
\begin{itemize}
\item Random walks and Markov chains and relation with diffusion equation (project 5)
\item Metropolis algorithm, detailed balance and ergodicity (project 5)
\item Applications to quantum mechanical systems (project 5)
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Exam FYS3150/FYS4150}
  \begin{block}{Ordinary differential equations (Chapter 8)}
\begin{itemize}
\item Euler's method and improved Euler's method, truncation errors (project 3)
\item Runge Kutta methods, 2nd and 4th order, truncation errors (project 3)
\item Leap-frog and Verlet algoritm (project 5)
\item How to implement a second-order differential equation, 
both linear and non-linear. How to make your equations dimensionless.
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Exam FYS3150/FYS4150}
  \begin{block}{Partial differential equations, chapter 10}
\begin{itemize}
\item Set up diffusion, Poisson and wave equations up to 2
spatial dimensions and time  
\item Set up the mathematical model and algorithms for these equations,
with boundary and initial conditions. The stability conditions for the diffusion equation.
\item Explicit, implicit and Crank-Nicolson schemes, and how to solve them.
Remember that they result in triangular matrices (project 4).  
\item Diffusion equation in two dimensions (project 5)
\item How to compute the Laplacian in Poisson's equation (project 5).
\item How to solve the wave equation in one and two dimensions using an explicit scheme.
\end{itemize}
  \end{block}
} 
x

\frame
{ 
  \frametitle{Other courses in Computational Science at UiO}
  \begin{block}{Bachelor/Master/PhD Courses}
\begin{itemize}
\item INF-MAT4350 Numerical linear algebra
\item MAT-INF3300/3310/4300/4310, PDEs and Sobolev spaces I and II
\item INF-MAT3360 Partial differential equation
\item INF3380 Parallell programming for scientific problems
\item INF5620 Numerical methods for PDEs, finite element method
\item FYS4411 Computational physics II, computational quantum mechanics.
\item FYS4460: Computational statistical mechanics
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{AND, GOOD LUCK TO YOU ALL!}
\vspace{-10cm}
\begin{figure}
\includegraphics[scale=0.6]{Nebbdyr2}
\end{figure}
} 






\end{document}






\end{document}

\frame[containsverbatim]
{
  \frametitle{Version control, Git and dropbox}
\begin{small}
{\scriptsize
Why version control?
\begin{itemize}
\item It allows you to keep track of different change made
\item It allows people you collaborate with to see the recent changes
\item It becomes an excellent logbook and allows people to verify your results
\item It can aso be used to build up a large directory of codes, with validation examples as well
\end{itemize}
Git is a very popular version control software, and easy to use. To install on ubuntu just write
\begin{verbatim}
sudo apt-get  install git-core 
\end{verbatim}
This applies to Dropbox as well.
}
\end{small}
} 

\frame[containsverbatim]
{
  \frametitle{Version control, Git}
\begin{small}
{\scriptsize
When you want to start tracking a project just go to the project's directory and type
\begin{verbatim}
git init
\end{verbatim}
This creates a subdirectory .git tat contains all of your necessary repository files. At this point nothing in your project is tracked yet.  To start, there are a few commands you need in the beginning. As an example
\begin{verbatim}
git add *.cpp  *.hpp
git add ANOTHERFILE
git commit -m 'This is first setup of my superduper project'
\end{verbatim}
using 
\begin{verbatim}
git status
\end{verbatim}
Tells you about the modifcations made. You can also standard unix commands like $mv$, $rm$ etc
\begin{verbatim}
git mv file_from file_to
\end{verbatim}
}

\end{small}
} 


\frame[containsverbatim]
{
  \frametitle{Version control, Git}
\begin{small}
{\scriptsize
If you want to get a copy of an existing Git repository for example, 
a project you would like to contribute to, the command you need is 
\begin{verbatim}
git clone 
\end{verbatim}
If you are familiar with other VCS systems such as Subversion, you will 
notice that the command is clone and not checkout. This is an important distinction. 
Git receives a copy of nearly all data that the server has. 
Every version of every file for the history of the project is pulled down when you run git clone. 
In fact, if your server disk gets corrupted, you can use any of the clones on any client to set the server back to the state it was in when it was cloned.

\begin{verbatim}
git status
\end{verbatim}
}
\end{small}
} 







\frame[containsverbatim]
{
  \frametitle{Optimization and profiling, useful for project 3}
\begin{small}
{\scriptsize
Till now we have not paid much attention to speed and possible optimization possibilities
inherent in the various compilers. We have compiled and linked as
\begin{verbatim}
c++  -c  mycode.cpp
c++  -o  mycode.exe  mycode.o
\end{verbatim}
This is what we call a flat compiler option and should be used when we develop the code.
It produces normally a very large and slow code when translated to machine instructions.
We use this option for debugging and for establishing the correct program output because
every operation is done precisely as the user specified it.

It is instructive to look up the compiler manual for further instructions
\begin{verbatim}
man c++  >  out_to_file
\end{verbatim}
}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Optimization and profiling}
\begin{small}
{\scriptsize
We have additional compiler options for optimization. These may include procedure inlining where 
performance may be improved, moving constants inside loops outside the loop, 
identify potential parallelism, include automatic vectorization or replace a division with a reciprocal
and a multiplication if this speeds up the code.
\begin{verbatim}
c++  -O3 -c  mycode.cpp
c++  -O3 -o  mycode.exe  mycode.o
\end{verbatim}
This is the recommended option. {\bf But you must check that you get the same results as previously}.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Optimization and profiling}
\begin{small}
{\scriptsize
It is also useful to profile your program under the development stage.
You would then compile with (Mac and unix/linux)
\begin{verbatim}
c++  -pg -O3 -c  mycode.cpp
c++  -pg -O3 -o  mycode.exe  mycode.o
\end{verbatim}
After you have run the code you can obtain the profiling information via
\begin{verbatim}
gprof mycode.exe >  out_to_profile
\end{verbatim}
When you have profiled properly your code, you must take out this option as it 
increases your CPU expenditure.
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Optimization and profiling}
\begin{small}
{\scriptsize
Other hints
\begin{itemize}
\item avoid if tests or call to functions inside loops, if possible. 
\item avoid multiplication with constants inside loops if possible
\end{itemize}
Bad code
\begin{verbatim}
for i = 1:n
    a(i) = b(i) +c*d
    e = g(k)
end
\end{verbatim}
Better code
\begin{verbatim}
temp = c*d
for i = 1:n
    a(i) = b(i) + temp
end
e = g(k)
\end{verbatim}

}
\end{small}
}











 













\frame
{
  \frametitle{What do we have then?}
\begin{itemize}
\item Several functions under the program link, see the file OOcodes.tar.gz
\item Matrix and array manipulations (similar to Blitz++/Armadillo)
\item Random numbers and numerical integration
\item Functions to convert arrays from C++ to Numpy and viceversa 
\item Numerical derivatives and differential equation solvers.
\end{itemize}
These files can serve as a help when you want to start to write your own classes. We will discuss some of these
classes during the course. 
} 


\frame
{
  \frametitle{But what should I do else????? Some hints.}
If your needs (common in most problems) include handling of large arrays and linear
algebra problem, I would not recommend to write your own vector-matrix or more general array handling class. 
It is easyto make error.  
\begin{itemize}
\item Old-fashioned allocation of arrays and explicit handling of all loops in for example
matrix-matrix multiplication.
\item You can use what we have developed for Blitz++ or the Array class (but more limited than Blitz++)
\item or (recommended) you can use armadillo, a great C++ library for handling arrays and doing linear algebra.
\item Armadillo provides a user friendly interface to lapack and blas functions.  Here an example of 
using the Blas function {\bf DGEMM} for matrix-matrix multiplication. 
\item After having installed armadillo, compile with {\bf c++ -O3 -o test.x test.cpp -lblas -lamardillo -llapack}.
\end{itemize}
} 


\frame[containsverbatim]
{
  \frametitle{Matrix-matrix multiplication}
\begin{lstlisting}
#include <cstdlib>
#include <ios>
#include <iostream>
#include <armadillo>
using namespace std;
using namespace arma;

/*Because fortran files don't have any header files,
 *we need to declare the functions ourself.*/
extern "C"
{
    void dgemm_(char*, char*, int*, int*, int*, double*,
            double*, int*, double*, int*, double*, double*, int*);
}
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Matrix-matrix multiplication}
\begin{lstlisting}
int main(int argc, char** argv)
{
    //Dimensions
    int n = atoi(argv[1]);
    int m = n;
    int p = m;

    /*Create random matrices
     * (note that older versions of armadillo uses "rand" instead of "randu") */
    srand(time(NULL));
    mat A(n, p);
    A.randu();
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Matrix-matrix multiplication}
\begin{lstlisting}
    // Pretty print, and pretty save, are as easy as the two following lines.
    //    cout << A << endl;
    //    A.save("A.mat", raw_ascii);
    mat A_trans = trans(A);
    mat B(p, m);
    B.randu();
    mat C(n, m);
    //    cout << B << endl;
    //    B.save("B.mat", raw_ascii);
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Matrix-matrix multiplication}
\begin{lstlisting}
    //   ARMADILLO  TEST  
    cout << "Starting armadillo multiplication\n";
    //Simple wall_clock timer is a part of armadillo.
    wall_clock timer;
    timer.tic();
    C = A*B;
    double num_sec = timer.toc();
    cout << "-- Finished in " << num_sec << " seconds.\n\n";
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Matrix-matrix multiplication}
\begin{lstlisting}
    C = zeros<mat> (n, m);
    cout << "Starting blas multiplication.\n";
    
        char trans = 'N';
        double alpha = 1.0;
        double beta = 0.0;
        int _numRowA = A.n_rows;
        int _numColA = A.n_cols;
        int _numRowB = B.n_rows;
        int _numColB = B.n_cols;
        int _numRowC = C.n_rows;
        int _numColC = C.n_cols;
        int lda = (A.n_rows >= A.n_cols) ? A.n_rows : A.n_cols;
        int ldb = (B.n_rows >= B.n_cols) ? B.n_rows : B.n_cols;
        int ldc = (C.n_rows >= C.n_cols) ? C.n_rows : C.n_cols;
\end{lstlisting}
}


\frame[containsverbatim]
{
  \frametitle{Matrix-matrix multiplication, calling DGEMM}
\begin{lstlisting}
        dgemm_(&trans, &trans, &_numRowA, &_numColB, &_numColA, &alpha,
                A.memptr(), &lda, B.memptr(), &ldb, &beta, C.memptr(), &ldc);
    
\end{lstlisting}
}






















\section{Week 44}
\frame
{
  \frametitle{Week 44}
  \begin{block}{Monte Carlo methods, chapter 14}
\begin{itemize}
\item Monday: Repetition from last week
\item Statistical physics and the Metropolis algorithm (covered by chapter 13)
\item Discussion of project 4.
%\item How to use titan.uio.no
\item Wednesday: 
\item Statistical physics and the Metropolis algorithm (covered by chapter 13)
\item Discussion of project 4.
\item Introduction to differential equations
\item Differential equations, general properties.
\item Runge-Kutta methods
\end{itemize}
Chapter 14 on quantum mechanical Monte Carlo is not part of this year's curriculum.
  \end{block}
} 




\frame
{
  \frametitle{Most Common Ensembles in Statistical Physics}
%  \begin{block}{C++ and Fortran declarations}
\begin{small}
{\scriptsize
\begin{tabular}{|l|c|c|c|c|}
\hline
&&&&\\
&\multicolumn{1}{|c}{Microcanonical}&\multicolumn{1}{|c}{Canonical}&
\multicolumn{1}{|c}{Grand can.}&\multicolumn{1}{|c|}{Pressure can.}\\
&&&&\\
\hline 
&&&&\\
Exchange of heat  &no&yes&yes&yes\\
with the environment&&&&\\
&&&&\\
Exchange of particles&no&no&yes&no\\
with the environemt&&&&\\
&&&&\\
Thermodynamical&$V, \cal M, \cal D$&$V, \cal M, \cal D$&$V, \cal M, \cal D$
&$P, \cal H, \cal E$\\
parameters&     $E$&$T$&$T$&$T$\\
	    &$N$&$N$&$\mu$&$N$\\
&&&&\\
Potential& Entropy& Helmholtz & $PV$ & Gibbs\\
&&&&\\
Energy &Internal& Internal & Internal & Enthalpy\\
&&&&\\

&&&&\\
&&&&\\ \hline
\end{tabular}
}
\end{small}
%  \end{block}
}

\frame
{
  \frametitle{Microcanonical Ensemble}
\begin{small}
{\scriptsize
Entropy
\be S=k_{B}ln\Omega\ee
\be dS=\frac{1}{T}dE+\frac{p}{T}dV-\frac{\mu}{T}dN\ee
Temperature
\be \frac{1}{k_{B}T}=\left(\frac{\partial ln\Omega}{\partial E}\right)_{N, V}\ee
Pressure
\be \frac{p}{k_{B}T}=\left(\frac{\partial ln\Omega}{\partial V}\right)_{N, E}\ee
Chemical potential
\be \frac{\mu}{k_{B}T}=-\left(\frac{\partial ln\Omega}{\partial N}\right)_{V, E}\ee
}
\end{small}
}


\frame
{
  \frametitle{Canonical Ensemble}
\begin{small}
{\scriptsize
Helmholtz Free Energy
\be F=-k_{B}TlnZ\ee
\be dF=-SdT-pdV+\mu dN\ee
Entropy
\be S =k_{B}lnZ
+k_{B}T\left(\frac{\partial lnZ}{\partial T}\right)_{N, V}\ee
Pressure
\be p=k_{B}T\left(\frac{\partial lnZ}{\partial V}\right)_{N, T}\ee
Chemical Potential
\be \mu =-k_{B}T\left(\frac{\partial lnZ}{\partial N}\right)_{V, T}\ee
Energy (internal only)
\be E =k_{B}T^{2}\left(\frac{\partial lnZ}{\partial T}\right)_{V, N}\ee

}
\end{small}
}


\frame
{
  \frametitle{Grand Canonical Ensemble}
\begin{small}
{\scriptsize
Potential
\be pV=k_{B}Tln\Xi\ee
\be d(pV)=SdT+Nd\mu +pdV\ee
Entropy
\be S =k_{B}ln\Xi 
+k_{B}T\left(\frac{\partial ln\Xi}{\partial T}\right)_{V, \mu}\ee
Particles
\be N =k_{B}T\left(\frac{\partial ln\Xi}{\partial \mu}\right)_{V, T}\ee
Pressure
\be p =k_{B}T\left(\frac{\partial ln\Xi}{\partial V}\right)_{\mu, T}\ee
}
\end{small}
}



\frame
{
  \frametitle{Pressure Canonical Ensemble}
\begin{small}
{\scriptsize
Gibbs Free Energy
\be G=-k_{B}Tln\Delta\ee
\be dG=-SdT+Vdp+\mu dN\ee
Entropy
\be S =k_{B}ln\Delta 
+k_{B}T\left(\frac{\partial ln\Delta}{\partial T}\right)_{p, N}\ee
Volume
\be V =-k_{B}T\left(\frac{\partial ln\Delta}{\partial p}\right)_{N, T}\ee
Chemical potential
\be \mu =-k_{B}T\left(\frac{\partial ln\Delta}{\partial N}\right)_{p, T}\ee
}
\end{small}
}


\frame
{
  \frametitle{Expectation Values}
\begin{small}
{\scriptsize
At a given temperature we have the probability distribution 
\be
  P_i(\beta) = \frac{e^{-\beta E_i}}{Z}
\ee
with $\beta=1/kT$ being the inverse temperature, $k$ the 
Boltzmann constant, $E_i$ is the energy of a state $i$ while 
$Z$ is the partition function for the canonical ensemble
defined as
\be
Z=\sum_{i=1}^{M}e^{-\beta E_i},
\ee
where the sum extends over all states
$M$. 
$P_i$ expresses the probability of finding the system in a given 
configuration $i$.
}
\end{small}
}


\frame
{
  \frametitle{Expectation Values}
\begin{small}
{\scriptsize
For a system described by the canonical ensemble, the energy is an
expectation value since we allow energy to be exchanged with the surroundings
(a heat bath with temperature $T$). This expectation value, the mean energy,
can be calculated using the probability distribution
$P_i$ as
\be
  \langle E \rangle = \sum_{i=1}^M E_i P_i(\beta)= 
  \frac{1}{Z}\sum_{i=1}^M E_ie^{-\beta E_i},
\ee
with a corresponding variance defined as
\be
\sigma_E^2=\langle E^2 \rangle-\langle E \rangle^2=
         \frac{1}{Z}\sum_{i=1}^M E_i^2e^{-\beta E_i}-
          \left(\frac{1}{Z}\sum_{i=1}^M E_ie^{-\beta E_i}\right)^2.
\ee
If we divide the latter quantity with
$kT^2$ we obtain the specific heat at constant volume 
\be
   C_V= \frac{1}{kT^2}\left(\langle E^2 \rangle-\langle E \rangle^2\right).
\ee
}
\end{small}
}


\frame
{
  \frametitle{Expectation Values}
\begin{small}
{\scriptsize
We can also write 
\be
   \langle E \rangle=-\frac{\partial lnZ}{\partial \beta}.
\ee
The specific heat is
\be
   C_V=\frac{1}{kT^2}\frac{\partial^2 lnZ}{\partial\beta^2}
\ee
These expressions link a physical quantity (in thermodynamics) with the microphysics
given by the partition function. Statistical physics is the field where one relates
microscopic quantities to observables at finite temperature.
}
\end{small}
}


\frame
{
  \frametitle{Expectation Values}
\begin{small}
{\scriptsize
\be
  \langle {\cal M} \rangle = \sum_i^M {\cal M}_i P_i(\beta)= 
  \frac{1}{Z}\sum_i^M {\cal M}_ie^{-\beta E_i},
\ee
and the corresponding variance
\be
\sigma_{{\cal M}}^2=\langle {\cal M}^2 \rangle-\langle {\cal M} \rangle^2=
         \frac{1}{Z}\sum_{i=1}^M {\cal M}_i^2e^{-\beta E_i}-
          \left(\frac{1}{Z}\sum_{i=1}^M {\cal M}_ie^{-\beta E_i}\right)^2.
\ee
This quantity defines also the susceptibility 
$\chi$ 
\be
  \chi=\frac{1}{kT}\left(\langle {\cal M}^2 \rangle-\langle {\cal M} \rangle^2\right).
\ee
}
\end{small}
}


\frame
{
  \frametitle{Phase Transitions}
\begin{small}
{\scriptsize
NOTE: Helmholtz free energy and canonical ensemble
\[
   F= \langle E\rangle -TS  = -kTln Z
\]
meaning $ lnZ = -F/kT  = -F\beta$
and 
\[
   \langle E \rangle=-\frac{\partial lnZ}{\partial \beta} =\frac{\partial (\beta F)}{\partial \beta}.
\]
and
\[
   C_V=-\frac{1}{kT^2}\frac{\partial^2 (\beta F)}{\partial\beta^2}.
\]
We can relate observables to various derivatives of the partition 
function and the free energy. When a given derivative of the free energy or the partition function is discontinuous 
or diverges (logarithmic divergence for the heat capacity from the Ising model) we talk of a phase transition
of order of the derivative.
}
\end{small}
}


\frame
{
  \frametitle{Phase Transitions}
\begin{small}
{\scriptsize
\begin{itemize}
\item An important quantity is the correlation length. The correlation length
defines the length scale at which the overall properties of a material start to differ from its bulk properties.
It is the distance over which the fluctuations of the microscopic degrees of freedom (for example the position of atoms)
are significantly correlated with each other. Usually it is of the order of few interatomic spacings for a solid.
\item The correlation length $\xi$ depends however on external conditions such as pressure and temperature.
\item A phase transition is marked by abrupt macroscopic changes as external parameters are changed, such as an increase
of temperature.
\item The point where a phase transition takes place is called a critical point.
\end{itemize}
}
\end{small}
}


\frame
{
  \frametitle{Two Scenarios for Phase Transitions}
\begin{small}
{\scriptsize
\begin{enumerate}
\item First order/discontinuous phase transitions: Two or more states on either side of the critical point also coexist exactly at the critical point.
As we pass through the critical point we observe a discontinuous behavior of thermodynamical functions. The correlation length is normally finite at the critical point.  Phenomena such as hysteris
occur, viz. there is a continuation of  state below the critical point into one above the critical point. This continuation
is metastable so that the system may take a macroscopically long time to readjust. Classical example, melting of ice.
\item Second order or continuous transitions: The correlation length diverges at the critical point, fluctuations are correlated
over all distance scales, which forces the system to be in a unique critical phase. The two phases on either side of the 
critical point become identical. Smooth behavior of first derivatives of the partition function, while second derivatives
diverge. Strong correlations make a perturbative treatment impossible. Renormalization group theory.
\end{enumerate}
}
\end{small}
}



\frame
{
  \frametitle{Examples of  Phase Transitions}
\begin{small}
{\scriptsize
\begin{tabular}{c|c|c|}
\hline
&&\\
\multicolumn{1}{|c}{System}&\multicolumn{1}{|c}{Transition}&\multicolumn{1}{|c}{Order Parameter}\\
&&\\
\hline 
&&\\
Liquid-gas  &          Condensation/evaporation&  Density difference $\Delta\rho=\rho_{liquid}-\rho_{gas}$ \\
Binary liquid&  mixture/Unmixing  &   Composition difference \\
Quantum liquid  &      Normal fluid/superfluid &  $<\phi>$, $\psi$ = wavefunction  \\
Liquid-solid    &      Melting/crystallisation & Reciprocal lattice vector \\
Magnetic solid   &     Ferromagnetic&  Spontaneous magnetisation $M$\\
       &               Antiferromagnetic &  Sublattice magnetisation $M$\\
Dielectric solid  &    Ferroelectric&  Polarization $P$ \\
                  &    Antiferroelectric & Sublattice polarisation $P$\\
&&\\
&&\\ \hline
\end{tabular}
}
\end{small}
}

\frame
{
  \frametitle{Ising Model}
\begin{small}
{\scriptsize
The model we will employ in our studies of phase transitions at finite temperature for 
magnetic systems is the so-called Ising model. In its simplest form
the energy is expressed as
\be
  E=-J\sum_{<kl>}^{N}s_ks_l-B\sum_k^Ns_k,
\ee
with  $s_k=\pm 1$, $N$ is the total number of spins, 
$J$ is a coupling constant expressing the strength of the interaction
between neighboring spins and 
$B$ is an external magnetic field interacting with the magnetic
moment set up by the spins.
The symbol $<kl>$ indicates that we sum over nearest
neighbors only. 
}
\end{small}
}


\frame
{
  \frametitle{Ising Model}
\begin{small}
{\scriptsize
Notice that for $J>0$ it is energetically favorable for neighboring spins 
to be aligned. This feature leads to, at low enough temperatures,
to a cooperative phenomenon called spontaneous magnetization. That is, 
through interactions between nearest neighbors, a given magnetic
moment can influence the alignment of spins  that are separated 
from the given spin by a macroscopic distance. These long range correlations
between spins are associated with a long-range order in which
the lattice has a net magnetization in the absence of a magnetic field. 
This phase is normally called the ferromagnetic phase. With $J< 0$, we have a 
so-called antiferromagnetic case.
At a critical temperature we have a phase transition to a disordered phase, a so-called
paramagnetic phase.
}
\end{small}
}




\frame
{
  \frametitle{Treatment of Boundaries}
\begin{small}
{\scriptsize
With two spins, since each spin takes two values only, it means that in total
we have $2^2=4$ possible arrangements of the two spins. 
These four possibilities are 
\[
   1= \uparrow\uparrow\hspace{1cm}
    2= \uparrow\downarrow\hspace{1cm}
   3= \downarrow\uparrow\hspace{1cm}
      4=\downarrow\downarrow
\]

What is the energy of each of these configurations? 

For small systems, the way we treat the ends matters. 
In the first case we employ what is called 
free ends. For the one-dimensional case, the energy is then written as 
a sum over a single index
\[
   E_i =-J\sum_{j=1}^{N-1}s_js_{j+1},
\]
If we  label the first spin as $s_1$ and the second as $s_2$ 
we obtain the following 
expression for the energy 
\[
   E=-Js_1s_2.
\]
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Treatment of Boundaries}
\begin{small}
{\scriptsize
The calculation of the energy for the one-dimensional lattice
with free ends for one specific spin-configuration 
can easily be implemented in the following lines
\begin{verbatim}
    for ( j=1; j < N; j++) {
        energy += spin[j]*spin[j+1]; 
    }
\end{verbatim}
where the vector $spin[]$ contains the spin value $s_k=\pm 1$. 
For the specific state $E_1$, we have chosen all spins up. The energy of
this configuration becomes then
\[
   E_1=E_{\uparrow\uparrow}=-J.
\]
The other configurations give
\[
   E_2=E_{\uparrow\downarrow}=+J,
\]
\[
   E_3=E_{\downarrow\uparrow}=+J,
\]
and
\[
   E_4=E_{\downarrow\downarrow}=-J.
\]

}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Treatment of Boundaries}
\begin{small}
{\scriptsize
We can also choose so-called periodic boundary conditions.
This means that if $i=N$, we set the spin number to $i=1$. 
In this case the energy for the one-dimensional lattice
reads 
\[
   E_i =-J\sum_{j=1}^{N}s_js_{j+1},
\]
and we obtain the following expression for the two-spin case
\[
   E=-J(s_1s_2+s_2s_1).
\]
If we choose to use periodic boundary conditions we can code the above
expression as
\begin{verbatim}
    jm=N;
    for ( j=1; j <=N ; j++) {
        energy += spin[j]*spin[jm]; 
        jm = j ;
    }
\end{verbatim}

}
\end{small}
}


\frame
{
  \frametitle{Treatment of Boundaries}
\begin{small}
{\scriptsize
\begin{tabular}{crrr}\\\hline
State& Energy (FE) & Energy (PBC) & Magnetization \\ \hline
$1= \uparrow\uparrow$& $-J$ & $-2J$ & 2\\
$2=\uparrow\downarrow$&  $J$ & $2J$  & 0\\
$   3= \downarrow\uparrow$&  $J$ & $2J$ &  0\\
$      4=\downarrow\downarrow$&  $-J$ & $-2J$ & -2\\ \hline
\end{tabular}
\begin{tabular}{llrrr}\\\hline 
Number spins up& Degeneracy& Energy (FE)& Energy (PBC) & Magnetization \\ \hline
2& 1&$-J$ &$-2J$ & 2\\
1& 2 & $J$ &$2J$ & 0\\
0& 1 & $-J$ &$-2J$ & -2 \\ \hline
\end{tabular}
}
\end{small}
}


\frame
{
  \frametitle{Treatment of Boundaries}
\begin{small}
{\scriptsize
It is worth noting that for small dimensions of the lattice,
the energy differs depending on whether we use
periodic boundary conditions or free ends. This means also
that the partition functions will be different, as discussed
below. In the thermodynamic limit however, $N\rightarrow \infty$,
the final results do not depend on the kind of boundary conditions
we choose. 
The magnetization is however the same, defined as
\[
   {\cal M}_i=\sum_{j=1}^N s_j,
\]
where we  sum over all spins for a given configuration $i$. 
}
\end{small}
}


\frame
{
  \frametitle{Treatment of Boundaries}
\begin{small}
{\scriptsize
In a similar way, we could enumerate the number of states for
a two-dimensional system consisting of two spins, i.e., 
a $2\times 2$ Ising model on a square lattice with {\em periodic
boundary conditions}. In this case we have a total of 
$2^4=16$ states. 
Some 
examples of configurations with their respective energies are 
listed here
\[
  E=-8J\hspace{1cm}\begin{array}{cc}\uparrow & \uparrow \\
                                    \uparrow & \uparrow\end{array}
\hspace{0.5cm}
  E=0\hspace{1cm}\begin{array}{cc}\uparrow & \uparrow \\
                                    \uparrow & \downarrow\end{array}
\hspace{0.5cm}
  E=0\hspace{1cm}\begin{array}{cc}\downarrow & \downarrow \\
                                    \uparrow & \downarrow\end{array}
\]
}
\end{small}
}


\frame
{
  \frametitle{Treatment of Boundaries}
\begin{small}
{\scriptsize
We can group these configurations
according to their total energy and magnetization.
\begin{tabular}{llrr}\\\hline 
Number spins up& Degeneracy& Energy & Magnetization \\ \hline
4&1 & $-8J$ & 4\\
3& 4 & $0$ & 2\\
2& 4 & $0$ & 0 \\
2& 2 & $8J$ & 0 \\
1& 4 & $0$ & -2 \\
0& 1 & $-8J$ & -4\\ \hline
\end{tabular}
}
\end{small}
}




\frame
{
  \frametitle{Modelling the Ising Model}
\begin{small}
{\scriptsize
The code uses periodic boundary conditions with energy

\[
   E_i =-J\sum_{j=1}^{N}s_js_{j+1},
\]
In our case we have as the Monte Carlo sampling function the probability
for finding the system in a state $s$ given by
\[
P_s=\frac{e^{-(\beta E_s)}}{Z},
\]
with energy $E_s$, $\beta=1/kT$ and $Z$ is a normalization constant which
defines the partition function in the canonical ensemble
\[
  Z(\beta)=\sum_se^{-(\beta E_s)}
\]
This is difficult to compute since we need all states. In a calculation 
of the Ising model in two dimensions, the number of configurations is
given by $2^N$ with $N=L\times L$ the number of spins for a lattice 
of length $L$. Fortunately, the Metropolis algorithm considers
only ratios between probabilities and we do not need to
compute the partition function at all. 
}
\end{small}
}



\frame
{
  \frametitle{Metropolis Algorithm}
\begin{small}
{\scriptsize
\begin{enumerate}
\item Establish an initial state with energy $E_b$ by positioning
yourself at a random position in the lattice
\item Change the initial configuration by flipping 
e.g., one spin only. Compute the energy of this trial state
$E_t$. 
\item Calculate $\Delta E=E_t-E_b$. The number of values $\Delta E$
is limited to five for the Ising model in two dimensions, see the discussion
below.
\item If $\Delta E \le 0$ we accept the new configuration, meaning that the
energy is lowered and we are hopefully moving towards the energy minimum
at a given temperature. Go to step 7. 
\item If $\Delta E >  0$, calculate $w=e^{-(\beta \Delta E)}$.
\item Compare $w$ with a random number $r$. If
      \[
         r \le w,
\]
then accept the new configuration, else we keep the old configuration and its values.
\item The next step is to update various expectations values.
\item The steps (2)-(7) are then repeated in order to obtain a
sufficently good representation of states. 
\end{enumerate}                                                       
}
\end{small}
}


 

\frame
{
  \frametitle{Modelling the Ising Model}
\begin{small}
{\scriptsize
In the calculation of the energy difference from one spin configuration
to the other, we will limit the change to the flipping of one
spin only. For the Ising model in two dimensions it means that there
will only be a limited set of values for $\Delta E$. Actually, there are
only five  possible values. To see this, 
select first a random spin position $x,y$ 
and assume that this spin
and its nearest neighbors are all pointing up. 
The energy for this
configuration is $E=-4J$. Now we flip this spin as shown below.
The energy of the new configuration is $E=4J$, yielding $\Delta E=8J$. 
\[
  E=-4J\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \uparrow & \uparrow\\
                                  & \uparrow & \end{array}
\hspace{1cm}\Longrightarrow\hspace{1cm}
  E=4J\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \downarrow & \uparrow\\
                                  & \uparrow & \end{array}
\]
}
\end{small}
}


\frame
{
  \frametitle{Modelling the Ising Model}
\begin{small}
{\scriptsize
The four other possibilities are as follows
\[
  E=-2J\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \uparrow & \uparrow\\
                                  & \uparrow & \end{array}
\hspace{1cm}\Longrightarrow\hspace{1cm}
  E=2J\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \downarrow & \uparrow\\
                                  & \uparrow & \end{array}
\]
with $\Delta E=4J$,
\[
  E=0\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \uparrow & \uparrow\\
                                  & \downarrow & \end{array}
\hspace{1cm}\Longrightarrow\hspace{1cm}
  E=0\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \downarrow & \downarrow & \uparrow\\
                                  & \downarrow & \end{array}
\]
with $\Delta E=0$
}
\end{small}
}

\frame
{
  \frametitle{Modelling the Ising Model}
\begin{small}
{\scriptsize
\[
  E=2J\hspace{1cm}\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \uparrow & \uparrow\\
                                  & \downarrow & \end{array}
\hspace{1cm}\Longrightarrow\hspace{1cm}
  E=-2J\hspace{1cm}\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \downarrow & \uparrow\\
                                  & \downarrow & \end{array}
\]
with $\Delta E=-4J$ and finally
\[
  E=4J\hspace{1cm}\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \uparrow & \downarrow\\
                                  & \downarrow & \end{array}
\hspace{1cm}\Longrightarrow\hspace{1cm}
  E=-4J\hspace{1cm}\begin{array}{ccc}         & \downarrow &         \\
                         \downarrow & \downarrow & \downarrow\\
                                  & \downarrow & \end{array}
\]
with $\Delta E=-8J$.
This means in turn that we could construct an array which contains all values
of $e^{\beta \Delta E}$ before doing the Metropolis sampling. Else, we
would have to evaluate the exponential  at each Monte Carlo sampling. 
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{The loop over $T$ in main }
\begin{small}
{\scriptsize
\begin{verbatim}
  for ( double temp = initial_temp; temp <= final_temp; temp+=temp_step){
    //    initialise energy and magnetization 
    E = M = 0.;
    // setup array for possible energy changes
    for( int de =-8; de <= 8; de++) w[de+8] = 0;
    for( int de =-8; de <= 8; de+=4) w[de+8] = exp(-de/temp);
    // initialise array for expectation values
    for( int i = 0; i < 5; i++) average[i] = 0.;
    initialize(n_spins, temp, spin_matrix, E, M);
    // start Monte Carlo computation
    for (int cycles = 1; cycles <= mcs; cycles++){
      Metropolis(n_spins, idum, spin_matrix, E, M, w);
      // update expectation values
      average[0] += E;    average[1] += E*E;
      average[2] += M;    average[3] += M*M; average[4] += fabs(M);
    }
    // print results
    output(n_spins, mcs, temp, average);
  }
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{The Initialise function}
\begin{small}
{\scriptsize
\begin{verbatim}
void initialize(int n_spins, double temp, int **spin_matrix, 
		double& E, double& M)
{
  // setup spin matrix and intial magnetization
  for(int y =0; y < n_spins; y++) {
    for (int x= 0; x < n_spins; x++){
      spin_matrix[y][x] = 1; // spin orientation for the ground state
      M +=  (double) spin_matrix[y][x];
    }
  }
  // setup initial energy
  for(int y =0; y < n_spins; y++) {
    for (int x= 0; x < n_spins; x++){
      E -=  (double) spin_matrix[y][x]*
	(spin_matrix[periodic(y,n_spins,-1)][x] +
	 spin_matrix[y][periodic(x,n_spins,-1)]);
    }
  }
}// end function initialise
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{The periodic function}
\begin{small}
{\scriptsize
A compact way of dealing with periodic boundary conditions is given as follows:
\begin{verbatim}
// inline function for periodic boundary conditions
inline int periodic(int i, int limit, int add) { 
  return (i+limit+add) % (limit);
\end{verbatim}
with the following example from the function initialise
\begin{verbatim}
      E -=  (double) spin_matrix[y][x]*
	(spin_matrix[periodic(y,n_spins,-1)][x] +
	 spin_matrix[y][periodic(x,n_spins,-1)]);
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Alternative way for periodic boundary conditions}
\begin{small}
{\scriptsize
A more pedagogical way is given by the (here Fortran as example)  program
\begin{verbatim}
   DO y = 1,lattice_y
       DO x = 1,lattice_x
          right = x+1 ; IF(x == lattice_x  ) right = 1
          left = x-1 ; IF(x == 1  ) left = lattice_x
          up = y+1 ; IF(y == lattice_y  ) up = 1
          down = y-1 ; IF(y == 1  ) down = lattice_y
          energy=energy - spin_matrix(x,y)*(spin_matrix(right,y)+&
               spin_matrix(left,y)+spin_matrix(x,up)+ &
               spin_matrix(x,down) )
          magnetization = magnetization + spin_matrix(x,y)
       ENDDO
    ENDDO
    energy = energy*0.5
\end{verbatim}
}
\end{small}
}



\frame
{
  \frametitle{Computing $\Delta E$ and $\Delta M$}
\begin{small}
{\scriptsize
The energy difference between a state $E_1$ and a state $E_2$ with zero magnetic field is
\[
   \Delta E = E_2-E_1 =J\sum_{<kl>}^{N}s_k^1s_{l}^1-J\sum_{<kl>}^{N}s_k^2s_{l}^2,
\]
which we can rewrite as 
\[
   \Delta E  = -J \sum_{<kl>}^{N}s_k^2(s_l^2-s_{l}^1),
\]
where the sum now runs only over the nearest neighbors $k$ of the spin. 
Since the spin to be flipped takes only two values, $s_l^1=\pm 1$ and $s_l^2=\pm 1$, it means that if
$s_l^1= 1$, then $s_l^2=-1$ and if $s_l^1= -1$, then $s_l^2=1$. 
The other spins keep their values, meaning that
$s_k^1=s_k^2$.
If $s_l^1= 1$ we must have $s_l^1-s_{l}^2=2$, and 
if $s_l^1= -1$ we must have $s_l^1-s_{l}^2=-2$. From these results we see that the energy difference
can be coded efficiently as 
\[
   \Delta E  = 2Js_l^1\sum_{<k>}^{N}s_k,
\]
where the sum runs only over the nearest neighbors $k$ of spin $l$

The difference in magnetisation is given by the difference 
$s_l^1-s_{l}^2=\pm 2$, or in a more compact way as
\[
M_2 = M_1+2s_l^2,
\]
where $M_1$ and $M_2$ are the magnetizations before and after the spin flip, respectively.  
}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{The Metropolis function}
\begin{small}
{\scriptsize
\begin{verbatim}
  // loop over all spins
  for(int y =0; y < n_spins; y++) {
    for (int x= 0; x < n_spins; x++){
      int ix = (int) (ran1(&idum)*(double)n_spins);   // RANDOM SPIN
      int iy = (int) (ran1(&idum)*(double)n_spins);  // RANDOM SPIN
      int deltaE =  2*spin_matrix[iy][ix]*
	(spin_matrix[iy][periodic(ix,n_spins,-1)]+
	 spin_matrix[periodic(iy,n_spins,-1)][ix] +
	 spin_matrix[iy][periodic(ix,n_spins,1)] +
	 spin_matrix[periodic(iy,n_spins,1)][ix]);
      if ( ran1(&idum) <= w[deltaE+8] ) {
	spin_matrix[iy][ix] *= -1;  // flip one spin and accept new spin config
        M += (double) 2*spin_matrix[iy][ix];
        E += (double) deltaE;
      }
    }
  }
\end{verbatim}
}
\end{small}
}


\frame[containsverbatim]
{
  \frametitle{Expectation Values}
\begin{small}
{\scriptsize

\begin{verbatim}
  double norm = 1/((double) (mcs));// divided by total number of cycles 
  double Eaverage = average[0]*norm;
  double E2average = average[1]*norm;
  double Maverage = average[2]*norm;
  double M2average = average[3]*norm;
  double Mabsaverage = average[4]*norm;
  // all expectation values are per spin, divide by 1/n_spins/n_spins
  double Evariance = (E2average- Eaverage*Eaverage)/n_spins/n_spins;
  double Mvariance = (M2average - Mabsaverage*Mabsaverage)/n_spins/n_spins;
  ofile << setiosflags(ios::showpoint | ios::uppercase);
  ofile << setw(15) << setprecision(8) << temp;
  ofile << setw(15) << setprecision(8) << Eaverage/n_spins/n_spins;
  ofile << setw(15) << setprecision(8) << Evariance/temp/temp;
  ofile << setw(15) << setprecision(8) << Maverage/n_spins/n_spins;
  ofile << setw(15) << setprecision(8) << Mvariance/temp;
  ofile << setw(15) << setprecision(8) << Mabsaverage/n_spins/n_spins << endl;
\end{verbatim}
}
\end{small}
}















\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
int Jacobi_P(int mynode, int numnodes, int N, double **A, double *x, double *b, double abstol){
  int i,j,k,i_global;
  int maxit = 100000;
  int rows_local,local_offset,last_rows_local,*count,*displacements;
  double sum1,sum2,*xold;
  double error_sum_local, error_sum_global;
  MPI_Status status;

  rows_local = (int) floor((double)N/numnodes);
  local_offset = mynode*rows_local;
  if(mynode == (numnodes-1)) 
    rows_local = N - rows_local*(numnodes-1);
\end{lstlisting}

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
  /*Distribute the Matrix and R.H.S. among the processors */
  if(mynode == 0){
    for(i=1;i<numnodes-1;i++){
      for(j=0;j<rows_local;j++)
	MPI_Send(A[i*rows_local+j],N,MPI_DOUBLE,i,j,MPI_COMM_WORLD);
      MPI_Send(b+i*rows_local,rows_local,MPI_DOUBLE,i,rows_local,
	       MPI_COMM_WORLD);
    }
    last_rows_local = N-rows_local*(numnodes-1);
    for(j=0;j<last_rows_local;j++)
      MPI_Send(A[(numnodes-1)*rows_local+j],N,MPI_DOUBLE,numnodes-1,j,
	       MPI_COMM_WORLD);
    MPI_Send(b+(numnodes-1)*rows_local,last_rows_local,MPI_DOUBLE,numnodes-1,
	     last_rows_local,MPI_COMM_WORLD);
  }
\end{lstlisting}

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
  else{
    A = CreateMatrix(rows_local,N);
    x = new double[rows_local];    
    b = new double[rows_local];
    for(i=0;i<rows_local;i++)
      MPI_Recv(A[i],N,MPI_DOUBLE,0,i,MPI_COMM_WORLD,&status);
    MPI_Recv(b,rows_local,MPI_DOUBLE,0,rows_local,MPI_COMM_WORLD,&status);
  }

\end{lstlisting}

}
\end{small}
}




\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
  xold = new double[N];
  count = new int[numnodes];
  displacements = new int[numnodes];
  //set initial guess to all 1.0
  for(i=0; i<N; i++){
    xold[i] = 1.0;
  }
  for(i=0;i<numnodes;i++){
    count[i] = (int) floor((double)N/numnodes);
    displacements[i] = i*count[i];
  }
  count[numnodes-1] = N - ((int)floor((double)N/numnodes))*(numnodes-1);
\end{lstlisting}

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
  for(k=0; k<maxit; k++){
    error_sum_local = 0.0;
    for(i = 0; i<rows_local; i++){
      i_global = local_offset+i;
      sum1 = 0.0; sum2 = 0.0;
      for(j=0; j < i_global; j++)
	sum1 = sum1 + A[i][j]*xold[j];
      for(j=i_global+1; j < N; j++)
	sum2 = sum2 + A[i][j]*xold[j];
      
      x[i] = (-sum1 - sum2 + b[i])/A[i][i_global];
      error_sum_local += (x[i]-xold[i_global])*(x[i]-xold[i_global]);
    }
\end{lstlisting}

}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
    MPI_Allreduce(&error_sum_local,&error_sum_global,1,MPI_DOUBLE,
		  MPI_SUM,MPI_COMM_WORLD);
    MPI_Allgatherv(x,rows_local,MPI_DOUBLE,xold,count,displacements,
		   MPI_DOUBLE,MPI_COMM_WORLD);
\end{lstlisting}

}
\end{small}
}



\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
    if(sqrt(error_sum_global)<abstol){
      if(mynode == 0){
	for(i=0;i<N;i++)
	  x[i] = xold[i];
      }
      else{
	DestroyMatrix(A,rows_local,N);
	delete[] x;
        delete[] b;
      }
      delete[] xold;
      delete[] count;
      delete[] displacements;
      return k;
    }
  }

\end{lstlisting}

}
\end{small}
}
\frame[containsverbatim]
{
  \frametitle{Code example for the parallel two-dimensional diff equation}
\begin{small}
{\scriptsize
\begin{lstlisting}
  cerr << "Jacobi: Maximum Number of Interations Reached Without Convergence\n";
  if(mynode == 0){
    for(i=0;i<N;i++)
      x[i] = xold[i];
  }
  else{
    DestroyMatrix(A,rows_local,N);
    delete[] x;
    delete[] b;
  }
  delete[] xold;
  delete[] count;
  delete[] displacements;
  
  return maxit;
}

\end{lstlisting}

}
\end{small}
}




\end{document}







\frame{
  \frametitle{How do I use the titan.uio.no cluster?}
  \begin{block}{hpc@usit.uio.no}
    \begin{itemize}
    \item Computational Physics requires High Performance Computing
      (HPC) resources
    \item USIT and the Research Computing Services (RCS) provides
      HPC resources and HPC support
    \item Resources: \url{titan.uio.no}
    \item Support: 14 people
    \item Contact: \url{hpc@usit.uio.no}
    \end{itemize}
  \end{block}
}


\frame{
  \frametitle{Titan}
  \begin{block}{Hardware}
    \begin{itemize}
    \item 304 dual-cpu quad-core SUN X2200 Opteron nodes (total 2432 cores), 2.2 Ghz, and 8 - 16 GB RAM and 250 - 1000 GB disk on each node
    \item 3 eight-cpu quad-core Sun X4600 AMD Opteron nodes (total 96 cores), 2.5 Ghz, and 128, 128 and 256 GB memory, respectively
    \item Infiniband interconnect
    \item Heterogenous cluster!
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Titan}
  \begin{block}{Software}
    \begin{itemize}
      \item Batch system: SLURM and MAUI
      \item Message Passing Interface (MPI):
        \begin{itemize}
        \item OpenMPI
        \item Scampi
        \item MPICH2
        \end{itemize}
      \item Compilers: GCC, Intel, Portland and Pathscale
      \item Optimized math libraries and scientific applications
      \item All you need may be found under \texttt{/site}
      \item Available software: \url{http://www.hpc.uio.no/index.php/Titan_software}
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Getting started}
  \begin{block}{Batch systems}
    \begin{itemize}
    \item A batch system controls the use of the cluster resources
    \item Submits the job to the right resource
    \item Monitors the job while executing
    \item Restarts the job in case of failure
    \item Takes care of priorities and queues to control execution
      order of unrelated jobs
    \end{itemize}
  \end{block}
  \begin{block}{Sun Grid Engine}
    \begin{itemize}
    \item SGE is the batch system used on Titan
    \item Jobs are executed either interactively or through job scripts
    \item Useful commands: \texttt{showq}, \texttt{qlogin}, \texttt{sbatch}
    \item \url{http://hpc.uio.no/index.php/Titan_User_Guide}
    \end{itemize}
  \end{block}
}

\frame{
  \frametitle{Getting started}
  \begin{block}{Modules}
    \begin{itemize}
    \item Different compilers, MPI-versions and applications need
      different sets of user environment variables
    \item The \texttt{modules} package lets you load and remove the
      different variable sets
    \item Useful commands:
      \begin{itemize}
      \item List available modules: \texttt{module avail}
      \item Load module: \texttt{module load <environment>}
      \item Unload module: \texttt{module unload <environment>}
      \item Currently loaded: \texttt{module list}
      \end{itemize}
    \item \url{http://hpc.uio.no/index.php/Titan_User_Guide}
    \end{itemize}
  \end{block}
}

\lstset{basicstyle=\tiny}
\frame{
  \frametitle{Example}
  \begin{block}{Interactively}
    \lstinputlisting{interactive.sh}
  \end{block}
}
\frame{
  \frametitle{The job script}
  \begin{block}{job.sge}
    \lstinputlisting{job.sge}
  \end{block}
}

\lstset{basicstyle=\small}

\frame{
  \frametitle{Example}
  \begin{block}{Submitting}
    \lstinputlisting{submitting.sh}
  \end{block}
}

\lstset{basicstyle=\tiny}
\frame{
  \frametitle{Example}
  \begin{block}{Checking execution}
    \lstinputlisting{checking.sh}
  \end{block}
}

\lstset{basicstyle=\small}

\frame{
  \frametitle{Tips and admonitions}
  \begin{block}{Tips}
    \begin{itemize}
    \item Titan FAQ:
      \url{http://www.hpc.uio.no/index.php/FAQ}
    \item man-pages, e.g. \texttt{man sbatch}
    \item Ask us
    \end{itemize}
  \end{block}
  \begin{block}{Admonitions}
    \begin{itemize}
    \item Remember to exit from qlogin-sessions; the resource is
      reserved for you untill you exit
    \item Don't run jobs on login-nodes; these are only for compiling
      and editing files
    \end{itemize}
  \end{block}
}






  
\frame
 {
   \frametitle{Projects: quantum mechanics}
 \begin{small}
 {\scriptsize

The expectation value of the kinetic energy expressed in atomic units for electron $i$ is 
\[
K_i = -\frac{1}{2}\frac{\nabla_{i}^{2} \Psi}{\Psi}.
\]
\begin{eqnarray}
\frac{\nabla^2 \Psi}{\Psi}
& = & \frac{\nabla^2 \Psi_{D}}{\Psi_{D}} + \frac{\nabla^2  \Psi_J}{ \Psi_J} + 2 \frac{\Grad \Psi_{D}}{\Psi_{D}}\cdot\frac{\Grad  \Psi_J}{ \Psi_J}
\end{eqnarray}
 }
 \end{small}
 }

\frame
 {
   \frametitle{Quantum mechanical project}
 \begin{small}
 {\scriptsize
We define the correlated function as
\[
\Psi_J=\prod_{i< j}g(r_{ij})=\prod_{i< j}^Ng(r_{ij})= \prod_{i=1}^N\prod_{j=i+1}^Ng(r_{ij}),
\]
with 
$r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}$ for three dimensions and
$r_{ij}=|{\bf r}_i-{\bf r}_j|=\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}$ for two dimensions.

In our particular case we have
\[
\Psi_J=\prod_{i< j}g(r_{ij})=\exp{\left\{\sum_{i<j}f(r_{ij})\right\}}=
\exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
 }
 \end{small}
 }

\frame
{
  \frametitle{Quantum mechanical project}
\begin{small}
{\scriptsize

The second derivative of the Jastrow factor divided by the Jastrow factor (the way it enters the kinetic energy) is
\[
\left[\frac{\nabla^2 \Psi_J}{\Psi_J}\right]_x =\  
2\sum_{k=1}^{N}
\sum_{i=1}^{k-1}\frac{\partial^2 g_{ik}}{\partial x_k^2}\ +\ 
\sum_{k=1}^N
\left(
\sum_{i=1}^{k-1}\frac{\partial g_{ik}}{\partial x_k} -
\sum_{i=k+1}^{N}\frac{\partial g_{ki}}{\partial x_i}
\right)^2
\]
But we have a simple form for the function, namely
\[
\Psi_{J}=\prod_{i< j}\exp{f(r_{ij})}= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
and it is easy to see that for particle $k$
we have
\[
  \frac{\nabla^2_k \Psi_J}{\Psi_J }=
\sum_{ij\ne k}\frac{({\bf r}_k-{\bf r}_i)({\bf r}_k-{\bf r}_j)}{r_{ki}r_{kj}}f'(r_{ki})f'(r_{kj})+
\sum_{j\ne k}\left( f''(r_{kj})+\frac{2}{r_{kj}}f'(r_{kj})\right)
\]
}
\end{small}
}

\frame
{
  \frametitle{Jastrow gradient}
\begin{small}
{\scriptsize
We have
\[
\Psi_J=\prod_{i< j}g(r_{ij})= \exp{\left\{\sum_{i<j}\frac{ar_{ij}}{1+\beta r_{ij}}\right\}},
\]
the gradient needed for the local energy is easy to compute.  
We get for particle $k$
\[
\frac{ \nabla_k \Psi_J}{ \Psi_J }= \sum_{j\ne k}\frac{{\bf r}_{kj}}{r_{kj}}\frac{a}{(1+\beta r_{kj})^2},
\]
which is rather easy to code.  Remember to sum over all particles  when you compute the local energy.
}
\end{small}
}







\frame
{
  \frametitle{Two-dimensional wave equation, Tsunami model}
\begin{small}
{\scriptsize
We assume that we can approximate the coastline with a quadratic grid. 
As boundary condition at the coastline we will employ 
\[
\frac{\partial u}{\partial n} = \nabla u\cdot {\bf n} = 0,
\]
where $\partial u/\partial n$ is the derivative in the direction normal to the boundary. 

Here you must pay particular attention to the endpoints.
}
\end{small}
}



\frame
{
  \frametitle{Two-dimensional wave equation}
\begin{small}
{\scriptsize
We are going to model the impact of an earthquake on sea water. This is normally modelled 
via an elevation of the sea bottom. We will assume that the movement of the sea bottom 
is very rapid compared with the period of the propagating waves.  This means that we can
approximate the bottom elevation with an initial surface elevation.
The initial conditions are then given by (with $L$ the length of the grid)
\[
   u(x,y,0) = f(x,y)\hspace{1.0cm} x,y\in (0,L),
\]
and 
\[
   \partial u/\partial t|_{t=0}=0 \hspace{1.0cm} x,y\in (0,L).
\]

We will approximate the initial elevation with the function
\[
 f(x,y) =  A_0 \exp{\left(-\left[\frac{x-x_c}{\sigma_x}\right]^2-\left[\frac{y-y_c}{\sigma_y}\right]^2\right)},
\]
where $A_0$ is the elevation of the surface and is typically $1-2$ m.  The variables $\sigma_x$ and
$\sigma_y$ represent the extensions of the surface elevation. Here we  let $\sigma_x=80$ km
and $\sigma_y=200$ km. The 2004 tsunami had extensions of approximately 200 and 1000 km, respectively.

The variables $x_c$ and $y_c$ represent the epicentre of the earthquake. 
}
\end{small}
}







\end{document}



%\section{Statistical physics}





















\frame
{ 
  \frametitle{Exam FYS3150 place FV329, Lab}
  \begin{block}{Place and duration}
\begin{itemize}
\item 13/12-17/12. 
\item duration: $\sim 45$ min 
\item ca 20-25 min for discussion of the project, your presentation, reproduction of results, test runs etc
\item 20-25 min for questions from one of the five topics listed below.
\item Check final exam list by the end of this week, see webpage. Changes must be communicated to me at mhjensen@fys.uio.no.
\end{itemize}
  \end{block}
} 


\frame
{ 
  \frametitle{Exam FYS3150}
  \begin{block}{How to prepare your Talk}
\begin{itemize}
\item You can use slides, ps, pdf, ppt etc files (projector and slide projector at room FV329). Latex example on webpage.
Choose among one of the two versions of project 5.
\item 10-15 mins (5-15 slides)  
for your presentation, rest of 10-15 mins questions and test of code from classfronter
\end{itemize}
You should discuss
\begin{itemize}
\item The mathematical model and the physics
\item Your algorithm and how you implemented it, with selected
results
\item How you dealt with eventual comments and your corrections  of these
\item Any improvement you can think of 
\end{itemize}
  \end{block}
} 




\end{document}




\end{document}




\section{Week 44, 26-30 Oct}
\frame
{
  \frametitle{Week 44, October 26-30}
  \begin{block}{Ising model, phase transitions and project 4}
\begin{itemize}
\item Monday: Repetition from last week
\item Discussion of project 4, results near the phase transition
\item Definition of correlation time and equilibration time  (optional part of project 4).
\item Wednesday: 
\item End Monte Carlo discussion and summary of chapters 8-10. Chapter 11 on quantum Monte Carlo will not be discussed and is not part of the curriculum.
\end{itemize}
  \end{block}
} 

\frame
{
  \frametitle{Mean Field Theory and the Ising Model}
\begin{small}
{\scriptsize
In studies of phase transitions we are interested in minimizing the free energy
by varying the average magnetization, which is the order parameter (disappears at
$T_C$).

In mean field theory the local magnetization is a treated as a constant, all effects from fluctuations
are neglected. A way to achieve this is to rewrite by adding and subtracting the mean magnetization
$\langle s \rangle$ 
\be
s_is_j=(s_i-\langle s \rangle+\langle s \rangle)(s_i-\langle s \rangle+\langle s \rangle)
\approx \langle s \rangle^2+\langle s \rangle(s_i-\langle s \rangle)+\langle s \rangle(s_j-\langle s \rangle),
\ee
where we have ignored terms of the order 
$(s_i-\langle s \rangle)(s_i-\langle s \rangle)$, which leads to correlations between neighbouring
spins. In mean field theory we ignore correlations. 
}
\end{small}
}


\frame
{
  \frametitle{Mean Field Theory and the Ising Model}
\begin{small}
{\scriptsize
This means that we can rewrite the  Hamiltonian
\be
  E=-J\sum_{<ij>}^{N}s_ks_l-B\sum_i^Ns_i,
\ee
as
\be
  E=-J\sum_{<ij>}\langle s \rangle^2+\langle s \rangle(s_i-\langle s \rangle)+\langle s \rangle(s_j-\langle s \rangle)    -B\sum_i s_i,
\ee
resulting in
\be
  E=-(B+zJ\langle s \rangle) \sum_i s_i +zJ\langle s \rangle^2,
\ee
with $z$ the number of nearest neighbours for a given site $i$.
We can define an effective field which all spins see, namely
\be
  B_{\mathrm{eff}}=(B+zJ\langle s \rangle).
\ee
}
\end{small}
}

\frame
{
  \frametitle{Mean Field Theory and the Ising Model}
\begin{small}
{\scriptsize
How do we get $\langle s \rangle)$?

Here we use the canonical ensemble. The partition function reads in this case
\be
Z=e^{-NzJ\langle s \rangle^2/kT}\left(2cosh(B_{\mathrm{eff}}/kT)\right)^N,
\ee
with a free energy
\be
F=-kTlnZ=-NkTln(2)+NzJ\langle s \rangle^2-NkTln\left(cosh(B_{\mathrm{eff}}/kT)\right)
\ee
and minimizing $F$ wrt $\langle s \rangle$ we arrive at
\be
\langle s \rangle = tanh(2cosh\left(B_{\mathrm{eff}}/kT)\right).
\ee
}
\end{small}
}


\frame
{
  \frametitle{Connecting to Landau Theory}
\begin{small}
{\scriptsize
Close to the phase transition we expect $\langle s \rangle$ to become small
and eventually vanish. We can then expand $F$ in powers of $\langle s \rangle$ as
\be
F=-NkTln(2)+NzJ\langle s \rangle^2-NkT-BN\langle s \rangle+NkT\left(\frac{1}{2}\langle s \rangle^2+
\frac{1}{12}\langle s \rangle^4+\dots\right),
\ee
and using $\langle M\rangle = N\langle s \rangle$ we can rewrite as
\be
F=F_0-B\langle M\rangle +\frac{1}{2}a\langle M \rangle^2+
\frac{1}{4}b\langle M \rangle^4+\dots
\ee
}
\end{small}
}


\frame
{
  \frametitle{Connecting to Landau Theory}
\begin{small}
{\scriptsize
Let $\langle M \rangle = m$ and
\be
F=F_0+\frac{1}{2}am^2+
\frac{1}{4}bm^4+\frac{1}{6}cm^6
\ee
$F$ has a minimum at equilibrium $F'(m) =0$ and $F''(m) > 0$
\[
F'(m)=0=m(a+bm^2+cm^4),
\]
and if we assume that $m$ is real we have two solutions
\[
   m=0,
\]
or
\[
  m^2 = \frac{b}{2c}\left(-1\pm\sqrt{1-4ac/b^2}\right)
\]
}
\end{small}
}



\frame
{
  \frametitle{Second Order Phase Transition}
\begin{small}
{\scriptsize

Can describe both first and second-order phase transitions. Here we consider the second case.
Assume $b > 0$ and $a\ll 1$ small since we want to study a perturbation around $m=0$. We reach the critical point when 
$a=0$. 
\[
  m^2 = \frac{b}{2c}\left((-1\pm\sqrt{1-4ac/b^2}\right)\approx -a/b
\]
Assume that
\[ a(T) = \alpha (T-T_C), \]
with $\alpha > 0$ and $T_C$ the critical temperature where the magnetization vanishes. If $a$ is negative we have two solutions
\[    m = \pm \sqrt{-a/b}  = \pm \sqrt{\frac{\alpha (T_C-T)}{b}}\]
$m$ evolves continuously to the critical temperature where $F=0$ for $T \le T_C$ (see separate graph).
}
\end{small}
}


\frame
{
  \frametitle{Entropy and Specific Heat}
\begin{small}
{\scriptsize
We can now compute the entropy 
\[ S= -\left(\frac{\partial F}{\partial T}\right)\]
For $T \ge T_C$ we have $m=0$ and 
\[ S= -\left(\frac{\partial F_0}{\partial T}\right)\]
and for $T \le T_C$
\[ S= -\left(\frac{\partial F_0}{\partial T}\right)-\alpha^2(T_C-T)/2b,\]
and we see that there is a smooth  crossover at $T_C$. 
}
\end{small}
}



\frame
{
  \frametitle{Entropy and Specific Heat}
\begin{small}
{\scriptsize
We can now compute the specific heat
\[ C_V= T\left(\frac{\partial S}{\partial T}\right)\]
and $T_C$ we get a discontinuity of 
\[\Delta C_V  = -\alpha^2/2b,\]
signalling a second-order phase transition.  
Landau theory gives irrespective of dimension critical exponents
\[ m \sim (T_C-T)^{\beta},\] 
and 
\[ C_V \sim (T_C-T)^{\alpha},\]
with $\beta =1/2$ and $\alpha = 1$. 
It predicts a phase transition for one dimension as well.
For the Ising model there is no phase transition for $d=1$.
In two dimensions we have $\beta=1/8$ and $\alpha =0$.
}
\end{small}
}


\frame
{
  \frametitle{Scaling Results}
\begin{small}
{\scriptsize
Near $T_C$ we can characterize the behavior of many physical quantities
by a power law behavior.
As an example, the mean magnetization is given by
\begin{equation}
  \langle {\cal M}(T) \rangle \sim \left(T-T_C\right)^{\beta},
\end{equation}
where $\beta$ is a so-called critical exponent. A similar relation
applies to the heat capacity 
\begin{equation}
  C_V(T) \sim \left|T_C-T\right|^{-\alpha},
\end{equation}
the susceptibility
\begin{equation}
  \chi(T) \sim \left|T_C-T\right|^{\gamma}.
\end{equation}
and the correlation length
\begin{equation}
  \xi(T) \sim \left|T_C-T\right|^{-\nu}.
\end{equation}
$\alpha = 0$, $\beta = 1/8$, $\gamma = 7/4$ and $\nu = 1$. Below we will derive these coefficients
from finite size scaling theories.
}
\end{small}
}

\frame
{
  \frametitle{Scaling Results}
\begin{small}
{\scriptsize
Through finite size scaling relations 
it is possible to relate the behavior at finite lattices with the 
results for an infinitely large lattice.
The critical temperature scales then as
\begin{equation}
 T_C(L)-T_C(L=\infty) \sim aL^{-1/\nu},
 \label{eq:tc}
\end{equation}
\begin{equation}
  \langle {\cal M}(T) \rangle \sim \left(T-T_C\right)^{\beta}
  \rightarrow L^{-\beta/\nu},
  \label{eq:scale1}
\end{equation}
\begin{equation}
  C_V(T) \sim \left|T_C-T\right|^{-\gamma} \rightarrow L^{\gamma/\nu},
  \label{eq:scale2}
\end{equation}
and
\begin{equation}
  \chi(T) \sim \left|T_C-T\right|^{-\alpha} \rightarrow L^{\alpha/\nu}.
  \label{eq:scale3}
\end{equation}
We can compute the slope of the curves for $M$, $C_V$ and $\chi$ as function of lattice sites and extract
the exponent $\nu$.
}
\end{small}
}

\frame
{
  \frametitle{Analytic Results: two-dimensional Ising model}
\begin{small}
{\scriptsize
The analytic expression for the Ising model in two dimensions was obtained
in 1944 by the Norwegian chemist Lars Onsager (Nobel prize in chemistry).
The exact partition function 
for $N$ spins in two dimensions and with zero magnetic field $\cal H$ is given by 
\be
   Z_N=\left[2cosh(\beta J)e^I\right]^N,
\ee
with 
\be
I=\frac{1}{2\pi}\int_0^{\pi}d\phi ln
        \left[\frac{1}{2}\left(1+(1-\kappa^2sin^2\phi)^{1/2}\right)\right],
\ee
and
\be
   \kappa=2sinh(2\beta J)/cosh^2(2\beta J).
\ee
}
\end{small}
}

\frame
{
  \frametitle{Analytic Results: two-dimensional Ising model}
\begin{small}
{\scriptsize
The resulting energy is given by 
\be 
  \langle E\rangle =  -Jcoth(2\beta J)\left[1+\frac{2}{\pi}(2tanh^2(2\beta J)-1)K_1(q)\right],
\ee
with 
$q=2sinh(2\beta J)/cosh^2(2\beta J)$ and the complete elliptic integral of the first kind
\be
K_1(q) = \int_0^{\pi/2}\frac{d\phi}{\sqrt{1-q^2sin^2\phi}}.
\ee
}
\end{small}
}

\frame
{
  \frametitle{Analytic Results: two-dimensional Ising model}
\begin{small}
{\scriptsize
Differentiating once more with respect to temperature we obtain the specific heat given by
\[
C_V=\frac{4k_B}{\pi}(\beta Jcoth(2\beta J))^2
\]
\[
\left\{K_1(q)-K_2(q)-(1-tanh^2(2\beta J))\left[\frac{\pi}{2}+(2tanh^2(2\beta J)-1)K_1(q)\right]\right\},
\]
with
\be
   K_2(q) = \int_0^{\pi/2}d\phi\sqrt{1-q^2sin^2\phi}.
\ee
is the complete elliptic integral of the second kind.
Near the critical temperature $T_C$ the specific heat behaves as
\be
C_V \approx -\frac{2}{k_B\pi}\left(\frac{J}{T_C}\right)^2ln\left|1-\frac{T}{T_C}\right|+\mathrm{const}.
\ee
}
\end{small}
}

\frame
{
  \frametitle{Analytic Results: two-dimensional Ising model}
\begin{small}
{\scriptsize
In theories of critical phenomena one has that
\be
C_V\sim \left|1-\frac{T}{T_C}\right|^{-\alpha},
\ee
and Onsager's result is a special case of this power law behavior. The limiting form of the function
\be
lim_{\alpha\rightarrow 0} \frac{1}{\alpha}(Y^{-\alpha}-1)=-lnY,
\ee
meaning that the analytic result is a special case of the power law singularity with 
$\alpha =0$.
}
\end{small}
}

\frame
{
  \frametitle{Analytic Results: two-dimensional Ising model}
\begin{small}
{\scriptsize
One can also show that the mean magnetization per spin is
\[
    \left[1-\frac{(1-tanh^2 (\beta J))^4}{16tanh^4(\beta J)}\right]^{1/8}
\]
for $T < T_C$ and $0$ for $T> T_C$. The behavior is thus as $T\rightarrow T_C$ from below
\[
M(T) \sim (T_C-T)^{1/8}
\]
The susceptibility behaves as 
\[
\chi(T) \sim |T_C-T|^{-7/4}
\]
}
\end{small}
}









\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
The so-called time-displacement autocorrelation $\phi(t)$ for the magnetization is given by
\[
\phi(t) = \int dt' \left[{\cal M}(t')-\langle {\cal M} \rangle\right]\left[{\cal M}(t'+t)-\langle {\cal M} \rangle\right],
\]
which can be rewritten as 
\[
\phi(t) = \int dt' \left[{\cal M}(t'){\cal M}(t'+t)-\langle {\cal M} \rangle^2\right],
\]
where $\langle {\cal M} \rangle$ is the average value of the magnetization and 
${\cal M}(t)$ its instantaneous value. We can discretize this function as follows, where we used our
set of computed values ${\cal M}(t)$ for a set of discretized times (our Monte Carlo cycles corresponding
to a sweep over the lattice) 
\[
\phi(t)  = \frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t'){\cal M}(t'+t)
-\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t')\times
\frac{1}{t_{\mathrm{max}}-t}\sum_{t'=0}^{t_{\mathrm{max}}-t}{\cal M}(t'+t).\label{eq:phitf}
\]
}
\end{small}
}

\frame[containsverbatim]
{
  \frametitle{Correlation Time, how to compute}
\begin{small}
{\scriptsize
\begin{verbatim}
// define m-value at each cycle within loop over cycles
m_matrix[cycles] = Maverage/(n_spins**2)/cycles
//  Then compute phi(i)  as (in pseudocode)
    for  i = 1, mcs
         r = 1.0/(mcs-i)
         s = 0.0; v = 0.0; p= 0.0
         for j = 1, mcs-i
             p = p+ m_matrix(j)*m_matrix(j+i)
             s = s+ m_matrix(j)
             v = v+ m_matrix(j+i)
         end for
         phi(i)  = r*p-r*r*s*v
    end for
\end{verbatim}
}
\end{small}
}



\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
One should be careful with times close to $t_{\mathrm{max}}$, the upper limit of the sums 
becomes small and we end up integrating over a rather small time interval. This means that the statistical
error in $\phi(t)$ due to the random nature of the fluctuations in ${\cal M}(t)$ can become large.

One should therefore choose $t \ll t_{\mathrm{max}}$.

Note also that we could replace the magnetization with the mean energy, or any other expectation values of interest.



The time-correlation function for the magnetization gives a measure of the correlation between the magnetization
at a time $t'$ and a time $t'+t$. If we multiply the magnetizations at these two different times,
we will get a positive contribution if the magnetizations are fluctuating in the same direction, or a negative value
if they fluctuate in the opposite direction. If we then integrate over time, or use the discretized version of, the time correlation function $\phi(t)$ should take a non-zero value if the fluctuations are 
correlated, else it should gradually go to zero. For times a long way apart the magnetizations are most likely 
uncorrelated and $\phi(t)$ should be zero.
}
\end{small}
}




\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
We can derive the correlation time by observing that our Metropolis algorithm is based on a random
walk in the space of all  possible spin configurations. 
Our probability 
distribution function ${\bf \hat{w}}(t)$ after a given number of time steps $t$ could be written as
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}^t\hat{w}}(0),
\]
with ${\bf \hat{w}}(0)$ the distribution at $t=0$ and ${\bf \hat{W}}$ representing the 
transition probability matrix. 
We can always expand ${\bf \hat{w}}(0)$ in terms of the right eigenvectors of 
${\bf \hat{v}}$ of ${\bf \hat{W}}$ as 
\[
    {\bf \hat{w}}(0)  = \sum_i\alpha_i{\bf \hat{v}}_i,
\]
resulting in 
\[
   {\bf \hat{w}}(t) = {\bf \hat{W}}^t{\bf \hat{w}}(0)={\bf \hat{W}}^t\sum_i\alpha_i{\bf \hat{v}}_i=
\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i,
\]
with $\lambda_i$ the $i^{\mathrm{th}}$ eigenvalue corresponding to  
the eigenvector ${\bf \hat{v}}_i$. 
}
\end{small}
}



\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
If we assume that $\lambda_0$ is the largest eigenvector we see that in the limit $t\rightarrow \infty$,
${\bf \hat{w}}(t)$ becomes proportional to the corresponding eigenvector 
${\bf \hat{v}}_0$. This is our steady state or final distribution. 

We can relate this property to an observable like the mean magnetization.
With the probabilty ${\bf \hat{w}}(t)$ (which in our case is the Boltzmann distribution) we
can write the mean magnetization as 
\[
 \langle {\cal M}(t) \rangle  = \sum_{\mu} {\bf \hat{w}}(t)_{\mu}{\cal M}_{\mu},
\]  
or as the scalar of a  vector product
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m},
\]  
with ${\bf m}$ being the vector whose elements are the values of ${\cal M}_{\mu}$ in its 
various microstates $\mu$.
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
We rewrite this relation  as
 \[
 \langle {\cal M}(t) \rangle  = {\bf \hat{w}}(t){\bf m}=\sum_i\lambda_i^t\alpha_i{\bf \hat{v}}_i{\bf m}_i.
\]  
If we define $m_i={\bf \hat{v}}_i{\bf m}_i$ as the expectation value of
${\cal M}$ in the $i^{\mathrm{th}}$ eigenstate we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \sum_i\lambda_i^t\alpha_im_i.
\] 
Since we have that in the limit $t\rightarrow \infty$ the mean magnetization is dominated by the 
the largest eigenvalue $\lambda_0$, we can rewrite the last equation as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\lambda_i^t\alpha_im_i.
\] 
We define the quantity
\[
   \tau_i=-\frac{1}{log\lambda_i},
\]
and rewrite the last expectation value as
 \[
 \langle {\cal M}(t) \rangle  = \langle {\cal M}(\infty) \rangle+\sum_{i\ne 0}\alpha_im_ie^{-t/\tau_i}.
\label{eq:finalmeanm}
\] 
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
The quantities $\tau_i$ are the correlation times for the system. They control also the auto-correlation function 
discussed above.  The longest correlation time is obviously given by the second largest
eigenvalue $\tau_1$, which normally defines the correlation time discussed above. For large times, this is the 
only correlation time that survives. If higher eigenvalues of the transition matrix are well separated from 
$\lambda_1$ and we simulate long enough,  $\tau_1$ may well define the correlation time. 
In other cases we may not be able to extract a reliable result for $\tau_1$. 
Coming back to the time correlation function $\phi(t)$ we can present a more general definition in terms
of the mean magnetizations $ \langle {\cal M}(t) \rangle$. Recalling that the mean value is equal 
to $ \langle {\cal M}(\infty) \rangle$ we arrive at the expectation values
\[
\phi(t) =\langle {\cal M}(0)-{\cal M}(\infty)\rangle \langle {\cal M}(t)-{\cal M}(\infty)\rangle,
\]
resulting in
\[
\phi(t) =\sum_{i,j\ne 0}m_i\alpha_im_j\alpha_je^{-t/\tau_i},
\]
which is appropriate for all times.
}
\end{small}
}



\frame
{
  \frametitle{Correlation Time}
\begin{small}
{\scriptsize
If the correlation function decays exponentially
\[ \phi (t) \sim \exp{(-t/\tau)}\]
then the exponential correlation time can be computed as the average
\[   \tau_{\mathrm{exp}}  =  -\langle  \frac{t}{log|\frac{\phi(t)}{\phi(0)}|} \rangle. \]

If the decay is exponential, then
\[  \int_0^{\infty} dt \phi(t)  = \int_0^{\infty} dt \phi(0)\exp{(-t/\tau)}  = \tau \phi(0),\] 
which  suggests another measure of correlation
\[   \tau_{\mathrm{int}} = \sum_k \frac{\phi(k)}{\phi(0)}, \]
called the integrated correlation time.
}
\end{small}
}








\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
Here we mention that  
one can show, using scaling relations, that at the critical temperature the 
correlation time $\tau$ relates to the lattice size $L$ as 
\[
    \tau \sim L^{d+z},
\]
with $d$ the dimensionality of the system.
For the Metropolis algorithm based on a single spin-flip process, Nightingale and Bl\"ote obtained
$z=2.1665\pm 0.0012$. This is a rather high value, meaning that our algorithm is not the best
choice when studying properties of the Ising model near $T_C$. 

We can understand this behavior by studying the development of the two-dimensional 
Ising model as function of temperature. 
}
\end{small}
}


\frame
{
  \frametitle{Time Auto-correlation Function}
\begin{small}
{\scriptsize
Cooling the system down to the critical temperature we observe clusters pervading larger areas of the lattice.
The reason for the large correlation time (and the parameter $z$) for the single-spin flip Metropolis
algorithm is the development of these large domains or clusters with all spins pointing in one direction.
It is quite difficult for the algorithm to flip over one of these large domains because it has to do it spin by spin,
with each move having a high probability of being rejected due to the ferromagnetic interaction 
between spins.
Since all spins point in the same direction, the chance of performing the flip
\[
  E=-4J\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \uparrow & \uparrow\\
                                  & \uparrow & \end{array}
\hspace{1cm}\Longrightarrow\hspace{1cm}
  E=4J\hspace{1cm}\begin{array}{ccc}         & \uparrow &         \\
                         \uparrow & \downarrow & \uparrow\\
                                  & \uparrow & \end{array}
\] 
leads to an energy difference of $\Delta E = 8J$. Using the exact critical temperature $k_BT_C/J\approx 2.2.69$,
we obtain a probability $\exp{-(8/2.269)}=0.029429$ which is rather small. 
The increase in large correlation times due to increasing lattices can be diminished by using  so-called
cluster algorithms, such as that  
introduced by Ulli Wolff in 1989 and the Swendsen-Wang algorithm from 1987. 
The two-dimensional Ising model with the Wolff or Swendsen-Wang algorithms exhibits a much smaller
correlation time, with the variable $z=0.25\pm 001$. Here, instead of flipping a single spin, one flips an entire cluster
of spins pointing in the same direction.
}
\end{small}
}






\frame
{
  \frametitle{Better  Algorithm needed}
\begin{small}
{\scriptsize
Monte Carlo simulations close to a phase transition are affected by critical slowing down. In the 2-D Ising system,
the correlation length  becomes very large, and the correlation time, which measures the number of steps between
independent Monte Carlo configurations behaves like
\[  \tau \sim \xi^z, \]
with $z\approx 2.1$ for the Metropolis algorithm.
The exponent $z$ is  called the dynamic critical exponent.
The maximum possible value for $\xi$  in a finite system of $N = L \times L$ spins is   $\xi\sim L$, because  $\xi$ 
cannot be larger than the
lattice size! 
This implies that   $\tau\sim L^z \approx N$. This makes simulations difficult because the Metropolis algorithm time
scales like $N$, so the time to generate independent Metropolis configurations scales like 
\[ N\tau\sim   N^2 = L^4.\]  
If the lattice 
size 
\[
L \rightarrow  \sqrt{10}L\approx 3.2L,
\] the simulation time increases by a factor of $100$.
}
\end{small}
}


\frame
{
  \frametitle{Better Algorithm  needed}
\begin{small}
{\scriptsize
There is a simple physical argument which helps understand why $z = 2$, The Metropolis algorithm is a local algorithm,
i.e., one spin is tested and flipped at a time. Near $T_C$ the system develops large domains of correlated spins which are
difficult to break up. So the most likely change in configuration is the movement of a whole domain of spins. But one
Metropolis sweep of the lattice can move a domain at most by approximately one lattice spacing in each time step. 
This motion is stochastic, i.e., like a random walk. 
The distance traveled in a random walk scales like $\sqrt{\mathrm{time}}$, so to move a
domain a distance of order  $\xi$ takes    $\tau\sim \xi^2$ Monte Carlo steps.
This argument suggests that the way to speed up a Monte Carlo simulation near $T_C$ is to use a non-local algorithm.
}
\end{small}
}







