<!--
Automatically generated HTML file from Doconce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Doconce: https://github.com/hplgit/doconce/" />
<meta name="description" content="Slides from FYS3150/4150 Lectures">




<link href="https://raw.githubusercontent.com/hplgit/doconce/master/bundled/html_styles/style_solarized_box/css/solarized_light_code.css" rel="stylesheet" type="text/css" title="light"/>
<script src="http://www.peterhaschke.com/assets/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<link href="http://thomasf.github.io/solarized-css/solarized-light.min.css" rel="stylesheet">
<style type="text/css">
h1 {color: #b58900;}  /* yellow */
/* h1 {color: #cb4b16;}  orange */
/* h1 {color: #d33682;}  magenta, the original choice of thomasf */
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #93a1a1;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #eee8d5;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_notice.png); }
.alert-summary  { background-image:url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_summary.png); }
.alert-warning { background-image: url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_warning.png); }
.alert-question {background-image:url(https://raw.github.com/hplgit/doconce/master/bundled/html_images/small_yellow_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>

</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [(' Week 34 ', 1, None, '___sec0'),
              (' Overview of week 34 ', 2, None, '___sec1'),
              (' Lectures and ComputerLab ', 2, None, '___sec2'),
              (' Course Format ', 2, None, '___sec3'),
              (' ComputerLab ', 2, None, '___sec4'),
              (' Topics covered in this course ', 2, None, '___sec5'),
              (' Syllabus FYS3150 ', 2, None, '___sec6'),
              (' Syllabus FYS3150 ', 2, None, '___sec7'),
              (' Syllabus FYS3150 ', 2, None, '___sec8'),
              (' Syllabus FYS3150 ', 2, None, '___sec9'),
              (' Syllabus FYS3150 ', 2, None, '___sec10'),
              (' Syllabus FYS3150 ', 2, None, '___sec11'),
              (' Overarching aims of this course ', 2, None, '___sec12'),
              (' And, there is nothing like a code which gives correct results!! ',
               2,
               None,
               '___sec13'),
              (' Other courses in Computational Science at UiO ',
               2,
               None,
               '___sec14'),
              (' Extremely useful tools, strongly recommended ',
               2,
               None,
               '___sec15'),
              (' A structured programming approach ', 2, None, '___sec16'),
              (' A structured programming approach ', 2, None, '___sec17'),
              (' Getting Started ', 2, None, '___sec18'),
              (' Makefiles and simple scripts ', 2, None, '___sec19'),
              (' Hello world ', 2, None, '___sec20'),
              (' Hello World ', 2, None, '___sec21'),
              (' Hello World ', 2, None, '___sec22'),
              (' Hello World ', 2, None, '___sec23'),
              (' C++ Hello World ', 2, None, '___sec24'),
              (' Brief summary ', 2, None, '___sec25'),
              (' Brief summary ', 2, None, '___sec26'),
              (' Serious problems and representation of numbers ',
               2,
               None,
               '___sec27'),
              (' Limits, you must declare variables ', 2, None, '___sec28'),
              (' From decimal to binary representation ',
               2,
               None,
               '___sec29'),
              (' From decimal to binary representation ',
               2,
               None,
               '___sec30'),
              (' From decimal to binary representation ',
               2,
               None,
               '___sec31'),
              (' Integer Numbers ', 2, None, '___sec32'),
              (' Loss of Precision ', 2, None, '___sec33'),
              (' Loss of Precision ', 2, None, '___sec34'),
              (' Loss of Precision ', 2, None, '___sec35'),
              (' Loss of Precision ', 2, None, '___sec36'),
              (' Loss of Precision ', 2, None, '___sec37'),
              (' Loss of Precision ', 2, None, '___sec38'),
              (' Loss of Precision ', 2, None, '___sec39'),
              (' Loss of numerical precision ', 2, None, '___sec40'),
              (' Loss of numerical precision ', 2, None, '___sec41'),
              (' Loss of numerical precision ', 2, None, '___sec42'),
              (' Loss of precision can cuae serious problems ',
               2,
               None,
               '___sec43'),
              (' Loss of precision, real numbers ', 2, None, '___sec44'),
              (' Roundoff errors ', 3, None, '___sec45'),
              (' More on loss of precision ', 2, None, '___sec46'),
              (' A problematic case ', 2, None, '___sec47'),
              (' Program to compute $\\exp{(-x)}$ ', 2, None, '___sec48'),
              (' Program to compute $\\exp{(-x)}$ ', 2, None, '___sec49'),
              (' Program to compute $\\exp{(-x)}$ ', 2, None, '___sec50'),
              (' Results $\\exp{(-x)}$ ', 2, None, '___sec51'),
              (' Program to compute $\\exp{(-x)}$ ', 2, None, '___sec52'),
              (' Program to compute $\\exp{(-x)}$ ', 2, None, '___sec53'),
              (' Results $\\exp{(-x)}$ ', 2, None, '___sec54'),
              (' Most used formula for derivatives ', 2, None, '___sec55'),
              (' Error Analysis ', 2, None, '___sec56'),
              (' Error Analysis ', 2, None, '___sec57'),
              (' Error Analysis ', 2, None, '___sec58'),
              (' Error Analysis ', 2, None, '___sec59'),
              (' Error Analysis ', 2, None, '___sec60'),
              (' Error Analysis ', 2, None, '___sec61'),
              (' Week 35 ', 1, None, '___sec62'),
              (' Overview of week 35 ', 2, None, '___sec63'),
              (' Technical Matter in C/C++: Pointers ', 2, None, '___sec64'),
              (' Technical Matter in C/C++: Pointer example I ',
               2,
               None,
               '___sec65'),
              (' Dissection: Pointer example I ', 2, None, '___sec66'),
              (' Pointer example II ', 2, None, '___sec67'),
              (' Dissection: Pointer example II ', 2, None, '___sec68'),
              (' Output of Pointer example II ', 2, None, '___sec69'),
              (' File handling; C-way ', 2, None, '___sec70'),
              (' File handling; C way cont. ', 2, None, '___sec71'),
              (' File handling, C++-way ', 2, None, '___sec72'),
              (' File handling, C++-way ', 2, None, '___sec73'),
              (' File handling, C++-way ', 2, None, '___sec74'),
              (' File handling, C++-way ', 2, None, '___sec75'),
              (' File handling, C++-way ', 2, None, '___sec76'),
              (' Call by value and reference ', 2, None, '___sec77'),
              (' Call by value and reference ', 2, None, '___sec78'),
              (' Call by value and reference ', 2, None, '___sec79'),
              (' Call by value and reference ', 2, None, '___sec80'),
              (' Call by value and reference ', 2, None, '___sec81'),
              (' Call by value and reference ', 2, None, '___sec82'),
              (' Call by value and reference ', 2, None, '___sec83'),
              (' Call by value and reference, F90/95 ', 2, None, '___sec84'),
              (' Important Matrix and vector handling packages ',
               2,
               None,
               '___sec85'),
              (' Basic Matrix Features ', 2, None, '___sec86'),
              (' Basic Matrix Features ', 2, None, '___sec87'),
              (' Some famous Matrices ', 2, None, '___sec88'),
              (' Basic Matrix Features ', 2, None, '___sec89'),
              (' Important Mathematical Operations ', 2, None, '___sec90'),
              (' Important Mathematical Operations ', 2, None, '___sec91'),
              (' Matrix Handling in C/C++, Static and Dynamical allocation ',
               2,
               None,
               '___sec92'),
              (' Matrix Handling in C/C++ ', 2, None, '___sec93'),
              (' Matrix Handling in C/C++ ', 2, None, '___sec94'),
              (' Matrix Handling in Fortran 90/95 ', 2, None, '___sec95'),
              (' Dynamic memory allocation in C/C++ ', 2, None, '___sec96'),
              (' Matrix Handling in C/C++, Dynamic Allocation ',
               2,
               None,
               '___sec97'),
              (' Armadillo, recommended!! ', 2, None, '___sec98'),
              (' Armadillo, simple examples ', 2, None, '___sec99'),
              (' Armadillo, how to compile and install ',
               2,
               None,
               '___sec100'),
              (' Armadillo, simple examples ', 2, None, '___sec101'),
              (' Armadillo, simple examples ', 2, None, '___sec102'),
              (' Armadillo, simple examples ', 2, None, '___sec103'),
              (' Armadillo, simple examples ', 2, None, '___sec104'),
              (' Armadillo, simple examples ', 2, None, '___sec105'),
              (' Armadillo, simple examples ', 2, None, '___sec106'),
              (' Armadillo, simple examples ', 2, None, '___sec107'),
              (' Gaussian Elimination ', 2, None, '___sec108'),
              (' Gaussian Elimination ', 2, None, '___sec109'),
              (' Gaussian Elimination ', 2, None, '___sec110'),
              (' Gaussian Elimination ', 2, None, '___sec111'),
              (' Gaussian Elimination ', 2, None, '___sec112'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec113'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec114'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec115'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec116'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec117'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec118'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec119'),
              (' Gaussian Elimination and Tridiagonal matrices, project 1 ',
               2,
               None,
               '___sec120'),
              (' Project 1, hints ', 2, None, '___sec121'),
              (' Project 1, hints ', 2, None, '___sec122'),
              (' Linear Algebra Methods ', 2, None, '___sec123'),
              (' LU Decomposition ', 2, None, '___sec124'),
              (' LU Decomposition, why? ', 2, None, '___sec125'),
              (' LU Decomposition, linear equations ', 2, None, '___sec126'),
              (' LU Decomposition, linear equations ', 2, None, '___sec127'),
              (' LU Decomposition, why? ', 2, None, '___sec128'),
              (' LU Decomposition, linear equations ', 2, None, '___sec129'),
              (' LU Decomposition, the inverse of a matrix ',
               2,
               None,
               '___sec130'),
              (' LU Decomposition, the inverse ', 2, None, '___sec131'),
              (' How to use the Library functions ', 2, None, '___sec132'),
              (' How to use the Library functions ', 2, None, '___sec133'),
              (' How to use the Library functions ', 2, None, '___sec134'),
              (' How to use the Library functions ', 2, None, '___sec135'),
              (' How to use the Library functions ', 2, None, '___sec136'),
              (' Week 36 ', 1, None, '___sec137'),
              (' Overview of week 36 ', 2, None, '___sec138'),
              (' Object orientation ', 2, None, '___sec139'),
              (' Object orientation ', 2, None, '___sec140'),
              (' Programming classes ', 2, None, '___sec141'),
              (' Programming classes ', 2, None, '___sec142'),
              (' Programming classes ', 2, None, '___sec143'),
              (' Programming classes ', 2, None, '___sec144'),
              (' Programming classes ', 2, None, '___sec145'),
              (' Programming classes ', 2, None, '___sec146'),
              (' Programming classes ', 2, None, '___sec147'),
              (' Programming classes ', 2, None, '___sec148'),
              (' Programming classes ', 2, None, '___sec149'),
              (' Programming classes ', 2, None, '___sec150'),
              (' Programming classes ', 2, None, '___sec151'),
              (' Programming classes ', 2, None, '___sec152'),
              (' Programming classes ', 2, None, '___sec153'),
              (' Programming classes ', 2, None, '___sec154'),
              (' Programming classes ', 2, None, '___sec155'),
              (' Programming classes ', 2, None, '___sec156'),
              (' Programming classes ', 2, None, '___sec157'),
              (' Programming classes ', 2, None, '___sec158'),
              (' Programming classes ', 2, None, '___sec159'),
              (' Programming classes ', 2, None, '___sec160'),
              (' Programming classes ', 2, None, '___sec161'),
              (' Programming classes ', 2, None, '___sec162'),
              (' Programming classes ', 2, None, '___sec163'),
              (' Programming classes ', 2, None, '___sec164'),
              (' Programming classes ', 2, None, '___sec165'),
              (' Programming classes ', 2, None, '___sec166'),
              (' Programming classes ', 2, None, '___sec167'),
              (' Programming classes ', 2, None, '___sec168'),
              (' Programming classes ', 2, None, '___sec169'),
              (' Programming classes ', 2, None, '___sec170'),
              (' Programming classes ', 2, None, '___sec171'),
              (' Programming classes ', 2, None, '___sec172'),
              (' Programming classes ', 2, None, '___sec173'),
              (' Programming classes, templates ', 2, None, '___sec174'),
              (' Programming classes ', 2, None, '___sec175'),
              (' Programming classes ', 2, None, '___sec176'),
              (' Programming classes ', 2, None, '___sec177'),
              (' Programming classes ', 2, None, '___sec178'),
              (' Programming classes ', 2, None, '___sec179'),
              (' Programming classes ', 2, None, '___sec180'),
              (' Week 37 ', 1, None, '___sec181'),
              (' Overview of week 37 ', 2, None, '___sec182'),
              (' Eigenvalue problems, basic definitions ',
               2,
               None,
               '___sec183'),
              (' Eigenvalue problems, basic definitions ',
               2,
               None,
               '___sec184'),
              (' Eigenvalue problems, basic definitions ',
               2,
               None,
               '___sec185'),
              (' Abel-Ruffini Impossibility Theorem ', 2, None, '___sec186'),
              (' Abel-Ruffini Impossibility Theorem ', 2, None, '___sec187'),
              (' Eigenvalue problems, basic definitions ',
               2,
               None,
               '___sec188'),
              (' Eigenvalue problems, basic definitions ',
               2,
               None,
               '___sec189'),
              (' Eigenvalue problems, basic definitions ',
               2,
               None,
               '___sec190'),
              (' Eigenvalue problems, basic definitions ',
               2,
               None,
               '___sec191'),
              (' Discussion of project 2 ', 2, None, '___sec192'),
              (' Discussion of project 2 ', 2, None, '___sec193'),
              (' Discussion of project 2 ', 2, None, '___sec194'),
              (' Discussion of project 2 ', 2, None, '___sec195'),
              (' Discussion of project 2 ', 2, None, '___sec196'),
              (' Discussion of project 2 ', 2, None, '___sec197'),
              (' Discussion of project 2 ', 2, None, '___sec198'),
              (' Discussion of project 2 ', 2, None, '___sec199'),
              (' Discussion of project 2 ', 2, None, '___sec200'),
              (' Discussion of project 2 ', 2, None, '___sec201'),
              (' Discussion of project 2 ', 2, None, '___sec202'),
              (' Discussion of project 2 ', 2, None, '___sec203'),
              (' Discussion of project 2 ', 2, None, '___sec204'),
              (' Discussion of project 2 ', 2, None, '___sec205'),
              (' Discussion of project 2 ', 2, None, '___sec206'),
              (' Discussion of project 2 ', 2, None, '___sec207'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec208'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec209'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec210'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec211'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec212'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec213'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec214'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec215'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec216'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec217'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec218'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec219'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec220'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec221'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec222'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec223'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec224'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec225'),
              (" Discussion of Jacobi's method for eigenvalues ",
               2,
               None,
               '___sec226'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec227'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec228'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec229'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec230'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec231'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec232'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec233'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec234'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec235'),
              (" Discussion of Householder's method for eigenvalues ",
               2,
               None,
               '___sec236')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->


<title>Slides from FYS3150/4150 Lectures</title>

<center><h1>Slides from FYS3150/4150 Lectures</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>


<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics and Center of Mathematics for Applications, University of Oslo</b></center>
<center>[2] <b>National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<p>
<center><h4>Sep 10, 2014</h4></center> <!-- date -->
<p>
<!-- !split -->

<h1>Week 34  <a name="___sec0"></a></h1>

<!-- !split -->

<h2>Overview of week 34  <a name="___sec1"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>

<ul>
  <li> Monday: First lecture: Presentation of the course, aims and content</li>
  <li> Monday: Second Lecture: Introduction to C++ programming and numerical precision.</li>
  <li> Tuesday: Numerical precision and C++ programming, continued and exercises for first week</li>
  <li> Numerical differentiation and loss of numerical precision (chapter 3 lecture notes)</li>
  <li> Computer lab: Thursday and Friday. First time: Thursday and Friday this week, Presentation of hardware and software at room FV329 first hour of every labgroup and solution of first simple exercises.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Lectures and ComputerLab  <a name="___sec2"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>

<ul>
  <li> Lectures: Monday (4.15pm-6pm) and Tuesday (12.15pm-2pm) only this and next week. Thereafter weeks 38 and 39, and weeks 43 and 44 and finally weeks 47 and 48.</li>
  <li> Weekly reading assignments needed to solve projects.</li>
  <li> First hour of each lab session used to discuss technicalities, address questions etc linked with projects.</li>
  <li> Detailed lecture notes, exercises, all programs presented, projects etc can be found at the homepage of the course.</li>
  <li> Computerlab: Thursday (9am-7pm) and Friday (9am-7pm) room FV329.</li>
  <li> Weekly plans and all other information are on the official webpage.</li>
  <li> Final written exam December 12, 9am (four hours).</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Course Format  <a name="___sec3"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>

<ul>
  <li> Several computer exercises, 5 compulsory projects. Electronic reports only.</li>
  <li> Evaluation and grading: The last project (50% of final grade) and a  final written exam (50% of final grade). Final written exam December 12.</li>
  <li> The computer lab (room FV329)consists of 16 Linux PCs, but many prefer own laptops. C/C++ is the default programming language, but Fortran2008 and Python are also used. All source codes discussed during the lectures can be found at the webpage of the course. We recommend either C/C++, Fortran2008 or Python as languages.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>ComputerLab  <a name="___sec4"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>

<p>
<table border="1">
<thead>
<tr><th align="center">                day                </th> <th align="center">              teacher              </th> </tr>
</thead>
<tbody>
<tr><td align="left">   Thursday 9am-1pm                       </td> <td align="left">   Anders, Morten L., H&#229;vard, MHJ    </td> </tr>
<tr><td align="left">   Thursday 1pm-5pm                       </td> <td align="left">   Anders, Morten L., H&#229;vard, MHJ    </td> </tr>
<tr><td align="left">   Friday 9am-1pm                         </td> <td align="left">   Anders, Morten L., H&#229;vard, MHJ    </td> </tr>
<tr><td align="left">   Friday 1pm-5pm                         </td> <td align="left">   Anders, Morten L., H&#229;vard, MHJ    </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>Topics covered in this course  <a name="___sec5"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>

<ul>
  <li> Numerical precision and intro to C++ programming</li>
  <li> Numerical derivation and integration</li>
  <li> Random numbers and Monte Carlo integration</li>
  <li> Monte Carlo methods in statistical physics</li>
  <li> Quantum Monte Carlo methods</li>
  <li> Linear algebra and eigenvalue problems</li>
  <li> Non-linear equations and roots of polynomials</li>
  <li> Ordinary differential equations</li>
  <li> Partial differential equations</li>
  <li> Parallelization of codes</li>
  <li> Programming av GPUs (optional)</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Syllabus FYS3150  <a name="___sec6"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Linear algebra and eigenvalue problems, chapters 6 and 7.</b>

<ul>
  <li> Know Gaussian elimination and LU decomposition</li>
  <li> How to solve linear equations</li>
  <li> How to obtain the inverse and the determinant of a real symmetric matrix</li>
  <li> Cholesky and tridiagonal matrix decomposition</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Syllabus FYS3150  <a name="___sec7"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Linear algebra and eigenvalue problems, chapters 6 and 7.</b>

<ul>
  <li> Householder's tridiagonalization technique and finding eigenvalues based on this</li>
  <li> Jacobi's method for finding eigenvalues</li>
  <li> Singular value decomposition</li>
  <li> Qubic Spline interpolation</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Syllabus FYS3150  <a name="___sec8"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Numerical integration, standard methods and Monte Carlo methods (chapters 4 and 11).</b>

<ul>
  <li> Trapezoidal, rectangle and Simpson's rules</li>
  <li> Gaussian quadrature, emphasis on Legendre polynomials, but you need to know about other polynomials as well.</li>
  <li> Brute force Monte Carlo integration</li>
  <li> Random numbers (simplest algo, ran0) and probability distribution functions, expectation values</li>
  <li> Improved Monte Carlo integration and importance sampling.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Syllabus FYS3150  <a name="___sec9"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Monte Carlo methods in physics (chapters 12, 13, and 14).</b>

<ul>
  <li> Random walks and Markov chains and relation with diffusion equation</li>
  <li> Metropolis algorithm, detailed balance and ergodicity</li>
  <li> Simple spin systems and phase transitions</li>
  <li> Variational Monte Carlo</li>
  <li> How to construct trial wave functions for quantum systems</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Syllabus FYS3150  <a name="___sec10"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Ordinary differential equations (chapters 8 and 9).</b>

<ul>
  <li> Euler's method and improved Euler's method, truncation errors</li>
  <li> Runge Kutta methods, 2nd and 4th order, truncation errors</li>
  <li> How to implement a second-order differential equation, both linear and non-linear. How to make your equations dimensionless.</li>
  <li> Boundary value problems, shooting and matching method (chap 9).</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Syllabus FYS3150  <a name="___sec11"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Partial differential equations, chapter 10.</b>

<ul>
  <li> Set up diffusion, Poisson and wave equations up to 2 spatial dimensions and time</li>
  <li> Set up the mathematical model and algorithms for these equations, with boundary and initial conditions. Their stability conditions.</li>
  <li> Explicit, implicit and Crank-Nicolson schemes, and how to solve them. Remember that they result in triangular matrices.</li>
  <li> How to compute the Laplacian in Poisson's equation.</li>
  <li> How to solve the wave equation in one and two dimensions.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Overarching aims of this course  <a name="___sec12"></a></h2>

<ul>
  <li> Develop a critical approach to all steps in a project, which methods are most relevant, which natural laws and physical processes are important. Sort out initial conditions and boundary conditions etc.</li>
  <li> This means to teach you structured scientific computing, learn to structure a project.</li>
  <li> A critical understanding of central mathematical algorithms and methods from numerical analysis. In particular their limits and stability criteria.</li>
  <li> Always try to find good checks of your codes (like solutions on closed form)</li>
  <li> To enable you to develop a critical view on the mathematical model and the physics.</li>
</ul>

<!-- !split -->

<h2>And, there is nothing like a code which gives correct results!!  <a name="___sec13"></a></h2>

<center><p><img src="fig-slides1/Nebbdyr2.png" align="bottom" width=500></p></center>

<ul>
 <li> J. J. Barton and L. R. Nackman,*Scientific and Engineering C++*, Addison Wesley, 3rd edition 2000.</li>
 <li> B. Stoustrup, <em>The C++ programming language</em>, Pearson, 1997.</li>
 <li> H. P. Langtangen INF-VERK3830 <a href="http://heim.ifi.uio.no/~hpl/INF-VERK4830/" target="_self"><tt>http://heim.ifi.uio.no/~hpl/INF-VERK4830/</tt></a></li>
 <li> D. Yang, <em>C++ and Object-oriented Numeric Computing for Scientists and Engineers</em>, Springer 2000.</li>
 <li> More books reviewed at <a href="http:://www.accu.org/" target="_self"><tt>http:://www.accu.org/</tt></a> and <a href="http://www.comeaucomputing.com/booklist/" target="_self"><tt>http://www.comeaucomputing.com/booklist/</tt></a></li>
</ul>

<!-- !split -->

<h2>Other courses in Computational Science at UiO  <a name="___sec14"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Bachelor/Master/PhD Courses.</b>

<ul>
  <li> INF-MAT4350 Numerical linear algebra</li>
  <li> MAT-INF3300/3310, PDEs and Sobolev spaces I and II</li>
  <li> INF-MAT3360 PDEs</li>
  <li> INF5620 Numerical methods for PDEs, finite element method</li>
  <li> FYS4411 Computational physics II (Parallelization (MPI), object orientation, quantum mechanical systems with many interacting particles), spring semester</li>
  <li> FYS4460 Computational physics III (Parallelization (MPI), object orientation, classical statistical physics, simulation of phase transitions, spring semester</li>
  <li> INF3331 Problem solving with high-level languages (Python), fall semester</li>
  <li> INF3380 Parallel computing for problems in the Natural Sciences (mostly PDEs), spring semester</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Extremely useful tools, strongly recommended  <a name="___sec15"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>and discussed at the lab sessions the first week.</b>

<ul>
  <li> GIT for version control (see webpage)</li>
  <li> ipython notebook</li>
  <li> QTcreator for editing and mastering computational projects (for C++ codes, see webpage of course)</li>
  <li> Armadillo as a useful numerical library for C++, highly recommended</li>
  <li> Unit tests, see also webpage</li>
  <li> Devilry for handing in projects</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>A structured programming approach  <a name="___sec16"></a></h2>

<ul>
  <li> Before writing a single line, have the algorithm clarified and understood. It is crucial to have a logical structure of e.g., the flow and organization of data before one starts writing.</li>
  <li> Always try to choose the simplest algorithm. Computational speed can be improved upon later.</li>
  <li> Try to write a as clear program as possible. Such programs are easier to debug, and although it may take more time, in the long run it may save you time. If you collaborate with other people, it reduces spending time on debuging and trying to understand what the codes do. A clear program will also allow you to remember better what the program really does!</li>
</ul>

<!-- !split -->

<h2>A structured programming approach  <a name="___sec17"></a></h2>

<ul>
  <li> The planning of the program should be from top down to bottom, trying to keep the flow as linear as possible. Avoid jumping back and forth in the program. First you need to arrange the major tasks to be achieved. Then try to break the major tasks into subtasks. These can be represented by functions or subprograms. They should accomplish limited tasks and as far as possible be independent of each other. That will allow you to use them in other programs as well.</li>
  <li> Try always to find some cases where an analytical solution exists or where simple test cases can be applied. If possible, devise different algorithms for solving the same problem. If you get the same answers, you may have coded things correctly or made the same error twice or more.</li>
</ul>

<!-- !split -->

<h2>Getting Started  <a name="___sec18"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Compiling and linking, without QTcreator.</b>
In order to obtain an executable file for a C++ program, the following
instructions under Linux/Unix can be used

<p>
<!-- begin verbatim block  sys-->
<pre><code>c++ -c -Wall myprogram.cpp
c++ -o myprogram myprogram.o
</code></pre>
<!-- end verbatim block -->
where the compiler is called through the command c++/g++. The compiler
option -Wall means that a warning is issued in case of non-standard
language. The executable file is in this case <code>myprogram</code>. The option
<code>-c</code> is for compilation only, where the program is translated into machine code,
while the <code>-o</code> option links the produced object file <code>myprogram.o</code>
and produces the executable <code>myprogram</code> .

<p>
For Fortran2008 we use the Intel compiler, replace <code>c++</code> with <code>ifort</code>.
Also, to speed up the code use compile options like

<p>
<!-- begin verbatim block  sys-->
<pre><code>c++ -O3 -c -Wall myprogram.cpp
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Makefiles and simple scripts  <a name="___sec19"></a></h2>

Under Linux/Unix it is often convenient to create a
so-called makefile, which is a script which includes possible
compiling commands.

<p>
<!-- begin verbatim block  cppcod-->
<pre><code># Comment lines
# General makefile for c - choose PROG =   name of given program
# Here we define compiler option, libraries and the  target
CC= g++ -Wall
PROG= myprogram
# this is the math library in C, not necessary for C++
LIB = -lm
# Here we make the executable file
${PROG} :          ${PROG}.o
                   ${CC} ${PROG}.o ${LIB} -o ${PROG}
# whereas here we create the object file
${PROG}.o :       ${PROG}.c
                  ${CC} -c ${PROG}.c
</code></pre>
<!-- end verbatim block -->
If you name your file for <code>makefile</code>, simply type the command
<code>make</code> and Linux/Unix executes all of the statements in the above
makefile. Note that C++ files have the extension <code>.cpp</code>.

<p>
<!-- !split -->

<h2>Hello world  <a name="___sec20"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>The C encounter.</b>
Here we present first the C version.

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>/* comments in C begin like this and end with */
#include &lt;stdlib.h&gt; /* atof function */
#include &lt;math.h&gt;   /* sine function */
#include &lt;stdio.h&gt;  /* printf function */
int main (int argc, char* argv[])
{
  double r, s;        /* declare variables */
  r = atof(argv[1]);  /* convert the text argv[1] to double */
  s = sin(r);
  printf(&quot;Hello, World! sin(%g)=%g\n&quot;, r, s);
  return 0;           /* success execution of the program */

</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Hello World  <a name="___sec21"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Dissection I.</b>
The compiler must see a declaration of a function before you can
call it (the compiler checks the argument and return types).
The declaration of library functions appears
in so-called &quot;header files&quot; that must be included in the program, e.g.,

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   #include &lt;stdlib.h&gt; /* atof function */
</code></pre>
<!-- end verbatim block -->
We call three functions (atof, sin, printf)
and these are declared in three different header files.
The main program is a function called main
with a return value set to an integer, int (0 if success).
The operating system stores the return value,
and other programs/utilities can check whether
the execution was successful or not.
The command-line arguments are transferred to the main function through

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   int main (int argc, char* argv[])
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Hello World  <a name="___sec22"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Dissection II.</b>
The command-line arguments are transferred to the main function through

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   int main (int argc, char* argv[])
</code></pre>
<!-- end verbatim block -->
The integer <code>argc</code> is the no of command-line arguments, set to
one in our case, while
<code>argv</code> is a vector of strings containing the command-line arguments
with <code>argv[0]</code> containing  the name of the program
and <code>argv[1]</code>, <code>argv[2]</code>, ... are the command-line args, i.e., the number of
lines of input to the program.
Here we define floating points, see also below,
through the keywords <code>float</code> for single precision real numbers and
<code>double</code> for double precision. The function
<code>atof</code> transforms a text (<code>argv[1]</code>) to a float.
The sine function is declared in math.h, a library which
is not automatically included and needs to be linked when computing
an executable file.

<p>
With the command <code>printf</code> we obtain a formatted printout.
The <code>printf</code> syntax is used for formatting output
in many C-inspired languages (Perl, Python, Awk, partly C++).
</div>


<p>
<!-- !split -->

<h2>Hello World  <a name="___sec23"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Now in C++.</b>
Here we present first the C++ version.

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>// A comment line begins like this in C++ programs
// Standard ANSI-C++ include files
using namespace std
#include &lt;iostream&gt;  // input and output
int main (int argc, char* argv[])
{
  // convert the text argv[1] to double using atof:
  double r = atof(argv[1]);
  double s = sin(r);
  cout &lt;&lt; &quot;Hello, World! sin(&quot; &lt;&lt; r &lt;&lt; &quot;)=&quot; &lt;&lt; s &lt;&lt; '\n';
  // success
  return 0;
}
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>C++ Hello World  <a name="___sec24"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Dissection I.</b>
We have replaced the call to <code>printf</code> with the standard C++ function
<code>cout</code>. The header file <code><iostream.h></code> is then needed.
In addition, we don't need to
declare variables like <code>r</code> and <code>s</code>  at the beginning of the program.
I personally prefer
however to declare all variables at the beginning of a function, as this
gives <em>me</em> a feeling of greater readability.
</div>


<p>
<!-- !split -->

<h2>Brief summary  <a name="___sec25"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>C/C++ program.</b>

<ul>
  <li> A C/C++ program begins with include statements of header files (libraries,intrinsic functions etc)</li>
  <li> Functions which are used are normally defined at top (details next week)</li>
  <li> The main program is set up as an integer, it returns 0 (everything correct) or 1 (something went wrong)</li>
  <li> Standard <code>if</code>, <code>while</code> and <code>for</code> statements as in Java, Fortran, Python...</li>
  <li> Integers have a very limited range.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Brief summary  <a name="___sec26"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Arrays.</b>

<ul>
  <li> A C/C++ array begins by indexing at 0!</li>
  <li> Array allocations are done by size, not by the final index value.If you allocate an array with 10 elements, you should index them from \( 0,1,\dots, 9 \).</li>
  <li> Initialize always an array before a computation.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Serious problems and representation of numbers  <a name="___sec27"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Integer and Real Numbers.</b>

<ul>
  <li> Overflow</li>
  <li> Underflow</li>
  <li> Roundoff errors</li>
  <li> Loss of precision</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Limits, you must declare variables  <a name="___sec28"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>C++ and Fortran declarations.</b>

<p>
<table border="1">
<thead>
<tr><th align="center">            type in C/C++ and Fortran2008             </th> <th align="center">                         bits                         </th> <th align="center">                        range                         </th> </tr>
</thead>
<tbody>
<tr><td align="left">   <code>int/INTEGER</code> (2)                              </td> <td align="center">   16                                                        </td> <td align="left">   -32768 to 32767                                           </td> </tr>
<tr><td align="left">   <code>unsigned int</code>                                 </td> <td align="center">   16                                                        </td> <td align="left">   0 to 65535                                                </td> </tr>
<tr><td align="left">   <code>signed int</code>                                   </td> <td align="center">   16                                                        </td> <td align="left">   -32768 to 32767                                           </td> </tr>
<tr><td align="left">   <code>short int</code>                                    </td> <td align="center">   16                                                        </td> <td align="left">   -32768 to 32767                                           </td> </tr>
<tr><td align="left">   <code>unsigned short int</code>                           </td> <td align="center">   16                                                        </td> <td align="left">   0 to 65535                                                </td> </tr>
<tr><td align="left">   <code>signed short int</code>                             </td> <td align="center">   16                                                        </td> <td align="left">   \( -32768 \) to 32767                                     </td> </tr>
<tr><td align="left">   <code>int/long int/INTEGER</code> (4)                     </td> <td align="center">   32                                                        </td> <td align="left">   -2147483648 to 2147483647                                 </td> </tr>
<tr><td align="left">   <code>signed long int</code>                              </td> <td align="center">   32                                                        </td> <td align="left">   -2147483648 to 2147483647                                 </td> </tr>
<tr><td align="left">   <code>float/REAL(4)</code>                                </td> <td align="center">   32                                                        </td> <td align="left">   \( 3.4\times 10^{-44} \) to \( 3.4\times 10^{+38} \)      </td> </tr>
<tr><td align="left">   <code>double/REAL(8)</code>                               </td> <td align="center">   64                                                        </td> <td align="left">   \( 1.7\times 10^{-322} \) to \( 1.7\times 10^{+308} \)    </td> </tr>
<tr><td align="left">   <code>long double</code>                                  </td> <td align="center">   64                                                        </td> <td align="left">   \( 1.7\times 10^{-322} \) to \( 1.7\times 10^{+308} \)    </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>From decimal to binary representation  <a name="___sec29"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>How to do it.</b>
$$
  a_n2^n+a_{n-1}2^{n-1}  +a_{n-2}2^{n-2}  +\dots +a_{0}2^{0}.
$$


<p>
In binary notation we have thus \( (417)_{10} =(110110001)_2 \)
since we have

$$
(110100001)_2
=1\times2^8+1\times 2^{7}+0\times 2^{6}+1\times 2^{5}+0\times 2^{4}+0\times 2^{3}+0\times 2^{2}+0\times 2^{2}+0\times 2^{1}+1\times 2^{0}.
$$

To see this, we have performed the following divisions by 2

<p>
<table border="1">
<tr></tr>
<tbody>
<tr><td align="center">   417/2=208                          </td> <td align="center">   remainder 1                        </td> <td align="center">   coefficient of \( 2^{0} \) is 1    </td> </tr>
<tr><td align="center">   208/2=104                          </td> <td align="center">   remainder 0                        </td> <td align="center">   coefficient of \( 2^{1} \) is 0    </td> </tr>
<tr><td align="center">   104/2=52                           </td> <td align="center">   remainder 0                        </td> <td align="center">   coefficient of \( 2^{2} \) is 0    </td> </tr>
<tr><td align="center">   52/2=26                            </td> <td align="center">   remainder 0                        </td> <td align="center">   coefficient of \( 2^{3} \) is 0    </td> </tr>
<tr><td align="center">   26/2=13                            </td> <td align="center">   remainder 1                        </td> <td align="center">   coefficient of \( 2^{4} \) is 0    </td> </tr>
<tr><td align="center">   13/2= 6                            </td> <td align="center">   remainder 1                        </td> <td align="center">   coefficient of \( 2^{5} \) is 1    </td> </tr>
<tr><td align="center">   6/2= 3                             </td> <td align="center">   remainder 0                        </td> <td align="center">   coefficient of \( 2^{6} \) is 0    </td> </tr>
<tr><td align="center">   3/2= 1                             </td> <td align="center">   remainder 1                        </td> <td align="center">   coefficient of \( 2^{7} \) is 1    </td> </tr>
<tr><td align="center">   1/2= 0                             </td> <td align="center">   remainder 1                        </td> <td align="center">   coefficient of \( 2^{8} \) is 1    </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>From decimal to binary representation  <a name="___sec30"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Integer numbers.</b>
<!-- begin verbatim block  cppcod-->
<pre><code>using namespace std;
#include &lt;iostream&gt;
int main (int argc, char* argv[])
{
  int i;
  int terms[32]; // storage of a0, a1, etc, up to 32 bits
  int number = atoi(argv[1]);
  // initialise the term a0, a1 etc
  for (i=0; i &lt; 32 ; i++){ terms[i] = 0;}
  for (i=0; i &lt; 32 ; i++){
    terms[i] = number%2;
    number /= 2;

  // write out results
  cout &lt;&lt; &quot;Number of bytes used= &quot; &lt;&lt; sizeof(number) &lt;&lt; endl;
  for (i=0; i &lt; 32 ; i++){
    cout &lt;&lt; &quot; Term nr: &quot; &lt;&lt; i &lt;&lt; &quot;Value= &quot; &lt;&lt; terms[i];
    cout &lt;&lt; endl;

  return 0;

</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>From decimal to binary representation  <a name="___sec31"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Integer numbers, Fortran.</b>
<!-- begin verbatim block  fcod-->
<pre><code>PROGRAM binary_integer
IMPLICIT NONE
  INTEGER  i, number, terms(0:31) ! storage of a0, a1, etc, up to 32 bits

  WRITE(*,*) 'Give a number to transform to binary notation'
  READ(*,*) number
! Initialise the terms a0, a1 etc
  terms = 0
! Fortran takes only integer loop variables
  DO i=0, 31
     terms(i) = MOD(number,2)
     number = number/2
  ENDDO
! write out results
  WRITE(*,*) 'Binary representation '
  DO i=0, 31
    WRITE(*,*)' Term nr and value', i, terms(i)
  ENDDO

END PROGRAM binary_integer
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Integer Numbers  <a name="___sec32"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Possible Overflow for Integers.</b>
<!-- begin verbatim block  cppcod-->
<pre><code>// A comment line begins like this in C++ programs
// Program to calculate 2**n
// Standard ANSI-C++ include files */
using namespace std
#include &lt;iostream&gt;
#include &lt;cmath&gt;
int main()
{
   int  int1, int2, int3;
// print to screen
   cout &lt;&lt; &quot;Read in the exponential N for 2^N =\n&quot;;
// read from screen
   cin &gt;&gt; int2;
   int1 = (int) pow(2., (double) int2);
   cout &lt;&lt; &quot; 2^N * 2^N = &quot; &lt;&lt; int1*int1 &lt;&lt; &quot;\n&quot;;
   int3 = int1 - 1;
   cout &lt;&lt; &quot; 2^N*(2^N - 1) = &quot; &lt;&lt; int1 * int3  &lt;&lt; &quot;\n&quot;;
   cout &lt;&lt; &quot; 2^N- 1 = &quot; &lt;&lt; int3  &lt;&lt; &quot;\n&quot;;
   return 0;

// End: program main()
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Loss of Precision  <a name="___sec33"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Machine Numbers.</b>
In the decimal system we would write a number like \( 9.90625 \)
in what is called the normalized scientific notation.

$$
  9.90625=0.990625\times 10^{1},
$$

and a real non-zero number could be generalized as

$$
\begin{equation}
    x=\pm r\times 10^{{\mbox{n}}},
\end{equation}
$$

with \( r \) a number in the range \( 1/10 \le r < 1 \).
In a similar way we can use represent a binary number in
scientific notation as

$$
\begin{equation}
    x=\pm q\times 2^{{\mbox{m}}},
\end{equation}
$$

with \( q \) a number in the range \( 1/2 \le q < 1 \).
This means that the mantissa of a binary number would be represented by
the general formula

$$
\begin{equation}
(0.a_{-1}a_{-2}\dots a_{-n})_2=a_{-1}\times 2^{-1}
+a_{-2}\times 2^{-2}+\dots+a_{-n}\times 2^{-n}.
\end{equation}
$$
</div>


<p>
<!-- !split -->

<h2>Loss of Precision  <a name="___sec34"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Machine Numbers.</b>
In a typical computer, floating-point numbers are represented
in the way described above, but with certain restrictions
on \( q \) and \( m \) imposed by the available word length.
In the machine, our
number \( x \) is represented as

$$
\begin{equation}
    x=(-1)^s\times {\mbox{mantissa}}\times 2^{{\mbox{exponent}}},
\end{equation}
$$


<p>
where \( s \) is the sign bit, and the exponent gives the available range.
With a single-precision word, 32 bits, 8 bits would typically be reserved
for the exponent,  1 bit for the sign and 23 for the mantissa.
</div>


<p>
<!-- !split -->

<h2>Loss of Precision  <a name="___sec35"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Machine Numbers.</b>
A modification of the scientific notation for binary numbers is to
require that the leading binary digit 1 appears to the left of the binary point.
In this case the representation of the mantissa \( q \) would be
\( (1.f)_2 \) and $ 1 \le q < 2$. This form is rather useful when storing
binary numbers in a computer word, since we can always assume that the leading
bit 1 is there. One bit of space can then be saved meaning that a 23 bits
mantissa has actually 24 bits. This means explicitely that a binary number with 23 bits
for the mantissa reads

$$
\begin{equation}
(1.a_{-1}a_{-2}\dots a_{-23})_2=1\times 2^0+a_{-1}\times 2^{-1}+
+a_{-2}\times 2^{-2}+\dots+a_{-23}\times 2^{-23}.
\end{equation}
$$

As an example, consider the 32 bits binary number

$$
(10111110111101000000000000000000)_2,
$$

where the first bit is reserved for the sign, 1 in this case yielding a
negative sign. The exponent \( m \) is given by the next 8 binary numbers
\( 01111101 \) resulting in 125 in the decimal system.
</div>


<p>
<!-- !split -->

<h2>Loss of Precision  <a name="___sec36"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Machine Numbers.</b>
However, since the
exponent has eight bits, this means it has  \( 2^8-1=255 \) possible numbers in the interval
\( -128 \le m \le 127 \), our final
exponent is \( 125-127=-2 \) resulting in \( 2^{-2} \).
Inserting the sign and the mantissa yields the final number in the decimal representation as

$$
 -2^{-2}\left(1\times 2^0+1\times 2^{-1}+
1\times 2^{-2}+1\times 2^{-3}+0\times 2^{-4}+1\times 2^{-5}\right)=$$


$$
(-0.4765625)_{10}.
$$

In this case we have an exact machine representation with 32 bits (actually, we need less than
23 bits for the mantissa).

<p>
If our number \( x \) can be exactly represented in the machine, we call
\( x \) a machine number. Unfortunately, most numbers cannot  and are thereby
only approximated in the machine. When such a number occurs as the result
of reading some input data or of a computation, an inevitable error
will arise in representing it as accurately as possible by
a machine number.
</div>


<p>
<!-- !split -->

<h2>Loss of Precision  <a name="___sec37"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Machine Numbers.</b>
A floating number x, labelled \( fl(x) \) will therefore always be represented as

$$
\begin{equation}
  fl(x) = x(1\pm \epsilon_x),
\end{equation}
$$

with \( x \) the exact number and the error \( |\epsilon_x| \le |\epsilon_M| \), where
\( \epsilon_M \) is the precision assigned. A number like \( 1/10 \) has no exact binary representation
with single or double precision. Since the mantissa

$$
\left(1.a_{-1}a_{-2}\dots a_{-n}\right)_2
$$

is always truncated at some stage \( n \) due to its limited number of bits, there is only a
limited number of real binary numbers. The spacing between every real binary number is given by the
chosen machine precision.
For a 32 bit words this number is approximately
$ \epsilon_M \sim 10^{-7}$ and for double precision (64 bits) we have
$ \epsilon_M \sim 10^{-16}$, or in terms of a binary base
as \( 2^{-23} \) and \( 2^{-52} \) for single and double precision, respectively.
</div>


<p>
<!-- !split -->

<h2>Loss of Precision  <a name="___sec38"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Machine Numbers.</b>
In the machine a number is represented as

$$
\begin{equation}
  fl(x)= x(1+\epsilon)
\end{equation}
$$


<p>
where \( |\epsilon| \leq \epsilon_M \) and \( \epsilon \) is given by the
specified precision, \( 10^{-7} \) for single and \( 10^{-16} \) for double
precision, respectively.
\( \epsilon_M \) is the given precision.
In case of a subtraction \( a=b-c \), we have

$$
\begin{equation}
   fl(a)=fl(b)-fl(c) = a(1+\epsilon_a),
\end{equation}
$$

or

$$
\begin{equation}
   fl(a)=b(1+\epsilon_b)-c(1+\epsilon_c),
\end{equation}
$$


<p>
meaning that

$$
\begin{equation}
   fl(a)/a=1+\epsilon_b\frac{b}{a}- \epsilon_c\frac{c}{a},
\end{equation}
$$


<p>
and if \( b\approx c \) we see that there is a potential for an increased
error in \( fl(a) \).
</div>


<p>
<!-- !split -->

<h2>Loss of Precision  <a name="___sec39"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Machine Numbers.</b>
Define
the absolute error as

$$
\begin{equation}
   |fl(a)-a|,
\end{equation}
$$

whereas the relative error is

$$
\begin{equation}
   \frac{ |fl(a)-a|}{a} \le \epsilon_a.
\end{equation}
$$

The above subraction is thus

$$
\begin{equation}
   \frac{ |fl(a)-a|}{a}=\frac{ |fl(b)-fl(c)-(b-c)|}{a},
\end{equation}
$$

yielding

$$
\begin{equation}
   \frac{ |fl(a)-a|}{a}=\frac{ |b\epsilon_b- c\epsilon_c|}{a}.
\end{equation}
$$

The relative error
is the quantity of interest in scientific work. Information about the
absolute error is normally of little use in the absence of the magnitude
of the quantity being measured.
</div>


<p>
<!-- !split -->

<h2>Loss of numerical precision  <a name="___sec40"></a></h2>

Suppose we wish to evaluate the function

$$
   f(x)=\frac{1-\cos(x)}{\sin(x)},
$$

for small values of \( x \). Five leading digits. If we multiply the denominator and numerator
with \( 1+\cos(x) \) we obtain the equivalent expression

$$
   f(x)=\frac{\sin(x)}{1+\cos(x)}.
$$


<p>
If we now choose \( x=0.007 \) (in radians) our choice of precision results in

$$
   \sin(0.007)\approx 0.69999\times 10^{-2},
$$

and

$$
   \cos(0.007)\approx 0.99998.
$$


<p>
<!-- !split -->

<h2>Loss of numerical precision  <a name="___sec41"></a></h2>

The first expression for \( f(x) \) results in

$$
   f(x)=\frac{1-0.99998}{0.69999\times 10^{-2}}=\frac{0.2\times 10^{-4}}{0.69999\times 10^{-2}}=0.28572\times 10^{-2},
$$

while the second expression results in

$$
   f(x)=\frac{0.69999\times 10^{-2}}{1+0.99998}=
\frac{0.69999\times 10^{-2}}{1.99998}=0.35000\times 10^{-2},
$$

which is also the exact result. In the first expression, due to our
choice of precision, we have
only one relevant digit in the numerator, after the
subtraction. This leads to a loss of precision and a wrong result due to
a cancellation of two nearly equal numbers.
If we had chosen a precision of six leading digits, both expressions
yield the same answer.

<p>
<!-- !split -->

<h2>Loss of numerical precision  <a name="___sec42"></a></h2>

If we were to evaluate \( x\sim \pi \), then the second expression for \( f(x) \)
can lead to potential losses of precision due to cancellations of nearly
equal numbers.

<p>
This simple example demonstrates  the loss of numerical precision due
to roundoff errors, where the number of leading digits is lost
in a subtraction of two near equal numbers.
The lesson to be drawn is that we cannot blindly compute a function.
We will always need to carefully analyze our algorithm in the search for
potential pitfalls. There is no magic recipe however, the only guideline
is an understanding of the fact that a machine cannot represent
correctly <em>all</em> numbers.

<p>
<!-- !split -->

<h2>Loss of precision can cuae serious problems  <a name="___sec43"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Real Numbers.</b>

<ul>
  <li> <b>Overflow</b>: When the positive exponent exceeds the max value, e.g., 308 for <code>DOUBLE PRECISION</code> (64 bits). Under such circumstances the program will terminate and some compilers may give you the warning <code>OVERFLOW</code>.</li>
  <li> <b>Underflow</b>: When the negative exponent becomes smaller than the min value, e.g., -308 for <code>DOUBLE PRECISION</code>. Normally, the variable is then set to zero and the program continues. Other compilers (or compiler options) may warn you with the <code>UNDERFLOW</code> message and the program terminates.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Loss of precision, real numbers  <a name="___sec44"></a></h2>

<h3>Roundoff errors  <a name="___sec45"></a></h3>

A floating point number like

$$
\begin{equation}
   x= 1.234567891112131468 = 0.1234567891112131468\times 10^{1}
\end{equation}
$$

may be stored in the following way. The exponent is  small
and is stored in full precision. However,
the mantissa is not stored fully. In double precision (64 bits), digits
beyond the
15th are lost since the mantissa is normally stored in two words,
one which is the most significant one representing
123456 and the least significant one containing 789111213. The digits
beyond 3 are lost. Clearly, if we are summing alternating series
with large numbers, subtractions between two large numbers may lead
to roundoff errors, since not all relevant digits are kept.
This leads eventually to the next problem, namely

<p>
<!-- !split -->

<h2>More on loss of precision  <a name="___sec46"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Real Numbers.</b>

<ul>
  <li> <b>Loss of precision</b>: When one has to e.g., multiply two large numbers where one suspects that the outcome may be beyond the bonds imposed by the variable declaration, one could represent the numbers by logarithms, or rewrite the equations to be solved in terms of dimensionless variables. When dealing with problems in e.g., particle physics or nuclear physics where distance is measured in fm (\( 10^{-15} \) m), it can be quite convenient to redefine the variables for distance in terms of a dimensionless variable of the order of unity. To give an example, suppose you work with single precision and wish to perform the addition \( 1+10^{-8} \). In this case, the information containing in \( 10^{-8} \) is simply lost in the addition. Typically, when performing the addition, the computer equates first the exponents of the two numbers to be added. For \( 10^{-8} \) this has however catastrophic consequences since in order to obtain an exponent equal to \( 10^0 \), bits in the mantissa are shifted to the right. At the end, all bits in the mantissa are zeros.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>A problematic case  <a name="___sec47"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Three ways of computing \( e^{-x} \).</b>
Brute force:

$$\exp{(-x)}=\sum_{n=0}^{\infty}(-1)^n\frac{x^n}{n!}$$


<p>
Recursion relation for

$$ \exp{(-x)}=\sum_{n=0}^{\infty}s_n=\sum_{n=0}^{\infty}(-1)^n\frac{x^n}{n!} $$


$$ s_n=-s_{n-1}\frac{x}{n}, $$


$$ \exp{(x)}=\sum_{n=0}^{\infty}s_n $$


$$ \exp{(-x)}=\frac{1}{\exp{(x)}} $$
</div>


<p>
<!-- !split -->

<h2>Program to compute \( \exp{(-x)} \)  <a name="___sec48"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Brute Force.</b>
<!-- begin verbatim block  cppcod-->
<pre><code>// Program to calculate function exp(-x)
// using straightforward summation with differing  precision
using namespace std
#include &lt;iostream&gt;
#include &lt;cmath&gt;
// type float:  32 bits precision
// type double: 64 bits precision
#define   TYPE          double
#define   PHASE(a)      (1 - 2 * (abs(a) % 2))
#define   TRUNCATION    1.0E-10
// function declaration
TYPE factorial(int);
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Program to compute \( \exp{(-x)} \)  <a name="___sec49"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Still Brute Force.</b>
<!-- begin verbatim block  cppcod-->
<pre><code>int main()
{
   int   n;
   TYPE  x, term, sum;
   for(x = 0.0; x &lt; 100.0; x += 10.0)  {
     sum  = 0.0;                //initialization
     n    = 0;
     term = 1;
     while(fabs(term) &gt; TRUNCATION)  {
         term =  PHASE(n) * (TYPE) pow((TYPE) x,(TYPE) n)
                / factorial(n);
         sum += term;
         n++;
     }  // end of while() loop
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Program to compute \( \exp{(-x)} \)  <a name="___sec50"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Oh, it never ends!</b>
<!-- begin verbatim block  cppcod-->
<pre><code>      printf(&quot;\nx = %4.1f   exp = %12.5E  series = %12.5E
              number of terms = %d&quot;,
              x, exp(-x), sum, n);
   } // end of for() loop

   printf(&quot;\n&quot;);           // a final line shift on output
   return 0;
} // End: function main()
//     The function factorial()
//     calculates and returns n!
TYPE factorial(int n)
{
   int  loop;
   TYPE fac;
   for(loop = 1, fac = 1.0; loop &lt;= n; loop++)  {
      fac *= loop;

   return fac;
} // End: function factorial()
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Results \( \exp{(-x)} \)  <a name="___sec51"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>What is going on?</b>

<p>
<table border="1">
<thead>
<tr><td align="center">         \( x \)         </td> <td align="center">     \( \exp{(-x)} \)    </td> <th align="center">          Series         </th> <th align="center">Number of terms in series</th> </tr>
</thead>
<tbody>
<tr><td align="center">   0.0                          </td> <td align="center">   0.100000E+01                 </td> <td align="center">   0.100000E+01                 </td> <td align="center">   1                            </td> </tr>
<tr><td align="center">   10.0                         </td> <td align="center">   0.453999E-04                 </td> <td align="center">   0.453999E-04                 </td> <td align="center">   44                           </td> </tr>
<tr><td align="center">   20.0                         </td> <td align="center">   0.206115E-08                 </td> <td align="center">   0.487460E-08                 </td> <td align="center">   72                           </td> </tr>
<tr><td align="center">   30.0                         </td> <td align="center">   0.935762E-13                 </td> <td align="center">   -0.342134E-04                </td> <td align="center">   100                          </td> </tr>
<tr><td align="center">   40.0                         </td> <td align="center">   0.424835E-17                 </td> <td align="center">   -0.221033E+01                </td> <td align="center">   127                          </td> </tr>
<tr><td align="center">   50.0                         </td> <td align="center">   0.192875E-21                 </td> <td align="center">   -0.833851E+05                </td> <td align="center">   155                          </td> </tr>
<tr><td align="center">   60.0                         </td> <td align="center">   0.875651E-26                 </td> <td align="center">   -0.850381E+09                </td> <td align="center">   171                          </td> </tr>
<tr><td align="center">   70.0                         </td> <td align="center">   0.397545E-30                 </td> <td align="center">   NaN                          </td> <td align="center">   171                          </td> </tr>
<tr><td align="center">   80.0                         </td> <td align="center">   0.180485E-34                 </td> <td align="center">   NaN                          </td> <td align="center">   171                          </td> </tr>
<tr><td align="center">   90.0                         </td> <td align="center">   0.819401E-39                 </td> <td align="center">   NaN                          </td> <td align="center">   171                          </td> </tr>
<tr><td align="center">   100.0                        </td> <td align="center">   0.372008E-43                 </td> <td align="center">   NaN                          </td> <td align="center">   171                          </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>Program to compute \( \exp{(-x)} \)  <a name="___sec52"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
<!-- begin verbatim block  cppcod-->
<pre><code>// program to compute exp(-x) without exponentials
using namespace std
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#define  TRUNCATION     1.0E-10

int main()
{
   int       loop, n;
   double    x, term, sum;
   for(loop = 0; loop &lt;= 100; loop += 10)
   {
     x    = (double) loop;          // initialization
     sum  = 1.0;
     term = 1;
     n    = 1;
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Program to compute \( \exp{(-x)} \)  <a name="___sec53"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Last statements.</b>
<!-- begin verbatim block  cppcod-->
<pre><code>     while(fabs(term) &gt; TRUNCATION)
       {
	 term *= -x/((double) n);
	 sum  += term;
	 n++;
       } // end while loop
     cout &lt;&lt; &quot;x = &quot; &lt;&lt; x   &lt;&lt; &quot; exp = &quot; &lt;&lt; exp(-x) &lt;&lt;&quot;series = &quot;
          &lt;&lt; sum  &lt;&lt; &quot; number of terms =&quot; &lt;&lt; n &lt;&lt; &quot;\n&quot;;
   } // end of for() loop

   cout &lt;&lt; &quot;\n&quot;;           // a final line shift on output

}  /*    End: function main() */
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Results \( \exp{(-x)} \)  <a name="___sec54"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>More Problems.</b>

<p>
<table border="1">
<thead>
<tr><td align="center">         \( x \)         </td> <td align="center">     \( \exp{(-x)} \)    </td> <th align="center">          Series         </th> <th align="center">Number of terms in series</th> </tr>
</thead>
<tbody>
<tr><td align="center">   0.000000                     </td> <td align="center">   0.10000000E+01               </td> <td align="center">   0.10000000E+01               </td> <td align="center">   1                            </td> </tr>
<tr><td align="center">   10.000000                    </td> <td align="center">   0.45399900E-04               </td> <td align="center">   0.45399900E-04               </td> <td align="center">   44                           </td> </tr>
<tr><td align="center">   20.000000                    </td> <td align="center">   0.20611536E-08               </td> <td align="center">   0.56385075E-08               </td> <td align="center">   72                           </td> </tr>
<tr><td align="center">   30.000000                    </td> <td align="center">   0.93576230E-13               </td> <td align="center">   -0.30668111E-04              </td> <td align="center">   100                          </td> </tr>
<tr><td align="center">   40.000000                    </td> <td align="center">   0.42483543E-17               </td> <td align="center">   -0.31657319E+01              </td> <td align="center">   127                          </td> </tr>
<tr><td align="center">   50.000000                    </td> <td align="center">   0.19287498E-21               </td> <td align="center">   0.11072933E+05               </td> <td align="center">   155                          </td> </tr>
<tr><td align="center">   60.000000                    </td> <td align="center">   0.87565108E-26               </td> <td align="center">   -0.33516811E+09              </td> <td align="center">   182                          </td> </tr>
<tr><td align="center">   70.000000                    </td> <td align="center">   0.39754497E-30               </td> <td align="center">   -0.32979605E+14              </td> <td align="center">   209                          </td> </tr>
<tr><td align="center">   80.000000                    </td> <td align="center">   0.18048514E-34               </td> <td align="center">   0.91805682E+17               </td> <td align="center">   237                          </td> </tr>
<tr><td align="center">   90.000000                    </td> <td align="center">   0.81940126E-39               </td> <td align="center">   -0.50516254E+22              </td> <td align="center">   264                          </td> </tr>
<tr><td align="center">   100.000000                   </td> <td align="center">   0.37200760E-43               </td> <td align="center">   -0.29137556E+26              </td> <td align="center">   291                          </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>Most used formula for derivatives  <a name="___sec55"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>3 point formulae.</b>
First derivative  (\( f_0 = f(x_0) \), \( f_{-h}=f(x_0-h) \) and \( f_{+h}=f(x_0+h) \)

$$
   \frac{f_h-f_{-h}}{2h}=f'_0+\sum_{j=1}^{\infty}\frac{f_0^{(2j+1)}}{(2j+1)!}h^{2j}.
$$

Second derivative

$$
 \frac{ f_h -2f_0 +f_{-h}}{h^2}=f_0''+2\sum_{j=1}^{\infty}\frac{f_0^{(2j+2)}}{(2j+2)!}h^{2j}.
$$
</div>


<p>
<!-- !split -->

<h2>Error Analysis  <a name="___sec56"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
$$
   \epsilon=log_{10}\left(\left|\frac{f''_{\mbox{computed}}-f''_{\mbox{exact}}}
                 {f''_{\mbox{exact}}}\right|\right),
$$


$$
   \epsilon_{\mbox{tot}}=\epsilon_{\mbox{approx}}+\epsilon_{\mbox{ro}}.
$$


<p>
For the computed second derivative  we have

$$
 f_0''=\frac{ f_h -2f_0 +f_{-h}}{h^2}-2\sum_{j=1}^{\infty}\frac{f_0^{(2j+2)}}{(2j+2)!}h^{2j},
$$

and the truncation or approximation error goes like

$$
  \epsilon_{\mbox{approx}}\approx \frac{f_0^{(4)}}{12}h^{2}.
$$
</div>


<p>
<!-- !split -->

<h2>Error Analysis  <a name="___sec57"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
If we were not to worry about loss of precision, we could in principle
make \( h \) as small as possible.
However, due to the computed expression in the above program example

$$
 f_0''=\frac{ f_h -2f_0 +f_{-h}}{h^2}=\frac{ (f_h -f_0) +(f_{-h}-f_0)}{h^2},
$$

we reach fairly quickly a limit for where loss of precision due to the subtraction
of two nearly equal numbers becomes crucial.

<p>
If \( (f_{\pm h} -f_0) \) are very close, we have
\( (f_{\pm h} -f_0)\approx \epsilon_M \), where \( |\epsilon_M|\le 10^{-7} \) for single and
\( |\epsilon_M|\le 10^{-15} \) for double precision, respectively.

<p>
We have then

$$
 \left|f_0''\right|=
 \left|\frac{ (f_h -f_0) +(f_{-h}-f_0)}{h^2}\right|\le \frac{ 2 \epsilon_M}{h^2}.
$$
</div>


<p>
<!-- !split -->

<h2>Error Analysis  <a name="___sec58"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
Our total error becomes

$$
   \left|\epsilon_{\mbox{tot}}\right|\le  \frac{2 \epsilon_M}{h^2} +
                          \frac{f_0^{(4)}}{12}h^{2}.
    \label{eq:experror}
$$

It is then natural to ask which value of \( h \) yields the smallest
total error. Taking the derivative of \( \left|\epsilon_{\mbox{tot}}\right| \)
with respect to \( h \) results in

$$
   h= \left(\frac{ 24\epsilon_M}{f_0^{(4)}}\right)^{1/4}.
$$

With double precision and \( x=10 \) we obtain

$$
   h\approx 10^{-4}.
$$

Beyond this value, it is essentially the loss of numerical precision
which takes over.
</div>


<p>
<!-- !split -->

<h2>Error Analysis  <a name="___sec59"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
Due to the subtractive cancellation in the expression
for \( f'' \) there is a pronounced detoriation in accuracy as \( h \) is made smaller
and smaller.

<p>
It is instructive in this analysis to rewrite the numerator of
the computed derivative as

$$
   (f_h -f_0) +(f_{-h}-f_0)=(e^{x+h}-e^{x}) + (e^{x-h}-e^{x}),
$$

as

$$
   (f_h -f_0) +(f_{-h}-f_0)=e^x(e^{h}+e^{-h}-2),
$$

since it is the difference \( (e^{h}+e^{-h}-2) \) which causes
the loss of precision.
</div>


<p>
<!-- !split -->

<h2>Error Analysis  <a name="___sec60"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>

<p>
<table border="1">
<thead>
<tr><td align="center">     \( x \)     </td> <td align="center">   \( h=0.01 \)  </td> <td align="center">  \( h=0.001 \)  </td> <td align="center">  \( h=0.0001 \) </td> <td align="center">\( h=0.0000001 \)</td> <th align="center">      Exact      </th> </tr>
</thead>
<tbody>
<tr><td align="center">   0.0                  </td> <td align="center">   1.000008             </td> <td align="center">   1.000000             </td> <td align="center">   1.000000             </td> <td align="center">   1.010303             </td> <td align="center">   1.000000             </td> </tr>
<tr><td align="center">   1.0                  </td> <td align="center">   2.718304             </td> <td align="center">   2.718282             </td> <td align="center">   2.718282             </td> <td align="center">   2.753353             </td> <td align="center">   2.718282             </td> </tr>
<tr><td align="center">   2.0                  </td> <td align="center">   7.389118             </td> <td align="center">   7.389057             </td> <td align="center">   7.389056             </td> <td align="center">   7.283063             </td> <td align="center">   7.389056             </td> </tr>
<tr><td align="center">   3.0                  </td> <td align="center">   20.085704            </td> <td align="center">   20.085539            </td> <td align="center">   20.085537            </td> <td align="center">   20.250467            </td> <td align="center">   20.085537            </td> </tr>
<tr><td align="center">   4.0                  </td> <td align="center">   54.598605            </td> <td align="center">   54.598155            </td> <td align="center">   54.598151            </td> <td align="center">   54.711789            </td> <td align="center">   54.598150            </td> </tr>
<tr><td align="center">   5.0                  </td> <td align="center">   148.414396           </td> <td align="center">   148.413172           </td> <td align="center">   148.413161           </td> <td align="center">   150.635056           </td> <td align="center">   148.413159           </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>Error Analysis  <a name="___sec61"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
The results for \( x=10 \) are shown in the Table

<p>
<table border="1">
<thead>
<tr><td align="center">                \( h \)                </td> <td align="center">           \( e^{h}+e^{-h} \)          </td> <td align="center">          \( e^{h}+e^{-h}-2 \)         </td> </tr>
</thead>
<tbody>
<tr><td align="center">   \( 10^{-1} \)                              </td> <td align="center">   2.0100083361116070                         </td> <td align="center">   \( 1.0008336111607230\times 10^{-2} \)     </td> </tr>
<tr><td align="center">   \( 10^{-2} \)                              </td> <td align="center">   2.0001000008333358                         </td> <td align="center">   \( 1.0000083333605581\times 10^{-4} \)     </td> </tr>
<tr><td align="center">   \( 10^{-3} \)                              </td> <td align="center">   2.0000010000000836                         </td> <td align="center">   \( 1.0000000834065048\times 10^{-6} \)     </td> </tr>
<tr><td align="center">   \( 10^{-5} \)                              </td> <td align="center">   2.0000000099999999                         </td> <td align="center">   \( 1.0000000050247593\times 10^{-8} \)     </td> </tr>
<tr><td align="center">   \( 10^{-5} \)                              </td> <td align="center">   2.0000000001000000                         </td> <td align="center">   \( 9.9999897251734637\times 10^{-11} \)    </td> </tr>
<tr><td align="center">   \( 10^{-6} \)                              </td> <td align="center">   2.0000000000010001                         </td> <td align="center">   \( 9.9997787827987850\times 10^{-13} \)    </td> </tr>
<tr><td align="center">   \( 10^{-7} \)                              </td> <td align="center">   2.0000000000000098                         </td> <td align="center">   \( 9.9920072216264089\times 10^{-15} \)    </td> </tr>
<tr><td align="center">   \( 10^{-8} \)                              </td> <td align="center">   2.0000000000000000                         </td> <td align="center">   \( 0.0000000000000000\times 10^{0} \)      </td> </tr>
<tr><td align="center">   \( 10^{-9} \)                              </td> <td align="center">   2.0000000000000000                         </td> <td align="center">   \( 1.1102230246251565\times 10^{-16} \)    </td> </tr>
<tr><td align="center">   \( 10^{-10} \)                             </td> <td align="center">   2.0000000000000000                         </td> <td align="center">   \( 0.0000000000000000\times 10^{0} \)      </td> </tr>
</tbody>
</table>
<p>
</div>


<h1>Week 35  <a name="___sec62"></a></h1>

<!-- !split -->

<h2>Overview of week 35  <a name="___sec63"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>

<ul>
  <li> Monday: Repetition from last week</li>
  <li> Numerical differentiation</li>
  <li> C/C++ programming details, pointers, read/write to/from file</li>
  <li> Tuesday: Intro to linear Algebra and presentation of project 1.</li>
  <li> Matrices in C++ and Fortran2008</li>
  <li> Gaussian elimination and discussion of project 1.</li>
  <li> Computer-Lab: start project 1. Reading asssignments and preparation for project 1: sections 2.5 and 3.1 for general C++ and Fortran features. Sections 6.1-6.4 (till page 182) are relevant for project 1.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Technical Matter in C/C++: Pointers  <a name="___sec64"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
A pointer specifies where a value resides in the computer's memory (like a house number specifies where a particular family resides on a street).

<p>
A pointer points to an address not to a data container of any kind!

<p>
Simple example declarations:

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>  using namespace std; // note use of namespace
  int main()
 {
   // what are the differences?
   int var;
   cin &gt;&gt; var;
   int *p, q;
   int *s, *t;
   int * a new[var];    // dynamic memory allocation
   delete [] a;
}
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Technical Matter in C/C++: Pointer example I  <a name="___sec65"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
<!-- begin verbatim block  cppcod-->
<pre><code>using namespace std; // note use of namespace
int main()
{
  int var;
  int *p;
  p = &amp;var;
  var  = 421;
  printf(&quot;Address of integer variable var : %p\n&quot;,&amp;var);
  printf(&quot;Its value: %d\n&quot;, var);
  printf(&quot;Value of integer pointer p : %p\n&quot;,p);
  printf(&quot;The value p points at :  %d\n&quot;,*p);
  printf(&quot;Address of the pointer p : %p\n&quot;,&amp;p);
  return 0;
}
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Dissection: Pointer example I  <a name="___sec66"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Discussion.</b>

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>int main()
{
  int var;     // Define an integer variable var
  int *p;      // Define a pointer to an integer
  p = &amp;var;    // Extract the address of var
  var = 421;   // Change content of var
  printf(&quot;Address of integer variable var : %p\n&quot;, &amp;var);
  printf(&quot;Its value: %d\n&quot;, var);  // 421
  printf(&quot;Value of integer pointer p : %p\n&quot;, p);  // = &amp;var
  // The content of the variable pointed to by p is *p
  printf(&quot;The value p points at :  %d\n&quot;, *p);
  // Address where the pointer is stored in memory
  printf(&quot;Address of the pointer p : %p\n&quot;, &amp;p);
  return 0;
}
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Pointer example II  <a name="___sec67"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
<!-- begin verbatim block  cppcod-->
<pre><code>int matr[2];
int *p;
p = &amp;matr[0];
matr[0] = 321;
matr[1] = 322;
printf(&quot;\nAddress of matrix element matr[1]: %p&quot;,&amp;matr[0]);
printf(&quot;\nValue of the  matrix element  matr[1]; %d&quot;,matr[0]);
printf(&quot;\nAddress of matrix element matr[2]: %p&quot;,&amp;matr[1]);
printf(&quot;\nValue of the matrix element  matr[2]: %d\n&quot;, matr[1]);
printf(&quot;\nValue of the pointer p: %p&quot;,p);
printf(&quot;\nThe value p points to: %d&quot;,*p);
printf(&quot;\nThe value that (p+1) points to  %d\n&quot;,*(p+1));
printf(&quot;\nAddress of pointer p : %p\n&quot;,&amp;p);
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Dissection: Pointer example II  <a name="___sec68"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
<!-- begin verbatim block  cppcod-->
<pre><code>int matr[2];    // Define integer array with two elements
int *p;         // Define pointer to integer
p = &amp;matr[0];   // Point to the address of the first element in matr
matr[0] = 321;  // Change the first element
matr[1] = 322;  // Change the second element
printf(&quot;\nAddress of matrix element matr[1]: %p&quot;, &amp;matr[0]);
printf(&quot;\nValue of the  matrix element  matr[1]; %d&quot;, matr[0]);
printf(&quot;\nAddress of matrix element matr[2]: %p&quot;, &amp;matr[1]);
printf(&quot;\nValue of the matrix element  matr[2]: %d\n&quot;, matr[1]);
printf(&quot;\nValue of the pointer p: %p&quot;, p);
printf(&quot;\nThe value p points to: %d&quot;, *p);
printf(&quot;\nThe value that (p+1) points to  %d\n&quot;, *(p+1));
printf(&quot;\nAddress of pointer p : %p\n&quot;, &amp;p);
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Output of Pointer example II  <a name="___sec69"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b></b>
<!-- begin verbatim block -->
<pre><code>Address of the matrix element matr[1]: 0xbfffef70
Value of the  matrix element  matr[1]; 321
Address of the matrix element matr[2]: 0xbfffef74
Value of the matrix element  matr[2]: 322
Value of the pointer: 0xbfffef70
The value pointer points at: 321
The value that (pointer+1) points at:  322
Address of the pointer variable : 0xbfffef6c
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>File handling; C-way  <a name="___sec70"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>using namespace std;
#include &lt;iostream&gt;
int main(int argc, char *argv[])
{
  FILE *in_file, *out_file;
  if( argc &lt; 3)  {
    printf(&quot;The programs has the following structure :\n&quot;);
    printf(&quot;write in the name of the input and output files \n&quot;);
    exit(0);
  }
  in_file = fopen( argv[1], &quot;r&quot;);// returns pointer to the  input file
  if( in_file == NULL )  { // NULL means that the file is missing
    printf(&quot;Can't find the input file %s\n&quot;, argv[1]);
    exit(0);
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>File handling; C way cont.  <a name="___sec71"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code> out_file = fopen( argv[2], &quot;w&quot;); // returns a pointer to the output file
 if( out_file == NULL )  {       // can't find the file
    printf(&quot;Can't find the output file%s\n&quot;, argv[2]);
    exit(0);
  }
  fclose(in_file);
  fclose(out_file);
  return 0;
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>File handling, C++-way  <a name="___sec72"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>#include &lt;fstream&gt;

// input and output file as global variable
ofstream ofile;
ifstream ifile;
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>File handling, C++-way  <a name="___sec73"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>int main(int argc, char* argv[])
{
  char *outfilename;
  //Read in output file, abort if there are too
  //few command-line arguments
  if( argc &lt;= 1 ){
    cout &lt;&lt; &quot;Bad Usage: &quot; &lt;&lt; argv[0] &lt;&lt;
      &quot; read also output file on same line&quot; &lt;&lt; endl;
    exit(1);
  }
  else{
    outfilename=argv[1];
  }
  ofile.open(outfilename);
  .....
  ofile.close();  // close output file
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>File handling, C++-way  <a name="___sec74"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>void output(double r_min , double r_max, int max_step,
            double *d)
{
int i;
ofile &lt;&lt; &quot;RESULTS:&quot; &lt;&lt; endl;
ofile &lt;&lt; setiosflags(ios::showpoint | ios::uppercase);
ofile &lt;&lt;&quot;R_min = &quot; &lt;&lt; setw(15) &lt;&lt; setprecision(8) &lt;&lt;r_min &lt;&lt;endl;
ofile &lt;&lt;&quot;R_max = &quot; &lt;&lt; setw(15) &lt;&lt; setprecision(8) &lt;&lt;r_max &lt;&lt;endl;
ofile &lt;&lt;&quot;Number of steps = &quot; &lt;&lt; setw(15) &lt;&lt; max_step &lt;&lt; endl;
ofile &lt;&lt; &quot;Five lowest eigenvalues:&quot; &lt;&lt; endl;
for(i = 0; i &lt; 5; i++) {
    ofile &lt;&lt; setw(15) &lt;&lt; setprecision(8) &lt;&lt; d[i] &lt;&lt; endl;

}  // end of function output
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>File handling, C++-way  <a name="___sec75"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>int main(int argc, char* argv[])
{
  char *infilename;
  // Read in input file, abort if there are too
  // few command-line arguments
  if( argc &lt;= 1 ){
    cout &lt;&lt; &quot;Bad Usage: &quot; &lt;&lt; argv[0] &lt;&lt;
      &quot; read also input file on same line&quot; &lt;&lt; endl;
    exit(1);
  }
  else{
    infilename=argv[1];
  }
  ifile.open(infilename);
  ....
  ifile.close();  // close input file
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>File handling, C++-way  <a name="___sec76"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>const char* filename1 = &quot;myfile&quot;;
ifstream ifile(filename1);
string filename2 = filename1 + &quot;.out&quot;
ofstream ofile(filename2);  // new output file
ofstream ofile(filename2, ios_base::app);  // append

//      Read something from the file:

double a; int b; char c[200];
ifile &gt;&gt; a &gt;&gt; b &gt;&gt; c;  // skips white space in between

//      Can test on success of reading:

if (!(ifile &gt;&gt; a &gt;&gt; b &gt;&gt; c)) ok = 0;
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Call by value and reference  <a name="___sec77"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>int main(int argc, char argv[]) {
int  a:
int *b;
a = 10;
b = new int[10];
for (i = 0; i &lt; 10; i++) {
  b[i] = i;
}
func(a, b);
delete [] b;
return 0;
}
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Call by value and reference  <a name="___sec78"></a></h2>

Morten: Too complicated LaTeX code for computer code to be
decoded....

<p>
<!-- !split -->

<h2>Call by value and reference  <a name="___sec79"></a></h2>

<ul>
  <li> Lines 1,2: Declaration of two variables <code>a</code> and <code>b</code>. The compiler reserves two locations in memory. The size of the location depends on the type of variable. Two properties are important for these locations: the address in memory and the content in the location. The value of <code>a</code> is <code>a</code>. The address of <code>a</code> is <code>&a</code>. The value of <code>b</code> is <code>*b</code>. The address of <code>b</code> is <code>&b</code>.</li>
  <li> Line 3: The value of <code>a</code> is now 10.</li>
  <li> Line 4: Memory to store 10 integers is reserved. The address to the first location is stored in b. Address to element number 6 is given by the expression (b + 6).</li>
  <li> Line 5: All 10 elements of <code>b</code> are given values: <code>b[0] = 0</code>, <code>b[1] = 1</code>, ....., <code>b[9] = 9</code></li>
  <li> line 7: here we deallocate the variable <code>b</code>.</li>
</ul>

<!-- !split -->

<h2>Call by value and reference  <a name="___sec80"></a></h2>

<ul>
  <li> Line 6: The <code>main()</code> function calls the function <code>func()</code> and the program counter transfers to the first statement in <code>func()</code>. With respect to data the following happens. The content of <code>a</code> (= 10) and the content of <code>b</code> (a memory address) are copied to a stack (new memory location) associated with the function <code>func()</code></li>
  <li> Line 7: The variable <code>x</code> and <code>y</code> are local variables in <code>func()</code>. They have the values  <code>x = 10</code>, <code>y</code> is the address of the first element in <code>b</code> in <code>main()</code>.</li>
  <li> Line 8: The local variable <code>x</code> stored in the stack memory is changed to 17. Nothing happens with the value <code>a</code> in <code>main()</code>.</li>
</ul>

<!-- !split -->

<h2>Call by value and reference  <a name="___sec81"></a></h2>

<ul>
  <li> Line 9: The value of y is an address and the symbol <code>*y</code> means the position in memory which has this address. The value in this location is now increased by 10. This means that the value of <code>b[0]</code> in the main program is equal to 10. Thus func() has modified a value in <code>main()</code>.</li>
  <li> Line 10: This statement has the same effect as line 9 except that it modifies the element <code>b[6]</code> in <code>main()</code> by adding a value of 10 to what was there originally, namely 5.</li>
  <li> Line 11: The program counter returns to <code>main()</code>, the next expression after <code>func(a,b)</code>. All data on the stack associated with <code>func()</code> are destroyed.</li>
</ul>

<!-- !split -->

<h2>Call by value and reference  <a name="___sec82"></a></h2>

<ul>
  <li> The value of a is transferred to <code>func()</code> and stored in a new memory location called <code>x</code>. Any modification of <code>x</code> in <code>func()</code> does not affect in any way the value of <code>a</code> in <code>main()</code>. This is called <em>transfer of data by value</em>.</li>
  <li> On the other hand the next argument in <code>func()</code> is an address which is transferred to <code>func()</code>. This address can be used to modify the corresponding value in <code>main()</code>. In the C language it is expressed as a modification of the value which <code>y</code> points to, namely the first element of <code>b</code>. This is called <em>transfer of data by reference</em> and is a method to transfer data back to the calling function, in this case <code>main()</code>.</li>
</ul>

<!-- !split -->

<h2>Call by value and reference  <a name="___sec83"></a></h2>

C++ allows however the programmer to use solely call by reference
(note that call by reference is implemented as pointers).
To see the difference between C and C++, consider the following simple
examples. In C we would write

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   int n; n =8;
   func(&amp;n); /* &amp;n is a pointer to n */
   ....
   void func(int *i)
   {
     *i = 10; /* n is changed to 10 */
     ....
   }
</code></pre>
<!-- end verbatim block -->

<p>
whereas in C++ we would write

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   int n; n =8;
   func(n); // just transfer n itself
   ....
   void func(int&amp; i)
   {
     i = 10; // n is changed to 10
     ....
   }
</code></pre>
<!-- end verbatim block -->
The reason why we emphasize the difference between call by value and call
by reference is that it allows the programmer to avoid pitfalls
like unwanted changes of variables. However, many people feel that this
reduces the readability of the code.

<p>
<!-- !split -->

<h2>Call by value and reference, F90/95  <a name="___sec84"></a></h2>

In Fortran we can use <code>INTENT(IN)</code>, <code>INTENT(OUT)</code>, <code>INTENT(INOUT)</code> to let the
program know which values should or should not be changed.

<p>
<!-- begin verbatim block  fcod-->
<pre><code>SUBROUTINE coulomb_integral(np,lp,n,l,coulomb)
  USE effective_interaction_declar
  USE energy_variables
  USE wave_functions
  IMPLICIT NONE
  INTEGER, INTENT(IN)  :: n, l, np, lp
  INTEGER :: i
  REAL(KIND=8), INTENT(INOUT) :: coulomb
  REAL(KIND=8) :: z_rel, oscl_r, sum_coulomb
  ...
</code></pre>
<!-- end verbatim block -->
This hinders unwanted changes and increases readability.

<p>
<!-- !split -->

<h2>Important Matrix and vector handling packages  <a name="___sec85"></a></h2>

The Numerical Recipes codes have been rewritten in Fortran 90/95 and
C/C++ by us.  The original source codes are taken from the widely used
software package LAPACK, which follows two other popular packages
developed in the 1970s, namely EISPACK and LINPACK.

<ul>
  <li> LINPACK: package for linear equations and least square problems.</li>
  <li> LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <a href="http://www.netlib.org" target="_self"><tt>http://www.netlib.org</tt></a> it is possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.</li>
  <li> BLAS (I, II and III): (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. Highly parallelized and efficient codes, all available for download from <a href="http://www.netlib.org" target="_self"><tt>http://www.netlib.org</tt></a>.</li>
</ul>

<!-- !split -->

<h2>Basic Matrix Features  <a name="___sec86"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Matrix properties reminder.</b>
$$
 {\bf A} =
      \left( \begin{array}{cccc} a_{11} & a_{12} & a_{13} & a_{14} \\
                                 a_{21} & a_{22} & a_{23} & a_{24} \\
                                   a_{31} & a_{32} & a_{33} & a_{34} \\
                                  a_{41} & a_{42} & a_{43} & a_{44}
             \end{array} \right)\qquad
{\bf I} =
      \left( \begin{array}{cccc} 1 & 0 & 0 & 0 \\
                                 0 & 1 & 0 & 0 \\
                                 0 & 0 & 1 & 0 \\
                                 0 & 0 & 0 & 1
             \end{array} \right)
$$


<p>
The inverse of a matrix is defined by

$$
{\bf A}^{-1} \cdot {\bf A} = I
$$
</div>


<p>
<!-- !split -->

<h2>Basic Matrix Features  <a name="___sec87"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Matrix Properties Reminder.</b>

<p>
<table border="1">
<thead>
<tr><th align="center">                               Relations                               </th> <th align="center">                                  Name                                 </th> <th align="center">                            matrix elements                            </th> </tr>
</thead>
<tbody>
<tr><td align="center">   \( A = A^{T} \)                                                            </td> <td align="center">   symmetric                                                                  </td> <td align="center">   \( a_{ij} = a_{ji} \)                                                      </td> </tr>
<tr><td align="center">   \( A = \left (A^{T} \right )^{-1} \)                                       </td> <td align="center">   real orthogonal                                                            </td> <td align="center">   \( \sum_k a_{ik} a_{jk} = \sum_k a_{ki} a_{kj} = \delta_{ij} \)            </td> </tr>
<tr><td align="center">   \( A = A^{*} \)                                                            </td> <td align="center">   real matrix                                                                </td> <td align="center">   \( a_{ij} = a_{ij}^{*} \)                                                  </td> </tr>
<tr><td align="center">   \( A = A^{\dagger} \)                                                      </td> <td align="center">   hermitian                                                                  </td> <td align="center">   \( a_{ij} = a_{ji}^{*} \)                                                  </td> </tr>
<tr><td align="center">   \( A = \left (A^{\dagger} \right )^{-1} \)                                 </td> <td align="center">   unitary                                                                    </td> <td align="center">   \( \sum_k a_{ik} a_{jk}^{*} = \sum_k a_{ki}^{*} a_{kj} = \delta_{ij} \)    </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>Some famous Matrices  <a name="___sec88"></a></h2>

<ul>
  <li> Diagonal if \( a_{ij}=0 \) for \( i\ne j \)</li>
  <li> Upper triangular if \( a_{ij}=0 \) for \( i > j \)</li>
  <li> Lower triangular if \( a_{ij}=0 \) for \( i < j \)</li>
  <li> Upper Hessenberg if \( a_{ij}=0 \) for \( i > j+1 \)</li>
  <li> Lower Hessenberg if \( a_{ij}=0 \) for \( i < j+1 \)</li>
  <li> Tridiagonal if \( a_{ij}=0 \) for \( |i -j| > 1 \)</li>
  <li> Lower banded with bandwidth \( p \): \( a_{ij}=0 \) for \( i > j+p \)</li>
  <li> Upper banded with bandwidth \( p \): \( a_{ij}=0 \) for \( i < j+p \)</li>
  <li> Banded, block upper triangular, block lower triangular....</li>
</ul>

<!-- !split -->

<h2>Basic Matrix Features  <a name="___sec89"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Some Equivalent Statements.</b>
For an \( N\times N \) matrix  \( {\bf A} \) the following properties are all equivalent

<ul>
  <li> If the inverse of \( {\bf A} \) exists, \( {\bf A} \) is nonsingular.</li>
  <li> The equation \( {\bf Ax}=0 \) implies \( {\bf x}=0 \).</li>
  <li> The rows of \( {\bf A} \) form a basis of \( R^N \).</li>
  <li> The columns of \( {\bf A} \) form a basis of \( R^N \).</li>
  <li> \( {\bf A} \) is a product of elementary matrices.</li>
  <li> \( 0 \) is not eigenvalue of \( {\bf A} \).</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Important Mathematical Operations  <a name="___sec90"></a></h2>

The basic matrix operations that we will deal with are addition and subtraction

$$
\begin{equation}
{\bf A}= {\bf B}\pm{\bf C}  \Longrightarrow a_{ij} = b_{ij}\pm c_{ij},
\label{eq:mtxadd}
\end{equation}
$$

scalar-matrix multiplication

$$
\begin{equation}
{\bf A}= \gamma{\bf B}  \Longrightarrow a_{ij} = \gamma b_{ij},
\end{equation}
$$

vector-matrix multiplication

$$
\begin{equation}
{\bf y}={\bf Ax}   \Longrightarrow y_{i} = \sum_{j=1}^{n} a_{ij}x_j,
\label{eq:vecmtx}
\end{equation}
$$

matrix-matrix multiplication

$$
\begin{equation}
{\bf A}={\bf BC}   \Longrightarrow a_{ij} = \sum_{k=1}^{n} b_{ik}c_{kj},
\label{eq:mtxmtx}
\end{equation}
$$

and transposition

$$
\begin{equation}
{\bf A}={\bf B}^T   \Longrightarrow a_{ij} = b_{ji}
\end{equation}
$$


<p>
<!-- !split -->

<h2>Important Mathematical Operations  <a name="___sec91"></a></h2>

Similarly, important vector operations that we will deal with are addition and subtraction

$$
\begin{equation}
{\bf x}= {\bf y}\pm{\bf z}  \Longrightarrow x_{i} = y_{i}\pm z_{i},
\end{equation}
$$

scalar-vector multiplication

$$
\begin{equation}
{\bf x}= \gamma{\bf y}  \Longrightarrow x_{i} = \gamma y_{i},
\end{equation}
$$

vector-vector multiplication (called Hadamard multiplication)

$$
\begin{equation}
{\bf x}={\bf yz}   \Longrightarrow x_{i} = y_{i}z_i,
\end{equation}
$$

the inner or so-called dot product  resulting in a constant

$$
\begin{equation}
x={\bf y}^T{\bf z}   \Longrightarrow x = \sum_{j=1}^{n} y_{j}z_{j},
\label{eq:innerprod}
\end{equation}
$$

and the outer product, which yields a matrix,

$$
\begin{equation}
{\bf A}=  {\bf yz}^T \Longrightarrow  a_{ij} = y_{i}z_{j},
\label{eq:outerprod}
\end{equation}
$$


<p>
<!-- !split -->

<h2>Matrix Handling in C/C++, Static and Dynamical allocation  <a name="___sec92"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Static.</b>
We have  an \( N\times N \) matrix A  with \( N=100 \)
In C/C++ this would be  defined as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   int N = 100;
   double A[100][100];
   //   initialize all elements to zero
   for(i=0 ; i &lt; N ; i++) {
      for(j=0 ; j &lt; N ; j++) {
         A[i][j] = 0.0;

</code></pre>
<!-- end verbatim block -->
Note the way the matrix is organized, row-major order.
</div>


<p>
<!-- !split -->

<h2>Matrix Handling in C/C++  <a name="___sec93"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Row Major Order, Addition.</b>
We have  \( N\times N \) matrices A, B and C and we wish to
evaluate \( A=B+C \).

$$
{\bf A}= {\bf B}\pm{\bf C}  \Longrightarrow a_{ij} = b_{ij}\pm c_{ij},
$$

In C/C++ this would be coded like

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   for(i=0 ; i &lt; N ; i++) {
      for(j=0 ; j &lt; N ; j++) {
         a[i][j] = b[i][j]+c[i][j]

</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Matrix Handling in C/C++  <a name="___sec94"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Row Major Order, Multiplication.</b>
We have  \( N\times N \) matrices A, B and C and we wish to
evaluate \( A=BC \).

$$
{\bf A}={\bf BC}   \Longrightarrow a_{ij} = \sum_{k=1}^{n} b_{ik}c_{kj},
$$

In C/C++ this would be coded like

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   for(i=0 ; i &lt; N ; i++) {
      for(j=0 ; j &lt; N ; j++) {
         for(k=0 ; k &lt; N ; k++) {
            a[i][j]+=b[i][k]*c[k][j];

</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Matrix Handling in Fortran 90/95  <a name="___sec95"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Column Major Order.</b>
<!-- begin verbatim block  fcod-->
<pre><code>   ALLOCATE (a(N,N), b(N,N), c(N,N))
   DO j=1,  N
      DO i=1, N
         a(i,j)=b(i,j)+c(i,j)
      ENDDO
   ENDDO
   ...
   DEALLOCATE(a,b,c)
</code></pre>
<!-- end verbatim block -->
Fortran 90 writes the above statements in a much simpler way

<p>
<!-- begin verbatim block  fcod-->
<pre><code>   a=b+c
</code></pre>
<!-- end verbatim block -->
Multiplication

<p>
<!-- begin verbatim block  fcod-->
<pre><code>   a=MATMUL(b,c)
</code></pre>
<!-- end verbatim block -->
Fortran contains also the intrinsic functions TRANSPOSE and CONJUGATE.
</div>


<p>
<!-- !split -->

<h2>Dynamic memory allocation in C/C++  <a name="___sec96"></a></h2>

At least three possibilities in this course

<ul>
  <li> Do it yourself</li>
  <li> Use the functions provided in the library package lib.cpp</li>
  <li> Use Armadillo <a href="http://arma.sourceforgenet" target="_self"><tt>http://arma.sourceforgenet</tt></a> (a C++ linear algebra library, discussion next two weeks, both here and at lab). !split</li>
</ul>

<h2>Matrix Handling in C/C++, Dynamic Allocation  <a name="___sec97"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Do it yourself.</b>
<!-- begin verbatim block  cppcod-->
<pre><code>int N;
double **  A;
A = new double*[N]
for ( i = 0; i &lt; N; i++)
    A[i] = new double[N];
</code></pre>
<!-- end verbatim block -->
Always free space when you don't need an array anymore.

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>for ( i = 0; i &lt; N; i++)
    delete[] A[i];
delete[] A;
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Armadillo, recommended!!  <a name="___sec98"></a></h2>

<ul>
  <li> Armadillo is a C++ linear algebra library (matrix maths) aiming towards a good balance between speed and ease of use. The syntax is deliberately similar to Matlab.</li>
  <li> Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. Various matrix decompositions are provided through optional integration with LAPACK, or one of its high performance drop-in replacements (such as the multi-threaded MKL or ACML libraries).</li>
  <li> A delayed evaluation approach is employed (at compile-time) to combine several operations into one and reduce (or eliminate) the need for temporaries. This is accomplished through recursive templates and template meta-programming.</li>
  <li> Useful for conversion of research code into production environments, or if C++ has been decided as the language of choice, due to speed and/or integration capabilities.</li>
  <li> The library is open-source software, and is distributed under a license that is useful in both open-source and commercial/proprietary contexts.</li>
</ul>

<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec99"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>#include &lt;iostream&gt;
#include &lt;armadillo&gt;

using namespace std;
using namespace arma;

int main(int argc, char** argv)
  {
  mat A = randu&lt;mat&gt;(5,5);
  mat B = randu&lt;mat&gt;(5,5);

  cout &lt;&lt; A*B &lt;&lt; endl;

  return 0;

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Armadillo, how to compile and install  <a name="___sec100"></a></h2>

For people using Ubuntu, Debian, Linux Mint, simply go to the synaptic package manager and install
armadillo from there.
You may have to install Lapack as well.
For Mac and Windows users, follow the instructions from the webpage
<a href="http://arma.sourceforge.net" target="_self"><tt>http://arma.sourceforge.net</tt></a>.
To compile, use for example

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>c++ -O2 -o program.x program.cpp  -larmadillo -llapack -lblas
</code></pre>
<!-- end verbatim block -->
where the <code>-l</code> option indicates the library you wish to link to.

<p>
<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec101"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>#include &lt;iostream&gt;
#include &quot;armadillo&quot;
using namespace arma;
using namespace std;

int main(int argc, char** argv)
  {
  // directly specify the matrix size (elements are uninitialised)
  mat A(2,3);
  // .n_rows = number of rows    (read only)
  // .n_cols = number of columns (read only)
  cout &lt;&lt; &quot;A.n_rows = &quot; &lt;&lt; A.n_rows &lt;&lt; endl;
  cout &lt;&lt; &quot;A.n_cols = &quot; &lt;&lt; A.n_cols &lt;&lt; endl;
  // directly access an element (indexing starts at 0)
  A(1,2) = 456.0;
  A.print(&quot;A:&quot;);
  // scalars are treated as a 1x1 matrix,
  // hence the code below will set A to have a size of 1x1
  A = 5.0;
  A.print(&quot;A:&quot;);
  // if you want a matrix with all elements set to a particular value
  // the .fill() member function can be used
  A.set_size(3,3);
  A.fill(5.0);  A.print(&quot;A:&quot;);
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec102"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>  mat B;

  // endr indicates &quot;end of row&quot;
  B &lt;&lt; 0.555950 &lt;&lt; 0.274690 &lt;&lt; 0.540605 &lt;&lt; 0.798938 &lt;&lt; endr
    &lt;&lt; 0.108929 &lt;&lt; 0.830123 &lt;&lt; 0.891726 &lt;&lt; 0.895283 &lt;&lt; endr
    &lt;&lt; 0.948014 &lt;&lt; 0.973234 &lt;&lt; 0.216504 &lt;&lt; 0.883152 &lt;&lt; endr
    &lt;&lt; 0.023787 &lt;&lt; 0.675382 &lt;&lt; 0.231751 &lt;&lt; 0.450332 &lt;&lt; endr;

  // print to the cout stream
  // with an optional string before the contents of the matrix
  B.print(&quot;B:&quot;);

  // the &lt;&lt; operator can also be used to print the matrix
  // to an arbitrary stream (cout in this case)
  cout &lt;&lt; &quot;B:&quot; &lt;&lt; endl &lt;&lt; B &lt;&lt; endl;
  // save to disk
  B.save(&quot;B.txt&quot;, raw_ascii);
  // load from disk
  mat C;
  C.load(&quot;B.txt&quot;);
  C += 2.0 * B;
  C.print(&quot;C:&quot;);
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec103"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>  // submatrix types:
  //
  // .submat(first_row, first_column, last_row, last_column)
  // .row(row_number)
  // .col(column_number)
  // .cols(first_column, last_column)
  // .rows(first_row, last_row)

  cout &lt;&lt; &quot;C.submat(0,0,3,1) =&quot; &lt;&lt; endl;
  cout &lt;&lt; C.submat(0,0,3,1) &lt;&lt; endl;

  // generate the identity matrix
  mat D = eye&lt;mat&gt;(4,4);

  D.submat(0,0,3,1) = C.cols(1,2);
  D.print(&quot;D:&quot;);

  // transpose
  cout &lt;&lt; &quot;trans(B) =&quot; &lt;&lt; endl;
  cout &lt;&lt; trans(B) &lt;&lt; endl;

  // maximum from each column (traverse along rows)
  cout &lt;&lt; &quot;max(B) =&quot; &lt;&lt; endl;
  cout &lt;&lt; max(B) &lt;&lt; endl;

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec104"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>  // maximum from each row (traverse along columns)
  cout &lt;&lt; &quot;max(B,1) =&quot; &lt;&lt; endl;
  cout &lt;&lt; max(B,1) &lt;&lt; endl;
  // maximum value in B
  cout &lt;&lt; &quot;max(max(B)) = &quot; &lt;&lt; max(max(B)) &lt;&lt; endl;
  // sum of each column (traverse along rows)
  cout &lt;&lt; &quot;sum(B) =&quot; &lt;&lt; endl;
  cout &lt;&lt; sum(B) &lt;&lt; endl;
  // sum of each row (traverse along columns)
  cout &lt;&lt; &quot;sum(B,1) =&quot; &lt;&lt; endl;
  cout &lt;&lt; sum(B,1) &lt;&lt; endl;
  // sum of all elements
  cout &lt;&lt; &quot;sum(sum(B)) = &quot; &lt;&lt; sum(sum(B)) &lt;&lt; endl;
  cout &lt;&lt; &quot;accu(B)     = &quot; &lt;&lt; accu(B) &lt;&lt; endl;
  // trace = sum along diagonal
  cout &lt;&lt; &quot;trace(B)    = &quot; &lt;&lt; trace(B) &lt;&lt; endl;
  // random matrix -- values are uniformly distributed in the [0,1] interval
  mat E = randu&lt;mat&gt;(4,4);
  E.print(&quot;E:&quot;);

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec105"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>  // row vectors are treated like a matrix with one row
  rowvec r;
  r &lt;&lt; 0.59499 &lt;&lt; 0.88807 &lt;&lt; 0.88532 &lt;&lt; 0.19968;
  r.print(&quot;r:&quot;);

  // column vectors are treated like a matrix with one column
  colvec q;
  q &lt;&lt; 0.81114 &lt;&lt; 0.06256 &lt;&lt; 0.95989 &lt;&lt; 0.73628;
  q.print(&quot;q:&quot;);

  // dot or inner product
  cout &lt;&lt; &quot;as_scalar(r*q) = &quot; &lt;&lt; as_scalar(r*q) &lt;&lt; endl;

    // outer product
  cout &lt;&lt; &quot;q*r =&quot; &lt;&lt; endl;
  cout &lt;&lt; q*r &lt;&lt; endl;


  // sum of three matrices (no temporary matrices are created)
  mat F = B + C + D;
  F.print(&quot;F:&quot;);

    return 0;

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec106"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>#include &lt;iostream&gt;
#include &quot;armadillo&quot;
using namespace arma;
using namespace std;

int main(int argc, char** argv)
  {
  cout &lt;&lt; &quot;Armadillo version: &quot; &lt;&lt; arma_version::as_string() &lt;&lt; endl;

  mat A;

  A &lt;&lt; 0.165300 &lt;&lt; 0.454037 &lt;&lt; 0.995795 &lt;&lt; 0.124098 &lt;&lt; 0.047084 &lt;&lt; endr
    &lt;&lt; 0.688782 &lt;&lt; 0.036549 &lt;&lt; 0.552848 &lt;&lt; 0.937664 &lt;&lt; 0.866401 &lt;&lt; endr
    &lt;&lt; 0.348740 &lt;&lt; 0.479388 &lt;&lt; 0.506228 &lt;&lt; 0.145673 &lt;&lt; 0.491547 &lt;&lt; endr
    &lt;&lt; 0.148678 &lt;&lt; 0.682258 &lt;&lt; 0.571154 &lt;&lt; 0.874724 &lt;&lt; 0.444632 &lt;&lt; endr
    &lt;&lt; 0.245726 &lt;&lt; 0.595218 &lt;&lt; 0.409327 &lt;&lt; 0.367827 &lt;&lt; 0.385736 &lt;&lt; endr;

  A.print(&quot;A =&quot;);

  // determinant
  cout &lt;&lt; &quot;det(A) = &quot; &lt;&lt; det(A) &lt;&lt; endl;
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Armadillo, simple examples  <a name="___sec107"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>  // inverse
  cout &lt;&lt; &quot;inv(A) = &quot; &lt;&lt; endl &lt;&lt; inv(A) &lt;&lt; endl;
  double k = 1.23;

  mat    B = randu&lt;mat&gt;(5,5);
  mat    C = randu&lt;mat&gt;(5,5);

  rowvec r = randu&lt;rowvec&gt;(5);
  colvec q = randu&lt;colvec&gt;(5);


  // examples of some expressions
  // for which optimised implementations exist
  // optimised implementation of a trinary expression
  // that results in a scalar
  cout &lt;&lt; &quot;as_scalar( r*inv(diagmat(B))*q ) = &quot;;
  cout &lt;&lt; as_scalar( r*inv(diagmat(B))*q ) &lt;&lt; endl;

  // example of an expression which is optimised
  // as a call to the dgemm() function in BLAS:
  cout &lt;&lt; &quot;k*trans(B)*C = &quot; &lt;&lt; endl &lt;&lt; k*trans(B)*C;

    return 0;

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Gaussian Elimination  <a name="___sec108"></a></h2>

We start with the linear set of equations

$$
   {\bf A}{\bf x} = {\bf w}.
$$

We assume also that the matrix \( {\bf A} \) is non-singular and that the
matrix elements along the diagonal satisfy \( a_{ii} \ne 0 \). Simple \( 4\times 4  \) example

$$
\left(\begin{array}{cccc}
                           a_{11}& a_{12} &a_{13}& a_{14}\\
                           a_{21}& a_{22} &a_{23}& a_{24}\\
                           a_{31}& a_{32} &a_{33}& a_{34}\\
                           a_{41}& a_{42} &a_{43}& a_{44}\\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           w_1\\
                           w_2\\
                           w_3 \\
                           w_4\\
                      \end{array} \right).
$$

or

$$
\begin{align}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4. \nonumber
\end{align}
$$


<p>
<!-- !split -->

<h2>Gaussian Elimination  <a name="___sec109"></a></h2>

The basic idea of Gaussian elimination is to use the first equation to eliminate the first unknown \( x_1 \)
from the remaining \( n-1 \) equations. Then we use the new second equation to eliminate the second unknown
\( x_2 \) from the remaining \( n-2 \) equations. With \( n-1 \) such eliminations
we obtain a so-called upper triangular set of equations of the form

$$
\begin{align}label{eq:gaussbacksub}
 b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
 b_{22}x_2 + b_{23}x_3 + b_{24}x_4=&y_2 \nonumber \\
b_{33}x_3 + b_{34}x_4=&y_3 \nonumber \\
b_{44}x_4=&y_4. \nonumber
\end{align}
$$

We can solve this system of equations recursively starting from \( x_n \) (in our case \( x_4 \)) and proceed with
what is called a backward substitution. This process can be expressed mathematically as

$$
\begin{equation}
   x_m = \frac{1}{b_{mm}}\left(y_m-\sum_{k=m+1}^nb_{mk}x_k\right)\quad m=n-1,n-2,\dots,1.
\end{equation}
$$

To arrive at such an upper triangular system of equations, we start by eliminating
the unknown \( x_1 \) for \( j=2,n \). We achieve this by multiplying the first equation by \( a_{j1}/a_{11} \) and then subtract
the result from the $j$th equation. We assume obviously that \( a_{11}\ne 0 \) and that
\( {\bf A} \) is not singular.

<p>
<!-- !split -->

<h2>Gaussian Elimination  <a name="___sec110"></a></h2>

Our actual \( 4\times 4 \) example reads after the first operation

$$
\left(\begin{array}{cccc}
                           a_{11}& a_{12} &a_{13}& a_{14}\\
                           0& (a_{22}-\frac{a_{21}a_{12}}{a_{11}}) &(a_{23}-\frac{a_{21}a_{13}}{a_{11}}) & (a_{24}-\frac{a_{21}a_{14}}{a_{11}})\\
0& (a_{32}-\frac{a_{31}a_{12}}{a_{11}})& (a_{33}-\frac{a_{31}a_{13}}{a_{11}})& (a_{34}-\frac{a_{31}a_{14}}{a_{11}})\\
0&(a_{42}-\frac{a_{41}a_{12}}{a_{11}}) &(a_{43}-\frac{a_{41}a_{13}}{a_{11}}) & (a_{44}-\frac{a_{41}a_{14}}{a_{11}}) \\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           y_1\\
                           w_2^{(2)}\\
                           w_3^{(2)} \\
                           w_4^{(2)}\\
                      \end{array} \right),
$$

or

$$
\begin{align}
 b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
 a^{(2)}_{22}x_2 + a^{(2)}_{23}x_3 + a^{(2)}_{24}x_4=&w^{(2)}_2 \nonumber \\
 a^{(2)}_{32}x_2 + a^{(2)}_{33}x_3 + a^{(2)}_{34}x_4=&w^{(2)}_3 \nonumber \\
 a^{(2)}_{42}x_2 + a^{(2)}_{43}x_3 + a^{(2)}_{44}x_4=&w^{(2)}_4, \nonumber \\
\end{align}
$$


<p>
<!-- !split -->

<h2>Gaussian Elimination  <a name="___sec111"></a></h2>

The new coefficients are

$$
\begin{equation}
   b_{1k} = a_{1k}^{(1)} \quad k=1,\dots,n,
\end{equation}
$$

where each \( a_{1k}^{(1)} \) is equal to the original \( a_{1k} \) element. The other coefficients are

$$
\begin{equation}
a_{jk}^{(2)} = a_{jk}^{(1)}-\frac{a_{j1}^{(1)}a_{1k}^{(1)}}{a_{11}^{(1)}} \quad j,k=2,\dots,n,
\end{equation}
$$

with a new right-hand side given by

$$
\begin{equation}
y_{1}=w_1^{(1)}, \quad w_j^{(2)} =w_j^{(1)}-\frac{a_{j1}^{(1)}w_1^{(1)}}{a_{11}^{(1)}} \quad j=2,\dots,n.
\end{equation}
$$

We have also set \( w_1^{(1)}=w_1 \), the original vector element.
We see that the system of unknowns \( x_1,\dots,x_n \) is transformed into an \( (n-1)\times (n-1) \) problem.

<p>
<!-- !split -->

<h2>Gaussian Elimination  <a name="___sec112"></a></h2>

This step is called forward substitution.
Proceeding with these substitutions, we obtain the
general expressions for the new coefficients

$$
\begin{equation}
   a_{jk}^{(m+1)} = a_{jk}^{(m)}-\frac{a_{jm}^{(m)}a_{mk}^{(m)}}{a_{mm}^{(m)}} \quad j,k=m+1,\dots,n,
\end{equation}
$$

with \( m=1,\dots,n-1 \) and a
right-hand side given by

$$
\begin{equation}
   w_j^{(m+1)} =w_j^{(m)}-\frac{a_{jm}^{(m)}w_m^{(m)}}{a_{mm}^{(m)}}\quad j=m+1,\dots,n.
\end{equation}
$$

This set of \( n-1 \) elimations leads us to an equations which is solved by back substitution.
If the arithmetics is exact and the matrix \( {\bf A} \) is not singular, then the computed answer will be exact.

<p>
Even though the matrix elements along the diagonal are not zero,
numerically small numbers may appear and subsequent divisions may lead to large numbers, which, if added
to a small number may yield losses of precision. Suppose for example that our first division in \( (a_{22}-a_{21}a_{12}/a_{11}) \)
results in \( -10^{-7} \) and that \( a_{22} \) is one.
one. We are then
adding \( 10^7+1 \). With single precision this results in \( 10^7 \).

<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec113"></a></h2>

Suppose we want to solve the following boundary value equation

$$
  -\frac{d^2u(x)}{dx^2} = f(x,u(x)),
$$

with \( x\in (a,b) \) and with boundary conditions \( u(a)=u(b) = 0 \).
We assume that \( f \) is a continuous function in the domain \( x\in (a,b) \).
Since, except the few cases where it is possible to find analytic solutions, we
will seek after approximate solutions, we choose to represent the approximation to the second derivative
from the previous chapter

$$
  f''=\frac{f_h -2f_0 +f_{-h}}{h^2} +O(h^2).
$$

We subdivide our interval \( x\in (a,b) \) into \( n \) subintervals by setting \( x_i = ih \), with \( i=0,1,\dots,n+1 \).
The step size is then given by \( h=(b-a)/(n+1) \) with \( n\in {\mathbb{N}} \).
For the internal grid points \( i=1,2,\dots n \) we replace the differential operator with the above formula
resulting in

$$
u''(x_i) \approx  \frac{u(x_i+h) -2u(x_i) +u(x_i-h)}{h^2},
$$

which we rewrite as

$$
u^{''}_i \approx  \frac{u_{i+1} -2u_i +u_{i-i}}{h^2}.
$$


<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec114"></a></h2>

We can rewrite our original differential equation in terms of a discretized equation with approximations to the
derivatives as

$$
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
$$

with \( i=1,2,\dots, n \). We need to add to this system the two boundary conditions \( u(a) =u_0 \) and \( u(b) = u_{n+1} \).
If we define a matrix

$$
    {\bf A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 & 2 & -1 & & & \\
                           & -1 & 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 & 2 \\
                      \end{array} \right)
$$

and the corresponding vectors \( {\bf u} = (u_1, u_2, \dots,u_n)^T \) and
\( {\bf f}({\bf u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T \)  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns

$$
   {\bf A}{\bf u} = {\bf f}({\bf u}).
$$


<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec115"></a></h2>

We start with the linear set of equations

$$
   {\bf A}{\bf u} = {\bf f},
$$

where \( {\bf A} \) is a tridiagonal matrix which we rewrite as

$$
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_n & b_n \\
                      \end{array} \right)
$$

where \( a,b,c \) are one-dimensional arrays of length \( 1:n \).
In project 1 the arrays \( a \) and \( c \) are equal, namely \( a_i=c_i=-1/h^2 \).
The matrix is  also positive definite.

<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec116"></a></h2>

We can rewrite as

$$
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_n & b_n \\
                      \end{array} \right)\left(\begin{array}{c}
                           u_1\\
                           u_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           u_n\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           f_1\\
                           f_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           f_n\\
                      \end{array} \right).
$$


<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec117"></a></h2>

A tridiagonal matrix is a special form of banded matrix where all the elements are zero except for
those on and immediately above and below the leading diagonal.
The above tridiagonal system   can be written as

$$
  a_iu_{i-1}+b_iu_i+c_iu_{i+1} = f_i,
$$

for \( i=1,2,\dots,n \). We see that \( u_{-1} \) and \( u_{n+1} \) are not required and we can set \( a_1=c_n=0 \).
In many applications the matrix is symmetric and we have \( a_i=c_i \).
The algorithm for solving this set of equations is rather simple and requires two steps only,
a forward substitution and a backward substitution. These steps are also
common to the algorithms based on
Gaussian elimination that
we discussed previously. However, due to its simplicity, the number of floating point operations
is in this
case proportional with \( O(n) \) while Gaussian elimination requires \( 2n^3/3+O(n^2) \) floating point operations.

<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec118"></a></h2>

In case your system of equations leads to a tridiagonal matrix, it is clearly an overkill to employ
Gaussian elimination or the standard LU decomposition.
You will encounter several applications involving tridiagonal matrices in our discussion of
partial differential equations in chapter 10.

<p>
Our algorithm starts with forward substitution with a loop over of the elements \( i \) and can be expressed via the
following piece of code

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   btemp = b[1];
   u[1] = f[1]/btemp;
   for(i=2 ; i &lt;= n ; i++) {
      temp[i] = c[i-1]/btemp;
      btemp = b[i]-a[i]*temp[i];
      u[i] = (f[i] - a[i]*u[i-1])/btemp;

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec119"></a></h2>

Note that you should avoid cases with \( b_1=0 \). If that is the case, you should rewrite the equations
as a set of order \( n-1 \) with \( u_2 \) eliminated.
Finally we perform the backsubstitution leading to the following code

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   for(i=n-1 ; i &gt;= 1 ; i--) {
      u[i] -= temp[i+1]*u[i+1];

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Gaussian Elimination and Tridiagonal matrices, project 1  <a name="___sec120"></a></h2>

Note that our sums start with \( i=1 \) and that one  should avoid cases with \( b_1=0 \). If that is the case, you should rewrite the equations
as a set of order \( n-1 \) with \( u_2 \) eliminated. However, a tridiagonal matrix problem is not a guarantee that we
can find a solution. The matrix \( {\bf A} \) which rephrases a second derivative in a discretized form

$$
    {\bf A} = \left(\begin{array}{cccccc}
                          2 & -1 & 0 & 0  &0  & 0\\
                          -1 & 2 & -1 &0 &0 &0 \\
                          0 & -1 & 2 & -1 & 0& 0 \\
                          0 & \dots   & \dots & \dots   &\dots & \dots \\
                          0 &0   &0  &-1  &2& -1 \\
                          0 &  0  &0  &0   &-1 & 2 \\
                      \end{array} \right),
$$

fulfills the condition of a weak dominance of the diagonal, with
\( |b_1| > |c_1| \), \( |b_n| > |a_n| \) and  \( |b_k| \ge |a_k|+|c_k| \) for \( k=2,3,\dots,n-1 \).
This is a relevant but not sufficient condition to guarantee that the matrix \( {\bf A} \) yields a solution to a linear
equation problem. The matrix needs also to be irreducible. A tridiagonal irreducible matrix means that all the elements \( a_i \) and
\( c_i \) are non-zero. If these two conditions are present, then \( {\bf A} \) is nonsingular and has a unique LU decomposition.

<p>
<!-- !split -->

<h2>Project 1, hints  <a name="___sec121"></a></h2>

When setting up the algo it is useful to note that the different
operations on the matrix (here as a \( 4\times 4 \) case  with diagonals
\( d_i \) and off-diagonals \( e_i \)

$$
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          e_1 & d_2 & e_2 & 0 \\
                          0 & e_2 & d_3 & e_3 \\
                          0 & 0 & e_3 & d_4
                      \end{array} \right)\rightarrow
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & e_2 & d_3 & e_3 \\
                          0 & 0 & e_3 & d_4
                      \end{array} \right)\rightarrow
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & 0 & \tilde{d}_3 & e_3 \\
                          0 & 0 & e_3 & d_4
                      \end{array} \right)
$$

and finally

$$
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & 0 & \tilde{d}_3 & e_3 \\
                          0 & 0 & 0 & \tilde{d}_4
                      \end{array} \right)
$$


<p>
<!-- !split -->

<h2>Project 1, hints  <a name="___sec122"></a></h2>

We notice the sub-blocks which get repeated

$$
   \left(\begin{array}{cccc}
                          d_1 & e_1 & 0 & 0 \\
                          0 & \tilde{d}_2 & e_2 & 0 \\
                          0 & 0 & \tilde{d}_3 & e_3 \\
                          0 & 0 & 0 & \tilde{d}_4
                      \end{array} \right)
$$

The matrices we often end up with in rewriting for for example partial differential equations,
have the feature that all leading principal submatrices are non-singular. If the matrix
is symmetric as well it can be rewritten as \( A=LDL^T \) with \( D \) the diagonal and we have the
following relations
\( a_{11} = d_1 \), \( a_{k,k-1}=e_{k-1}d_{k-1} \) for \( k=2,\dots,n \) and finally

$$
a_{kk} = d_k+e_{k-1}^2d_{k-1}=d_k+e_{k-1}a_{k,k-1}
$$

for \( k=2,\dots,n \).

<p>
<!-- !split -->

<h2>Linear Algebra Methods  <a name="___sec123"></a></h2>

<ul>
  <li> Gaussian elimination, \( O(2/3n^3) \) flops, general matrix</li>
  <li> LU decomposition, upper triangular and lower tridiagonal matrices, \( O(2/3n^3) \) flops, general matrix. Get easily the inverse, determinant and can solve linear equations with back-substitution only, \( O(n^2) \) flops</li>
  <li> Cholesky decomposition \( A=LL^T \). Real symmetric or hermitian positive definite matrix, \( O(1/3n^3) \) flops.</li>
  <li> Tridiagonal linear systems, important for differential equations. Normally positive definite and non-singular. \( O(8n) \) flops for symmetric. \( A=LDL^T \) with \( D \) the diagonal. Special case of banded matrices.</li>
  <li> Singular value decomposition</li>
  <li> the QR method will be discussed in chapter 7 in connection with eigenvalue systems. \( O(4/3n^3) \) flops. !split</li>
</ul>

<h2>LU Decomposition  <a name="___sec124"></a></h2>

The LU decomposition method means that we can rewrite
this matrix as the product of two matrices \( {\bf L} \) and \( {\bf U} \)
where

$$
\label{eq3}
   \left(\begin{array}{cccc}
                          a_{11} & a_{12} & a_{13} & a_{14} \\
                          a_{21} & a_{22} & a_{23} & a_{24} \\
                          a_{31} & a_{32} & a_{33} & a_{34} \\
                          a_{41} & a_{42} & a_{43} & a_{44}
                      \end{array} \right)
                      = \left( \begin{array}{cccc}
                              1  & 0      & 0      & 0 \\
                          l_{21} & 1      & 0      & 0 \\
                          l_{31} & l_{32} & 1      & 0 \\
                          l_{41} & l_{42} & l_{43} & 1
                      \end{array} \right)
                        \left( \begin{array}{cccc}
                          u_{11} & u_{12} & u_{13} & u_{14} \\
                               0 & u_{22} & u_{23} & u_{24} \\
                               0 & 0      & u_{33} & u_{34} \\
                               0 & 0      &  0     & u_{44}
             \end{array} \right).
$$

LU decomposition forms the backbone of other algorithms in linear algebra, such as the
solution of linear equations given by

$$
\begin{align}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4.  \nonumber
\end{align}
$$

The above set of equations is conveniently solved by using LU decomposition as an intermediate step.

<p>
The matrix \( {\bf A}\in \mathbb{R}^{n\times n} \) has an LU factorization if the determinant
is different from zero. If the LU factorization exists and \( {\bf A} \) is non-singular, then the LU factorization
is unique and the determinant is given by

$$
det\{{\bf A}\}=det\{{\bf LU}\}= det\{{\bf L}\}det\{{\bf U}\}=u_{11}u_{22}\dots u_{nn}.
$$


<p>
<!-- !split -->

<h2>LU Decomposition, why?  <a name="___sec125"></a></h2>

There are at least three main advantages with LU decomposition compared with standard Gaussian elimination:

<ul>
  <li> It is straightforward to compute the determinant of a matrix</li>
  <li> If we have to solve sets of linear equations with the same matrix but with different vectors \( {\bf y} \), the number of FLOPS is of the order \( n^3 \).</li>
  <li> The invers is such an operation !split</li>
</ul>

<h2>LU Decomposition, linear equations  <a name="___sec126"></a></h2>

With the LU decomposition it is rather
simple to solve a system of linear equations

$$
\begin{align}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4. \nonumber
\end{align}
$$


<p>
This can be written in matrix form as

$$ {\bf Ax}={\bf w}. $$


<p>
where \( {\bf A} \) and \( {\bf w} \) are known and we have to solve for
\( {\bf x} \). Using the LU dcomposition we write

$$ {\bf A} {\bf x} \equiv {\bf L} {\bf U} {\bf x} ={\bf w}. $$


<p>
<!-- !split -->

<h2>LU Decomposition, linear equations  <a name="___sec127"></a></h2>

The previous equation can be calculated in two steps

$$ {\bf L} {\bf y} = {\bf w};\qquad {\bf Ux}={\bf y}. $$


<p>
To show that this is correct we use to the LU decomposition
to rewrite our system of linear equations as

$$  {\bf LUx}={\bf w}, $$

and since the determinat of \( {\bf L} \) is equal to 1 (by construction
since the diagonals of \( {\bf L} \) equal 1) we can use the inverse of
\( {\bf L} \) to obtain

$$
   {\bf Ux}={\bf L^{-1}w}={\bf y},
$$

which yields the intermediate step

$$
   {\bf L^{-1}w}={\bf y}
$$

and as soon as we have \( {\bf y} \) we can obtain \( {\bf x} \)
through \( {\bf Ux}={\bf y} \).

<p>
<!-- !split -->

<h2>LU Decomposition, why?  <a name="___sec128"></a></h2>

For our four-dimentional example this takes the form

$$
\begin{align}
 y_1=&w_1 \nonumber\\
l_{21}y_1 + y_2=&w_2\nonumber \\
l_{31}y_1 + l_{32}y_2 + y_3 =&w_3\nonumber \\
l_{41}y_1 + l_{42}y_2 + l_{43}y_3 + y_4=&w_4. \nonumber
\end{align}
$$


<p>
and

$$
\begin{align}
 u_{11}x_1 +u_{12}x_2 +u_{13}x_3 + u_{14}x_4=&y_1 \nonumber\\
u_{22}x_2 + u_{23}x_3 + u_{24}x_4=&y_2\nonumber \\
u_{33}x_3 + u_{34}x_4=&y_3\nonumber \\
u_{44}x_4=&y_4  \nonumber
\end{align}
$$


<p>
This example shows the basis for the algorithm
needed to solve the set of \( n \) linear equations.

<p>
<!-- !split -->

<h2>LU Decomposition, linear equations  <a name="___sec129"></a></h2>

The algorithm goes as follows

<ul>
  <li> Set up the matrix \( \bf A \) and the vector \( \bf w \) with their correct dimensions. This determines the dimensionality of the unknown vector \( \bf x \).</li>
  <li> Then LU decompose the matrix \( \bf A \) through a call to the function <code>ludcmp(double a, int n, int indx, double &d)</code>. This functions returns the LU decomposed matrix \( \bf A \), its determinant and the vector indx which keeps track of the number of interchanges of rows. If the determinant is zero, the solution is malconditioned.</li>
  <li> Thereafter you call the function  <code>lubksb(double a, int n, int indx, double w)</code> which uses the LU decomposed matrix \( \bf A \) and the vector \( \bf w \) and returns \( \bf x \) in the same place as \( \bf w \). Upon exit the original content in \( \bf w \) is destroyed. If you wish to keep this information, you should make a backup of it in your calling function.</li>
</ul>

<!-- !split -->

<h2>LU Decomposition, the inverse of a matrix  <a name="___sec130"></a></h2>

If the inverse exists then

$$
   {\bf A}^{-1}{\bf A}={\bf I},
$$

the identity matrix. With an LU decomposed matrix we can rewrite the last equation as

$$
   {\bf LU}{\bf A}^{-1}={\bf I}.
$$

If we assume that the first column (that is column 1) of the inverse matrix
can be written as a vector with unknown entries

$$
    {\bf A}_1^{-1}= \left( \begin{array}{c}

                              a_{11}^{-1} \\
                              a_{21}^{-1} \\
                              \dots \\
                              a_{n1}^{-1} \\
                    \end{array} \right),
$$

then we have a linear set of equations

$$
    {\bf LU}\left( \begin{array}{c}

                              a_{11}^{-1} \\
                              a_{21}^{-1} \\
                              \dots \\
                              a_{n1}^{-1} \\
                    \end{array} \right) =\left( \begin{array}{c}
                               1 \\
                              0 \\
                              \dots \\
                              0 \\
                    \end{array} \right).
$$


<p>
<!-- !split -->

<h2>LU Decomposition, the inverse  <a name="___sec131"></a></h2>

In a similar way we can compute the unknow entries of the second column,

$$
    {\bf LU}\left( \begin{array}{c}

                              a_{12}^{-1} \\
                              a_{22}^{-1} \\
                              \dots \\
                              a_{n2}^{-1} \\
                    \end{array} \right) =\left( \begin{array}{c}
                                0 \\
                              1 \\
                              \dots \\
                              0 \\
                    \end{array} \right),
$$

and continue till we have solved all \( n \) sets of linear equations.

<p>
<!-- !split -->

<h2>How to use the Library functions  <a name="___sec132"></a></h2>

Standard C/C++: fetch the files <code>lib.cpp</code> and <code>lib.h</code>. You can make a directory where you store
these files, and eventually its compiled version lib.o. The example here is program1.cpp from
chapter 6 and performs the matrix inversion.

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>//  Simple matrix inversion example
#include &lt;iostream&gt;
#include &lt;new&gt;
#include &lt;cstdio&gt;
#include &lt;cstdlib&gt;
#include &lt;cmath&gt;
#include &lt;cstring&gt;
#include &quot;lib.h&quot;

using namespace std;

/* function declarations */

void inverse(double **, int);

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>How to use the Library functions  <a name="___sec133"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>void inverse(double **a, int n)
{
  int          i,j, *indx;
  double       d, *col, **y;
  // allocate space in memory
  indx = new int[n];
  col  = new double[n];
  y    = (double **) matrix(n, n, sizeof(double));
  ludcmp(a, n, indx, &amp;d);   // LU decompose  a[][]
  printf(&quot;\n\nLU form of matrix of a[][]:\n&quot;);
  for(i = 0; i &lt; n; i++) {
    printf(&quot;\n&quot;);
    for(j = 0; j &lt; n; j++) {
      printf(&quot; a[%2d][%2d] = %12.4E&quot;,i, j, a[i][j]);

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>How to use the Library functions  <a name="___sec134"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>  // find inverse of a[][] by columns
  for(j = 0; j &lt; n; j++) {
    // initialize right-side of linear equations
    for(i = 0; i &lt; n; i++) col[i] = 0.0;
    col[j] = 1.0;
    lubksb(a, n, indx, col);
    // save result in y[][]
    for(i = 0; i &lt; n; i++) y[i][j] = col[i];
  }   //j-loop over columns
  // return the inverse matrix in a[][]
  for(i = 0; i &lt; n; i++) {
    for(j = 0; j &lt; n; j++) a[i][j] = y[i][j];

  free_matrix((void **) y);     // release local memory
  delete [] col;
  delete []indx;
}  // End: function inverse()
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>How to use the Library functions  <a name="___sec135"></a></h2>

For Fortran users:

<p>
<!-- begin verbatim block  fcod-->
<pre><code>PROGRAM matrix
  USE constants
  USE F90library
  IMPLICIT NONE
  !      The definition of the matrix, using dynamic allocation
  REAL(DP), ALLOCATABLE, DIMENSION(:,:) :: a, ainv, unity
  !      the determinant
  REAL(DP) :: d
  !      The size of the matrix
  INTEGER :: n
  ....
  !      Allocate now place in heap for a
  ALLOCATE ( a(n,n), ainv(n,n), unity(n,n) )
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>How to use the Library functions  <a name="___sec136"></a></h2>

For Fortran users:

<p>
<!-- begin verbatim block  fcod-->
<pre><code>  WRITE(6,*) ' The matrix before inversion'
  WRITE(6,'(3F12.6)') a
  ainv=a
  CALL matinv (ainv, n, d)
  ....
  !      get the unity matrix
  unity=MATMUL(ainv,a)
  WRITE(6,*) ' The unity matrix'
  WRITE(6,'(3F12.6)') unity
  !      deallocate all arrays
  DEALLOCATE (a, ainv, unity)
END PROGRAM matrix

</code></pre>
<!-- end verbatim block -->

<h1>Week 36  <a name="___sec137"></a></h1>

<!-- !split -->

<h2>Overview of week 36  <a name="___sec138"></a></h2>

<div class="alert alert-block alert-block alert-text-normal"><b>Linear Algebra and project 1.</b>

<ul>
  <li> Discussion of Project 1</li>
  <li> Object orientation and unit testing (see detailed instruction on webpage)</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Object orientation  <a name="___sec139"></a></h2>

Why object orientation?

<ul>
  <li> Three main topics: objects, class hierarchies and polymorphism</li>
  <li> The aim here is to be to be able to write a more general code which can easily be tailored to new situations.</li>
  <li> {\bf Polymorphism} is a term used in software development to describe a variety of techniques employed by programmers to create flexible and reusable software components. The term is Greek and it loosely translates to "many forms". Strategy: try to single out the variables needed to describe a given system and those needed to describe a given solver. !split</li>
</ul>

<h2>Object orientation  <a name="___sec140"></a></h2>

In programming languages, a polymorphic object is an entity, such as a variable or a procedure, that can hold or operate on values of differing types during the program's execution. Because a polymorphic object can operate on a variety of values and types, it can also be used in a variety of programs, sometimes with little or no change by the programmer. The idea of write once, run many, also known as code reusability, is an important characteristic to the programming paradigm known as Object-Oriented Programming (OOP).

<p>
OOP describes an approach to programming where a program is viewed as a collection of interacting, but mostly independent software components. These software components are known as objects in OOP and they are typically implemented in a programming language as an entity that encapsulates both data and procedures.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec141"></a></h2>

In Fortran a vector or matrix start with \( 1 \), but it is easy
to change a vector so that it starts with zero or even a negative number.
If we have a double precision Fortran vector  which starts at \( -10 \) and ends at \( 10 \), we could declare it as
<code>REAL(KIND=8) ::  vector(-10:10)</code>. Similarly, if we want to start at zero and end at 10 we could write
<code>REAL(KIND=8) ::  vector(0:10)</code>.
We have also seen that Fortran  allows us to write a matrix addition \( {\bf A} = {\bf B}+{\bf C} \) as
<code>A = B + C</code>.  This means that we have overloaded the addition operator so that it translates this operation into
two loops and an addition of two matrix elements \( a_{ij} = b_{ij}+c_{ij} \).

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec142"></a></h2>

The way the matrix addition is written is very close to the way we express this relation mathematically. The benefit for the
programmer is that our code is easier to read. Furthermore, such a way of coding makes it  more likely  to spot eventual
errors as well.

<p>
In Ansi C and C++ arrays start by default from \( i=0 \).  Moreover, if we  wish to add two matrices we need to explicitely write out
the two loops as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>   for(i=0 ; i &lt; n ; i++) {
      for(j=0 ; j &lt; n ; j++) {
         a[i][j]=b[i][j]+c[i][j]

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec143"></a></h2>

However,
the strength of C++ is the possibility
to define new data types, tailored to some particular problem.
Via new data types and overloading of operations such as addition and subtraction, we can easily define
sets of operations and data types which allow us to write a matrix addition in exactly the same
way as we would do in Fortran.  We could also change the way we declare a C++ matrix elements \( a_{ij} \), from  \( a[i][j] \)
to say \( a(i,j) \), as we would do in Fortran. Similarly, we could also change the default range from \( 0:n-1 \) to \( 1:n \).

<p>
To achieve this we need to introduce two important entities in C++ programming, classes and templates.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec144"></a></h2>

The function and class declarations are fundamental concepts within C++.  Functions are abstractions
which encapsulate an algorithm or parts of it and perform specific tasks in a program.
We have already met several examples on how to use  functions.
Classes can be defined as abstractions which encapsulate
data and operations on these data.
The data can be very complex data structures  and the class can contain particular functions
which operate on these data. Classes allow therefore for a higher level of abstraction in computing.
The elements (or components) of the data
type are the class data members, and the procedures are the class
member functions.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec145"></a></h2>

Classes are user-defined tools used to create multi-purpose software which can be reused by other classes or functions.
These user-defined data types contain data (variables) and
functions operating on the data.

<p>
A simple example is that of a point in two dimensions.
The data could be the \( x \) and \( y \) coordinates of a given  point. The functions
we define could be simple read and write functions or the possibility to compute the distance between two points.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec146"></a></h2>

C++ has a class complex in its standard
template library (STL). The standard usage in a given function could then look like

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>// Program to calculate addition and multiplication of two complex numbers
using namespace std;
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;complex&gt;
int main()
{
  complex&lt;double&gt; x(6.1,8.2), y(0.5,1.3);
  // write out x+y
  cout &lt;&lt; x + y &lt;&lt; x*y  &lt;&lt; endl;
  return 0;

</code></pre>
<!-- end verbatim block -->
where we add and multiply two complex numbers \( x=6.1+\imath 8.2 \) and \( y=0.5+\imath 1.3 \) with the obvious results
\( z=x+y=6.6+\imath 9.5 \) and \( z=x\cdot y= -7.61+\imath 12.03 \).

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec147"></a></h2>

We proceed by  splitting our task in three files.

<p>
We define first a header file complex.h  which contains the declarations of
the class. The header file contains the class declaration (data and
functions), declaration of stand-alone functions, and all inlined
functions, starting as follows

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>#ifndef Complex_H
#define Complex_H
//   various include statements and definitions
#include &lt;iostream&gt;          // Standard ANSI-C++ include files
#include &lt;new&gt;
#include ....

class Complex
{...
definition of variables and their character
};
//   declarations of various functions used by the class
...
#endif
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec148"></a></h2>

Next we provide a file complex.cpp where the code and algorithms of
different functions (except inlined functions) declared within the
class are written.  The files <code>complex.h</code> and <code>complex.cpp</code> are normally
placed in a directory with other classes and libraries we have
defined.

<p>
Finally, we discuss here an example of a main program which uses this
particular class.  An example of a program which uses our complex
class is given below. In particular we would like our class to perform
tasks like declaring complex variables, writing out the real and
imaginary part and performing algebraic operations such as adding or
multiplying two complex numbers.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec149"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>#include &quot;Complex.h&quot;
...  other include and declarations
int main ()
{
  Complex a(0.1,1.3);    // we declare a complex variable a
  Complex b(3.0), c(5.0,-2.3);  // we declare  complex variables b and c
  Complex d = b;         //  we declare  a new complex variable d
  cout &lt;&lt; &quot;d=&quot; &lt;&lt; d &lt;&lt; &quot;, a=&quot; &lt;&lt; a &lt;&lt; &quot;, b=&quot; &lt;&lt; b &lt;&lt; endl;
  d = a*c + b/a;  //   we add, multiply and divide two complex numbers
  cout &lt;&lt; &quot;Re(d)=&quot; &lt;&lt; d.Re() &lt;&lt; &quot;, Im(d)=&quot; &lt;&lt; d.Im() &lt;&lt; endl;  // write out of the real and imaginary parts

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec150"></a></h2>

We include the header file complex.h and define four different complex variables. These
are \( a=0.1+\imath 1.3 \), \( b=3.0+\imath 0 \) (note that if you don't define a value for the imaginary part  this is set to
zero), \( c=5.0-\imath 2.3 \) and \( d=b \).  Thereafter we have defined standard algebraic operations and the member functions
of the class which allows us to print out the real and imaginary part of a given variable.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec151"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>class Complex
{
private:
   double re, im; // real and imaginary part
public:
   Complex ();                              // Complex c;
   Complex (double re, double im = 0.0); // Definition of a complex variable;
   Complex (const Complex&amp; c);              // Usage: Complex c(a);   // equate two complex variables
   Complex&amp; operator= (const Complex&amp; c); // c = a;   //  equate two complex variables, same as previous
....

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec152"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>  ~Complex () {}                        // destructor
   double   Re () const;        // double real_part = a.Re();
   double   Im () const;        // double imag_part = a.Im();
   double   abs () const;       // double m = a.abs(); // modulus
   friend Complex operator+ (const Complex&amp;  a, const Complex&amp; b);
   friend Complex operator- (const Complex&amp;  a, const Complex&amp; b);
   friend Complex operator* (const Complex&amp;  a, const Complex&amp; b);
   friend Complex operator/ (const Complex&amp;  a, const Complex&amp; b);
};
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec153"></a></h2>

The class is defined via the statement <code>class Complex</code>. We must first use the key word
<code>class</code>, which in turn is followed by the user-defined variable name  <code>Complex</code>.
The body of the class, data and functions, is encapsulated  within the parentheses <code>{...}</code>.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec154"></a></h2>

Data and specific functions can be private, which means that they cannot be accessed from outside the class.
This means also that access cannot be inherited by other functions outside the class. If we use <code>protected</code>
instead of <code>private</code>, then data and functions can be inherited outside the class.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec155"></a></h2>

The key word <code>public</code> means  that data and functions can be accessed from outside the class.
Here we have defined several functions  which can be accessed by functions outside the class.
The declaration <code>friend</code> means that stand-alone functions can work on privately declared  variables  of the type
<code>(re, im)</code>.  Data members of a class should be declared as private variables.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec156"></a></h2>

The first public function we encounter is a so-called
constructor, which  tells how we declare a variable of type <code>Complex</code>
and how this variable is initialized. We have chose  three possibilities in the example above:

<p>
A declaration like <code>Complex c;</code> calls the member function <code>Complex()</code> which can have the following implementation

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>Complex:: Complex () { re = im = 0.0; }
</code></pre>
<!-- end verbatim block -->

<p>
meaning that it sets the real and imaginary parts to zero. Note the
way a member function is defined. The constructor is the first
function that is called when an object is instantiated.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec157"></a></h2>

Another possibility is

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>Complex:: Complex () {}
</code></pre>
<!-- end verbatim block -->
which means that there is no initialization of the real and imaginary parts. The drawback is that a given compiler can then assign random values to a given variable.

<p>
A call like <code>Complex a(0.1,1.3);</code> means that we could call the member function `Complex(double, double)`as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>Complex:: Complex (double re_a, double im_a) {
    re = re_a; im = im_a; }
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec158"></a></h2>

The simplest member function are those we defined to extract
the real and imaginary part of a variable. Here you have to recall that these are private data,
that is they invisible for users of the class.  We obtain a copy of these variables by defining the
functions

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>double Complex:: Re () const { return re; }} //  getting the real part
double Complex:: Im () const { return im; }  //   and the imaginary part
</code></pre>
<!-- end verbatim block -->
Note that we have introduced   the declaration  <code>const</code>.  What does it mean?
This declaration means that a variabale cannot be changed within  a called function.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec159"></a></h2>

If we define a variable as
<code>const double p = 3;</code> and then try to change its value, we will get an error when we
compile our program. This means that constant arguments in functions cannot be changed.

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>// const arguments (in functions) cannot be changed:
void myfunc (const Complex&amp; c)
{ c.re = 0.2; /* ILLEGAL!! compiler error... */  }
</code></pre>
<!-- end verbatim block -->
If we declare the function and try to change the value to \( 0.2 \), the compiler will complain by sending
an error message.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec160"></a></h2>

If we define a function to compute the absolute value of complex variable like

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>double Complex:: abs ()  { return sqrt(re*re + im*im);}
</code></pre>
<!-- end verbatim block -->
without the constant declaration  and define thereafter a function
<code>myabs</code> as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>double myabs (const Complex&amp; c)
{ return c.abs(); }   // Not ok because c.abs() is not a const func.
</code></pre>
<!-- end verbatim block -->
the compiler would not allow the c.abs() call in myabs
since <code>Complex::abs</code> is not a constant member function.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec161"></a></h2>

Constant functions cannot change the object's state.
To avoid this we declare the function <code>abs</code> as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>double Complex:: abs () const { return sqrt(re*re + im*im); }
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec162"></a></h2>

C++ (and Fortran) allow for overloading of operators. That means we
can define algebraic operations on for example vectors or any
arbitrary object.  As an example, a vector addition of the type \( {\bf
c} = {\bf a} + {\bf b} \) means that we need to write a small part of
code with a for-loop over the dimension of the array.  We would rather
like to write this statement as <code>c = a+b;</code> as this makes the code much
more readable and close to eventual equations we want to code.  To
achieve this we need to extend the definition of operators.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec163"></a></h2>

Let us study the declarations in our complex class.
In our main function we have a statement like <code>d = b;</code>, which means
that we call <code>d.operator= (b)</code> and we have defined a so-called assignment operator
as a part of the class defined as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>Complex&amp; Complex:: operator= (const Complex&amp; c)
{
   re = c.re;
   im = c.im;
   return *this;
}
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec164"></a></h2>

With this function, statements like
<code>Complex d = b;</code> or <code>Complex d(b);</code>
make a new object \( d \), which becomes a copy of \( b \).
We can make simple implementations in terms of the assignment

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>Complex:: Complex (const Complex&amp; c)
{ *this = c; }
</code></pre>
<!-- end verbatim block -->
which  is a pointer to "this object", <code>*this</code> is the present object,
so <code>*this = c;</code> means setting the present object equal to \( c \), that is
<code>this->operator= (c);</code>.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec165"></a></h2>

The meaning of the addition operator \( + \) for Complex objects is defined in the
function
<code>Complex operator+ (const Complex& a, const Complex& b); // a+b</code>
The compiler translates <code>c = a + b;</code> into <code>c = operator+ (a, b);</code>.
Since this implies the call to function, it brings in an additional overhead. If speed
is crucial and this function call is performed inside a loop, then it is more difficult for a
given compiler to perform optimizations of a loop.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec166"></a></h2>

The solution to this is to inline functions.   We discussed inlining in chapter
2 of the lecture notes.
Inlining means that the function body is copied directly into
the calling code, thus avoiding calling the function.
Inlining is enabled by the inline keyword

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>inline Complex operator+ (const Complex&amp; a, const Complex&amp; b)
{ return Complex (a.re + b.re, a.im + b.im); }
</code></pre>
<!-- end verbatim block -->
Inline functions, with complete bodies must be written in the header file  complex.h.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec167"></a></h2>

Consider  the case <code>c = a + b;</code>
that is,  <code>c.operator= (operator+ (a,b));</code>
If <code>operator+</code>, <code>operator=</code> and the constructor <code>Complex(r,i)</code> all
are inline functions, this transforms to

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>c.re = a.re + b.re;
c.im = a.im + b.im;
</code></pre>
<!-- end verbatim block -->
by the compiler, i.e., no function calls

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec168"></a></h2>

The stand-alone function <code>operator+</code> is a friend of the Complex  class

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>class Complex
{
   ...
   friend Complex operator+ (const Complex&amp; a, const Complex&amp; b);
   ...
};
</code></pre>
<!-- end verbatim block -->
so it can read (and manipulate) the private data parts \( re \) and
\( im \) via

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>inline Complex operator+ (const Complex&amp; a, const Complex&amp; b)
{ return Complex (a.re + b.re, a.im + b.im); }
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec169"></a></h2>

Since we do not need to alter the re and im variables, we can
get the values by Re() and Im(), and there is no need to be a
friend function

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>inline Complex operator+ (const Complex&amp; a, const Complex&amp; b)
{ return Complex (a.Re() + b.Re(), a.Im() + b.Im()); }
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec170"></a></h2>

The multiplication functionality can now be extended to imaginary numbers by the following code

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>inline Complex operator* (const Complex&amp; a, const Complex&amp; b)
{
  return Complex(a.re*b.re - a.im*b.im, a.im*b.re + a.re*b.im);

</code></pre>
<!-- end verbatim block -->
It will be convenient to inline all functions used by this operator.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec171"></a></h2>

To inline the complete expression <code>a*b;</code>, the constructors and
<code>operator=</code>  must also be inlined.  This can be achieved via the following piece of code

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>inline Complex:: Complex () { re = im = 0.0; }
inline Complex:: Complex (double re_, double im_)
{ ... }
inline Complex:: Complex (const Complex&amp; c)
{ ... }
inline Complex:: operator= (const Complex&amp; c)
{ ... }
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec172"></a></h2>

<!-- begin verbatim block  cppcod-->
<pre><code>// e, c, d are complex
e = c*d;
// first compiler translation:
e.operator= (operator* (c,d));
// result of nested inline functions
// operator=, operator*, Complex(double,double=0):
e.re = c.re*d.re - c.im*d.im;
e.im = c.im*d.re + c.re*d.im;
</code></pre>
<!-- end verbatim block -->
The definitions <code>operator-</code> and <code>operator/</code> follow the same set up.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec173"></a></h2>

Finally, if we wish to write to file or another device a complex number using the simple syntax
<code>cout << c;</code>, we obtain this by defining
the effect of \( << \) for a Complex object as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>ostream&amp; operator&lt;&lt; (ostream&amp; o, const Complex&amp; c)
{ o &lt;&lt; &quot;(&quot; &lt;&lt; c.Re() &lt;&lt; &quot;,&quot; &lt;&lt; c.Im() &lt;&lt; &quot;) &quot;; return o;}
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes, templates  <a name="___sec174"></a></h2>

What if we wanted to make a class which takes integers
or floating point numbers with single precision?
A simple way to achieve this is copy and paste our class and replace <code>double</code> with for
example <code>int</code>.

<p>
C++  allows us to do this automatically via the usage of templates, which
are the C++ constructs for parameterizing parts of
classes. Class templates  is a template for producing classes. The declaration consists
of the keyword <code>template</code> followed by a list of template arguments enclosed in brackets.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec175"></a></h2>

We can therefore make a more general class by rewriting our original example as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>template&lt;class T&gt;
class Complex
{
private:
   T re, im; // real and imaginary part
public:
   Complex ();                              // Complex c;
   Complex (T re, T im = 0); // Definition of a complex variable;
   Complex (const Complex&amp; c);              // Usage: Complex c(a);   // equate two complex variables
   Complex&amp; operator= (const Complex&amp; c); // c = a;   //  equate two complex variables, same as previous

</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec176"></a></h2>

We can therefore make a more general class by rewriting our original example as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>  ~Complex () {}                        // destructor
   T   Re () const;        // T real_part = a.Re();
   T   Im () const;        // T imag_part = a.Im();
   T   abs () const;       // T m = a.abs(); // modulus
   friend Complex operator+ (const Complex&amp;  a, const Complex&amp; b);
   friend Complex operator- (const Complex&amp;  a, const Complex&amp; b);
   friend Complex operator* (const Complex&amp;  a, const Complex&amp; b);
   friend Complex operator/ (const Complex&amp;  a, const Complex&amp; b);
};
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec177"></a></h2>

What it says is that <code>Complex</code> is a parameterized type with \( T \) as a parameter and \( T \) has to be a type such as double
or float.
The class complex is now a class template
and we would define variables in a code as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>Complex&lt;double&gt; a(10.0,5.1);
Complex&lt;int&gt; b(1,0);
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec178"></a></h2>

Member functions of our class are defined by preceding the name of the function with the <code>template</code> keyword.
Consider the function we defined as <code>Complex:: Complex (double re_a, double im_a)</code>.
We would rewrite this function as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>template&lt;class T&gt;
Complex&lt;T&gt;:: Complex (T re_a, T im_a)
{ re = re_a; im = im_a; }
</code></pre>
<!-- end verbatim block -->
The member functions  are otherwise defined following ordinary member function definitions.

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec179"></a></h2>

Here follows a very simple first class in the file squared.h

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>// Not all declarations here
// Class to compute the square of a number
template&lt;class T&gt;
class Squared{
  public:
    // Default constructor, not used here
    Squared(){}

    // Overload the function operator()
    T operator()(T x){return x*x;}

};
</code></pre>
<!-- end verbatim block -->

<p>
<!-- !split -->

<h2>Programming classes  <a name="___sec180"></a></h2>

and we would use it as

<p>
<!-- begin verbatim block  cppcod-->
<pre><code>#include &lt;iostream&gt;
#include &quot;squared.h&quot;
using namespace std;

int main(){
  Squared&lt;double&gt; s;
  cout &lt;&lt; s(3) &lt;&lt; endl;

</code></pre>
<!-- end verbatim block -->

<h1>Week 37  <a name="___sec181"></a></h1>

<!-- !split -->

<h2>Overview of week 37  <a name="___sec182"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b>Eigenvalue problems and project 2.</b>

<ul>
 <li> Discussion of Jacobi's algorithm, chapter 7.1-7.2</li>
 <li> Presentation of project 2.</li>
</ul>

Reading assignment this week: chapters 7.1-7.4 of lecture notes
</div>


<p>
<!-- !split -->

<h2>Eigenvalue problems, basic definitions  <a name="___sec183"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Let us consider the matrix \( {\bf A} \) of dimension \( n \). The eigenvalues of
\( {\bf A} \) are defined through the matrix equation 
$$
   {\bf A}{\bf x}^{(\nu)} = \lambda^{(\nu)}{\bf x}^{(\nu)},
$$

where \( \lambda^{(\nu)} \) are the eigenvalues and \( {\bf x}^{(\nu)} \) the
corresponding eigenvectors.
Unless otherwise stated, when we use the wording eigenvector we mean the
right eigenvector. The left eigenvalue problem is defined as 
$$
{\bf x}^{(\nu)}_L{\bf A} = \lambda^{(\nu)}{\bf x}^{(\nu)}_L
$$

The above right eigenvector problem is equivalent to a set of \( n \) equations with \( n \) unknowns
\( x_i \).
</div>


<p>
<!-- !split -->

<h2>Eigenvalue problems, basic definitions  <a name="___sec184"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The eigenvalue problem can be rewritten as 
$$
   \left( {\bf A}-\lambda^{(\nu)} {\bf I} \right) {\bf x}^{(\nu)} = 0,
$$

with \( {\bf I} \) being the unity matrix. This equation provides
a solution to the problem if and only if the determinant
is zero, namely
$$
   \left| {\bf A}-\lambda^{(\nu)}{\bf I}\right| = 0,
$$

which in turn means that the determinant is a polynomial
of degree \( n \) in \( \lambda \) and in general we will have 
\( n \) distinct zeros.
</div>


<p>
<!-- !split -->

<h2>Eigenvalue problems, basic definitions  <a name="___sec185"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The eigenvalues of a matrix 
\( {\bf A}\in {\mathbb{C}}^{n\times n} \)
are thus the \( n \) roots of its characteristic polynomial 
$$
P(\lambda) = det(\lambda{\bf I}-{\bf A}),
$$

or 
$$
  P(\lambda)= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right).
$$

The set of these roots is called the spectrum and is denoted as
\( \lambda({\bf A}) \).
If \( \lambda({\bf A})=\left\{\lambda_1,\lambda_2,\dots ,\lambda_n\right\} \) then we have
$$
   det({\bf A})= \lambda_1\lambda_2\dots\lambda_n, 
$$

and if we define the trace of \( {\bf A} \) as
$$
Tr({\bf A})=\sum_{i=1}^n a_{ii}$$

then
$$
Tr({\bf A})=\lambda_1+\lambda_2+\dots+\lambda_n.
$$
</div>


<p>
<!-- !split -->

<h2>Abel-Ruffini Impossibility Theorem  <a name="___sec186"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The <em>Abel-Ruffini</em> theorem (also known as Abel's impossibility theorem) 
states that there is no general solution in radicals to polynomial equations of degree five or higher.

<p>
The content of this theorem is frequently misunderstood. It does not assert that higher-degree polynomial equations are unsolvable. 
In fact, if the polynomial has real or complex coefficients, and we allow complex solutions, then every polynomial equation has solutions; this is the fundamental theorem of algebra. Although these solutions cannot always be computed exactly with radicals, they can be computed to any desired degree of accuracy using numerical methods such as the Newton-Raphson method or Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees.

<p>
The theorem only concerns the form that such a solution must take. The content of the theorem is 
that the solution of a higher-degree equation cannot in all cases be expressed in terms of the polynomial coefficients with a finite number of operations of addition, subtraction, multiplication, division and root extraction. Some polynomials of arbitrary degree, of which the simplest nontrivial example is the monomial equation \( ax^n = b \), are always solvable with a radical.
</div>


<p>
<!-- !split -->

<h2>Abel-Ruffini Impossibility Theorem  <a name="___sec187"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>

<p>
The <em>Abel-Ruffini</em> theorem says that there are some fifth-degree equations whose solution cannot be so expressed. 
The equation \( x^5 - x + 1 = 0 \) is an example. Some other fifth degree equations can be solved by radicals, 
for example \( x^5 - x^4 - x + 1 = 0 \). The precise criterion that distinguishes between those equations that can be solved 
by radicals and those that cannot was given by Galois and is now part of Galois theory: 
a polynomial equation can be solved by radicals if and only if its Galois group is a solvable group.

<p>
Today, in the modern algebraic context, we say that second, third and fourth degree polynomial 
equations can always be solved by radicals because the symmetric groups \( S_2, S_3 \) and \( S_4 \) are solvable groups, 
whereas \( S_n \) is not solvable for \( n \ge 5 \).
</div>


<p>
<!-- !split -->

<h2>Eigenvalue problems, basic definitions  <a name="___sec188"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
In the present discussion we assume that our matrix is real and symmetric, that is 
\( {\bf A}\in {\mathbb{R}}^{n\times n} \).
The matrix \( {\bf A} \) has \( n \) eigenvalues
\( \lambda_1\dots \lambda_n \) (distinct or not). Let \( {\bf D} \) be the
diagonal matrix with the eigenvalues on the diagonal
$$
{\bf D}=    \left( \begin{array}{ccccccc} \lambda_1 & 0 & 0   & 0    & \dots  &0     & 0 \\
                                0 & \lambda_2 & 0 & 0    & \dots  &0     &0 \\
                                0   & 0 & \lambda_3 & 0  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\lambda_{n-1} & \\
                                0   & \dots & \dots & \dots  &\dots       &0 & \lambda_n

             \end{array} \right).
$$

If \( {\bf A} \) is real and symmetric then there exists a real orthogonal matrix \( {\bf S} \) such that
$$
     {\bf S}^T {\bf A}{\bf S}= \mathrm{diag}(\lambda_1,\lambda_2,\dots ,\lambda_n),
$$

and for \( j=1:n \) we have \( {\bf A}{\bf S}(:,j) = \lambda_j {\bf S}(:,j) \).
</div>


<p>
<!-- !split -->

<h2>Eigenvalue problems, basic definitions  <a name="___sec189"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
To obtain the eigenvalues of \( {\bf A}\in {\mathbb{R}}^{n\times n} \),
the strategy is to
perform a series of similarity transformations on the original
matrix \( {\bf A} \), in order to reduce it either into a  diagonal form as above
or into a  tridiagonal form. 

<p>
We say that a matrix \( {\bf B} \) is a similarity
transform  of  \( {\bf A} \) if 
$$
     {\bf B}= {\bf S}^T {\bf A}{\bf S}, \hspace{1cm} \mathrm{where} \hspace{1cm}  {\bf S}^T{\bf S}={\bf S}^{-1}{\bf S} ={\bf I}.
$$

The importance of a similarity transformation lies in the fact that
the resulting matrix has the same
eigenvalues, but the eigenvectors are in general different.
</div>


<p>
<!-- !split -->

<h2>Eigenvalue problems, basic definitions  <a name="___sec190"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
To prove this we
start with  the eigenvalue problem and a similarity transformed matrix \( {\bf B} \).
$$
   {\bf A}{\bf x}=\lambda{\bf x} \hspace{1cm} \mathrm{and}\hspace{1cm} 
    {\bf B}= {\bf S}^T {\bf A}{\bf S}.
$$

We multiply the first equation on the left by \( {\bf S}^T \) and insert
\( {\bf S}^{T}{\bf S} = {\bf I} \) between \( {\bf A} \) and \( {\bf x} \). Then we get
$$
\begin{equation}
   ({\bf S}^T{\bf A}{\bf S})({\bf S}^T{\bf x})=\lambda{\bf S}^T{\bf x} ,
\end{equation}  
$$

which is the same as 
$$
   {\bf B} \left ( {\bf S}^T{\bf x} \right ) = \lambda \left ({\bf S}^T{\bf x}\right ).
$$

The variable  \( \lambda \) is an eigenvalue of \( {\bf B} \) as well, but with
eigenvector \( {\bf S}^T{\bf x} \).
</div>


<p>
<!-- !split -->

<h2>Eigenvalue problems, basic definitions  <a name="___sec191"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The basic philosophy is to

<ul>
 <li> Either apply subsequent similarity transformations (direct method) so that</li> 
</ul>

$$
\begin{equation}
   {\bf S}_N^T\dots {\bf S}_1^T{\bf A}{\bf S}_1\dots {\bf S}_N={\bf D} ,
\end{equation}
$$


<ul>
 <li> Or apply subsequent similarity transformations so that \( {\bf A} \) becomes tridiagonal (Householder) or upper/lower triangular (the <em>QR</em> method to be discussed later).</li> 
 <li> Thereafter, techniques for obtaining eigenvalues from tridiagonal matrices can be used.</li>
 <li> Or use so-called power methods</li>
 <li> Or use iterative methods (Krylov, Lanczos, Arnoldi). These methods are popular for huge matrix problems.</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec192"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
In project 1 we rewrote our original differential equation in terms of a discretized equation with approximations to the 
derivatives as
$$
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
$$

with \( i=1,2,\dots, n \). We need to add to this system the two boundary conditions \( u(a) =u_0 \) and \( u(b) = u_{n+1} \).
If we define a matrix
$$
    {\bf A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 &amp; 2 & -1 & & & \\
                           & -1 &amp; 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 &amp; 2 \\
                      \end{array} \right)
$$

and the corresponding vectors \( {\bf u} = (u_1, u_2, \dots,u_n)^T \) and 
\( {\bf f}({\bf u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T \)  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns 
$$
   {\bf A}{\bf u} = {\bf f}({\bf u}).
 $$
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec193"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We are first interested in the solution of the radial part of Schroedinger's equation for one electron. This equation reads
$$
  -\frac{\hbar^2}{2 m} \left ( \frac{1}{r^2} \frac{d}{dr} r^2
  \frac{d}{dr} - \frac{l (l + 1)}{r^2} \right )R(r) 
     + V(r) R(r) = E R(r).
$$

In our case \( V(r) \) is the harmonic oscillator potential \( (1/2)kr^2 \) with
\( k=m\omega^2 \) and \( E \) is
the energy of the harmonic oscillator in three dimensions.
The oscillator frequency is \( \omega \) and the energies are
$$
E_{nl}=  \hbar \omega \left(2n+l+\frac{3}{2}\right),
$$

with \( n=0,1,2,\dots \) and \( l=0,1,2,\dots \).
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec194"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Since we have made a transformation to spherical coordinates it means that 
\( r\in [0,\infty) \).  
The quantum number
\( l \) is the orbital momentum of the electron.   Then we substitute \( R(r) = (1/r) u(r) \) and obtain
$$
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r) 
       + \left ( V(r) + \frac{l (l + 1)}{r^2}\frac{\hbar^2}{2 m}
                                    \right ) u(r)  = E u(r) .
$$

The boundary conditions are \( u(0)=0 \) and \( u(\infty)=0 \).
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec195"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We introduce a dimensionless variable \( \rho = (1/\alpha) r \)
where \( \alpha \) is a constant with dimension length and get
$$
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       + \left ( V(\rho) + \frac{l (l + 1)}{\rho^2}
         \frac{\hbar^2}{2 m\alpha^2} \right ) u(\rho)  = E u(\rho) .
$$

In project 2 we choose \( l=0 \).
Inserting \( V(\rho) = (1/2) k \alpha^2\rho^2 \) we end up with
$$
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       + \frac{k}{2} \alpha^2\rho^2u(\rho)  = E u(\rho) .
$$

We multiply thereafter with \( 2m\alpha^2/\hbar^2 \) on both sides and obtain
$$
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec196"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We have thus
$$
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
$$

The constant \( \alpha \) can now be fixed
so that
$$
\frac{mk}{\hbar^2} \alpha^4 = 1,
$$

or 
$$
\alpha = \left(\frac{\hbar^2}{mk}\right)^{1/4}.
$$

Defining
$$
\lambda = \frac{2m\alpha^2}{\hbar^2}E,
$$

we can rewrite Schr\"odinger's equation as
$$
  -\frac{d^2}{d\rho^2} u(\rho) + \rho^2u(\rho)  = \lambda u(\rho) .
$$

This is the first equation to solve numerically. In three dimensions 
the eigenvalues for \( l=0 \) are 
\( \lambda_0=3,\lambda_1=7,\lambda_2=11,\dots . \)
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec197"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We use the by now standard
expression for the second derivative of a function \( u \)
$$
\begin{equation}
    u''=\frac{u(\rho+h) -2u(\rho) +u(\rho-h)}{h^2} +O(h^2),
    \label{eq:diffoperation}
\end{equation} 
$$

where \( h \) is our step.
Next we define minimum and maximum values for the variable \( \rho \),
\( \rho_{\mathrm{min}}=0 \)  and \( \rho_{\mathrm{max}} \), respectively.
You need to check your results for the energies against different values
\( \rho_{\mathrm{max}} \), since we cannot set
\( \rho_{\mathrm{max}}=\infty \).
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec198"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
With a given number of steps, \( n_{\mathrm{step}} \), we then 
define the step \( h \) as
$$
  h=\frac{\rho_{\mathrm{max}}-\rho_{\mathrm{min}} }{n_{\mathrm{step}}}.
$$

Define an arbitrary value of \( \rho \) as 
$$
    \rho_i= \rho_{\mathrm{min}} + ih \hspace{1cm} i=0,1,2,\dots , n_{\mathrm{step}}
$$

we can rewrite the Schr\"odinger equation for \( \rho_i \) as
$$
-\frac{u(\rho_i+h) -2u(\rho_i) +u(\rho_i-h)}{h^2}+\rho_i^2u(\rho_i)  = \lambda u(\rho_i),
$$

or in  a more compact way
$$
-\frac{u_{i+1} -2u_i +u_{i-1}}{h^2}+\rho_i^2u_i=-\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}+V_iu_i  = \lambda u_i,
$$

where \( V_i=\rho_i^2 \) is the harmonic oscillator potential.
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec199"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Define first the diagonal matrix element
$$
   d_i=\frac{2}{h^2}+V_i,
$$

and the non-diagonal matrix element 
$$
   e_i=-\frac{1}{h^2}.
$$

In this case the non-diagonal matrix elements are given by a mere constant. <em>All non-diagonal matrix elements are equal</em>.

<p>
With these definitions the Schroedinger equation takes the following form
$$
d_iu_i+e_{i-1}u_{i-1}+e_{i+1}u_{i+1}  = \lambda u_i,
$$

where \( u_i \) is unknown. We can write the 
latter equation as a matrix eigenvalue problem 
$$
\begin{equation}
    \left( \begin{array}{ccccccc} d_1 & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2 & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3 & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{n_{\mathrm{step}}-2} & e_{n_{\mathrm{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n_{\mathrm{step}}-1} & d_{n_{\mathrm{step}}-1}

             \end{array} \right)      \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right)=\lambda \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right) 
      \label{eq:sematrix}
\end{equation} 
$$

or if we wish to be more detailed, we can write the tridiagonal matrix as
$$
\begin{equation}
    \left( \begin{array}{ccccccc} \frac{2}{h^2}+V_1 & -\frac{1}{h^2} & 0   & 0    & \dots  &0     & 0 \\
                                -\frac{1}{h^2} & \frac{2}{h^2}+V_2 & -\frac{1}{h^2} & 0    & \dots  &0     &0 \\
                                0   & -\frac{1}{h^2} & \frac{2}{h^2}+V_3 & -\frac{1}{h^2}  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\frac{2}{h^2}+V_{n_{\mathrm{step}}-2} & -\frac{1}{h^2}\\
                                0   & \dots & \dots & \dots  &\dots       &-\frac{1}{h^2} & \frac{2}{h^2}+V_{n_{\mathrm{step}}-1}

             \end{array} \right)  
\label{eq:matrixse} 
\end{equation} 
$$

Recall that the solutions are known via the boundary conditions at
\( i=n_{\mathrm{step}} \) and at the other end point, that is for  \( \rho_0 \).
The solution is zero in both cases.
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec200"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We are going to study two electrons in a harmonic oscillator well which
also interact via a repulsive Coulomb interaction.
Let us start with the single-electron equation written as
$$
  -\frac{\hbar^2}{2 m} \frac{d^2}{dr^2} u(r) 
       + \frac{1}{2}k r^2u(r)  = E^{(1)} u(r),
$$

where \( E^{(1)} \) stands for the energy with one electron only.
For two electrons with no repulsive Coulomb interaction, we have the following 
Schroedinger equation
$$
\left(  -\frac{\hbar^2}{2 m} \frac{d^2}{dr_1^2} -\frac{\hbar^2}{2 m} \frac{d^2}{dr_2^2}+ \frac{1}{2}k r_1^2+ \frac{1}{2}k r_2^2\right)u(r_1,r_2)  = E^{(2)} u(r_1,r_2) .
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec201"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Note that we deal with a two-electron wave function \( u(r_1,r_2) \) and 
two-electron energy \( E^{(2)} \).

<p>
With no interaction this can be written out as the product of two
single-electron wave functions, that is we have a solution on closed form.

<p>
We introduce the relative coordinate \( {\bf r} = {\bf r}_1-{\bf r}_2 \)
and the center-of-mass coordinate \( {\bf R} = 1/2({\bf r}_1+{\bf r}_2) \).
With these new coordinates, the radial Schr\"odinger equation reads
$$
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2} -\frac{\hbar^2}{4 m} \frac{d^2}{dR^2}+ \frac{1}{4} k r^2+  kR^2\right)u(r,R)  = E^{(2)} u(r,R).
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec202"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The equations for \( r \) and \( R \) can be separated via the ansatz for the 
wave function \( u(r,R) = \psi(r)\phi(R) \) and the energy is given by the sum
of the relative energy \( E_r \) and the center-of-mass energy \( E_R \), that
is
$$
E^{(2)}=E_r+E_R.
$$

We add then the repulsive Coulomb interaction between two electrons,
namely a term 
$$
V(r_1,r_2) = \frac{\beta e^2}{|{\bf r}_1-{\bf r}_2|}=\frac{\beta e^2}{r},
$$

with \( \beta e^2=1.44 \) eVnm.
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec203"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Adding this term, the \( r \)-dependent Schroedinger equation becomes
$$
\left(  -\frac{\hbar^2}{m} \frac{d^2}{dr^2}+ \frac{1}{4}k r^2+\frac{\beta e^2}{r}\right)\psi(r)  = E_r \psi(r).
$$

This equation is similar to the one we had previously in parts (a) and (b) 
and we introduce
again a dimensionless variable \( \rho = r/\alpha \). Repeating the same
steps, we arrive at 
$$
  -\frac{d^2}{d\rho^2} \psi(\rho) 
       + \frac{mk}{4\hbar^2} \alpha^4\rho^2\psi(\rho)+\frac{m\alpha \beta e^2}{\rho\hbar^2}\psi(\rho)  = 
\frac{m\alpha^2}{\hbar^2}E_r \psi(\rho) .
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec204"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We want to manipulate this equation further to make it as similar to that in (a)
as possible. We define a 'frequency' 
$$
\omega_r^2=\frac{1}{4}\frac{mk}{\hbar^2} \alpha^4,
$$

and fix the constant \( \alpha \) by requiring 
$$
\frac{m\alpha \beta e^2}{\hbar^2}=1
$$

or 
$$
\alpha = \frac{\hbar^2}{m\beta e^2}.
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec205"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Defining 
$$
\lambda = \frac{m\alpha^2}{\hbar^2}E,
$$

we can rewrite Schroedinger's equation as
$$
  -\frac{d^2}{d\rho^2} \psi(\rho) + \omega_r^2\rho^2\psi(\rho) +\frac{1}{\rho}\psi(\rho) = \lambda \psi(\rho).
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec206"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We treat \( \omega_r \) as a parameter which reflects the strength of the oscillator potential.

<p>
Here we will study the cases \( \omega_r = 0.01 \), \( \omega_r = 0.5 \), \( \omega_r =1 \),
and \( \omega_r = 5 \)   
for the ground state only, that is the lowest-lying state.
</div>


<p>
<!-- !split -->

<h2>Discussion of project 2  <a name="___sec207"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
With no repulsive Coulomb interaction 
you should get a result which corresponds to 
the relative energy of a non-interacting system.   
Make sure your results are 
stable as functions of \( \rho_{\mathrm{max}} \) and the number of steps.

<p>
We are only interested in the ground state with \( l=0 \). We omit the 
center-of-mass energy.

<p>
For specific oscillator frequencies, the above equation has analytic answers,
see the article by M.&nbsp;Taut, Phys. Rev. A 48, 3561 - 3566 (1993).
The article can be retrieved from the following web address <a href="http://prola.aps.org/abstract/PRA/v48/i5/p3561_1" target="_self"><tt>http://prola.aps.org/abstract/PRA/v48/i5/p3561_1</tt></a>.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec208"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b>The general overview.</b>

<p>
One speaks normally of two main approaches to solving the eigenvalue problem.

<ul>
 <li> The first is the formal method, involving determinants and the  characteristic polynomial. This proves how many eigenvalues  there are, and is the way most of you learned about how to solve the eigenvalue problem, but for matrices of dimensions greater than 2 or 3, it is rather impractical.</li>
 <li> The other general approach is to use similarity or unitary tranformations  to reduce a matrix to diagonal form. This is normally done in two steps: first reduce to for example a <em>tridiagonal</em> form, and then to diagonal form. The main algorithms we will discuss in detail, Jacobi's and  Householder's  (so-called direct method) and Lanczos algorithms (an iterative method), follow this</li> 
</ul>

methodology.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec209"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Direct or non-iterative methods  require for matrices of dimensionality \( n\times n \) typically \( O(n^3) \) operations. These methods are normally called standard methods and are used for dimensionalities
\( n \sim 10^5 \) or smaller. A brief historical overview  

<p>
<table border="1">
<thead>
<tr><th align="center">       Year      </th> <td align="center">     \( n \)     </td> <th align="center">                 </th> </tr>
</thead>
<tbody>
<tr><td align="center">   1950                 </td> <td align="center">   \( n=20 \)           </td> <td align="center">   (Wilkinson)          </td> </tr>
<tr><td align="center">   1965                 </td> <td align="center">   \( n=200 \)          </td> <td align="center">   (Forsythe et al.)    </td> </tr>
<tr><td align="center">   1980                 </td> <td align="center">   \( n=2000 \)         </td> <td align="center">   Linpack              </td> </tr>
<tr><td align="center">   1995                 </td> <td align="center">   \( n=20000 \)        </td> <td align="center">   Lapack               </td> </tr>
<tr><td align="center">   2012                 </td> <td align="center">   \( n\sim 10^5 \)     </td> <td align="center">   Lapack               </td> </tr>
</tbody>
</table>
<p>
shows that in the course of 60 years the dimension that  direct diagonalization methods can handle  has increased by almost a factor of
\( 10^4 \). However, it pales beside the progress achieved by computer hardware, from flops to petaflops, a factor of almost \( 10^{15} \). We see clearly played out in history the \( O(n^3) \) bottleneck  of direct matrix algorithms.

<p>
Sloppily speaking, when  \( n\sim 10^4 \) is cubed we have \( O(10^{12}) \) operations, which is smaller than the \( 10^{15} \) increase in flops.  

<p>
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec210"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
If the matrix to diagonalize is large and sparse, direct methods simply become impractical, 
also because
many of the direct methods tend to destroy sparsity. As a result large dense matrices may arise during the diagonalization procedure.  The idea behind iterative methods is to project the 
$n-$dimensional problem in smaller spaces, so-called Krylov subspaces. 
Given a matrix \( {\bf A} \) and a vector \( {\bf v} \), the associated Krylov sequences of vectors
(and thereby subspaces) 
\( {\bf v} \), \( {\bf A}{\bf v} \), \( {\bf A}^2{\bf v} \), \( {\bf A}^3{\bf v},\dots \), represent
successively larger Krylov subspaces. 

<p>
<table border="1">
<thead>
<tr><th align="center">               Matrix              </th> <td align="center">    \( {\bf A}{\bf x}={\bf b} \)   </td> <td align="center">\( {\bf A}{\bf x}=\lambda{\bf x} \)</td> </tr>
</thead>
<tbody>
<tr><td align="left">   \( {\bf A}={\bf A}^* \)                </td> <td align="left">   Conjugate gradient                     </td> <td align="left">   Lanczos                                </td> </tr>
<tr><td align="left">   \( {\bf A}\ne {\bf A}^* \)             </td> <td align="left">   GMRES etc                              </td> <td align="left">   Arnoldi                                </td> </tr>
</tbody>
</table>
<p>
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec211"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>

<p>
The Numerical Recipes codes have been rewritten in Fortran 90/95 and C/C++ by us.
The original source codes are taken from the widely used software
package LAPACK, which follows two other popular packages developed in the 1970s, 
namely EISPACK and LINPACK.

<ul>
 <li> LINPACK: package for linear equations  and least square problems.</li>
 <li> LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <a href="http://www.netlib.org" target="_self"><tt>http://www.netlib.org</tt></a>  it is  possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.</li>
 <li> BLAS (I, II and III): (Basic Linear Algebra Subprograms)  are routines that provide standard building blocks for performing basic vector and matrix operations.   Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations.</li> 
</ul>
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec212"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Consider an  (\( n\times n \)) orthogonal transformation matrix 
$$
{\bf S}=
 \left( 
   \begin{array}{cccccccc}
   1  &    0  & \dots &   0        &    0  & \dots & 0 &   0       \\
   0  &    1  & \dots &   0        &    0  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\ 
   0  &    0  & \dots & \cos\theta  &    0  & \dots & 0 & \sin\theta \\
   0  &    0  & \dots &   0        &    1  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\
   0  &    0  & \dots &  -\sin\theta        &    0  & \dots & 1 &   \cos\theta       \\ 
   0  &    0  & \dots & 0 & \dots & \dots & 0 & 1  
   \end{array}
 \right)
$$

with property \( {\bf S^{T}} = {\bf S^{-1}} \).
It performs a plane rotation around an angle \( \theta \) in the Euclidean 
$n-$dimensional space.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec213"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
It means that its matrix elements that differ
from zero are given by
$$
    s_{kk}= s_{ll}=\cos\theta, 
    s_{kl}=-s_{lk}= -\sin\theta, 
    s_{ii}=-s_{ii}=1\hspace{0.5cm} i\ne k \hspace{0.5cm} i \ne l,
$$

A similarity transformation 
$$
     {\bf B}= {\bf S}^T {\bf A}{\bf S},
$$

results in 
$$
\begin{eqnarray*}
b_{ik} &=& a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} &=& a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} &=& a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} &=& a_{ll}\cos^2\theta +2a_{kl}\cos\theta sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} &=& (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{eqnarray*}
$$

The angle \( \theta \) is  arbitrary. The recipe is to choose  \( \theta \) so that all
non-diagonal matrix elements \( b_{kl} \) become zero.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec214"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The main idea is thus to reduce systematically the 
norm of the 
off-diagonal matrix elements  of a matrix  \( {\bf A} \) 
$$
\mathrm{off}({\bf A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}.
$$

 To demonstrate the algorithm, we consider the  simple \( 2\times 2 \)  similarity transformation
of the full matrix. The matrix is symmetric, we single out $ 1\le k < l \le n$  and 
use the abbreviations \( c=\cos\theta \) and \( s=\sin\theta \) to obtain
$$
 \left( \begin{array}{cc} b_{kk} & 0 \\
                          0 & b_{ll} \\\end{array} \right)  =  \left( \begin{array}{cc} c & -s \\
                          s &c \\\end{array} \right)  \left( \begin{array}{cc} a_{kk} & a_{kl} \\
                          a_{lk} &a_{ll} \\\end{array} \right) \left( \begin{array}{cc} c & s \\
                          -s & c \\\end{array} \right).
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec215"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We require that the non-diagonal matrix elements \( b_{kl}=b_{lk}=0 \), implying that 
$$
a_{kl}(c^2-s^2)+(a_{kk}-a_{ll})cs = b_{kl} = 0.
$$

If \( a_{kl}=0 \) one sees immediately that \( \cos\theta = 1 \) and \( \sin\theta=0 \).
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec216"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The Frobenius norm of an orthogonal transformation is always preserved. The Frobenius norm is defined
as 
$$
 \mathrm{norm}({\bf A})_F =  \sqrt{\sum_{i=1}^n\sum_{j=1}^n |a_{ij}|^2}.
$$

This means that for our \( 2\times 2 \) case  we have
$$
2a_{kl}^2+a_{kk}^2+a_{ll}^2 = b_{kk}^2+b_{ll}^2,
$$

which leads to
$$
\mathrm{off}({\bf B})^2 = \mathrm{norm}({\bf B})_F^2-\sum_{i=1}^nb_{ii}^2=\mathrm{off}({\bf A})^2-2a_{kl}^2,
$$

since
$$
  \mathrm{norm}({\bf B})_F^2-\sum_{i=1}^nb_{ii}^2=\mathrm{norm}({\bf A})_F^2-\sum_{i=1}^na_{ii}^2+(a_{kk}^2+a_{ll}^2 -b_{kk}^2-b_{ll}^2).
$$

This results means that  the matrix \( {\bf A} \) moves closer to diagonal form  for each transformation.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec217"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Defining the quantities \( \tan\theta = t= s/c \) and
$$\cot 2\theta=\tau = \frac{a_{ll}-a_{kk}}{2a_{kl}},
$$

we obtain the quadratic equation (using \( \cot 2\theta=1/2(\cot \theta-\tan\theta) \)
$$
t^2+2\tau t-1= 0,
$$

resulting in 
$$
  t = -\tau \pm \sqrt{1+\tau^2},
$$

and \( c \) and \( s \) are easily obtained via
$$
   c = \frac{1}{\sqrt{1+t^2}},
$$

and \( s=tc \).  Choosing \( t \) to be the smaller of the roots ensures that \( |\theta| \le \pi/4 \) and has the 
effect of minimizing the difference between the matrices \( {\bf B} \) and \( {\bf A} \) since
$$
\mathrm{norm}({\bf B}-{\bf A})_F^2=4(1-c)\sum_{i=1,i\ne k,l}^n(a_{ik}^2+a_{il}^2) +\frac{2a_{kl}^2}{c^2}.
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec218"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>

<ul>
 <li> Choose a tolerance \( \epsilon \), making it a small number, typically \( 10^{-8} \) or smaller.</li>
 <li> Setup a <em>while</em> test  where one compares the norm of the newly computed off-diagonal</li>
</ul>

matrix elements  \[ \mathrm{off}({\bf A}) = \sqrt{\sum_{i=1}^n\sum_{j=1,j\ne i}^n a_{ij}^2}   >  \epsilon. \]

<ul>
 <li> Now choose the matrix elements \( a_{kl} \) so that we have those with largest value, that is \( |a_{kl}|=\mathrm{max}_{i\ne j} |a_{ij}| \).</li>
</ul>

<li> Compute thereafter \( \tau = (a_{ll}-a_{kk})/2a_{kl} \), \( \tan\theta \), \( \cos\theta \) and \( \sin\theta \).</li>

<ul>
 <li> Compute thereafter the similarity transformation for this set of values \( (k,l) \), obtaining the new matrix \( {\bf B}= {\bf S}(k,l,\theta)^T {\bf A}{\bf S}(k,l,\theta) \).</li>
 <li> Compute the new norm of the off-diagonal matrix elements and continue till you have satisfied \( \mathrm{off}({\bf B})  \le  \epsilon \)</li>
</ul>
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec219"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The convergence rate of the Jacobi method is however poor, one needs typically \( 3n^2-5n^2 \) rotations and each rotation 
requires \( 4n \) operations, resulting in a total of \( 12n^3-20n^3 \) operations in order to zero out non-diagonal matrix elements.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec220"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We specialize to a symmetric \( 3\times 3  \) matrix \( {\bf A} \).
We start the process as follows (assuming that \( a_{23}=a_{32} \) is the largest non-diagonal)
with \( c=\cos{\theta} \) and \( s=\sin{\theta} \)
$$
 {\bf B} =
      \left( \begin{array}{ccc} 
                1 & 0 & 0    \\
                0 & c & -s     \\
                0 & s & c
             \end{array} \right)\left( \begin{array}{ccc} 
                a_{11} & a_{12} & a_{13}    \\
                a_{21} & a_{22} & a_{23}     \\
                a_{31} & a_{32} & a_{33}
             \end{array} \right)
              \left( \begin{array}{ccc} 
                1 & 0 & 0    \\
                0 & c & s     \\
                0 & -s & c
             \end{array} \right).
$$

We will choose the angle \( \theta \) in order to have \( a_{23}=a_{32}=0 \).
We get (symmetric matrix)
$$
 {\bf B} =\left( \begin{array}{ccc} 
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
$$

Note that \( a_{11} \) is unchanged! As it should.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec221"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We have
$$
 {\bf B} =\left( \begin{array}{ccc} 
                a_{11} & a_{12}c -a_{13}s& a_{12}s+a_{13}c    \\
                a_{12}c -a_{13}s & a_{22}c^2+a_{33}s^2 -2a_{23}sc& (a_{22}-a_{33})sc +a_{23}(c^2-s^2)     \\
                a_{12}s+a_{13}c & (a_{22}-a_{33})sc +a_{23}(c^2-s^2) & a_{22}s^2+a_{33}c^2 +2a_{23}sc
             \end{array} \right).
$$

or
$$
\begin{eqnarray*}
b_{11} &=& a_{11} \\
b_{12} &=& a_{12}\cos\theta - a_{13}\sin\theta , 1 \ne 2, 1 \ne 3 \\
b_{13} &=& a_{13}\cos\theta + a_{12}\sin\theta , 1 \ne 2, 1 \ne 3 \nonumber\\
b_{22} &=& a_{22}\cos^2\theta - 2a_{23}\cos\theta \sin\theta +a_{33}\sin^2\theta\nonumber\\
b_{33} &=& a_{33}\cos^2\theta +2a_{23}\cos\theta \sin\theta +a_{22}\sin^2\theta\nonumber\\
b_{23} &=& (a_{22}-a_{33})\cos\theta \sin\theta +a_{23}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{eqnarray*}
$$

We will fix the angle \( \theta \) so that \( b_{23}=0 \).
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec222"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
We get then a new matrix
$$
 {\bf B} =\left( \begin{array}{ccc} 
                b_{11} & b_{12}& b_{13}    \\
                b_{12}& b_{22}& 0    \\
                b_{13}& 0& a_{33}
             \end{array} \right).
$$

We repeat then assuming that \( b_{12} \) is the largest non-diagonal matrix element and get a
new matrix 
$$
 {\bf C} =
      \left( \begin{array}{ccc} 
                c & -s & 0    \\
                s & c & 0     \\
                0 & 0 & 1
             \end{array} \right)\left( \begin{array}{ccc} 
                b_{11} & b_{12} & b_{13}    \\
                b_{12} & b_{22} & 0     \\
                b_{13} & 0 & b_{33}
             \end{array} \right)
              \left( \begin{array}{ccc} 
                c & s & 0    \\
                -s & c & 0     \\
                0 & 0 & 1
             \end{array} \right).
$$

We continue this process till all non-diagonal matrix elements are zero (ideally).
You will notice that performing the above operations that the matrix element 
\( b_{23} \) which was previous zero becomes different from zero.  This is one of the problems which slows
down the jacobi procedure.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec223"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The more general expression for the new matrix elements are
$$
\begin{eqnarray*}
b_{ii} &=& a_{ii}, i \ne k, i \ne l \\
b_{ik} &=& a_{ik}\cos\theta - a_{il}\sin\theta , i \ne k, i \ne l \\
b_{il} &=& a_{il}\cos\theta + a_{ik}\sin\theta , i \ne k, i \ne l \nonumber\\
b_{kk} &=& a_{kk}\cos^2\theta - 2a_{kl}\cos\theta \sin\theta +a_{ll}\sin^2\theta\nonumber\\
b_{ll} &=& a_{ll}\cos^2\theta +2a_{kl}\cos\theta \sin\theta +a_{kk}\sin^2\theta\nonumber\\
b_{kl} &=& (a_{kk}-a_{ll})\cos\theta \sin\theta +a_{kl}(\cos^2\theta-\sin^2\theta)\nonumber 
\end{eqnarray*}
$$

This is what we will need to code.
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec224"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b>Code example.</b>
<!-- begin verbatim block  cppcod-->
<pre><code>//  we have defined a matrix A and a matrix R for the eigenvector, both of dim n x n
//  The final matrix R has the eigenvectors in its row elements, it is set to one
//  for the diagonal elements in the beginning, zero else.
....
double tolerance = 1.0E-10; 
int iterations = 0;
while ( maxnondiag &gt; tolerance &amp;&amp; iterations &lt;= maxiter)
{
   int p, q;
   maxnondiag  = offdiag(A, p, q, n);
   Jacobi_rotate(A, R, p, q, n);
   iterations++;
}
...
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec225"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Finding the max nondiagonal element
<!-- begin verbatim block  cppcod-->
<pre><code>//  the offdiag function, using Armadillo
double offdiag(mat A, int p, int q, int n);
{
   double max;
   for (int i = 0; i &lt; n; ++i)
   {
       for ( int j = i+1; j &lt; n; ++j)
       {
           double aij = fabs(A(i,j));
           if ( aij &gt; max)
           { 
              max = aij;  p = i; q = j;
           }
       }
   }
   return max;
}
// more statements
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Discussion of Jacobi's method for eigenvalues  <a name="___sec226"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Finding the new matrix elements
<!-- begin verbatim block  cppcod-->
<pre><code>void Jacobi_rotate ( mat A, mat R, int k, int l, int n )
{
  double s, c;
  if ( A(k,l) != 0.0 ) {
    double t, tau;
    tau = (A(l,l) - A(k,k))/(2*A(k,l));
    
    if ( tau &gt;= 0 ) {
      t = 1.0/(tau + sqrt(1.0 + tau*tau));
    } else {
      t = -1.0/(-tau +sqrt(1.0 + tau*tau));
    }
    
    c = 1/sqrt(1+t*t);
    s = c*t;
  } else {
    c = 1.0;
    s = 0.0;
  }
  double a_kk, a_ll, a_ik, a_il, r_ik, r_il;
  a_kk = A(k,k);
  a_ll = A(l,l);
  A(k,k) = c*c*a_kk - 2.0*c*s*A(k,l) + s*s*a_ll;
  A(l,l) = s*s*a_kk + 2.0*c*s*A(k,l) + c*c*a_ll;
  A(k,l) = 0.0;  // hard-coding non-diagonal elements by hand
  A(l,k) = 0.0;  // same here
  for ( int i = 0; i &lt; n; i++ ) {
    if ( i != k &amp;&amp; i != l ) {
      a_ik = A(i,k);
      a_il = A(i,l);
      A(i,k) = c*a_ik - s*a_il;
      A(k,i) = A(i,k);
      A(i,l) = c*a_il + s*a_ik;
      A(l,i) = A(i,l);
    }
//  And finally the new eigenvectors
    r_ik = R(i,k);
    r_il = R(i,l);

    R(i,k) = c*r_ik - s*r_il;
    R(i,l) = c*r_il + s*r_ik;
  }
  return;
} // end of function jacobi_rotate
</code></pre>
<!-- end verbatim block -->
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec227"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The drawbacks with Jacobi's method are rather obvious, with perhaps the most negative feature being the fact that we cannot tell * a priori* how many transformations are needed. Can we do better?  
The answer to this is yes and is given by a clever algorithm outlined by Householder. It was ranked among the top ten algorithms in the previous century.  We will discuss this algorithm in more detail during week 38.

<p>
The first step  consists in finding
an orthogonal  matrix \( {\bf S} \) which is the product of \( (n-2) \) orthogonal matrices 
$$ 
   {\bf S}={\bf S}_1{\bf S}_2\dots{\bf S}_{n-2},
$$

each of which successively transforms one row and one column of \( {\bf A} \) into the 
required tridiagonal form. Only \( n-2 \) transformations are required, since the last two
elements are already in tridiagonal form. In order to determine each \( {\bf S_i} \) let us
see what happens after the first multiplication, namely,
$$
    {\bf S}_1^T{\bf A}{\bf S}_1=    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &a'_{23}  & \dots    & \dots  &\dots &a'_{2n} \\
                                0   & a'_{32} &a'_{33}  & \dots    & \dots  &\dots &a'_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & a'_{n2} &a'_{n3}  & \dots    & \dots  &\dots &a'_{nn} \\

             \end{array} \right) 
$$

where the primed quantities represent a matrix \( {\bf A}' \) of dimension
\( n-1 \) which will subsequentely be transformed by \( {\bf S_2} \).
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec228"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
The factor  \( e_1 \) is a possibly non-vanishing element. The next
transformation produced by \( {\bf S_2} \) has the same effect as  \( {\bf
S_1} \) but now on the submatirx \( {\bf A^{'}} \) only
$$
   \left ({\bf S}_{1}{\bf S}_{2} \right )^{T} {\bf A}{\bf S}_{1} {\bf S}_{2}
 = \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} &e_2  & 0   & \dots  &\dots &0 \\
                                0   & e_2 &a''_{33}  & \dots    & \dots  &\dots &a''_{3n} \\
                                0   & \dots &\dots & \dots    & \dots  &\dots & \\
                                0   & 0 &a''_{n3}  & \dots    & \dots  &\dots &a''_{nn} \\

             \end{array} \right) 
$$

<em>Note that the effective size of the matrix on which we apply the transformation reduces for every new step. In the previous Jacobi method each similarity transformation is in principle performed on the full size of the original matrix</em>.
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec229"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
After a series of such transformations, we end with a set of diagonal
matrix elements
$$
  a_{11}, a'_{22}, a''_{33}\dots a^{n-1}_{nn},
$$

and off-diagonal matrix elements 
$$
   e_1, e_2,e_3,  \dots, e_{n-1}.
$$

The resulting matrix reads
$$
{\bf S}^{T} {\bf A} {\bf S} = 
    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & a''_{33} & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &a^{(n-1)}_{n-2} & e_{n-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n-1} & a^{(n-1)}_{n-1}

             \end{array} \right) .
$$
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec230"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
It remains to find a recipe for determining the transformation \( {\bf S}_n \).
We illustrate the method for \( {\bf S}_1 \) which we assume takes the form
$$
    {\bf S_{1}} = \left( \begin{array}{cc} 1 & {\bf 0^{T}} \\
                              {\bf 0}& {\bf P} \end{array} \right),
$$

with \( {\bf 0^{T}} \) being a zero row vector, \( {\bf 0^{T}} = \{0,0,\cdots\} \)
of dimension \( (n-1) \). The matrix \( {\bf P} \)  is symmetric 
with dimension (\( (n-1) \times (n-1) \)) satisfying
\( {\bf P}^2={\bf I} \)  and \( {\bf P}^T={\bf P} \). 
A possible choice which fullfils the latter two requirements is 
$$
    {\bf P}={\bf I}-2{\bf u}{\bf u}^T,
$$

where \( {\bf I} \) is the \( (n-1) \) unity matrix and \( {\bf u} \) is an \( n-1 \)
column vector with norm \( {\bf u}^T{\bf u} \) (inner product).
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec231"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
 Note that \( {\bf u}{\bf u}^T \) is an outer product giving a
matrix of dimension (\( (n-1) \times (n-1) \)). 
Each matrix element of \( {\bf P} \) then reads
$$
   P_{ij}=\delta_{ij}-2u_iu_j,
$$

where \( i \) and \( j \) range from \( 1 \) to \( n-1 \). Applying the transformation  
\( {\bf S}_1 \) results in 
$$
   {\bf S}_1^T{\bf A}{\bf S}_1 =  \left( \begin{array}{cc} a_{11} & ({\bf Pv})^T \\
                              {\bf Pv}& {\bf A}' \end{array} \right) ,
$$

where \( {\bf v^{T}} = \{a_{21}, a_{31},\cdots, a_{n1}\} \) and {\bf P}
must satisfy (\( {\bf Pv})^{T} = \{k, 0, 0,\cdots \} \). Then
$$
\begin{equation}
    {\bf Pv} = {\bf v} -2{\bf u}( {\bf u}^T{\bf v})= k {\bf e},
    \label{eq:palpha}
\end{equation}
$$

with \( {\bf e^{T}} = \{1,0,0,\dots 0\} \).
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec232"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Solving the latter equation gives us \( {\bf u} \) and thus the needed transformation
\( {\bf P} \). We do first however need to compute the scalar \( k \) by taking the scalar
product of the last equation with its transpose and using the fact that \( {\bf P}^2={\bf I} \).
We get then
$$
   ({\bf Pv})^T{\bf Pv} = k^{2} = {\bf v}^T{\bf v}=
   |v|^2 = \sum_{i=2}^{n}a_{i1}^2,
$$

which determines the constant $ k = \pm v$.
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec233"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
 Now we can rewrite Eq.&nbsp;\eqref{eq:palpha}
as 
$$
    {\bf v} - k{\bf e} = 2{\bf u}( {\bf u}^T{\bf v}),
$$

and taking the scalar product of this equation with itself and obtain
$$
\begin{equation}
    2( {\bf u}^T{\bf v})^2=(v^2\pm a_{21}v),
    \label{eq:pmalpha}
\end{equation}
$$

which finally determines 
$$
    {\bf u}=\frac{{\bf v}-k{\bf e}}{2( {\bf u}^T{\bf v})}.
$$

In solving Eq.&nbsp;\eqref{eq:pmalpha} great care has to be exercised so as to choose
those values which make the right-hand largest in order to avoid loss of numerical
precision. 
The above steps are then repeated for every transformations till we have a 
tridiagonal matrix suitable for obtaining the eigenvalues.
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec234"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
Our Householder transformation has given us a tridiagonal matrix. We discuss here how one can use
Householder's iterative procedure to obtain the eigenvalues. 
Let us specialize to a \( 4\times 4  \) matrix.
The tridiagonal matrix takes the form
$$
 {\bf A} =
      \left( \begin{array}{cccc} 
                d_{1} & e_{1} & 0     &  0    \\
                e_{1} & d_{2} & e_{2} &  0    \\
                 0    & e_{2} & d_{3} & e_{3} \\
                 0    &   0   & e_{3} & d_{4} 
             \end{array} \right).
$$

As a first observation, if any of the elements \( e_{i} \) are zero the
matrix can be separated into smaller pieces before
diagonalization. Specifically, if \( e_{1} = 0 \) then \( d_{1} \) is an
eigenvalue.
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec235"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
 Thus, let us introduce  a transformation \( {\bf S_{1}} \) which operates like
$$
 {\bf S_{1}} =
      \left( \begin{array}{cccc} 
                \cos \theta & 0 & 0 & \sin \theta\\
                 0       & 0 & 0 &      0      \\
                   0        & 0 & 0 &      0      \\
               \cos \theta & 0 & 0 & \cos \theta 
             \end{array} \right)
$$


<p>
Then the similarity transformation 
$$
{\bf S_{1}^{T} A  S_{1}} = {\bf A'} = 
      \left( \begin{array}{cccc}
              d'_{1} & e'_{1} &   0    &   0   \\
              e'_{1}  & d_{2}  & e_{2}  &   0   \\
                0    & e_{2}  & d_{3}  & e'{3} \\
                0    &   0    & e'_{3} & d'_{4}
             \end{array} \right)
$$

produces a matrix where the primed elements in \( {\bf A'} \) have been
changed by the transformation whereas the unprimed elements are unchanged.
If we now choose \( \theta \) to
give the element \( a_{21}^{'} = e^{'}= 0 \) then we have the first
eigenvalue  \( = a_{11}^{'} = d_{1}^{'} \).
(This is actually what you are doing in project 2!!)
</div>


<p>
<!-- !split -->

<h2>Discussion of Householder's method for eigenvalues  <a name="___sec236"></a></h2>
<div class="alert alert-block alert-block alert-text-normal"><b></b>
This procedure can be continued on the remaining three-dimensional
submatrix for the next eigenvalue. Thus after few transformations    
we have the wanted diagonal form.

<p>
What we see here is just a special case of the more general procedure 
developed by Francis in two articles in 1961 and 1962. Using Householder's method is not very efficient ether. 

<p>
The algorithm is based on the so-called <em>QR</em> method (or just <em>QR</em>-algorithm). It follows from a theorem by Schur which states that any square matrix can be written out in terms of an orthogonal matrix \( {\bf Q} \) and an upper triangular matrix \( {\bf U} \). Historically <em>R</em> was used instead of 
<em>U</em> since the wording right triangular matrix was first used.
The method is based on an iterative procedure similar to Jacobi's method, by a succession of
planar rotations. For a tridiagonal matrix it is simple to carry out in principle, but complicated in detail! We will discuss this in more detail during week 38.
</div>


<p>

<!-- ------------------- end of main content --------------- -->


</body>
</html>
    

